{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree(rootDir=os.getcwd(), ignore=['node_modules', 'images', '_helpers', '.next', '.git']):\n",
    "    tree = []\n",
    "    for root, dirs, files in os.walk(rootDir):\n",
    "        dirs[:] = [d for d in dirs if d not in ignore]\n",
    "        for file in files:\n",
    "            absolute_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(absolute_path, rootDir)            \n",
    "            tree.append(relative_path)\n",
    "            \n",
    "    return tree\n",
    "\n",
    "tree = get_tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports: ['fastapi', 'fastapi.middleware.cors', 'pydantic', 'typing', 'redis', 'os', 'json', 'langchain.vectorstores', 'langchain.chat_models', 'langchain.embeddings', 'langchain.llms', 'langchain.chains.qa_with_sources', 'langchain.chains.question_answering', 'langchain.memory', 'langchain.chains', 'langchain.prompts.prompt', 'langchain.schema', 'langchain.agents', 'langchain.memory', 'langchain.chat_models', 'langchain.agents', 'langchain.agents', 'uvicorn']\n",
      "Classes: ['BadRequestError', 'MissingUserIdError', 'MissingDatasetError', 'MissingQuestionError', 'LlmAnswerRequest', 'ClearCacheRequest']\n",
      "Functions: ['hello_world', 'llm_answer', 'clear_cache']\n",
      "Called functions/methods: {'keys', 'ConversationBufferMemory', 'PromptTemplate', 'ElasticVectorSearch', 'add_middleware', 'post', 'OpenAIEmbeddings', 'Tool', 'clear', 'load_qa_with_sources_chain', 'get', 'pop', 'as_retriever', 'OpenAI', 'RetrievalQAWithSourcesChain', 'ChatOpenAI', 'MissingQuestionError', 'FastAPI', 'replace', 'HTTPException', 'initialize_agent', 'BadRequestError', 'append', 'MissingDatasetError', 'print', 'run', 'MissingUserIdError'}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "class CodeAnalyzer(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.imports = []\n",
    "        self.classes = []\n",
    "        self.functions = []\n",
    "        self.calls = []\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        for alias in node.names:\n",
    "            self.imports.append(alias.name)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        self.imports.append(node.module)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "        self.classes.append(node.name)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        self.functions.append(node.name)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Call(self, node):\n",
    "        if isinstance(node.func, ast.Name):\n",
    "            self.calls.append(node.func.id)\n",
    "        elif isinstance(node.func, ast.Attribute):\n",
    "            self.calls.append(node.func.attr)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "def analyze_code(file_path):\n",
    "    with open(file_path, \"r\") as source_code:\n",
    "        tree = ast.parse(source_code.read())\n",
    "        analyzer = CodeAnalyzer()\n",
    "        analyzer.visit(tree)\n",
    "        \n",
    "        print(\"Imports:\", analyzer.imports)\n",
    "        print(\"Classes:\", analyzer.classes)\n",
    "        print(\"Functions:\", analyzer.functions)\n",
    "        print(\"Called functions/methods:\", set(analyzer.calls))\n",
    "\n",
    "# replace 'your_file.py' with the path of the Python file you want to analyze\n",
    "analyze_code('backend/chat_api/main.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "def parse_practice_problems_to_json(content):\n",
    "    guid = str(uuid.uuid4())\n",
    "    prompt = f\"\"\"\n",
    "Your job is to parse through python code and documentation with the goal of returning only\n",
    "information that would required or helpful to another agent answering higher level questions. \n",
    "The agent will give you a question or task and your response should be exerpts from the context \n",
    "provided. \n",
    "\n",
    "\n",
    "\n",
    "CONTENT: {content}\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    # print(prompt)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI Assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(response['choices'][0]['message']['content'])\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse the following content into JSON:\\n{content}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "practice_problems = find_practice_problems(\"./_helpers/output/level_1_volume_1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = parse_questions(practice_problems[1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = questions[22]\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from marvin import ai_fn, ai_model\n",
    "\n",
    "@ai_model(model=\"gpt-4\")\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    options: List[str]\n",
    "    question_data: Optional[List[dict]] = None\n",
    "\n",
    "\n",
    "@ai_fn\n",
    "def parse_question(question: str) -> list[Question]:\n",
    "    \"\"\"\n",
    "    Parse a question string into a structured Question object with fields, \"question\", \"options\", and \"question_data\" (OPTIONAL)\n",
    "    \"\"\"\n",
    "\n",
    "print(parse_question(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_practice_problems_to_json(obj[47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace problem numbers and choices with a unique symbol\n",
    "    text = re.sub(r'(\\d+)\\.', r'\\1<@>', text)\n",
    "    text = re.sub(r'(A|B|C)\\.', r'\\1<#>', text)\n",
    "    return text\n",
    "\n",
    "def postprocess_text(text):\n",
    "    # Replace the unique symbol with a period\n",
    "    text = text.replace('<@>', '.')\n",
    "    text = text.replace('<#>', '.')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_to_json(text):\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe('sentencizer') \n",
    "    doc = nlp(text)\n",
    "\n",
    "    problems_json = []\n",
    "    current_problem = None\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        if \"<@>\" in sentence.text: \n",
    "            # If the sentence contains a problem number, it's a new problem\n",
    "            if current_problem is not None:\n",
    "                problems_json.append(current_problem)\n",
    "            current_problem = {\"problem_number\": postprocess_text(sentence.text.split('<@>')[0].strip()), \"problem_text\": \"\", \"choices\": []}\n",
    "            print(current_problem)\n",
    "            problem_text = sentence.text.split('<@>')[1].strip()\n",
    "            print(problem_text)\n",
    "            if \"<#>\" in problem_text:\n",
    "                # If the problem text contains a choice, split it\n",
    "                current_problem[\"problem_text\"] = postprocess_text(problem_text.split('<#>')[0].strip())\n",
    "                current_problem[\"choices\"].append('A.' + postprocess_text(problem_text.split('<#>')[1].strip()))\n",
    "            else:\n",
    "                current_problem[\"problem_text\"] = postprocess_text(problem_text)\n",
    "        elif \"<#>\" in sentence.text:\n",
    "            # If the sentence contains a choice, it's a choice for the current problem\n",
    "            current_problem[\"choices\"].append(postprocess_text(sentence.text.strip()))\n",
    "        else:\n",
    "            # If the sentence doesn't contain a problem number or a choice, it's part of the problem text\n",
    "            current_problem[\"problem_text\"] += postprocess_text(sentence.text.strip())\n",
    "    problems_json.append(current_problem)\n",
    "\n",
    "    return problems_json\n",
    "print(parse_text_to_json(practice_problems[0]['content'][18:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_text_to_json(practice_problems[0]['content'][18:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_problems[0]['content'][18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pprint\n",
    "import site\n",
    "\n",
    "def get_directories_to_analyze():\n",
    "    directories = []\n",
    "    # Include your project directory\n",
    "    directories.append('/backend/chat_api')\n",
    "    # Include all site-packages directories\n",
    "    directories.extend(site.getsitepackages())\n",
    "    return directories\n",
    "\n",
    "def analyze_directories(directories):\n",
    "    codebase_data = {}\n",
    "    for directory in directories:\n",
    "        codebase_data.update(analyze_directory(directory))\n",
    "    return codebase_data\n",
    "\n",
    "directories_to_analyze = get_directories_to_analyze()\n",
    "\n",
    "class DependencyAnalyzer(ast.NodeVisitor):\n",
    "    def __init__(self, module_name):\n",
    "        self.module_name = module_name\n",
    "        self.dependencies = set()\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        for alias in node.names:\n",
    "            self.dependencies.add(alias.name)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        self.dependencies.add(node.module)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "def analyze_directory(directory_path):\n",
    "    codebase_data = {}\n",
    "    dependency_graph = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                module_name = os.path.splitext(file)[0]\n",
    "                with open(file_path, \"r\") as source_code:\n",
    "                    tree = ast.parse(source_code.read())\n",
    "                    analyzer = DependencyAnalyzer(module_name)\n",
    "                    analyzer.visit(tree)\n",
    "                    codebase_data[module_name] = analyzer.dependencies\n",
    "                    for dependency in analyzer.dependencies:\n",
    "                        dependency_graph.setdefault(dependency, set()).add(module_name)\n",
    "\n",
    "    return codebase_data, dependency_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "analyze_directory() missing 1 required positional argument: 'directory_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb Cell 16\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m codebase_data, dependency_graph \u001b[39m=\u001b[39m analyze_directory()\n",
      "\u001b[0;31mTypeError\u001b[0m: analyze_directory() missing 1 required positional argument: 'directory_path'"
     ]
    }
   ],
   "source": [
    "codebase_data, dependency_graph = analyze_directory(\"./backend/chat_api/\")\n",
    "\n",
    "# pp = pprint.PrettyPrinter(indent=4)\n",
    "# print(\"Codebase Data:\")\n",
    "# pp.pprint(codebase_data)\n",
    "# print(\"\\nDependency Graph:\")\n",
    "# pp.pprint(dependency_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "relevant_files = ['langchain.chat_models', 'langchain.embeddings', \n",
    "                  'langchain.llms', 'langchain.memory', 'langchain.vectorstores']\n",
    "\n",
    "relevant_data = {file: codebase_data[file] for file in relevant_files if file in codebase_data}\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(relevant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asgi': {'fastapi', 'main'},\n",
       " 'main': {'fastapi',\n",
       "  'fastapi.middleware.cors',\n",
       "  'json',\n",
       "  'langchain.agents',\n",
       "  'langchain.chains',\n",
       "  'langchain.chains.qa_with_sources',\n",
       "  'langchain.chains.question_answering',\n",
       "  'langchain.chat_models',\n",
       "  'langchain.embeddings',\n",
       "  'langchain.llms',\n",
       "  'langchain.memory',\n",
       "  'langchain.prompts.prompt',\n",
       "  'langchain.schema',\n",
       "  'langchain.vectorstores',\n",
       "  'os',\n",
       "  'pydantic',\n",
       "  'redis',\n",
       "  'typing',\n",
       "  'uvicorn'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
