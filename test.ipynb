{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import PyPDF2\n",
    "import fitz\n",
    "import pdfrw\n",
    "from pdfrw import PdfReader\n",
    "\n",
    "file = \"../../Downloads/CFA Level 1 Volume 1 (2023, CFA Institute) - libgen.li.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader = fitz.open(file)\n",
    "# print(reader.load_page(0))\n",
    "# print(reader.load_page(12).get_text(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_toc(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    toc = doc.get_toc()\n",
    "    toc_entries = []\n",
    "    for level, title, page_number in toc:\n",
    "        toc_entries.append({\"level\": level, \"title\": title, \"page_number\": page_number})\n",
    "    return toc_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_child_content(node):\n",
    "    if not node[\"children\"]:\n",
    "        return node[\"content\"]\n",
    "\n",
    "    child_content = \"\"\n",
    "    for child in node[\"children\"]:\n",
    "        child_content += remove_child_content(child)\n",
    "\n",
    "    node[\"content\"] = node[\"content\"].replace(child_content, \"\")\n",
    "    return child_content\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def build_nested_toc_with_content(file_path, toc):\n",
    "    nested_toc = []\n",
    "    stack = []\n",
    "\n",
    "    pdf = fitz.open(file_path)\n",
    "\n",
    "    for i, entry in enumerate(toc):\n",
    "        level = entry[\"level\"]\n",
    "        start_page = entry[\"page_number\"] - 1\n",
    "        end_page = toc[i + 1][\"page_number\"]  - 1 if i < len(toc) - 1 else len(pdf)\n",
    "\n",
    "        content = \"\"\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            if page < len(pdf):\n",
    "                content += pdf.load_page(page).get_text()\n",
    "\n",
    "        if level == 1:\n",
    "            title = f\"LearningModule{i+1}\"\n",
    "        else:\n",
    "            title = entry[\"title\"]\n",
    "\n",
    "        # Split content by the child's title\n",
    "        if i < len(toc) - 1 and toc[i + 1][\"page_number\"] == entry[\"page_number\"]:\n",
    "            child_title = toc[i + 1][\"title\"].upper()\n",
    "            child_title_regex = re.compile(r'{}'.format(child_title))\n",
    "            match = child_title_regex.search(content)\n",
    "            if match:\n",
    "                content = content[:match.start()]\n",
    "\n",
    "        if i < len(toc) - 1 and toc[i + 1][\"page_number\"] > entry[\"page_number\"]:\n",
    "\n",
    "            child_title = re.escape(toc[i + 1][\"title\"].upper())\n",
    "            child_title_regex = re.compile(r'{}'.format(child_title))\n",
    "            child_match = child_title_regex.search(content)\n",
    "\n",
    "            upper_title = re.escape(title.upper())\n",
    "            title_regex = re.compile(r'{}'.format(upper_title))\n",
    "            match = title_regex.search(content)\n",
    "\n",
    "            if match and child_match:\n",
    "                content = content[match.start():child_match.start()]\n",
    "            elif match:\n",
    "                content = content[match.start():]\n",
    "\n",
    "        # else:\n",
    "        #     if i < len(toc) - 1:\n",
    "        #         child_title = toc[i + 1][\"title\"]\n",
    "        #         child_title_regex = re.compile(r'\\b{}\\b'.format(child_title), re.IGNORECASE)\n",
    "        #         match = child_title_regex.search(content)\n",
    "        #         if match:\n",
    "        #             content = content[match.start():]\n",
    "\n",
    "        item = {\n",
    "            \"title\": title,\n",
    "            \"page_number\": entry[\"page_number\"],\n",
    "            \"content\": content,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        while len(stack) >= level:\n",
    "            stack.pop()\n",
    "\n",
    "        if not stack:\n",
    "            nested_toc.append(item)\n",
    "        else:\n",
    "            parent = stack[-1]\n",
    "            parent[\"children\"].append(item)\n",
    "\n",
    "        stack.append(item)\n",
    "\n",
    "    pdf.close()\n",
    "\n",
    "    # Remove content of child nodes from parent nodes\n",
    "    for item in nested_toc:\n",
    "        remove_child_content(item)\n",
    "\n",
    "    return nested_toc\n",
    "\n",
    "\n",
    "\n",
    "toc = extract_toc(file)\n",
    "sections = build_nested_toc_with_content(file, toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present Value of a Series of Equal and Unequal Cash Flows\n",
      "21\n",
      "PRESENT VALUE OF A SERIES OF EQUAL AND \n",
      "UNEQUAL CASH FLOWS\n",
      "calculate and interpret the future value (FV) and present value (PV) \n",
      "of a single sum of money, an ordinary annuity, an annuity due, a \n",
      "perpetuity (PV only), and a series of unequal cash flows\n",
      "demonstrate the use of a time line in modeling and solving time \n",
      "value of money problems\n",
      "Many applications in investment management involve assets that offer a series of \n",
      "cash flows over time. The cash flows may be highly uneven, relatively even, or equal. \n",
      "They may occur over relatively short periods of time, longer periods of time, or even \n",
      "stretch on indefinitely. In this section, we discuss how to find the present value of a \n",
      "series of cash flows.\n",
      "The Present Value of a Series of Equal Cash Flows\n",
      "We begin with an ordinary annuity. Recall that an ordinary annuity has equal annuity \n",
      "payments, with the first payment starting one period into the future. In total, the \n",
      "annuity makes N payments, with the first payment at t = 1 and the last at t = N. We \n",
      "can express the present value of an ordinary annuity as the sum of the present values \n",
      "of each individual annuity payment, as follows:\n",
      " PV =  A \n",
      "_ \n",
      " ( 1 + r )   +  \n",
      "A \n",
      "_ \n",
      " ( 1 + r ) 2   +  \n",
      "A \n",
      "_ \n",
      " ( 1 + r ) 3   + … +  \n",
      "A \n",
      "_ \n",
      " ( 1 + r ) N−1   +  \n",
      "A \n",
      "_ \n",
      " ( 1 + r ) N   \n",
      "(10)\n",
      "where\n",
      " \n",
      "A = the annuity amount\n",
      " \n",
      "r = the interest rate per period corresponding to the frequency of annuity \n",
      "payments (for example, annual, quarterly, or monthly)\n",
      " \n",
      "N = the number of annuity payments\n",
      "Because the annuity payment (A) is a constant in this equation, it can be factored out \n",
      "as a common term. Thus the sum of the interest factors has a shortcut expression:\n",
      " PV = A  [ \n",
      "1 −  \n",
      "1 \n",
      "_ \n",
      " ( 1 + r ) N    \n",
      "_ \n",
      "r \n",
      " ]  \n",
      "(11)\n",
      "In much the same way that we computed the future value of an ordinary annuity, we \n",
      "find the present value by multiplying the annuity amount by a present value annuity \n",
      "factor (the term in brackets in Equation 11).\n",
      "9\n",
      "© CFA Institute. For candidate use only. Not for distribution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sections[1][\"children\"][0]['children'][8]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'level': 1, 'title': 'How to Use the CFA Program Curriculum', 'page_number': 9}, {'level': 2, 'title': 'Errata', 'page_number': 9}, {'level': 3, 'title': 'Designing Your Personal Study Program', 'page_number': 9}, {'level': 3, 'title': 'CFA Institute Learning Ecosystem (LES)', 'page_number': 10}, {'level': 3, 'title': 'Feedback', 'page_number': 10}, {'level': 1, 'title': 'Quantitative Methods', 'page_number': 11}, {'level': 2, 'title': 'Learning Module 1\\tThe Time Value of Money', 'page_number': 13}, {'level': 3, 'title': 'Introduction', 'page_number': 13}, {'level': 3, 'title': 'Interest Rates', 'page_number': 14}, {'level': 3, 'title': 'Future Value of a Single Cash Flow', 'page_number': 16}, {'level': 3, 'title': 'Non-Annual Compounding (Future Value)', 'page_number': 20}, {'level': 3, 'title': 'Continuous Compounding', 'page_number': 22}, {'level': 4, 'title': 'Stated and Effective Rates', 'page_number': 24}, {'level': 3, 'title': 'A Series of Cash Flows', 'page_number': 25}, {'level': 4, 'title': 'Equal Cash Flows—Ordinary Annuity', 'page_number': 25}, {'level': 4, 'title': 'Unequal Cash Flows', 'page_number': 26}, {'level': 3, 'title': 'Present Value of a Single Cash Flow', 'page_number': 27}, {'level': 3, 'title': 'Non-Annual Compounding (Present Value)', 'page_number': 29}, {'level': 3, 'title': 'Present Value of a Series of Equal and Unequal Cash Flows', 'page_number': 31}, {'level': 4, 'title': 'The Present Value of a Series of Equal Cash Flows', 'page_number': 31}, {'level': 4, 'title': 'The Present Value of a Series of Unequal Cash Flows', 'page_number': 35}, {'level': 3, 'title': 'Present Value of a Perpetuity', 'page_number': 36}, {'level': 4, 'title': 'Present Values Indexed at Times Other than t = 0', 'page_number': 37}, {'level': 3, 'title': 'Solving for Interest Rates, Growth Rates, and Number of Periods', 'page_number': 38}, {'level': 4, 'title': 'Solving for Interest Rates and Growth Rates', 'page_number': 39}, {'level': 4, 'title': 'Solving for the Number of Periods', 'page_number': 41}, {'level': 3, 'title': 'Solving for Size of Annuity Payments', 'page_number': 42}, {'level': 3, 'title': 'Present and Future Value Equivalence and the Additivity Principle', 'page_number': 46}, {'level': 4, 'title': 'The Cash Flow Additivity Principle', 'page_number': 48}, {'level': 3, 'title': 'Summary', 'page_number': 49}, {'level': 3, 'title': 'Practice Problems', 'page_number': 50}, {'level': 3, 'title': 'Solutions', 'page_number': 55}, {'level': 2, 'title': 'Learning Module 2\\tOrganizing, Visualizing, and Describing Data', 'page_number': 69}, {'level': 3, 'title': 'Introduction', 'page_number': 69}, {'level': 3, 'title': 'Data Types', 'page_number': 70}, {'level': 4, 'title': 'Numerical versus Categorical Data', 'page_number': 71}, {'level': 4, 'title': 'Cross-Sectional versus Time-Series versus Panel Data', 'page_number': 73}, {'level': 4, 'title': 'Structured versus Unstructured Data', 'page_number': 74}, {'level': 4, 'title': 'Data Summarization', 'page_number': 78}, {'level': 3, 'title': 'Organizing Data for Quantitative Analysis', 'page_number': 78}, {'level': 3, 'title': 'Summarizing Data Using Frequency Distributions', 'page_number': 81}, {'level': 3, 'title': 'Summarizing Data Using a Contingency Table', 'page_number': 87}, {'level': 3, 'title': 'Data Visualization', 'page_number': 92}, {'level': 4, 'title': 'Histogram and Frequency Polygon', 'page_number': 92}, {'level': 4, 'title': 'Bar Chart', 'page_number': 94}, {'level': 4, 'title': 'Tree-Map', 'page_number': 97}, {'level': 4, 'title': 'Word Cloud', 'page_number': 98}, {'level': 4, 'title': 'Line Chart', 'page_number': 100}, {'level': 4, 'title': 'Scatter Plot', 'page_number': 102}, {'level': 4, 'title': 'Heat Map', 'page_number': 106}, {'level': 4, 'title': 'Guide to Selecting among Visualization Types', 'page_number': 108}, {'level': 3, 'title': 'Measures of Central Tendency', 'page_number': 110}, {'level': 4, 'title': 'The Arithmetic Mean', 'page_number': 111}, {'level': 4, 'title': 'The Median', 'page_number': 115}, {'level': 4, 'title': 'The Mode', 'page_number': 116}, {'level': 4, 'title': 'Other Concepts of Mean', 'page_number': 117}, {'level': 3, 'title': 'Quantiles', 'page_number': 126}, {'level': 4, 'title': 'Quartiles, Quintiles, Deciles, and Percentiles', 'page_number': 127}, {'level': 4, 'title': 'Quantiles in Investment Practice', 'page_number': 132}, {'level': 3, 'title': 'Measures of Dispersion', 'page_number': 133}, {'level': 4, 'title': 'The Range', 'page_number': 133}, {'level': 4, 'title': 'The Mean Absolute Deviation', 'page_number': 134}, {'level': 4, 'title': 'Sample Variance and Sample Standard Deviation', 'page_number': 135}, {'level': 3, 'title': 'Downside Deviation and Coefficient of Variation', 'page_number': 138}, {'level': 4, 'title': 'Coefficient of Variation', 'page_number': 141}, {'level': 3, 'title': 'The Shape of the Distributions', 'page_number': 143}, {'level': 4, 'title': 'The Shape of the Distributions: Kurtosis', 'page_number': 146}, {'level': 3, 'title': 'Correlation between Two Variables', 'page_number': 149}, {'level': 4, 'title': 'Properties of Correlation', 'page_number': 150}, {'level': 4, 'title': 'Limitations of Correlation Analysis', 'page_number': 153}, {'level': 3, 'title': 'Summary', 'page_number': 156}, {'level': 3, 'title': 'Practice Problems', 'page_number': 161}, {'level': 3, 'title': 'Solutions', 'page_number': 174}, {'level': 2, 'title': 'Learning Module 3\\tProbability Concepts', 'page_number': 183}, {'level': 3, 'title': 'Probability Concepts and Odds Ratios', 'page_number': 184}, {'level': 4, 'title': 'Probability, Expected Value, and Variance', 'page_number': 184}, {'level': 3, 'title': 'Conditional and Joint Probability', 'page_number': 189}, {'level': 3, 'title': 'Expected Value and Variance', 'page_number': 201}, {'level': 3, 'title': 'Portfolio Expected Return and Variance of Return', 'page_number': 207}, {'level': 3, 'title': 'Covariance Given a Joint Probability Function', 'page_number': 212}, {'level': 3, 'title': \"Bayes' Formula\", 'page_number': 216}, {'level': 4, 'title': 'Bayes’ Formula', 'page_number': 216}, {'level': 3, 'title': 'Principles of Counting', 'page_number': 222}, {'level': 3, 'title': 'Summary', 'page_number': 228}, {'level': 3, 'title': 'Practice Problems', 'page_number': 231}, {'level': 3, 'title': 'Solutions', 'page_number': 238}, {'level': 2, 'title': 'Learning Module 4\\tCommon Probability Distributions', 'page_number': 245}, {'level': 3, 'title': 'Discrete Random Variables', 'page_number': 246}, {'level': 4, 'title': 'Discrete Random Variables', 'page_number': 247}, {'level': 3, 'title': 'Discrete and Continuous Uniform Distribution', 'page_number': 251}, {'level': 4, 'title': 'Continuous Uniform Distribution', 'page_number': 253}, {'level': 3, 'title': 'Binomial Distribution', 'page_number': 256}, {'level': 3, 'title': 'Normal Distribution', 'page_number': 264}, {'level': 4, 'title': 'The Normal Distribution', 'page_number': 264}, {'level': 4, 'title': 'Probabilities Using the Normal Distribution', 'page_number': 268}, {'level': 4, 'title': 'Standardizing a Random Variable', 'page_number': 270}, {'level': 4, 'title': 'Probabilities Using the Standard Normal Distribution', 'page_number': 270}, {'level': 3, 'title': 'Applications of the Normal Distribution', 'page_number': 272}, {'level': 3, 'title': 'Lognormal Distribution and Continuous Compounding', 'page_number': 276}, {'level': 4, 'title': 'The Lognormal Distribution ', 'page_number': 276}, {'level': 4, 'title': 'Continuously Compounded Rates of Return ', 'page_number': 279}, {'level': 3, 'title': 'Student’s t-, Chi-Square, and F-Distributions', 'page_number': 282}, {'level': 4, 'title': 'Student’s t-Distribution ', 'page_number': 282}, {'level': 4, 'title': 'Chi-Square and F-Distribution', 'page_number': 284}, {'level': 3, 'title': 'Monte Carlo Simulation', 'page_number': 289}, {'level': 3, 'title': 'Summary', 'page_number': 295}, {'level': 3, 'title': 'Practice Problems', 'page_number': 298}, {'level': 3, 'title': 'Solutions', 'page_number': 306}, {'level': 2, 'title': 'Learning Module 5\\tSampling and Estimation', 'page_number': 313}, {'level': 3, 'title': 'Introduction', 'page_number': 314}, {'level': 3, 'title': 'Sampling Methods', 'page_number': 314}, {'level': 4, 'title': 'Simple Random Sampling', 'page_number': 315}, {'level': 4, 'title': 'Stratified Random Sampling', 'page_number': 316}, {'level': 4, 'title': 'Cluster Sampling', 'page_number': 318}, {'level': 4, 'title': 'Non-Probability Sampling', 'page_number': 319}, {'level': 4, 'title': 'Sampling from Different Distributions', 'page_number': 323}, {'level': 3, 'title': 'The Central Limit Theorem and Distribution of the Sample Mean', 'page_number': 325}, {'level': 4, 'title': 'The Central Limit Theorem', 'page_number': 325}, {'level': 4, 'title': 'Standard Error of the Sample Mean', 'page_number': 327}, {'level': 3, 'title': 'Point Estimates of the Population Mean', 'page_number': 330}, {'level': 4, 'title': 'Point Estimators', 'page_number': 330}, {'level': 3, 'title': 'Confidence Intervals for the Population Mean and Sample Size Selection ', 'page_number': 334}, {'level': 4, 'title': 'Selection of Sample Size', 'page_number': 340}, {'level': 3, 'title': 'Resampling', 'page_number': 342}, {'level': 3, 'title': 'Sampling Related  Biases', 'page_number': 345}, {'level': 4, 'title': 'Data Snooping Bias', 'page_number': 346}, {'level': 4, 'title': 'Sample Selection Bias', 'page_number': 347}, {'level': 4, 'title': 'Look-Ahead Bias', 'page_number': 349}, {'level': 4, 'title': 'Time-Period Bias', 'page_number': 350}, {'level': 3, 'title': 'Summary', 'page_number': 351}, {'level': 3, 'title': 'Practice Problems', 'page_number': 354}, {'level': 3, 'title': 'Solutions', 'page_number': 359}, {'level': 2, 'title': 'Learning Module 6\\tHypothesis Testing', 'page_number': 363}, {'level': 3, 'title': 'Introduction', 'page_number': 364}, {'level': 4, 'title': 'Why Hypothesis Testing?', 'page_number': 364}, {'level': 4, 'title': 'Implications from a Sampling Distribution', 'page_number': 365}, {'level': 3, 'title': 'The Process of Hypothesis Testing', 'page_number': 366}, {'level': 4, 'title': 'Stating the Hypotheses', 'page_number': 367}, {'level': 4, 'title': 'Two-Sided vs. One-Sided Hypotheses', 'page_number': 367}, {'level': 4, 'title': 'Selecting the Appropriate Hypotheses', 'page_number': 368}, {'level': 3, 'title': 'Identify the Appropriate Test Statistic', 'page_number': 369}, {'level': 4, 'title': 'Test Statistics', 'page_number': 369}, {'level': 4, 'title': 'Identifying the Distribution of the Test Statistic', 'page_number': 370}, {'level': 3, 'title': 'Specify the Level of Significance', 'page_number': 370}, {'level': 3, 'title': 'State the Decision Rule', 'page_number': 372}, {'level': 4, 'title': 'Determining Critical Values', 'page_number': 373}, {'level': 4, 'title': 'Decision Rules and Confidence Intervals', 'page_number': 374}, {'level': 4, 'title': 'Collect the Data and Calculate the Test Statistic', 'page_number': 375}, {'level': 3, 'title': 'Make a Decision', 'page_number': 376}, {'level': 4, 'title': 'Make a Statistical Decision', 'page_number': 376}, {'level': 4, 'title': 'Make an Economic Decision', 'page_number': 376}, {'level': 4, 'title': 'Statistically Significant but Not Economically Significant?', 'page_number': 376}, {'level': 3, 'title': 'The Role of p-Values', 'page_number': 377}, {'level': 3, 'title': 'Multiple Tests and Significance Interpretation', 'page_number': 380}, {'level': 3, 'title': 'Tests Concerning a Single Mean', 'page_number': 383}, {'level': 3, 'title': 'Test Concerning Differences between Means with Independent Samples', 'page_number': 387}, {'level': 3, 'title': 'Test Concerning Differences between Means with Dependent Samples', 'page_number': 389}, {'level': 3, 'title': 'Testing Concerning Tests of Variances', 'page_number': 393}, {'level': 4, 'title': 'Tests of a Single Variance', 'page_number': 393}, {'level': 4, 'title': 'Test Concerning the Equality of Two Variances (F-Test)', 'page_number': 397}, {'level': 3, 'title': 'Parametric vs. Nonparametric Tests', 'page_number': 402}, {'level': 4, 'title': 'Uses of Nonparametric Tests', 'page_number': 403}, {'level': 4, 'title': 'Nonparametric Inference: Summary', 'page_number': 403}, {'level': 3, 'title': 'Tests Concerning Correlation', 'page_number': 404}, {'level': 4, 'title': 'Parametric Test of a Correlation', 'page_number': 405}, {'level': 4, 'title': 'Tests Concerning Correlation: The Spearman Rank Correlation Coefficient', 'page_number': 407}, {'level': 3, 'title': 'Test of Independence Using Contingency Table Data', 'page_number': 409}, {'level': 3, 'title': 'Summary', 'page_number': 414}, {'level': 3, 'title': 'Practice Problems', 'page_number': 418}, {'level': 3, 'title': 'Solutions', 'page_number': 429}, {'level': 2, 'title': 'Learning Module 7\\tIntroduction to Linear Regression', 'page_number': 439}, {'level': 3, 'title': 'Simple Linear Regression', 'page_number': 439}, {'level': 3, 'title': 'Estimating the Parameters of a Simple Linear Regression', 'page_number': 442}, {'level': 4, 'title': 'The Basics of Simple Linear Regression', 'page_number': 442}, {'level': 4, 'title': 'Estimating the Regression Line', 'page_number': 443}, {'level': 4, 'title': 'Interpreting the Regression Coefficients', 'page_number': 446}, {'level': 4, 'title': 'Cross-Sectional vs. Time-Series Regressions', 'page_number': 447}, {'level': 3, 'title': 'Assumptions of the Simple Linear Regression Model', 'page_number': 450}, {'level': 4, 'title': 'Assumption 1: Linearity', 'page_number': 450}, {'level': 4, 'title': 'Assumption 2: Homoskedasticity', 'page_number': 452}, {'level': 4, 'title': 'Assumption 3: Independence', 'page_number': 454}, {'level': 4, 'title': 'Assumption 4: Normality', 'page_number': 455}, {'level': 3, 'title': 'Analysis of Variance', 'page_number': 457}, {'level': 4, 'title': 'Breaking down the Sum of Squares Total into Its Components', 'page_number': 458}, {'level': 4, 'title': 'Measures of Goodness of Fit', 'page_number': 459}, {'level': 4, 'title': 'ANOVA and Standard Error of Estimate in Simple Linear Regression', 'page_number': 460}, {'level': 3, 'title': 'Hypothesis Testing of Linear Regression Coefficients', 'page_number': 463}, {'level': 4, 'title': 'Hypothesis Tests of the Slope Coefficient', 'page_number': 463}, {'level': 4, 'title': 'Hypothesis Tests of the Intercept', 'page_number': 466}, {'level': 4, 'title': 'Hypothesis Tests of Slope When Independent Variable Is an Indicator Variable', 'page_number': 467}, {'level': 4, 'title': 'Test of Hypotheses: Level of Significance and p-Values', 'page_number': 469}, {'level': 3, 'title': 'Prediction Using Simple Linear Regression and Prediction Intervals', 'page_number': 470}, {'level': 3, 'title': 'Functional Forms for Simple Linear Regression', 'page_number': 474}, {'level': 4, 'title': 'The Log-Lin Model', 'page_number': 475}, {'level': 4, 'title': 'The Lin-Log Model', 'page_number': 476}, {'level': 4, 'title': 'The Log-Log Model', 'page_number': 478}, {'level': 4, 'title': 'Selecting the Correct Functional Form', 'page_number': 479}, {'level': 3, 'title': 'Summary', 'page_number': 481}, {'level': 3, 'title': 'Practice Problems', 'page_number': 484}, {'level': 3, 'title': 'Solutions', 'page_number': 498}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LearningModule6',\n",
       " 'page_number': 11,\n",
       " 'content': 'Quantitative Methods\\n© CFA Institute. For candidate use only. Not for distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\nThe Time Value of Money\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity \\nManagement (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ninterpret interest rates as required rates of return, discount rates, or \\nopportunity costs\\nexplain an interest rate as the sum of a real risk-free rate and \\npremiums that compensate investors for bearing distinct types of \\nrisk\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\ncalculate and interpret the effective annual rate, given the stated \\nannual interest rate and the frequency of compounding\\nINTRODUCTION\\nAs individuals, we often face decisions that involve saving money for a future use, or \\nborrowing money for current consumption. We then need to determine the amount \\nwe need to invest, if we are saving, or the cost of borrowing, if we are shopping for \\na loan. As investment analysts, much of our work also involves evaluating transac-\\ntions with present and future cash flows. When we place a value on any security, for \\nexample, we are attempting to determine the worth of a stream of future cash flows. \\nTo carry out all the above tasks accurately, we must understand the mathematics of \\ntime value of money problems. Money has time value in that individuals value a given \\namount of money more highly the earlier it is received. Therefore, a smaller amount \\n1\\nL E A R N I N G  M O D U L E\\n1\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       " 'children': [{'title': 'Learning Module 1\\tThe Time Value of Money',\n",
       "   'page_number': 13,\n",
       "   'content': 'The Time Value of Money\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity \\nManagement (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ninterpret interest rates as required rates of return, discount rates, or \\nopportunity costs\\nexplain an interest rate as the sum of a real risk-free rate and \\npremiums that compensate investors for bearing distinct types of \\nrisk\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\ncalculate and interpret the effective annual rate, given the stated \\nannual interest rate and the frequency of compounding\\n',\n",
       "   'children': [{'title': 'Introduction',\n",
       "     'page_number': 13,\n",
       "     'content': '',\n",
       "     'children': []},\n",
       "    {'title': 'Interest Rates',\n",
       "     'page_number': 14,\n",
       "     'content': 'INTEREST RATES\\ninterpret interest rates as required rates of return, discount rates, or \\nopportunity costs\\nexplain an interest rate as the sum of a real risk-free rate and \\npremiums that compensate investors for bearing distinct types of \\nrisk\\nIn this reading, we will continually refer to interest rates. In some cases, we assume \\na particular value for the interest rate; in other cases, the interest rate will be the \\nunknown quantity we seek to determine. Before turning to the mechanics of time \\nvalue of money problems, we must illustrate the underlying economic concepts. In \\nthis section, we briefly explain the meaning and interpretation of interest rates.\\nTime value of money concerns equivalence relationships between cash flows \\noccurring on different dates. The idea of equivalence relationships is relatively simple. \\nConsider the following exchange: You pay $10,000 today and in return receive $9,500 \\ntoday. Would you accept this arrangement? Not likely. But what if you received the \\n$9,500 today and paid the $10,000 one year from now? Can these amounts be considered \\nequivalent? Possibly, because a payment of $10,000 a year from now would probably \\nbe worth less to you than a payment of $10,000 today. It would be fair, therefore, \\nto discount the $10,000 received in one year; that is, to cut its value based on how \\nmuch time passes before the money is paid. An interest rate, denoted r, is a rate of \\nreturn that reflects the relationship between differently dated cash flows. If $9,500 \\ntoday and $10,000 in one year are equivalent in value, then $10,000 − $9,500 = $500 \\nis the required compensation for receiving $10,000 in one year rather than now. The \\ninterest rate—the required compensation stated as a rate of return—is $500/$9,500 \\n= 0.0526 or 5.26 percent.\\nInterest rates can be thought of in three ways. First, they can be considered required \\nrates of return—that is, the minimum rate of return an investor must receive in order \\nto accept the investment. Second, interest rates can be considered discount rates. In \\nthe example above, 5.26 percent is that rate at which we discounted the $10,000 future \\namount to find its value today. Thus, we use the terms “interest rate” and “discount \\nrate” almost interchangeably. Third, interest rates can be considered opportunity costs. \\n1 Examples in this reading and other readings in quantitative methods at Level I were updated in 2018 by \\nProfessor Sanjiv Sabherwal of the University of Texas, Arlington.\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\nInterest Rates\\n5\\nAn opportunity cost is the value that investors forgo by choosing a particular course \\nof action. In the example, if the party who supplied $9,500 had instead decided to \\nspend it today, he would have forgone earning 5.26 percent on the money. So we can \\nview 5.26 percent as the opportunity cost of current consumption.\\nEconomics tells us that interest rates are set in the marketplace by the forces of sup-\\nply and demand, where investors are suppliers of funds and borrowers are demanders \\nof funds. Taking the perspective of investors in analyzing market-determined interest \\nrates, we can view an interest rate r as being composed of a real risk-free interest rate \\nplus a set of four premiums that are required returns or compensation for bearing \\ndistinct types of risk:\\n \\nr = Real risk-free interest rate + Inflation premium + Default risk premium + \\nLiquidity premium + Maturity premium\\n■ \\nThe real risk-free interest rate is the single-period interest rate for a com-\\npletely risk-free security if no inflation were expected. In economic theory, \\nthe real risk-free rate reflects the time preferences of individuals for current \\nversus future real consumption.\\n■ \\nThe inflation premium compensates investors for expected inflation and \\nreflects the average inflation rate expected over the maturity of the debt. \\nInflation reduces the purchasing power of a unit of currency—the amount \\nof goods and services one can buy with it. The sum of the real risk-free \\ninterest rate and the inflation premium is the nominal risk-free interest \\nrate.2 Many countries have governmental short-term debt whose interest \\nrate can be considered to represent the nominal risk-free interest rate in that \\ncountry. The interest rate on a 90-day US Treasury bill (T-bill), for example, \\nrepresents the nominal risk-free interest rate over that time horizon.3 US \\nT-bills can be bought and sold in large quantities with minimal transaction \\ncosts and are backed by the full faith and credit of the US government.\\n■ \\nThe default risk premium compensates investors for the possibility that the \\nborrower will fail to make a promised payment at the contracted time and in \\nthe contracted amount.\\n■ \\nThe liquidity premium compensates investors for the risk of loss relative \\nto an investment’s fair value if the investment needs to be converted to cash \\nquickly. US T-bills, for example, do not bear a liquidity premium because \\nlarge amounts can be bought and sold without affecting their market price. \\nMany bonds of small issuers, by contrast, trade infrequently after they are \\nissued; the interest rate on such bonds includes a liquidity premium reflect-\\ning the relatively high costs (including the impact on price) of selling a \\nposition.\\n■ \\nThe maturity premium compensates investors for the increased sensitivity \\nof the market value of debt to a change in market interest rates as maturity \\nis extended, in general (holding all else equal). The difference between the \\n2 Technically, 1 plus the nominal rate equals the product of 1 plus the real rate and 1 plus the inflation rate. \\nAs a quick approximation, however, the nominal rate is equal to the real rate plus an inflation premium. \\nIn this discussion we focus on approximate additive relationships to highlight the underlying concepts.\\n3 Other developed countries issue securities similar to US Treasury bills. The French government issues \\nBTFs or negotiable fixed-rate discount Treasury bills (Bons du Trésor àtaux fixe et à intérêts précomptés) \\nwith maturities of up to one year. The Japanese government issues a short-term Treasury bill with matur-\\nities of 6 and 12 months. The German government issues at discount both Treasury financing paper \\n(Finanzierungsschätze des Bundes or, for short, Schätze) and Treasury discount paper (Bubills) with \\nmaturities up to 24 months. In the United Kingdom, the British government issues gilt-edged Treasury \\nbills with maturities ranging from 1 to 364 days. The Canadian government bond market is closely related \\nto the US market; Canadian Treasury bills have maturities of 3, 6, and 12 months.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n6\\ninterest rate on longer-maturity, liquid Treasury debt and that on short-term \\nTreasury debt reflects a positive maturity premium for the longer-term debt \\n(and possibly different inflation premiums as well).\\nUsing this insight into the economic meaning of interest rates, we now turn to a \\ndiscussion of solving time value of money problems, starting with the future value \\nof a single cash flow.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Future Value of a Single Cash Flow',\n",
       "     'page_number': 16,\n",
       "     'content': 'FUTURE VALUE OF A SINGLE CASH FLOW\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nIn this section, we introduce time value associated with a single cash flow or lump-sum \\ninvestment. We describe the relationship between an initial investment or present \\nvalue (PV), which earns a rate of return (the interest rate per period) denoted as r, \\nand its future value (FV), which will be received N years or periods from today.\\nThe following example illustrates this concept. Suppose you invest $100 (PV = \\n$100) in an interest-bearing bank account paying 5 percent annually. At the end of \\nthe first year, you will have the $100 plus the interest earned, 0.05 × $100 = $5, for a \\ntotal of $105. To formalize this one-period example, we define the following terms:\\n \\nPV = present value of the investment\\n FVN = future value of the investment N periods from today\\n \\nr = rate of interest per period\\nFor N = 1, the expression for the future value of amount PV is\\n FV1 = PV(1 + r)   \\n(1)\\nFor this example, we calculate the future value one year from today as FV1 = $100(1.05) \\n= $105.\\nNow suppose you decide to invest the initial $100 for two years with interest \\nearned and credited to your account annually (annual compounding). At the end of \\nthe first year (the beginning of the second year), your account will have $105, which \\nyou will leave in the bank for another year. Thus, with a beginning amount of $105 \\n(PV = $105), the amount at the end of the second year will be $105(1.05) = $110.25. \\nNote that the $5.25 interest earned during the second year is 5 percent of the amount \\ninvested at the beginning of Year 2.\\nAnother way to understand this example is to note that the amount invested at \\nthe beginning of Year 2 is composed of the original $100 that you invested plus the \\n$5 interest earned during the first year. During the second year, the original principal \\nagain earns interest, as does the interest that was earned during Year 1. You can see \\nhow the original investment grows:\\nOriginal investment\\n$100.00\\nInterest for the first year ($100 × 0.05)\\n5.00\\nInterest for the second year based on original investment ($100 × 0.05)\\n5.00\\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nFuture Value of a Single Cash Flow\\n7\\nInterest for the second year based on interest earned in the first year (0.05 × \\n$5.00 interest on interest)\\n0.25\\n\\xa0\\xa0\\xa0Total\\n$110.25\\nThe $5 interest that you earned each period on the $100 original investment is known \\nas simple interest (the interest rate times the principal). Principal is the amount of \\nfunds originally invested. During the two-year period, you earn $10 of simple interest. \\nThe extra $0.25 that you have at the end of Year 2 is the interest you earned on the \\nYear 1 interest of $5 that you reinvested.\\nThe interest earned on interest provides the first glimpse of the phenomenon \\nknown as compounding. Although the interest earned on the initial investment is \\nimportant, for a given interest rate it is fixed in size from period to period. The com-\\npounded interest earned on reinvested interest is a far more powerful force because, \\nfor a given interest rate, it grows in size each period. The importance of compounding \\nincreases with the magnitude of the interest rate. For example, $100 invested today \\nwould be worth about $13,150 after 100 years if compounded annually at 5 percent, \\nbut worth more than $20 million if compounded annually over the same time period \\nat a rate of 13 percent.\\nTo verify the $20 million figure, we need a general formula to handle compounding \\nfor any number of periods. The following general formula relates the present value of \\nan initial investment to its future value after N periods:\\n FVN = PV(1 + r)N   \\n(2)\\nwhere r is the stated interest rate per period and N is the number of compounding \\nperiods. In the bank example, FV2 = $100(1 + 0.05)2 = $110.25. In the 13 percent \\ninvestment example, FV100 = $100(1.13)100 = $20,316,287.42.\\nThe most important point to remember about using the future value equation is \\nthat the stated interest rate, r, and the number of compounding periods, N, must be \\ncompatible. Both variables must be defined in the same time units. For example, if \\nN is stated in months, then r should be the one-month interest rate, unannualized.\\nA time line helps us to keep track of the compatibility of time units and the interest \\nrate per time period. In the time line, we use the time index t to represent a point in \\ntime a stated number of periods from today. Thus the present value is the amount \\navailable for investment today, indexed as t = 0. We can now refer to a time N periods \\nfrom today as t = N. The time line in Exhibit 1 shows this relationship.\\nExhibit 1: The Relationship between an Initial Investment, PV, and Its Future \\nValue, FV\\n0\\n1\\n 2\\n 3\\n...\\nN – 1\\nN\\nPV\\nFVN = PV(1 + r)N\\nIn Exhibit 1, we have positioned the initial investment, PV, at t = 0. Using Equation \\n2, we move the present value, PV, forward to t = N by the factor (1 + r)N. This factor \\nis called a future value factor. We denote the future value on the time line as FV and \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n8\\nposition it at t = N. Suppose the future value is to be received exactly 10 periods from \\ntoday’s date (N = 10). The present value, PV, and the future value, FV, are separated \\nin time through the factor (1 + r)10.\\nThe fact that the present value and the future value are separated in time has \\nimportant consequences:\\n■ \\nWe can add amounts of money only if they are indexed at the same point in \\ntime.\\n■ \\nFor a given interest rate, the future value increases with the number of \\nperiods.\\n■ \\nFor a given number of periods, the future value increases with the interest \\nrate.\\nTo better understand these concepts, consider three examples that illustrate how \\nto apply the future value formula.\\nEXAMPLE 1\\nThe Future Value of a Lump Sum with Interim Cash \\nReinvested at the Same Rate\\n1. You are the lucky winner of your state’s lottery of $5 million after taxes. \\nYou invest your winnings in a five-year certificate of deposit (CD) at a local \\nfinancial institution. The CD promises to pay 7 percent per year compound-\\ned annually. This institution also lets you reinvest the interest at that rate for \\nthe duration of the CD. How much will you have at the end of five years if \\nyour money remains invested at 7 percent for five years with no withdraw-\\nals?\\nSolution: \\nTo solve this problem, compute the future value of the $5 million investment \\nusing the following values in Equation 2:\\n \\nPV = $5, 000, 000\\n  \\n \\nr = 7 %  = 0.07\\n  \\n \\nN = 5\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= $5,000,000  ( 1.07 ) 5 \\n  \\n \\n= $5,000,000  ( 1.402552 )  \\n  \\n \\n \\n= $7,012,758.65\\n \\n \\nAt the end of five years, you will have $7,012,758.65 if your money remains \\ninvested at 7 percent with no withdrawals.\\nIn this and most examples in this reading, note that the factors are reported at six \\ndecimal places but the calculations may actually reflect greater precision. For exam-\\nple, the reported 1.402552 has been rounded up from 1.40255173 (the calculation is \\nactually carried out with more than eight decimal places of precision by the calculator \\nor spreadsheet). Our final result reflects the higher number of decimal places carried \\nby the calculator or spreadsheet.4\\n4 We could also solve time value of money problems using tables of interest rate factors. Solutions using \\ntabled values of interest rate factors are generally less accurate than solutions obtained using calculators \\nor spreadsheets, so practitioners prefer calculators or spreadsheets.\\n© CFA Institute. For candidate use only. Not for distribution.\\nFuture Value of a Single Cash Flow\\n9\\nEXAMPLE 2\\nThe Future Value of a Lump Sum with No Interim Cash\\n1. An institution offers you the following terms for a contract: For an invest-\\nment of ¥2,500,000,  the institution promises to pay you a lump sum six \\nyears from now at an 8 percent annual interest rate. What future amount \\ncan you expect?\\nSolution: \\nUse the following data in Equation 2 to find the future value:\\n \\nPV = ¥2, 500, 000\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 6\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= ¥2, 500, 000  ( 1.08 ) 6 \\n  \\n \\n= ¥2, 500, 000  ( 1.586874 )  \\n  \\n \\n \\n= ¥3, 967, 186\\n \\n \\nYou can expect to receive ¥3,967,186 six years from now.\\nOur third example is a more complicated future value problem that illustrates the \\nimportance of keeping track of actual calendar time.\\nEXAMPLE 3\\nThe Future Value of a Lump Sum\\n1. A pension fund manager estimates that his corporate sponsor will make \\na $10 million contribution five years from now. The rate of return on plan \\nassets has been estimated at 9 percent per year. The pension fund manager \\nwants to calculate the future value of this contribution 15 years from now, \\nwhich is the date at which the funds will be distributed to retirees. What is \\nthat future value?\\nSolution:\\nBy positioning the initial investment, PV, at t = 5, we can calculate the future \\nvalue of the contribution using the following data in Equation 2:\\n \\nPV = $10 million\\n  \\n \\nr = 9 %  = 0.09\\n  \\n \\nN = 10\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= $10,000,000  ( 1.09 ) 10 \\n  \\n \\n= $10,000,000  ( 2.367364 )  \\n  \\n \\n \\n= $23,673,636.75\\n \\n \\nThis problem looks much like the previous two, but it differs in one im-\\nportant respect: its timing. From the standpoint of today (t = 0), the future \\namount of $23,673,636.75 is 15 years into the future. Although the future \\nvalue is 10 years from its present value, the present value of $10 million will \\nnot be received for another five years.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n10\\nExhibit 2: The Future Value of a Lump Sum, Initial Investment Not at \\nt = 0\\n \\nAs Exhibit 2 shows, we have followed the convention of indexing today \\nas t = 0 and indexing subsequent times by adding 1 for each period. The \\nadditional contribution of $10 million is to be received in five years, so it is \\nindexed as t = 5 and appears as such in the figure. The future value of the \\ninvestment in 10 years is then indexed at t = 15; that is, 10 years following \\nthe receipt of the $10 million contribution at t = 5. Time lines like this one \\ncan be extremely useful when dealing with more-complicated problems, \\nespecially those involving more than one cash flow.\\nIn a later section of this reading, we will discuss how to calculate the value today \\nof the $10 million to be received five years from now. For the moment, we can use \\nEquation 2. Suppose the pension fund manager in Example 3 above were to receive \\n$6,499,313.86 today from the corporate sponsor. How much will that sum be worth \\nat the end of five years? How much will it be worth at the end of 15 years?\\n \\nPV = $6,499,313.86\\n  \\n \\nr = 9 %  = 0.09\\n  \\n \\nN = 5\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= $6,499,313.86  ( 1.09 ) 5 \\n  \\n \\n= $6,499,313.86  ( 1.538624 )  \\n  \\n \\n \\n= $10,000,000 at the five-year mark\\n  \\nand\\n \\nPV = $6,499,313.86\\n  \\n \\nr = 9 %  = 0.09\\n  \\n \\nN = 15\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= $6,499,313.86  ( 1.09 ) 15 \\n  \\n \\n \\n= $6,499,313.86  ( 3.642482 )  \\n  \\n \\n \\n= $23,673,636.74 at the 15-year mark\\n  \\nThese results show that today’s present value of about $6.5 million becomes $10 \\nmillion after five years and $23.67 million after 15 years.\\nNON-ANNUAL COMPOUNDING (FUTURE VALUE)\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Non-Annual Compounding (Future Value)',\n",
       "     'page_number': 20,\n",
       "     'content': 'Learning Module 1 \\nThe Time Value of Money\\n10\\nExhibit 2: The Future Value of a Lump Sum, Initial Investment Not at \\nt = 0\\n \\nAs Exhibit 2 shows, we have followed the convention of indexing today \\nas t = 0 and indexing subsequent times by adding 1 for each period. The \\nadditional contribution of $10 million is to be received in five years, so it is \\nindexed as t = 5 and appears as such in the figure. The future value of the \\ninvestment in 10 years is then indexed at t = 15; that is, 10 years following \\nthe receipt of the $10 million contribution at t = 5. Time lines like this one \\ncan be extremely useful when dealing with more-complicated problems, \\nespecially those involving more than one cash flow.\\nIn a later section of this reading, we will discuss how to calculate the value today \\nof the $10 million to be received five years from now. For the moment, we can use \\nEquation 2. Suppose the pension fund manager in Example 3 above were to receive \\n$6,499,313.86 today from the corporate sponsor. How much will that sum be worth \\nat the end of five years? How much will it be worth at the end of 15 years?\\n \\nPV = $6,499,313.86\\n  \\n \\nr = 9 %  = 0.09\\n  \\n \\nN = 5\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= $6,499,313.86  ( 1.09 ) 5 \\n  \\n \\n= $6,499,313.86  ( 1.538624 )  \\n  \\n \\n \\n= $10,000,000 at the five-year mark\\n  \\nand\\n \\nPV = $6,499,313.86\\n  \\n \\nr = 9 %  = 0.09\\n  \\n \\nN = 15\\n  \\n FV N = PV  ( 1 + r ) N   \\n \\n= $6,499,313.86  ( 1.09 ) 15 \\n  \\n \\n \\n= $6,499,313.86  ( 3.642482 )  \\n  \\n \\n \\n= $23,673,636.74 at the 15-year mark\\n  \\nThese results show that today’s present value of about $6.5 million becomes $10 \\nmillion after five years and $23.67 million after 15 years.\\nNON-ANNUAL COMPOUNDING (FUTURE VALUE)\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nNon-Annual Compounding (Future Value)\\n11\\nIn this section, we examine investments paying interest more than once a year. For \\ninstance, many banks offer a monthly interest rate that compounds 12 times a year. \\nIn such an arrangement, they pay interest on interest every month. Rather than quote \\nthe periodic monthly interest rate, financial institutions often quote an annual interest \\nrate that we refer to as the stated annual interest rate or quoted interest rate. We \\ndenote the stated annual interest rate by rs. For instance, your bank might state that \\na particular CD pays 8 percent compounded monthly. The stated annual interest rate \\nequals the monthly interest rate multiplied by 12. In this example, the monthly interest \\nrate is 0.08/12 = 0.0067 or 0.67 percent.5 This rate is strictly a quoting convention \\nbecause (1 + 0.0067)12 = 1.083, not 1.08; the term (1 + rs) is not meant to be a future \\nvalue factor when compounding is more frequent than annual.\\nWith more than one compounding period per year, the future value formula can \\nbe expressed as\\n FV N = PV  ( 1 +  \\n r s  \\n_ \\nm  ) \\nmN\\n \\n(3)\\nwhere rs is the stated annual interest rate, m is the number of compounding \\nperiods per year, and N now stands for the number of years. Note the compatibility \\nhere between the interest rate used, rs/m, and the number of compounding periods, \\nmN. The periodic rate, rs/m, is the stated annual interest rate divided by the number \\nof compounding periods per year. The number of compounding periods, mN, is the \\nnumber of compounding periods in one year multiplied by the number of years. The \\nperiodic rate, rs/m, and the number of compounding periods, mN, must be compatible.\\nEXAMPLE 4\\nThe Future Value of a Lump Sum with Quarterly \\nCompounding\\n1. Continuing with the CD example, suppose your bank offers you a CD with \\na two-year maturity, a stated annual interest rate of 8 percent compounded \\nquarterly, and a feature allowing reinvestment of the interest at the same \\ninterest rate. You decide to invest $10,000. What will the CD be worth at \\nmaturity?\\nSolution:\\nCompute the future value with Equation 3 as follows:\\n \\nPV = $10,000\\n  \\n \\n r s = 8 %  = 0.08\\n  \\n \\nm = 4\\n  \\n r s \\u200a/\\u200am = 0.08\\u200a/\\u200a4 = 0.02\\n  \\n \\nN = 2\\n  \\nmN = 4  ( 2 )  = 8 interest periods  \\n \\n \\n FV N = PV  ( 1 +  \\n r s  \\n_ \\nm  ) \\nmN\\n \\n  \\n \\n= $10,000  ( 1.02 ) 8 \\n  \\n \\n= $10,000  ( 1.171659 )  \\n  \\n \\n= $11,716.59\\n \\n \\nAt maturity, the CD will be worth $11,716.59.\\n5 To avoid rounding errors when using a financial calculator, divide 8 by 12 and then press the %i key, \\nrather than simply entering 0.67 for %i, so we have (1 + 0.08/12)12 = 1.083000.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n12\\nThe future value formula in Equation 3 does not differ from the one in Equation 2. \\nSimply keep in mind that the interest rate to use is the rate per period and the expo-\\nnent is the number of interest, or compounding, periods.\\nEXAMPLE 5\\nThe Future Value of a Lump Sum with Monthly \\nCompounding\\n1. An Australian bank offers to pay you 6 percent compounded monthly. You \\ndecide to invest A$1 million for one year. What is the future value of your \\ninvestment if interest payments are reinvested at 6 percent?\\nSolution:\\nUse Equation 3 to find the future value of the one-year investment as fol-\\nlows:\\n \\nPV = A$1,000,000\\n  \\n \\n r s = 6 %  = 0.06\\n  \\n \\nm = 12\\n  \\n r s \\u200a/\\u200am = 0.06\\u200a/\\u200a12 = 0.0050\\n  \\n \\n \\nN = 1\\n  \\nmN = 12  ( 1 )  = 12 interest periods  \\n \\n \\n FV N = PV  ( 1 +  \\n r s  \\n_ \\nm  ) \\nmN\\n \\n  \\n \\n= A$1,000,000  ( 1.005 ) 12 \\n  \\n \\n \\n= A$1,000,000  ( 1.061678 )  \\n  \\n \\n \\n= A$1,061,677.81\\n \\n \\nIf you had been paid 6 percent with annual compounding, the future \\namount would be only A$1,000,000(1.06) = A$1,060,000 instead of \\nA$1,061,677.81 with monthly compounding.\\nCONTINUOUS COMPOUNDING\\ncalculate and interpret the effective annual rate, given the stated \\nannual interest rate and the frequency of compounding\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\nThe preceding discussion on compounding periods illustrates discrete compounding, \\nwhich credits interest after a discrete amount of time has elapsed. If the number of \\ncompounding periods per year becomes infinite, then interest is said to compound \\ncontinuously. If we want to use the future value formula with continuous compound-\\ning, we need to find the limiting value of the future value factor for m → ∞ (infinitely \\nmany compounding periods per year) in Equation 3. The expression for the future \\nvalue of a sum in N years with continuous compounding is\\n FV N = PV  e  r s   N \\n(4)\\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Continuous Compounding',\n",
       "     'page_number': 22,\n",
       "     'content': 'CONTINUOUS COMPOUNDING\\ncalculate and interpret the effective annual rate, given the stated \\nannual interest rate and the frequency of compounding\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\nThe preceding discussion on compounding periods illustrates discrete compounding, \\nwhich credits interest after a discrete amount of time has elapsed. If the number of \\ncompounding periods per year becomes infinite, then interest is said to compound \\ncontinuously. If we want to use the future value formula with continuous compound-\\ning, we need to find the limiting value of the future value factor for m → ∞ (infinitely \\nmany compounding periods per year) in Equation 3. The expression for the future \\nvalue of a sum in N years with continuous compounding is\\n FV N = PV  e  r s   N \\n(4)\\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nContinuous Compounding\\n13\\nThe term  e  r s   N is the transcendental number e ≈ 2.7182818 raised to the power rsN. \\nMost financial calculators have the function ex.\\nEXAMPLE 6\\nThe Future Value of a Lump Sum with Continuous \\nCompounding\\nSuppose a $10,000 investment will earn 8 percent compounded continuously \\nfor two years. We can compute the future value with Equation 4 as follows:\\n \\nPV = $10,000\\n  \\n \\n r s = 8 %  = 0.08\\n  \\n \\nN = 2\\n  \\n FV N = PV  e  r s   N   \\n \\n= $10,000  e 0.08  ( 2 )   \\n  \\n \\n= $10,000  ( 1.173511 )  \\n  \\n \\n= $11,735.11\\n \\n \\nWith the same interest rate but using continuous compounding, the $10,000 \\ninvestment will grow to $11,735.11 in two years, compared with $11,716.59 \\nusing quarterly compounding as shown in Example 4.\\nExhibit 3 shows how a stated annual interest rate of 8 percent generates different \\nending dollar amounts with annual, semiannual, quarterly, monthly, daily, and contin-\\nuous compounding for an initial investment of $1 (carried out to six decimal places).\\nAs Exhibit 3 shows, all six cases have the same stated annual interest rate of 8 \\npercent; they have different ending dollar amounts, however, because of differences \\nin the frequency of compounding. With annual compounding, the ending amount \\nis $1.08. More frequent compounding results in larger ending amounts. The ending \\ndollar amount with continuous compounding is the maximum amount that can be \\nearned with a stated annual rate of 8 percent.\\nExhibit 3: The Effect of Compounding Frequency on Future Value\\nFrequency\\nrs/m\\nmN\\nFuture Value of $1\\nAnnual\\n8%/1 = 8%\\n1 × 1 = 1\\n$1.00(1.08)\\n=\\n$1.08\\nSemiannual\\n8%/2 = 4%\\n2 × 1 = 2\\n$1.00(1.04)2\\n=\\n$1.081600\\nQuarterly\\n8%/4 = 2%\\n4 × 1 = 4\\n$1.00(1.02)4\\n=\\n$1.082432\\nMonthly\\n8%/12 = 0.6667%\\n12 × 1 = 12\\n$1.00(1.006667)12\\n=\\n$1.083000\\nDaily\\n8%/365 = 0.0219%\\n365 × 1 = 365\\n$1.00(1.000219)365\\n=\\n$1.083278\\nContinuous\\n\\xa0\\n\\xa0\\n$1.00e0.08(1)\\n=\\n$1.083287\\nExhibit 3 also shows that a $1 investment earning 8.16 percent compounded annually \\ngrows to the same future value at the end of one year as a $1 investment earning 8 \\npercent compounded semiannually. This result leads us to a distinction between the \\nstated annual interest rate and the effective annual rate (EAR).6 For an 8 percent \\nstated annual interest rate with semiannual compounding, the EAR is 8.16 percent.\\n6 Among the terms used for the effective annual return on interest-bearing bank deposits are annual \\npercentage yield (APY) in the United States and equivalent annual rate (EAR) in the United Kingdom. \\nBy contrast, the annual percentage rate (APR) measures the cost of borrowing expressed as a yearly \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n14\\n',\n",
       "     'children': [{'title': 'Stated and Effective Rates',\n",
       "       'page_number': 24,\n",
       "       'content': 'Stated and Effective Rates\\nThe stated annual interest rate does not give a future value directly, so we need a for-\\nmula for the EAR. With an annual interest rate of 8 percent compounded semiannually, \\nwe receive a periodic rate of 4 percent. During the course of a year, an investment of \\n$1 would grow to $1(1.04)2 = $1.0816, as illustrated in Exhibit 3. The interest earned \\non the $1 investment is $0.0816 and represents an effective annual rate of interest of \\n8.16 percent. The effective annual rate is calculated as follows:\\n EAR = (1 + Periodic interest rate)m – 1   \\n(5)\\nThe periodic interest rate is the stated annual interest rate divided by m, where m is \\nthe number of compounding periods in one year. Using our previous example, we can \\nsolve for EAR as follows: (1.04)2 − 1 = 8.16 percent.\\nThe concept of EAR extends to continuous compounding. Suppose we have a rate \\nof 8 percent compounded continuously. We can find the EAR in the same way as above \\nby finding the appropriate future value factor. In this case, a $1 investment would \\ngrow to $1e0.08(1.0) = $1.0833. The interest earned for one year represents an effective \\nannual rate of 8.33 percent and is larger than the 8.16 percent EAR with semiannual \\ncompounding because interest is compounded more frequently. With continuous \\ncompounding, we can solve for the effective annual rate as follows:\\n EAR =  e  r s  − 1 \\n(6)\\nWe can reverse the formulas for EAR with discrete and continuous compounding to \\nfind a periodic rate that corresponds to a particular effective annual rate. Suppose we \\nwant to find the appropriate periodic rate for a given effective annual rate of 8.16 per-\\ncent with semiannual compounding. We can use Equation 5 to find the periodic rate:\\n \\n0.0816 =  ( 1 + Periodic rate ) 2 − 1\\n  \\n \\n \\n1.0816 =  ( 1 + Periodic rate ) 2 \\n  \\n \\n \\n ( 1.0816 ) 1/2 − 1 = Periodic rate  \\n \\n \\n  ( 1.04 )  − 1 = Periodic rate\\n  \\n \\n \\n4% = Periodic rate\\n \\n \\nTo calculate the continuously compounded rate (the stated annual interest rate with \\ncontinuous compounding) corresponding to an effective annual rate of 8.33 percent, \\nwe find the interest rate that satisfies Equation 6:\\n 0.0833 =  e  r s  − 1  \\n \\n1.0833 =  e  r s    \\n \\nTo solve this equation, we take the natural logarithm of both sides. (Recall that the \\nnatural log of   e    r  s     is ln   e    r  s    =  r  s   .) Therefore, ln 1.0833 = rs, resulting in rs = 8 percent. We \\nsee that a stated annual rate of 8 percent with continuous compounding is equivalent \\nto an EAR of 8.33 percent.\\nrate. In the United States, the APR is calculated as a periodic rate times the number of payment periods \\nper year and, as a result, some writers use APR as a general synonym for the stated annual interest rate. \\nNevertheless, APR is a term with legal connotations; its calculation follows regulatory standards that vary \\ninternationally. Therefore, “stated annual interest rate” is the preferred general term for an annual interest \\nrate that does not account for compounding within the year.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'A Series of Cash Flows',\n",
       "     'page_number': 25,\n",
       "     'content': 'A Series of Cash Flows\\n15\\nA SERIES OF CASH FLOWS\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nIn this section, we consider series of cash flows, both even and uneven. We begin \\nwith a list of terms commonly used when valuing cash flows that are distributed over \\nmany time periods.\\n■ \\nAn annuity is a finite set of level sequential cash flows.\\n■ \\nAn ordinary annuity has a first cash flow that occurs one period from now \\n(indexed at t = 1).\\n■ \\nAn annuity due has a first cash flow that occurs immediately (indexed at t \\n= 0).\\n■ \\nA perpetuity is a perpetual annuity, or a set of level never-ending sequen-\\ntial cash flows, with the first cash flow occurring one period from now.\\n',\n",
       "     'children': [{'title': 'Equal Cash Flows—Ordinary Annuity',\n",
       "       'page_number': 25,\n",
       "       'content': '',\n",
       "       'children': []},\n",
       "      {'title': 'Unequal Cash Flows',\n",
       "       'page_number': 26,\n",
       "       'content': 'Unequal Cash Flows\\nIn many cases, cash flow streams are unequal, precluding the simple use of the future \\nvalue annuity factor. For instance, an individual investor might have a savings plan \\nthat involves unequal cash payments depending on the month of the year or lower \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Present Value of a Single Cash Flow',\n",
       "     'page_number': 27,\n",
       "     'content': 'Present Value of a Single Cash Flow\\n17\\nsavings during a planned vacation. One can always find the future value of a series \\nof unequal cash flows by compounding the cash flows one at a time. Suppose you \\nhave the five cash flows described in Exhibit 5, indexed relative to the present (t = 0).\\nExhibit 5: A Series of Unequal Cash Flows and Their Future \\nValues at 5 Percent\\nTime\\nCash Flow ($)\\nFuture Value at Year 5\\nt = 1\\n1,000\\n$1,000(1.05)4\\n=\\n$1,215.51\\nt = 2\\n2,000\\n$2,000(1.05)3\\n=\\n$2,315.25\\nt = 3\\n4,000\\n$4,000(1.05)2\\n=\\n$4,410.00\\nt = 4\\n5,000\\n$5,000(1.05)1\\n=\\n$5,250.00\\nt = 5\\n6,000\\n$6,000(1.05)0\\n=\\n$6,000.00\\n\\xa0\\n\\xa0\\nSum\\n=\\n$19,190.76\\nAll of the payments shown in Exhibit 5 are different. Therefore, the most direct \\napproach to finding the future value at t = 5 is to compute the future value of each \\npayment as of t = 5 and then sum the individual future values. The total future value \\nat Year 5 equals $19,190.76, as shown in the third column. Later in this reading, you \\nwill learn shortcuts to take when the cash flows are close to even; these shortcuts will \\nallow you to combine annuity and single-period calculations.\\nPRESENT VALUE OF A SINGLE CASH FLOW\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nJust as the future value factor links today’s present value with tomorrow’s future \\nvalue, the present value factor allows us to discount future value to present value. \\nFor example, with a 5 percent interest rate generating a future payoff of $105 in one \\nyear, what current amount invested at 5 percent for one year will grow to $105? The \\nanswer is $100; therefore, $100 is the present value of $105 to be received in one year \\nat a discount rate of 5 percent.\\nGiven a future cash flow that is to be received in N periods and an interest rate per \\nperiod of r, we can use the formula for future value to solve directly for the present \\nvalue as follows:\\n \\n FV N = PV  ( 1 + r ) N \\n  \\n \\nPV =  FV N  [ \\n1 \\n_ \\n ( 1 + r ) N   ]   \\n \\nPV =  FV N  ( 1 + r ) −N \\n  \\n(8)\\nWe see from Equation 8 that the present value factor, (1 + r)−N, is the reciprocal \\nof the future value factor, (1 + r)N.\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n18\\nEXAMPLE 8\\nThe Present Value of a Lump Sum\\n1. An insurance company has issued a Guaranteed Investment Contract (GIC) \\nthat promises to pay $100,000 in six years with an 8 percent return rate. \\nWhat amount of money must the insurer invest today at 8 percent for six \\nyears to make the promised payment?\\nSolution:\\nWe can use Equation 8 to find the present value using the following data:\\n \\n FV N = $100,000\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 6\\n  \\nPV =  FV N  ( 1 + r ) −N   \\n \\n= $100,000  [ \\n1 \\n_ \\n ( 1.08 ) 6   ]  \\n  \\n \\n= $100,000  ( 0.6301696 )  \\n  \\n \\n= $63,016.96\\n \\n \\nWe can say that $63,016.96 today, with an interest rate of 8 percent, is \\nequivalent to $100,000 to be received in six years. Discounting the $100,000 \\nmakes a future $100,000 equivalent to $63,016.96 when allowance is \\nmade for the time value of money. As the time line in Exhibit 6 shows, the \\n$100,000 has been discounted six full periods.\\n \\nExhibit 6: The Present Value of a Lump Sum to Be Received at Time \\nt = 6\\n \\n1\\n2\\n3\\n4\\n6\\nPV = $63,016.96\\n$100,000 = FV\\n5\\n0\\nEXAMPLE 9\\nThe Projected Present Value of a More Distant Future \\nLump Sum\\n1. Suppose you own a liquid financial asset that will pay you $100,000 in 10 \\nyears from today. Your daughter plans to attend college four years from to-\\nday, and you want to know what the asset’s present value will be at that time. \\n© CFA Institute. For candidate use only. Not for distribution.\\nNon-Annual Compounding (Present Value)\\n19\\nGiven an 8 percent discount rate, what will the asset be worth four years \\nfrom today?\\nSolution:\\nThe value of the asset is the present value of the asset’s promised payment. \\nAt t = 4, the cash payment will be received six years later. With this informa-\\ntion, you can solve for the value four years from today using Equation 8:\\n \\n FV N = $100,000\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 6\\n  \\nPV =  FV N  ( 1 + r ) −N   \\n \\n= $100,000 \\n1 \\n_ \\n ( 1.08 ) 6   \\n  \\n \\n= $100,000  ( 0.6301696 )  \\n  \\n \\n= $63,016.96\\n \\n \\n \\nExhibit 7: The Relationship between Present Value and Future Value\\n \\n0 \\n...\\n4\\n...\\n10\\n$46,319.35\\n$63,016.96\\n$100,000\\nThe time line in Exhibit 7 shows the future payment of $100,000 that is to \\nbe received at t = 10. The time line also shows the values at t = 4 and at t = 0. \\nRelative to the payment at t = 10, the amount at t = 4 is a projected present \\nvalue, while the amount at t = 0 is the present value (as of today).\\nPresent value problems require an evaluation of the present value factor, (1 + r)−N. \\nPresent values relate to the discount rate and the number of periods in the following \\nways:\\n■ \\nFor a given discount rate, the farther in the future the amount to be \\nreceived, the smaller that amount’s present value.\\n■ \\nHolding time constant, the larger the discount rate, the smaller the present \\nvalue of a future amount.\\nNON-ANNUAL COMPOUNDING (PRESENT VALUE)\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\nRecall that interest may be paid semiannually, quarterly, monthly, or even daily. To \\nhandle interest payments made more than once a year, we can modify the present \\nvalue formula (Equation 8) as follows. Recall that rs is the quoted interest rate and \\nequals the periodic interest rate multiplied by the number of compounding periods \\nin each year. In general, with more than one compounding period in a year, we can \\nexpress the formula for present value as\\n8\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Non-Annual Compounding (Present Value)',\n",
       "     'page_number': 29,\n",
       "     'content': 'Non-Annual Compounding (Present Value)\\n19\\nGiven an 8 percent discount rate, what will the asset be worth four years \\nfrom today?\\nSolution:\\nThe value of the asset is the present value of the asset’s promised payment. \\nAt t = 4, the cash payment will be received six years later. With this informa-\\ntion, you can solve for the value four years from today using Equation 8:\\n \\n FV N = $100,000\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 6\\n  \\nPV =  FV N  ( 1 + r ) −N   \\n \\n= $100,000 \\n1 \\n_ \\n ( 1.08 ) 6   \\n  \\n \\n= $100,000  ( 0.6301696 )  \\n  \\n \\n= $63,016.96\\n \\n \\n \\nExhibit 7: The Relationship between Present Value and Future Value\\n \\n0 \\n...\\n4\\n...\\n10\\n$46,319.35\\n$63,016.96\\n$100,000\\nThe time line in Exhibit 7 shows the future payment of $100,000 that is to \\nbe received at t = 10. The time line also shows the values at t = 4 and at t = 0. \\nRelative to the payment at t = 10, the amount at t = 4 is a projected present \\nvalue, while the amount at t = 0 is the present value (as of today).\\nPresent value problems require an evaluation of the present value factor, (1 + r)−N. \\nPresent values relate to the discount rate and the number of periods in the following \\nways:\\n■ \\nFor a given discount rate, the farther in the future the amount to be \\nreceived, the smaller that amount’s present value.\\n■ \\nHolding time constant, the larger the discount rate, the smaller the present \\nvalue of a future amount.\\nNON-ANNUAL COMPOUNDING (PRESENT VALUE)\\ncalculate the solution for time value of money problems with \\ndifferent frequencies of compounding\\nRecall that interest may be paid semiannually, quarterly, monthly, or even daily. To \\nhandle interest payments made more than once a year, we can modify the present \\nvalue formula (Equation 8) as follows. Recall that rs is the quoted interest rate and \\nequals the periodic interest rate multiplied by the number of compounding periods \\nin each year. In general, with more than one compounding period in a year, we can \\nexpress the formula for present value as\\n8\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n20\\n PV =  FV N  ( 1 +  \\n r s  \\n_ \\nm  ) \\n−mN\\n \\n(9)\\nwhere\\n \\nm = number of compounding periods per year\\n \\nrs = quoted annual interest rate\\n \\nN = number of years\\nThe formula in Equation 9 is quite similar to that in Equation 8. As we have already \\nnoted, present value and future value factors are reciprocals. Changing the frequency \\nof compounding does not alter this result. The only difference is the use of the periodic \\ninterest rate and the corresponding number of compounding periods.\\nThe following example illustrates Equation 9.\\nEXAMPLE 10\\nThe Present Value of a Lump Sum with Monthly \\nCompounding\\n1. The manager of a Canadian pension fund knows that the fund must make a \\nlump-sum payment of C$5 million 10 years from now. She wants to invest \\nan amount today in a GIC so that it will grow to the required amount. The \\ncurrent interest rate on GICs is 6 percent a year, compounded monthly. \\nHow much should she invest today in the GIC?\\nSolution:\\nUse Equation 9 to find the required present value:\\n \\n FV N = C$5,000,000\\n  \\n \\n r s = 6 %  = 0.06\\n  \\n \\nm = 12\\n  \\n r s \\u200a/\\u200am = 0.06\\u200a/\\u200a12 = 0.005\\n  \\n \\n \\nN = 10\\n  \\nmN = 12  ( 10 )  = 120  \\n \\nPV =  FV N  ( 1 +  \\n r s  \\n_ \\nm  ) \\n−mN\\n \\n  \\n \\n \\n= C$5,000,000  ( 1.005 ) −120 \\n  \\n \\n \\n= C$5,000,000  ( 0.549633 )  \\n  \\n \\n \\n= C$2,748,163.67\\n \\n \\nIn applying Equation 9, we use the periodic rate (in this case, the monthly \\nrate) and the appropriate number of periods with monthly compounding (in \\nthis case, 10 years of monthly compounding, or 120 periods).\\n© CFA Institute. For candidate use only. Not for distribution.\\nPresent Value of a Series of Equal and Unequal Cash Flows\\n21\\nPRESENT VALUE OF A SERIES OF EQUAL AND \\nUNEQUAL CASH FLOWS\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nMany applications in investment management involve assets that offer a series of \\ncash flows over time. The cash flows may be highly uneven, relatively even, or equal. \\nThey may occur over relatively short periods of time, longer periods of time, or even \\nstretch on indefinitely. In this section, we discuss how to find the present value of a \\nseries of cash flows.\\nThe Present Value of a Series of Equal Cash Flows\\nWe begin with an ordinary annuity. Recall that an ordinary annuity has equal annuity \\npayments, with the first payment starting one period into the future. In total, the \\nannuity makes N payments, with the first payment at t = 1 and the last at t = N. We \\ncan express the present value of an ordinary annuity as the sum of the present values \\nof each individual annuity payment, as follows:\\n PV =  A \\n_ \\n ( 1 + r )   +  \\nA \\n_ \\n ( 1 + r ) 2   +  \\nA \\n_ \\n ( 1 + r ) 3   + …\\u200a+\\u200a \\nA \\n_ \\n ( 1 + r ) N−1   +  \\nA \\n_ \\n ( 1 + r ) N   \\n(10)\\nwhere\\n \\nA = the annuity amount\\n \\nr = the interest rate per period corresponding to the frequency of annuity \\npayments (for example, annual, quarterly, or monthly)\\n \\nN = the number of annuity payments\\nBecause the annuity payment (A) is a constant in this equation, it can be factored out \\nas a common term. Thus the sum of the interest factors has a shortcut expression:\\n PV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n(11)\\nIn much the same way that we computed the future value of an ordinary annuity, we \\nfind the present value by multiplying the annuity amount by a present value annuity \\nfactor (the term in brackets in Equation 11).\\n9\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Present Value of a Series of Equal and Unequal Cash Flows',\n",
       "     'page_number': 31,\n",
       "     'content': 'Present Value of a Series of Equal and Unequal Cash Flows\\n21\\nPRESENT VALUE OF A SERIES OF EQUAL AND \\nUNEQUAL CASH FLOWS\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nMany applications in investment management involve assets that offer a series of \\ncash flows over time. The cash flows may be highly uneven, relatively even, or equal. \\nThey may occur over relatively short periods of time, longer periods of time, or even \\nstretch on indefinitely. In this section, we discuss how to find the present value of a \\nseries of cash flows.\\n',\n",
       "     'children': [{'title': 'The Present Value of a Series of Equal Cash Flows',\n",
       "       'page_number': 31,\n",
       "       'content': 'The Present Value of a Series of Equal Cash Flows\\nWe begin with an ordinary annuity. Recall that an ordinary annuity has equal annuity \\npayments, with the first payment starting one period into the future. In total, the \\nannuity makes N payments, with the first payment at t = 1 and the last at t = N. We \\ncan express the present value of an ordinary annuity as the sum of the present values \\nof each individual annuity payment, as follows:\\n PV =  A \\n_ \\n ( 1 + r )   +  \\nA \\n_ \\n ( 1 + r ) 2   +  \\nA \\n_ \\n ( 1 + r ) 3   + …\\u200a+\\u200a \\nA \\n_ \\n ( 1 + r ) N−1   +  \\nA \\n_ \\n ( 1 + r ) N   \\n(10)\\nwhere\\n \\nA = the annuity amount\\n \\nr = the interest rate per period corresponding to the frequency of annuity \\npayments (for example, annual, quarterly, or monthly)\\n \\nN = the number of annuity payments\\nBecause the annuity payment (A) is a constant in this equation, it can be factored out \\nas a common term. Thus the sum of the interest factors has a shortcut expression:\\n PV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n(11)\\nIn much the same way that we computed the future value of an ordinary annuity, we \\nfind the present value by multiplying the annuity amount by a present value annuity \\nfactor (the term in brackets in Equation 11).\\n9\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n22\\nEXAMPLE 11\\nThe Present Value of an Ordinary Annuity\\n1. Suppose you are considering purchasing a financial asset that promises to \\npay €1,000 per year for five years, with the first payment one year from now. \\nThe required rate of return is 12 percent per year. How much should you pay \\nfor this asset?\\nSolution:\\nTo find the value of the financial asset, use the formula for the present value \\nof an ordinary annuity given in Equation 11 with the following data:\\nA = €1,000\\nr = 12% = 0.12\\nN = 5\\nPV =  A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]   \\n = €1,000  [ \\n1 −  \\n1 \\n_ \\n ( 1.12 ) 5    \\n_ \\n0.12 \\n ]   \\n = €1,000(3.604776)\\n = €3,604.78\\nThe series of cash flows of €1,000 per year for five years is currently worth \\n€3,604.78 when discounted at 12 percent.\\nKeeping track of the actual calendar time brings us to a specific type of annuity \\nwith level payments: the annuity due. An annuity due has its first payment occurring \\ntoday (t = 0). In total, the annuity due will make N payments. Exhibit 8 presents the \\ntime line for an annuity due that makes four payments of $100.\\nExhibit 8: An Annuity Due of $100 per Period\\n|\\n|\\n|\\n|\\n0\\n1\\n2\\n3\\n$100\\n$100\\n$100\\n$100\\nAs Exhibit 8 shows, we can view the four-period annuity due as the sum of two parts: \\na $100 lump sum today and an ordinary annuity of $100 per period for three periods. \\nAt a 12 percent discount rate, the four $100 cash flows in this annuity due example \\nwill be worth $340.18.7\\nExpressing the value of the future series of cash flows in today’s dollars gives us a \\nconvenient way of comparing annuities. The next example illustrates this approach.\\n7 There is an alternative way to calculate the present value of an annuity due. Compared to an ordinary \\nannuity, the payments in an annuity due are each discounted one less period. Therefore, we can modify \\nEquation 11 to handle annuities due by multiplying the right-hand side of the equation by (1 + r):\\n PV  ( Annuity\\xa0due )  = A  {  [ 1 −  ( 1 + r ) −N ]  / r }   ( 1 + r )   \\n© CFA Institute. For candidate use only. Not for distribution.\\nPresent Value of a Series of Equal and Unequal Cash Flows\\n23\\nEXAMPLE 12\\nAn Annuity Due as the Present Value of an Immediate \\nCash Flow Plus an Ordinary Annuity\\n1. You are retiring today and must choose to take your retirement benefits \\neither as a lump sum or as an annuity. Your company’s benefits officer pres-\\nents you with two alternatives: an immediate lump sum of $2 million or an \\nannuity with 20 payments of $200,000 a year with the first payment starting \\ntoday. The interest rate at your bank is 7 percent per year compounded an-\\nnually. Which option has the greater present value? (Ignore any tax differ-\\nences between the two options.)\\nSolution:\\nTo compare the two options, find the present value of each at time t = 0 and \\nchoose the one with the larger value. The first option’s present value is $2 \\nmillion, already expressed in today’s dollars. The second option is an annuity \\ndue. Because the first payment occurs at t = 0, you can separate the annuity \\nbenefits into two pieces: an immediate $200,000 to be paid today (t = 0) and \\nan ordinary annuity of $200,000 per year for 19 years. To value this option, \\nyou need to find the present value of the ordinary annuity using Equation 11 \\nand then add $200,000 to it.\\n \\nA = $200,000\\n  \\n \\nN = 19\\n  \\nr = 7% = 0.07\\n  \\n \\nPV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n  \\n \\n= $200,000  [ \\n1 −  \\n1 \\n_ \\n ( 1.07 ) 19    \\n_ \\n0.07 \\n ]  \\n  \\n \\n \\n= $200,000  ( 10.335595 )  \\n  \\n \\n= $2,067,119.05\\n \\n \\nThe 19 payments of $200,000 have a present value of $2,067,119.05. Adding \\nthe initial payment of $200,000 to $2,067,119.05, we find that the total value \\nof the annuity option is $2,267,119.05. The present value of the annuity is \\ngreater than the lump sum alternative of $2 million.\\nWe now look at another example reiterating the equivalence of present and future \\nvalues.\\nEXAMPLE 13\\nThe Projected Present Value of an Ordinary Annuity\\n1. A German pension fund manager anticipates that benefits of €1 million per \\nyear must be paid to retirees. Retirements will not occur until 10 years from \\nnow at time t = 10. Once benefits begin to be paid, they will extend until \\nt = 39 for a total of 30 payments. What is the present value of the pension \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n24\\nliability if the appropriate annual discount rate for plan liabilities is 5 percent \\ncompounded annually?\\nSolution:\\nThis problem involves an annuity with the first payment at t = 10. From the \\nperspective of t = 9, we have an ordinary annuity with 30 payments. We can \\ncompute the present value of this annuity with Equation 11 and then look at \\nit on a time line.\\nA = €1,000,000\\nr = 5% = 0.05\\nN = 30\\nPV =  A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]   \\n = €1,000,000  [ \\n1 −  \\n1 \\n_ \\n ( 1.05 ) 30    \\n_ \\n0.05 \\n ]   \\n = €1,000,000(15.372451)\\n = €15,372,451.03\\n \\nExhibit 9: The Present Value of an Ordinary Annuity with First \\nPayment at Time t = 10 (in Millions) \\n \\n0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n10\\n11           12 . . . . . . . . . . . . . . . . . . . . 39\\n1 . . . . . . . . . . . . . . . . . . \\n1\\n1\\n1\\nOn the time line, we have shown the pension payments of €1 million ex-\\ntending from t = 10 to t = 39. The bracket and arrow indicate the process of \\nfinding the present value of the annuity, discounted back to t = 9. The pres-\\nent value of the pension benefits as of t = 9 is €15,372,451.03. The problem is \\nto find the present value today (at t = 0).\\nNow we can rely on the equivalence of present value and future value. As \\nExhibit 9 shows, we can view the amount at t = 9 as a future value from the \\nvantage point of t = 0. We compute the present value of the amount at t = 9 \\nas follows:\\nFVN = €15,372,451.03 (the present value at t = 9)\\nN = 9\\nr = 5% = 0.05\\nPV = FVN(1 + r)–N\\n = €15,372,451.03(1.05)–9\\n = €15,372,451.03(0.644609)\\n = €9,909,219.00\\nThe present value of the pension liability is €9,909,219.00.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPresent Value of a Series of Equal and Unequal Cash Flows\\n25\\nExample 13 illustrates three procedures emphasized in this reading:\\n■ \\nfinding the present or future value of any cash flow series;\\n■ \\nrecognizing the equivalence of present value and appropriately discounted \\nfuture value; and\\n■ \\nkeeping track of the actual calendar time in a problem involving the time \\nvalue of money.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'The Present Value of a Series of Unequal Cash Flows',\n",
       "       'page_number': 35,\n",
       "       'content': 'The Present Value of a Series of Unequal Cash Flows\\nWhen we have unequal cash flows, we must first find the present value of each indi-\\nvidual cash flow and then sum the respective present values. For a series with many \\ncash flows, we usually use a spreadsheet. Exhibit 10 lists a series of cash flows with \\nthe time periods in the first column, cash flows in the second column, and each cash \\nflow’s present value in the third column. The last row of Exhibit 10 shows the sum of \\nthe five present values.\\nExhibit 10: A Series of Unequal Cash Flows and Their \\nPresent Values at 5 Percent\\nTime Period\\nCash Flow ($)\\nPresent Value at Year 0\\n1\\n1,000\\n$1,000(1.05)−1\\n=\\n$952.38\\n2\\n2,000\\n$2,000(1.05)−2\\n=\\n$1,814.06\\n3\\n4,000\\n$4,000(1.05)−3\\n=\\n$3,455.35\\n4\\n5,000\\n$5,000(1.05)−4\\n=\\n$4,113.51\\n5\\n6,000\\n$6,000(1.05)−5\\n=\\n$4,701.16\\n\\xa0\\n\\xa0\\nSum\\n=\\n$15,036.46\\nWe could calculate the future value of these cash flows by computing them one at a \\ntime using the single-payment future value formula. We already know the present value \\nof this series, however, so we can easily apply time-value equivalence. The future value \\nof the series of cash flows from Table 2, $19,190.76, is equal to the single $15,036.46 \\namount compounded forward to t = 5:\\n \\nPV = $15,036.46\\n  \\n \\nN = 5\\n  \\nr = 5 %  = 0.05\\n  \\n \\n FV N = PV  ( 1 + r ) N   \\n \\n= $15,036.46  ( 1.05 ) 5 \\n  \\n \\n= $15,036.46  ( 1.276282 )  \\n  \\n \\n \\n= $19,190.76\\n \\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n26\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Present Value of a Perpetuity',\n",
       "     'page_number': 36,\n",
       "     'content': 'PRESENT VALUE OF A PERPETUITY\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\nConsider the case of an ordinary annuity that extends indefinitely. Such an ordinary \\nannuity is called a perpetuity (a perpetual annuity). To derive a formula for the present \\nvalue of a perpetuity, we can modify Equation 10 to account for an infinite series of \\ncash flows:\\n PV = A ∑ \\nt=1\\n  \\n∞   [ \\n1 \\n_ \\n ( 1 + r ) t   ]  \\n(12)\\nAs long as interest rates are positive, the sum of present value factors converges and\\n PV =  A \\n_ \\nr  \\n(13)\\nTo see this, look back at Equation 11, the expression for the present value of an ordi-\\nnary annuity. As N (the number of periods in the annuity) goes to infinity, the term \\n1/(1 + r)N approaches 0 and Equation 11 simplifies to Equation 13. This equation will \\nreappear when we value dividends from stocks because stocks have no predefined \\nlife span. (A stock paying constant dividends is similar to a perpetuity.) With the first \\npayment a year from now, a perpetuity of $10 per year with a 20 percent required \\nrate of return has a present value of $10/0.2 = $50.\\nEquation 13 is valid only for a perpetuity with level payments. In our development \\nabove, the first payment occurred at t = 1; therefore, we compute the present value \\nas of t = 0.\\nOther assets also come close to satisfying the assumptions of a perpetuity. Certain \\ngovernment bonds and preferred stocks are typical examples of financial assets that \\nmake level payments for an indefinite period of time.\\nEXAMPLE 14\\nThe Present Value of a Perpetuity\\n1. The British government once issued a type of security called a consol bond, \\nwhich promised to pay a level cash flow indefinitely. If a consol bond paid \\n£100 per year in perpetuity, what would it be worth today if the required \\nrate of return were 5 percent?\\nSolution:\\nTo answer this question, we can use Equation 13 with the following data:\\n \\nA = £100\\n  \\nr = 5 %  = 0.05\\n  \\n \\nPV = A\\u200a/\\u200ar  \\n= £100\\u200a/\\u200a0.05\\n  \\n \\n= £2, 000\\n \\n \\nThe bond would be worth £2,000.\\n10\\n© CFA Institute. For candidate use only. Not for distribution.\\nPresent Value of a Perpetuity\\n27\\n',\n",
       "     'children': [{'title': 'Present Values Indexed at Times Other than t = 0',\n",
       "       'page_number': 37,\n",
       "       'content': 'Present Values Indexed at Times Other than t = 0\\nIn practice with investments, analysts frequently need to find present values indexed \\nat times other than t = 0. Subscripting the present value and evaluating a perpetuity \\nbeginning with $100 payments in Year 2, we find PV1 = $100/0.05 = $2,000 at a 5 percent \\ndiscount rate. Further, we can calculate today’s PV as PV0 = $2,000/1.05 = $1,904.76.\\nConsider a similar situation in which cash flows of $6 per year begin at the end \\nof the 4th year and continue at the end of each year thereafter, with the last cash \\nflow at the end of the 10th year. From the perspective of the end of the third year, \\nwe are facing a typical seven-year ordinary annuity. We can find the present value of \\nthe annuity from the perspective of the end of the third year and then discount that \\npresent value back to the present. At an interest rate of 5 percent, the cash flows of \\n$6 per year starting at the end of the fourth year will be worth $34.72 at the end of \\nthe third year (t = 3) and $29.99 today (t = 0).\\nThe next example illustrates the important concept that an annuity or perpetuity \\nbeginning sometime in the future can be expressed in present value terms one period \\nprior to the first payment. That present value can then be discounted back to today’s \\npresent value.\\nEXAMPLE 15\\nThe Present Value of a Projected Perpetuity\\n1. Consider a level perpetuity of £100 per year with its first payment beginning \\nat t = 5. What is its present value today (at t = 0), given a 5 percent discount \\nrate?\\nSolution:\\nFirst, we find the present value of the perpetuity at t = 4 and then discount \\nthat amount back to t = 0. (Recall that a perpetuity or an ordinary annuity \\nhas its first payment one period away, explaining the t = 4 index for our \\npresent value calculation.)\\ni. \\nFind the present value of the perpetuity at t = 4:\\n \\nA = £100\\n  \\nr = 5 %  = 0.05\\n  \\n \\nPV = A / r  \\n= £100 / 0.05\\n  \\n= £2, 000\\n \\n \\nii. \\nFind the present value of the future amount at t = 4. From the \\nperspective of t = 0, the present value of £2,000 can be considered a \\nfuture value. Now we need to find the present value of a lump sum:\\n \\n FV N = £2, 000\\xa0(the\\xa0present\\xa0value\\xa0at\\xa0t = 4)\\n  \\n \\n \\n \\nr = 5 %  = 0.05\\n  \\n \\nN = 4\\n  \\nPV =  FV N  ( 1 + r ) −N   \\n \\n= £2, 000  ( 1.05 ) −4 \\n  \\n \\n= £2, 000   ( 0.822702 )  \\n  \\n \\n= £1, 645.40\\n \\n \\nToday’s present value of the perpetuity is £1,645.40.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n28\\nAs discussed earlier, an annuity is a series of payments of a fixed amount for a \\nspecified number of periods. Suppose we own a perpetuity. At the same time, we \\nissue a perpetuity obligating us to make payments; these payments are the same size \\nas those of the perpetuity we own. However, the first payment of the perpetuity we \\nissue is at t = 5; payments then continue on forever. The payments on this second \\nperpetuity exactly offset the payments received from the perpetuity we own at t = 5 \\nand all subsequent dates. We are left with level nonzero net cash flows at t = 1, 2, 3, \\nand 4. This outcome exactly fits the definition of an annuity with four payments. Thus \\nwe can construct an annuity as the difference between two perpetuities with equal, \\nlevel payments but differing starting dates. The next example illustrates this result.\\nEXAMPLE 16\\nThe Present Value of an Ordinary Annuity as the Present \\nValue of a Current Minus Projected Perpetuity\\n1. Given a 5 percent discount rate, find the present value of a four-year ordi-\\nnary annuity of £100 per year starting in Year 1 as the difference between \\nthe following two level perpetuities:\\n \\nPerpetuity 1\\n£100 per year starting in Year 1 (first payment at t = 1)\\nPerpetuity 2\\n£100 per year starting in Year 5 (first payment at t = 5)\\n \\nSolution:\\nIf we subtract Perpetuity 2 from Perpetuity 1, we are left with an ordinary \\nannuity of £100 per period for four years (payments at t = 1, 2, 3, 4). Sub-\\ntracting the present value of Perpetuity 2 from that of Perpetuity 1, we arrive \\nat the present value of the four-year ordinary annuity:\\n \\n PV 0  ( Perpetuity 1 )  = £100\\u200a/\\u200a0.05 = £2, 000\\n  \\n \\n \\n \\n PV 4  ( Perpetuity 2 )  = £100\\u200a/\\u200a0.05 = £2, 000\\n  \\n \\n \\n \\n PV 0  ( Perpetuity 2 )  = £2, 000\\u200a/\\u200a ( 1.05 ) 4 = £1, 645.40  \\n \\n \\n \\n \\n PV 0  ( Annuity )  =  PV 0  ( Perpetuity 1 )  −  PV 0  ( Perpetuity 2 )  \\n  \\n \\n \\n \\n \\n= £2, 000 − £1, 645.40\\n  \\n \\n= £354.60\\n \\n \\nThe four-year ordinary annuity’s present value is equal to £2,000 – £1,645.40 \\n= £354.60.\\nSOLVING FOR INTEREST RATES, GROWTH RATES, AND \\nNUMBER OF PERIODS\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\n11\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Solving for Interest Rates, Growth Rates, and Number of Periods',\n",
       "     'page_number': 38,\n",
       "     'content': 'Solving for Interest Rates, Growth Rates, and Number of Periods\\n29\\nIn the previous examples, certain pieces of information have been made available. \\nFor instance, all problems have given the rate of interest, r, the number of time peri-\\nods, N, the annuity amount, A, and either the present value, PV, or future value, FV. \\nIn real-world applications, however, although the present and future values may be \\ngiven, you may have to solve for either the interest rate, the number of periods, or \\nthe annuity amount. In the subsections that follow, we show these types of problems.\\n',\n",
       "     'children': [{'title': 'Solving for Interest Rates and Growth Rates',\n",
       "       'page_number': 39,\n",
       "       'content': 'Solving for Interest Rates and Growth Rates\\nSuppose a bank deposit of €100 is known to generate a payoff of €111 in one year. \\nWith this information, we can infer the interest rate that separates the present value \\nof €100 from the future value of €111 by using Equation 2, FVN = PV(1 + r)N, with N \\n= 1. With PV, FV, and N known, we can solve for r directly:\\n 1 + r = FV/PV\\n 1 + r = €111/€100 = 1.11\\n \\nr = 0.11, or 11%\\nThe interest rate that equates €100 at t = 0 to €111 at t = 1 is 11 percent. Thus we can \\nstate that €100 grows to €111 with a growth rate of 11 percent.\\nAs this example shows, an interest rate can also be considered a growth rate. The \\nparticular application will usually dictate whether we use the term “interest rate” or \\n“growth rate.” Solving Equation 2 for r and replacing the interest rate r with the growth \\nrate g produces the following expression for determining growth rates:\\n g = (FVN/PV)1/N – 1   \\n(14)\\nBelow are two examples that use the concept of a growth rate.\\nEXAMPLE 17\\nCalculating a Growth Rate (1)\\nHyundai Steel, the first Korean steelmaker, was established in 1953. Hyundai \\nSteel’s sales increased from ₩14,146.4 billion in 2012 to ₩19,166.0 billion in \\n2017. However, its net profit declined from ₩796.4 billion in 2012 to ₩727.5 \\nbillion in 2017. Calculate the following growth rates for Hyundai Steel for the \\nfive-year period from the end of 2012 to the end of 2017:\\n1. Sales growth rate.\\nSolution to 1:\\nTo solve this problem, we can use Equation 14, g = (FVN/PV)1/N – 1. We \\ndenote sales in 2012 as PV and sales in 2017 as FV5. We can then solve for \\nthe growth rate as follows:\\n \\ng =  \\n5 √ \\n____________________ \\n \\n₩19, 166.0\\u200a/\\u200a₩14, 146.4  − 1\\n  \\n \\n \\n=  \\n5 √ _ \\n1.354832  − 1  \\n \\n= 1.062618 − 1\\n  \\n \\n= 0.062618 or about 6.3%\\n \\n \\nThe calculated growth rate of about 6.3 percent a year shows that Hyundai \\nSteel’s sales grew during the 2012–2017 period.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n30\\n2. Net profit growth rate.\\nSolution to 2:\\nIn this case, we can speak of a positive compound rate of decrease or a nega-\\ntive compound growth rate. Using Equation 14, we find\\n \\ng =  \\n5 √ \\n______________ \\n \\n₩727.5\\u200a/\\u200a₩796.4  − 1\\n  \\n \\n \\n=  \\n5 √ _ \\n0.913486  − 1  \\n \\n= 0.982065 − 1\\n  \\n \\n= −\\u200a0.017935 or about − \\u200a1.8%\\n  \\nIn contrast to the positive sales growth, the rate of growth in net profit was \\napproximately –1.8 percent during the 2012–2017 period.\\nEXAMPLE 18\\nCalculating a Growth Rate (2)\\n1. Toyota Motor Corporation, one of the largest automakers in the world, had \\nconsolidated vehicle sales of 8.96 million units in 2018 (fiscal year ending \\n31 March 2018). This is substantially more than consolidated vehicle sales \\nof 7.35 million units six years earlier in 2012. What was the growth rate in \\nnumber of vehicles sold by Toyota from 2012 to 2018?\\nSolution:\\nUsing Equation 14, we find\\n \\ng =  \\n6 √ _ \\n8.96\\u200a/\\u200a7.35  − 1\\n  \\n \\n=  \\n6 √ _ \\n1.219048  − 1  \\n \\n= 1.033563 − 1\\n  \\n \\n= 0.033563 or about \\u200a3.4%\\n  \\nThe rate of growth in vehicles sold was approximately 3.4 percent during the \\n2012–2018 period. Note that we can also refer to 3.4 percent as the com-\\npound annual growth rate because it is the single number that compounds \\nthe number of vehicles sold in 2012 forward to the number of vehicles sold \\nin 2018. Exhibit 11 lists the number of vehicles sold by Toyota from 2012 to \\n2018.\\n \\nExhibit 11: Number of Vehicles Sold, 2012–2018\\n \\n \\nYear\\nNumber of Vehicles Sold \\n(Millions)\\n(1 + g)t\\nt\\n2012\\n7.35\\n\\xa0\\n0\\n2013\\n8.87\\n8.87/7.35 = 1.206803\\n1\\n2014\\n9.12\\n9.12/8.87 = 1.028185\\n2\\n2015\\n8.97\\n8.97/9.12 = 0.983553\\n3\\n2016\\n8.68\\n8.68/8.97 = 0.967670\\n4\\n2017\\n8.97\\n8.97/8.68 = 1.033410\\n5\\n2018\\n8.96\\n8.96/8.97 = 0.998885\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolving for Interest Rates, Growth Rates, and Number of Periods\\n31\\n \\nSource: www.toyota.com.\\nExhibit 11 also shows 1 plus the one-year growth rate in number of vehicles \\nsold. We can compute the 1 plus six-year cumulative growth in number of \\nvehicles sold from 2012 to 2018 as the product of quantities (1 + one-year \\ngrowth rate). We arrive at the same result as when we divide the ending \\nnumber of vehicles sold, 8.96 million, by the beginning number of vehicles \\nsold, 7.35 million:\\n \\n 8.96 \\n_ \\n7.35  =   ( 8.87 \\n_ \\n7.35  )   ( 9.12 \\n_ \\n8.87  )   ( 8.97 \\n_ \\n9.12  )   ( 8.68 \\n_ \\n8.97  )   ( 8.97 \\n_ \\n8.68  )   ( 8.96 \\n_ \\n8.97  )  \\n  \\n \\n \\n \\n \\n=   ( 1 +  g 1 )   ( 1 +  g 2 )   ( 1 +  g 3 )   ( 1 +  g 4 )   ( 1 +  g 5 )   ( 1 +  g 6 )    \\n \\n \\n \\n \\n1.219048 =   ( 1.206803 )   ( 1.028185 )   ( 0.983553 )   ( 0.967670 )   ( 1.033410 )   ( 0.998885 )   \\n \\nThe right-hand side of the equation is the product of 1 plus the one-year \\ngrowth rate in number of vehicles sold for each year. Recall that, using Equa-\\ntion 14, we took the sixth root of 8.96/7.35 = 1.219048. In effect, we were \\nsolving for the single value of g which, when compounded over six periods, \\ngives the correct product of 1 plus the one-year growth rates.8\\nIn conclusion, we do not need to compute intermediate growth rates as \\nin Exhibit 11 to solve for a compound growth rate g. Sometimes, however, \\nthe intermediate growth rates are interesting or informative. For example, \\nmost of the 21.9 percent increase in vehicles sold by Toyota from 2012 to \\n2018 occurred in 2013 as sales increased by 20.7 percent from 2012 to 2013. \\nElsewhere in Toyota Motor’s disclosures, the company noted that all regions \\nexcept Europe showed a substantial increase in sales in 2013. We can also \\nanalyze the variability in growth rates when we conduct an analysis as in \\nExhibit 11. Sales continued to increase in 2014 but then declined in 2015 \\nand 2016. Sales then increased but the sales in 2017 and 2018 are about the \\nsame as in 2015. \\nThe compound growth rate is an excellent summary measure of growth over \\nmultiple time periods. In our Toyota Motors example, the compound growth rate \\nof 3.4 percent is the single growth rate that, when added to 1, compounded over six \\nyears, and multiplied by the 2012 number of vehicles sold, yields the 2018 number \\nof vehicles sold.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Solving for the Number of Periods',\n",
       "       'page_number': 41,\n",
       "       'content': 'Solving for the Number of Periods\\nIn this section, we demonstrate how to solve for the number of periods given present \\nvalue, future value, and interest or growth rates.\\n8 The compound growth rate that we calculate here is an example of a geometric mean, specifically the \\ngeometric mean of the growth rates. We define the geometric mean in the reading on statistical concepts.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n32\\nEXAMPLE 19\\nThe Number of Annual Compounding Periods Needed for \\nan Investment to Reach a Specific Value\\n1. You are interested in determining how long it will take an investment of \\n€10,000,000 to double in value. The current interest rate is 7 percent com-\\npounded annually. How many years will it take €10,000,000 to double to \\n€20,000,000?\\nSolution:\\nUse Equation 2, FVN = PV(1 + r)N, to solve for the number of periods, N, as \\nfollows:\\n \\n ( 1 + r ) N =  FV N \\u200a/\\u200aPV = 2\\n  \\n \\n \\nN ln  ( 1 + r )  = ln  ( 2 )    \\n \\nN = ln  ( 2 )  \\u200a/\\u200aln  ( 1 + r )    \\n \\n= ln  ( 2 )  \\u200a/\\u200aln  ( 1.07 )  = 10.24\\n  \\nWith an interest rate of 7 percent, it will take approximately 10 years for the \\ninitial €10,000,000 investment to grow to €20,000,000. Solving for N in the \\nexpression (1.07)N = 2.0 requires taking the natural logarithm of both sides \\nand using the rule that ln(xN) = N ln(x). Generally, we find that N = [ln(FV/\\nPV)]/ln(1 + r). Here, N = ln(€20,000,000/€10,000,000)/ln(1.07) = ln(2)/\\nln(1.07) = 10.24.9\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Solving for Size of Annuity Payments',\n",
       "     'page_number': 42,\n",
       "     'content': 'SOLVING FOR SIZE OF ANNUITY PAYMENTS\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nIn this section, we discuss how to solve for annuity payments. Mortgages, auto loans, \\nand retirement savings plans are classic examples of applications of annuity formulas.\\n9 To quickly approximate the number of periods, practitioners sometimes use an ad hoc rule called the \\nRule of 72: Divide 72 by the stated interest rate to get the approximate number of years it would take to \\ndouble an investment at the interest rate. Here, the approximation gives 72/7 = 10.3 years. The Rule of 72 \\nis loosely based on the observation that it takes 12 years to double an amount at a 6 percent interest rate, \\ngiving 6 × 12 = 72. At a 3 percent rate, one would guess it would take twice as many years, 3 × 24 = 72.\\n12\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolving for Size of Annuity Payments\\n33\\nEXAMPLE 20\\nCalculating the Size of Payments on a Fixed-Rate \\nMortgage\\n1. You are planning to purchase a $120,000 house by making a down payment \\nof $20,000 and borrowing the remainder with a 30-year fixed-rate mortgage \\nwith monthly payments. The first payment is due at t = 1. Current mortgage \\ninterest rates are quoted at 8 percent with monthly compounding. What will \\nyour monthly mortgage payments be?\\nSolution:\\nThe bank will determine the mortgage payments such that at the stated \\nperiodic interest rate, the present value of the payments will be equal to the \\namount borrowed (in this case, $100,000). With this fact in mind, we can \\nuse Equation 11,  PV = A  \\n[\\n \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n \\n]\\n  , to solve for the annuity amount, A, \\nas the present value divided by the present value annuity factor:\\n \\nPV = $100, 000\\n  \\n \\n r s = 8 %  = 0.08\\n  \\n \\nm = 12\\n  \\n r s \\u200a/\\u200am = 0.08\\u200a/\\u200a12 = 0.006667\\n  \\n \\n \\nN = 30\\n  \\nmN = 12 × 30 = 360\\n  \\n \\nPresent value annuity factor =  \\n1 −  \\n1 \\n____________ \\n \\n [ 1 +   ( r s \\u200a/\\u200am )  ] \\nmN \\n   \\n \\n_____________ \\n r s \\u200a/\\u200am \\n =  \\n1 −  \\n1 \\n_ \\n ( 1.006667 ) 360    \\n \\n____________ \\n0.006667 \\n \\n  \\n \\n \\n \\n \\n \\n= 136.283494\\n  \\n \\nA = PV\\u200a/\\u200aPresent value annuity factor\\n  \\n \\n \\n \\n= $100, 000\\u200a/\\u200a136.283494\\n  \\n \\n= $733.76\\n \\n \\nThe amount borrowed, $100,000, is equivalent to 360 monthly payments of \\n$733.76 with a stated interest rate of 8 percent. The mortgage problem is a \\nrelatively straightforward application of finding a level annuity payment.\\nNext, we turn to a retirement-planning problem. This problem illustrates the \\ncomplexity of the situation in which an individual wants to retire with a specified \\nretirement income. Over the course of a life cycle, the individual may be able to save \\nonly a small amount during the early years but then may have the financial resources \\nto save more during later years. Savings plans often involve uneven cash flows, a topic \\nwe will examine in the last part of this reading. When dealing with uneven cash flows, \\nwe take maximum advantage of the principle that dollar amounts indexed at the same \\npoint in time are additive—the cash flow additivity principle.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n34\\nEXAMPLE 21\\nThe Projected Annuity Amount Needed to Fund a \\nFuture-Annuity Inflow\\n1. Jill Grant is 22 years old (at t = 0) and is planning for her retirement at age \\n63 (at t = 41). She plans to save $2,000 per year for the next 15 years (t = 1 \\nto t = 15). She wants to have retirement income of $100,000 per year for 20 \\nyears, with the first retirement payment starting at t = 41. How much must \\nGrant save each year from t = 16 to t = 40 in order to achieve her retirement \\ngoal? Assume she plans to invest in a diversified stock-and-bond mutual \\nfund that will earn 8 percent per year on average.\\nSolution:\\nTo help solve this problem, we set up the information on a time line. As \\nExhibit 12 shows, Grant will save $2,000 (an outflow) each year for Years \\n1 to 15. Starting in Year 41, Grant will start to draw retirement income of \\n$100,000 per year for 20 years. In the time line, the annual savings is record-\\ned in parentheses ($2) to show that it is an outflow. The problem is to find \\nthe savings, recorded as X, from Year 16 to Year 40.\\n \\nExhibit 12: Solving for Missing Annuity Payments (in Thousands) \\n \\n |\\n |\\n |       |\\n |\\n |    \\n   |\\n |  \\n  |    \\n    |\\n 0\\n 1\\n 2\\n15\\n16\\n17\\n40\\n41\\n42 \\n60\\n    ($2)      ($2)  ... ($2)      (X)       (X)  ...  (X)\\n$100\\n$100 ... $100\\n...\\n...\\n...\\nSolving this problem involves satisfying the following relationship: the pres-\\nent value of savings (outflows) equals the present value of retirement income \\n(inflows). We could bring all the dollar amounts to t = 40 or to t = 15 and \\nsolve for X.\\nLet us evaluate all dollar amounts at t = 15 (we encourage the reader to \\nrepeat the problem by bringing all cash flows to t = 40). As of t = 15, the \\nfirst payment of X will be one period away (at t = 16). Thus we can value the \\nstream of Xs using the formula for the present value of an ordinary annuity.\\nThis problem involves three series of level cash flows. The basic idea is that \\nthe present value of the retirement income must equal the present value of \\nGrant’s savings. Our strategy requires the following steps:\\n1. Find the future value of the savings of $2,000 per year and index it at t \\n= 15. This value tells us how much Grant will have saved.\\n2. Find the present value of the retirement income at t = 15. This value \\ntells us how much Grant needs to meet her retirement goals (as of t \\n= 15). Two substeps are necessary. First, calculate the present value \\nof the annuity of $100,000 per year at t = 40. Use the formula for the \\npresent value of an annuity. (Note that the present value is indexed at t \\n= 40 because the first payment is at t = 41.) Next, discount the present \\nvalue back to t = 15 (a total of 25 periods).\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolving for Size of Annuity Payments\\n35\\n3. Now compute the difference between the amount Grant has saved \\n(Step 1) and the amount she needs to meet her retirement goals (Step \\n2). Her savings from t = 16 to t = 40 must have a present value equal to \\nthe difference between the future value of her savings and the present \\nvalue of her retirement income.\\nOur goal is to determine the amount Grant should save in each of the 25 \\nyears from t = 16 to t = 40. We start by bringing the $2,000 savings to t = 15, \\nas follows:\\n \\nA = $2, 000\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 15\\n  \\nFV = A  [  ( 1 + r ) N − 1 \\n_ \\nr \\n ]    \\n \\n= $2, 000  [  ( 1.08 ) 15 − 1 \\n_ \\n0.08 \\n ]  \\n  \\n \\n= $2, 000  ( 27.152114 )  \\n  \\n \\n= $54, 304.23\\n \\n \\nAt t = 15, Grant’s initial savings will have grown to $54,304.23.\\nNow we need to know the value of Grant’s retirement income at t = 15. As \\nstated earlier, computing the retirement present value requires two sub-\\nsteps. First, find the present value at t = 40 with the formula in Equation \\n11; second, discount this present value back to t = 15. Now we can find the \\nretirement income present value at t = 40:\\n \\nA = $100, 000\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 20\\n  \\nPV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n  \\n \\n= $100, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1.08 ) 20    \\n_ \\n0.08 \\n ]  \\n  \\n \\n \\n= $100, 000  ( 9.818147 )  \\n  \\n \\n= $981, 814.74\\n \\n \\nThe present value amount is as of t = 40, so we must now discount it back as \\na lump sum to t = 15:\\n \\n FV N = $981, 814.74\\n  \\n \\nN = 25\\n  \\nr = 8 %  = 0.08\\n  \\n \\nPV =  FV N  ( 1 + r ) −N   \\n \\n= $981, 814.74  ( 1.08 ) −25 \\n  \\n \\n= $981, 814.74  ( 0.146018 )  \\n  \\n \\n \\n= $143, 362.53\\n \\n \\nNow recall that Grant will have saved $54,304.23 by t = 15. Therefore, \\nin present value terms, the annuity from t = 16 to t = 40 must equal the \\ndifference between the amount already saved ($54,304.23) and the amount \\nrequired for retirement ($143,362.53). This amount is equal to $143,362.53 − \\n$54,304.23 = $89,058.30. Therefore, we must now find the annuity payment, \\nA, from t = 16 to t = 40 that has a present value of $89,058.30. We find the \\nannuity payment as follows:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n36\\n \\nPV = $89, 058.30\\n  \\n \\nr = 8 %  = 0.08\\n  \\n \\nN = 25\\n  \\nPresent value annuity factor =   [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n  \\n \\n \\n \\n=   [ \\n1 −  \\n1 \\n_ \\n ( 1.08 ) 25    \\n_ \\n0.08 \\n ]  \\n  \\n \\n= 10.674776\\n  \\n \\nA = PV\\u200a/\\u200aPresent value annuity factor\\n  \\n \\n \\n \\n= $89, 058.30\\u200a/\\u200a10.674776\\n  \\n \\n \\n= $8, 342.87\\n \\n \\nGrant will need to increase her savings to $8,342.87 per year from t = 16 to t \\n= 40 to meet her retirement goal of having a fund equal to $981,814.74 after \\nmaking her last payment at t = 40.\\nPRESENT AND FUTURE VALUE EQUIVALENCE AND \\nTHE ADDITIVITY PRINCIPLE\\ncalculate and interpret the future value (FV) and present value (PV) \\nof a single sum of money, an ordinary annuity, an annuity due, a \\nperpetuity (PV only), and a series of unequal cash flows\\ndemonstrate the use of a time line in modeling and solving time \\nvalue of money problems\\nAs we have demonstrated, finding present and future values involves moving amounts \\nof money to different points on a time line. These operations are possible because \\npresent value and future value are equivalent measures separated in time. Exhibit 13 \\nillustrates this equivalence; it lists the timing of five cash flows, their present values \\nat t = 0, and their future values at t = 5.\\nTo interpret Exhibit 13, start with the third column, which shows the present val-\\nues. Note that each $1,000 cash payment is discounted back the appropriate number \\nof periods to find the present value at t = 0. The present value of $4,329.48 is exactly \\nequivalent to the series of cash flows. This information illustrates an important point: \\nA lump sum can actually generate an annuity. If we place a lump sum in an account \\nthat earns the stated interest rate for all periods, we can generate an annuity that is \\nequivalent to the lump sum. Amortized loans, such as mortgages and car loans, are \\nexamples of this principle.\\nExhibit 13: The Equivalence of Present and Future Values\\nTime\\nCash Flow ($)\\nPresent Value at t = 0\\n\\xa0\\nFuture Value at t = 5\\n1\\n1,000\\n$1,000(1.05)−1\\n=\\n$952.38\\n\\xa0\\n$1,000(1.05)4\\n=\\n$1,215.51\\n2\\n1,000\\n$1,000(1.05)−2\\n=\\n$907.03\\n\\xa0\\n$1,000(1.05)3\\n=\\n$1,157.63\\n3\\n1,000\\n$1,000(1.05)−3\\n=\\n$863.84\\n\\xa0\\n$1,000(1.05)2\\n=\\n$1,102.50\\n13\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Present and Future Value Equivalence and the Additivity Principle',\n",
       "     'page_number': 46,\n",
       "     'content': 'Present and Future Value Equivalence and the Additivity Principle\\n37\\nTime\\nCash Flow ($)\\nPresent Value at t = 0\\n\\xa0\\nFuture Value at t = 5\\n4\\n1,000\\n$1,000(1.05)−4\\n=\\n$822.70\\n\\xa0\\n$1,000(1.05)1\\n=\\n$1,050.00\\n5\\n1,000\\n$1,000(1.05)−5\\n=\\n$783.53\\n\\xa0\\n$1,000(1.05)0\\n=\\n$1,000.00\\n\\xa0\\n\\xa0\\nSum:\\n\\xa0\\n$4,329.48\\n\\xa0\\nSum:\\n\\xa0\\n$5,525.64\\nTo see how a lump sum can fund an annuity, assume that we place $4,329.48 in the \\nbank today at 5 percent interest. We can calculate the size of the annuity payments \\nby using Equation 11. Solving for A, we find\\n \\nA =  \\nPV \\n___________ \\n \\n 1 −   [ 1\\u200a/\\u200a ( 1 + r ) N ]   \\n \\n____________ \\nr \\n \\n  \\n  \\n \\n=  $4, 329.48 \\n_ \\n 1 −   [ 1\\u200a/\\u200a ( 1.05 ) 5 ]   \\n \\n____________ \\n0.05 \\n     \\n \\n= $1, 000\\n \\n \\nExhibit 14 shows how the initial investment of $4,329.48 can actually generate five \\n$1,000 withdrawals over the next five years.\\nTo interpret Exhibit 14, start with an initial present value of $4,329.48 at t = 0. \\nFrom t = 0 to t = 1, the initial investment earns 5 percent interest, generating a future \\nvalue of $4,329.48(1.05) = $4,545.95. We then withdraw $1,000 from our account, \\nleaving $4,545.95 − $1,000 = $3,545.95 (the figure reported in the last column for time \\nperiod 1). In the next period, we earn one year’s worth of interest and then make a \\n$1,000 withdrawal. After the fourth withdrawal, we have $952.38, which earns 5 per-\\ncent. This amount then grows to $1,000 during the year, just enough for us to make \\nthe last withdrawal. Thus the initial present value, when invested at 5 percent for five \\nyears, generates the $1,000 five-year ordinary annuity. The present value of the initial \\ninvestment is exactly equivalent to the annuity.\\nNow we can look at how future value relates to annuities. In Exhibit 13, we \\nreported that the future value of the annuity was $5,525.64. We arrived at this figure \\nby compounding the first $1,000 payment forward four periods, the second $1,000 \\nforward three periods, and so on. We then added the five future amounts at t = 5. \\nThe annuity is equivalent to $5,525.64 at t = 5 and $4,329.48 at t = 0. These two dollar \\nmeasures are thus equivalent. We can verify the equivalence by finding the present \\nvalue of $5,525.64, which is $5,525.64 × (1.05)−5 = $4,329.48. We found this result \\nabove when we showed that a lump sum can generate an annuity.\\nExhibit 14: How an Initial Present Value Funds an Annuity\\nTime \\nPeriod\\nAmount Available \\nat the Beginning of \\nthe Time Period ($)\\nEnding Amount before Withdrawal\\nWithdrawal ($)\\nAmount Available \\nafter Withdrawal ($)\\n1\\n4,329.48\\n$4,329.48(1.05)\\n=\\n$4,545.95\\n1,000\\n3,545.95\\n2\\n3,545.95\\n$3,545.95(1.05)\\n=\\n$3,723.25\\n1,000\\n2,723.25\\n3\\n2,723.25\\n$2,723.25(1.05)\\n=\\n$2,859.41\\n1,000\\n1,859.41\\n4\\n1,859.41\\n$1,859.41(1.05)\\n=\\n$1,952.38\\n1,000\\n952.38\\n5\\n952.38\\n$952.38(1.05)\\n=\\n$1,000\\n1,000\\n0\\nTo summarize what we have learned so far: A lump sum can be seen as equivalent to \\nan annuity, and an annuity can be seen as equivalent to its future value. Thus present \\nvalues, future values, and a series of cash flows can all be considered equivalent as \\nlong as they are indexed at the same point in time.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n38\\n',\n",
       "     'children': [{'title': 'The Cash Flow Additivity Principle',\n",
       "       'page_number': 48,\n",
       "       'content': 'The Cash Flow Additivity Principle\\nThe cash flow additivity principle—the idea that amounts of money indexed at the \\nsame point in time are additive—is one of the most important concepts in time value of \\nmoney mathematics. We have already mentioned and used this principle; this section \\nprovides a reference example for it.\\nConsider the two series of cash flows shown on the time line in Exhibit 15. The \\nseries are denoted A and B. If we assume that the annual interest rate is 2 percent, \\nwe can find the future value of each series of cash flows as follows. Series A’s future \\nvalue is $100(1.02) + $100 = $202. Series B’s future value is $200(1.02) + $200 = $404. \\nThe future value of (A + B) is $202 + $404 = $606 by the method we have used up \\nto this point. The alternative way to find the future value is to add the cash flows of \\neach series, A and B (call it A + B), and then find the future value of the combined \\ncash flow, as shown in Exhibit 15.\\nThe third time line in Exhibit 15 shows the combined series of cash flows. Series \\nA has a cash flow of $100 at t = 1, and Series B has a cash flow of $200 at t = 1. The \\ncombined series thus has a cash flow of $300 at t = 1. We can similarly calculate the \\ncash flow of the combined series at t = 2. The future value of the combined series \\n(A + B) is $300(1.02) + $300 = $606—the same result we found when we added the \\nfuture values of each series.\\nThe additivity and equivalence principles also appear in another common situa-\\ntion. Suppose cash flows are $4 at the end of the first year and $24 (actually separate \\npayments of $4 and $20) at the end of the second year. Rather than finding present \\nvalues of the first year’s $4 and the second year’s $24, we can treat this situation as a \\n$4 annuity for two years and a second-year $20 lump sum. If the discount rate were \\n6 percent, the $4 annuity would have a present value of $7.33 and the $20 lump sum \\na present value of $17.80, for a total of $25.13.\\nExhibit 15: The Additivity of Two Series of Cash Flows\\nt = 0\\nt = 1\\nt = 2\\nA\\n$100\\n$100\\nt = 0\\nt = 1\\nt = 2\\nB\\n$200\\n$200\\nt = 0\\nt = 1\\nt = 2\\nA + B\\n$300\\n$300\\n© CFA Institute. For candidate use only. Not for distribution.\\nPresent and Future Value Equivalence and the Additivity Principle\\n39\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 49,\n",
       "     'content': 'SUMMARY\\nIn this reading, we have explored a foundation topic in investment mathematics, the \\ntime value of money. We have developed and reviewed the following concepts for use \\nin financial applications:\\n■ \\nThe interest rate, r, is the required rate of return; r is also called the discount \\nrate or opportunity cost.\\n■ \\nAn interest rate can be viewed as the sum of the real risk-free interest \\nrate and a set of premiums that compensate lenders for risk: an inflation \\npremium, a default risk premium, a liquidity premium, and a maturity \\npremium.\\n■ \\nThe future value, FV, is the present value, PV, times the future value factor, \\n(1 + r)N.\\n■ \\nThe interest rate, r, makes current and future currency amounts equivalent \\nbased on their time value.\\n■ \\nThe stated annual interest rate is a quoted interest rate that does not \\naccount for compounding within the year.\\n■ \\nThe periodic rate is the quoted interest rate per period; it equals the stated \\nannual interest rate divided by the number of compounding periods per \\nyear.\\n■ \\nThe effective annual rate is the amount by which a unit of currency will \\ngrow in a year with interest on interest included.\\n■ \\nAn annuity is a finite set of level sequential cash flows.\\n■ \\nThere are two types of annuities, the annuity due and the ordinary annuity. \\nThe annuity due has a first cash flow that occurs immediately; the ordi-\\nnary annuity has a first cash flow that occurs one period from the present \\n(indexed at t = 1).\\n■ \\nOn a time line, we can index the present as 0 and then display equally \\nspaced hash marks to represent a number of periods into the future. This \\nrepresentation allows us to index how many periods away each cash flow \\nwill be paid.\\n■ \\nAnnuities may be handled in a similar approach as single payments if we use \\nannuity factors rather than single-payment factors.\\n■ \\nThe present value, PV, is the future value, FV, times the present value factor, \\n(1 + r)−N.\\n■ \\nThe present value of a perpetuity is A/r, where A is the periodic payment to \\nbe received forever.\\n■ \\nIt is possible to calculate an unknown variable, given the other relevant vari-\\nables in time value of money problems.\\n■ \\nThe cash flow additivity principle can be used to solve problems with \\nuneven cash flows by combining single payments and annuities.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n40\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 50,\n",
       "     'content': 'PRACTICE PROBLEMS\\n1. The table below gives current information on the interest rates for two two-year \\nand two eight-year maturity investments. The table also gives the maturity, \\nliquidity, and default risk characteristics of a new investment possibility (Invest-\\nment 3). All investments promise only a single payment (a payment at maturity). \\nAssume that premiums relating to inflation, liquidity, and default risk are con-\\nstant across all time horizons.\\nInvestment\\nMaturity (in Years)\\nLiquidity\\nDefault Risk\\nInterest Rate (%)\\n1\\n2\\nHigh\\nLow\\n2.0\\n2\\n2\\nLow\\nLow\\n2.5\\n3\\n7\\nLow\\nLow\\nr3\\n4\\n8\\nHigh\\nLow\\n4.0\\n5\\n8\\nLow\\nHigh\\n6.5\\nBased on the information in the above table, address the following:\\nA. Explain the difference between the interest rates on Investment 1 and \\nInvestment 2.\\nB. Estimate the default risk premium.\\nC. Calculate upper and lower limits for the interest rate on Investment 3, r3.\\n2. The nominal risk-free rate is best described as the sum of the real risk-free rate \\nand a premium for:\\nA. maturity.\\nB. liquidity.\\nC. expected inflation.\\n3. Which of the following risk premiums is most relevant in explaining the differ-\\nence in yields between 30-year bonds issued by the US Treasury and 30-year \\nbonds issued by a small private issuer?\\nA. Inflation\\nB. Maturity\\nC. Liquidity\\n4. The value in six years of $75,000 invested today at a stated annual interest rate of \\n7% compounded quarterly is closest to:\\nA. $112,555.\\nB. $113,330.\\nC. $113,733.\\n5. A bank quotes a stated annual interest rate of 4.00%. If that rate is equal to an \\neffective annual rate of 4.08%, then the bank is compounding interest:\\nA. daily.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n41\\nB. quarterly.\\nC. semiannually.\\n6. Given a €1,000,000 investment for four years with a stated annual rate of 3% com-\\npounded continuously, the difference in its interest earnings compared with the \\nsame investment compounded daily is closest to:\\nA. €1.\\nB. €6.\\nC. €455.\\n7. A couple plans to set aside $20,000 per year in a conservative portfolio projected \\nto earn 7 percent a year. If they make their first savings contribution one year \\nfrom now, how much will they have at the end of 20 years?\\n8. Two years from now, a client will receive the first of three annual payments of \\n$20,000 from a small business project. If she can earn 9 percent annually on her \\ninvestments and plans to retire in six years, how much will the three business \\nproject payments be worth at the time of her retirement?\\n9. A saver deposits the following amounts in an account paying a stated annual rate \\nof 4%, compounded semiannually:\\nYear\\nEnd of Year Deposits ($)\\n1\\n4,000\\n2\\n8,000\\n3\\n7,000\\n4\\n10,000\\nAt the end of Year 4, the value of the account is closest to:\\nA. $30,432\\nB. $30,447\\nC. $31,677\\n10. To cover the first year’s total college tuition payments for his two children, a \\nfather will make a $75,000 payment five years from now. How much will he need \\nto invest today to meet his first tuition goal if the investment earns 6 percent \\nannually?\\n11. Given the following timeline and a discount rate of 4% a year compounded \\nannually, the present value (PV), as of the end of Year 5 (PV5 ), of the cash flow \\nreceived at the end of Year 20 is closest to:\\n0\\n1\\n2\\n3\\n4\\n5\\nPV5\\n20\\n...\\n$50,000\\nA. $22,819.\\nB. $27,763.\\nC. $28,873.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n42\\n12. A client requires £100,000 one year from now. If the stated annual rate is 2.50% \\ncompounded weekly, the deposit needed today is closest to:\\nA. £97,500.\\nB. £97,532.\\nC. £97,561.\\n13. A client can choose between receiving 10 annual $100,000 retirement payments, \\nstarting one year from today, or receiving a lump sum today. Knowing that he can \\ninvest at a rate of 5 percent annually, he has decided to take the lump sum. What \\nlump sum today will be equivalent to the future annual payments?\\n14. You are considering investing in two different instruments. The first instrument \\nwill pay nothing for three years, but then it will pay $20,000 per year for four \\nyears. The second instrument will pay $20,000 for three years and $30,000 in the \\nfourth year. All payments are made at year-end. If your required rate of return on \\nthese investments is 8 percent annually, what should you be willing to pay for:\\nA. The first instrument?\\nB. The second instrument (use the formula for a four-year annuity)?\\n15. Suppose you plan to send your daughter to college in three years. You expect her \\nto earn two-thirds of her tuition payment in scholarship money, so you estimate \\nthat your payments will be $10,000 a year for four years. To estimate whether you \\nhave set aside enough money, you ignore possible inflation in tuition payments \\nand assume that you can earn 8 percent annually on your investments. How \\nmuch should you set aside now to cover these payments?\\n16. An investment pays €300 annually for five years, with the first payment occurring \\ntoday. The present value (PV) of the investment discounted at a 4% annual rate is \\nclosest to:\\nA. €1,336.\\nB. €1,389.\\nC. €1,625.\\n17. At a 5% interest rate per year compounded annually, the present value (PV) of a \\n10-year ordinary annuity with annual payments of $2,000 is $15,443.47. The PV \\nof a 10-year annuity due with the same interest rate and payments is closest to:\\nA. $14,708.\\nB. $16,216.\\nC. $17,443.\\n18. Grandparents are funding a newborn’s future university tuition costs, estimated \\nat $50,000/year for four years, with the first payment due as a lump sum in 18 \\nyears. Assuming a 6% effective annual rate, the required deposit today is closest \\nto:\\nA. $60,699.\\nB. $64,341.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n43\\nC. $68,201.\\n19. The present value (PV) of an investment with the following year-end cash flows \\n(CF) and a 12% required annual rate of return is closest to:\\nYear\\nCash Flow (€)\\n1\\n100,000\\n2\\n150,000\\n5\\n–10,000\\nA. €201,747.\\nB. €203,191.\\nC. €227,573.\\n20. A perpetual preferred stock makes its first quarterly dividend payment of $2.00 in \\nfive quarters. If the required annual rate of return is 6% compounded quarterly, \\nthe stock’s present value is closest to:\\nA. $31.\\nB. $126.\\nC. $133.\\n21. A sweepstakes winner may select either a perpetuity of £2,000 a month begin-\\nning with the first payment in one month or an immediate lump sum payment \\nof £350,000. If the annual discount rate is 6% compounded monthly, the present \\nvalue of the perpetuity is:\\nA. less than the lump sum.\\nB. equal to the lump sum.\\nC. greater than the lump sum.\\n22. For a lump sum investment of ¥250,000 invested at a stated annual rate of 3% \\ncompounded daily, the number of months needed to grow the sum to ¥1,000,000 \\nis closest to:\\nA. 555.\\nB. 563.\\nC. 576.\\n23. An investment of €500,000 today that grows to €800,000 after six years has a \\nstated annual interest rate closest to:\\nA. 7.5% compounded continuously.\\nB. 7.7% compounded daily.\\nC. 8.0% compounded semiannually.\\n24. A client plans to send a child to college for four years starting 18 years from now. \\nHaving set aside money for tuition, she decides to plan for room and board also. \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n44\\nShe estimates these costs at $20,000 per year, payable at the beginning of each \\nyear, by the time her child goes to college. If she starts next year and makes 17 \\npayments into a savings account paying 5 percent annually, what annual pay-\\nments must she make?\\n25. A couple plans to pay their child’s college tuition for 4 years starting 18 years \\nfrom now. The current annual cost of college is C$7,000, and they expect this cost \\nto rise at an annual rate of 5 percent. In their planning, they assume that they can \\nearn 6 percent annually. How much must they put aside each year, starting next \\nyear, if they plan to make 17 equal payments?\\n26. A sports car, purchased for £200,000, is financed for five years at an annual rate of \\n6% compounded monthly. If the first payment is due in one month, the monthly \\npayment is closest to:\\nA. £3,847.\\nB. £3,867.\\nC. £3,957.\\n27. Given a stated annual interest rate of 6% compounded quarterly, the level amount \\nthat, deposited quarterly, will grow to £25,000 at the end of 10 years is closest to:\\nA. £461.\\nB. £474.\\nC. £836.\\n28. A client invests €20,000 in a four-year certificate of deposit (CD) that annually \\npays interest of 3.5%. The annual CD interest payments are automatically rein-\\nvested in a separate savings account at a stated annual interest rate of 2% com-\\npounded monthly. At maturity, the value of the combined asset is closest to:\\nA. €21,670.\\nB. €22,890.\\nC. €22,950.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 55,\n",
       "     'content': 'Solutions\\n45\\nSOLUTIONS\\n1. \\nA. Investment 2 is identical to Investment 1 except that Investment 2 has low \\nliquidity. The difference between the interest rate on Investment 2 and \\nInvestment 1 is 0.5 percentage point. This amount represents the liquidity \\npremium, which represents compensation for the risk of loss relative to \\nan investment’s fair value if the investment needs to be converted to cash \\nquickly.\\nB. To estimate the default risk premium, find the two investments that have the \\nsame maturity but different levels of default risk. Both Investments 4 and 5 \\nhave a maturity of eight years. Investment 5, however, has low liquidity and \\nthus bears a liquidity premium. The difference between the interest rates of \\nInvestments 5 and 4 is 2.5 percentage points. The liquidity premium is 0.5 \\npercentage point (from Part A). This leaves 2.5 − 0.5 = 2.0 percentage points \\nthat must represent a default risk premium reflecting Investment 5’s high \\ndefault risk.\\nC. Investment 3 has liquidity risk and default risk comparable to Investment \\n2, but with its longer time to maturity, Investment 3 should have a higher \\nmaturity premium. The interest rate on Investment 3, r3, should thus be \\nabove 2.5 percent (the interest rate on Investment 2). If the liquidity of \\nInvestment 3 were high, Investment 3 would match Investment 4 except for \\nInvestment 3’s shorter maturity. We would then conclude that Investment \\n3’s interest rate should be less than the interest rate on Investment 4, which \\nis 4 percent. In contrast to Investment 4, however, Investment 3 has low \\nliquidity. It is possible that the interest rate on Investment 3 exceeds that of \\nInvestment 4 despite 3’s shorter maturity, depending on the relative size of \\nthe liquidity and maturity premiums. However, we expect r3 to be less than \\n4.5 percent, the expected interest rate on Investment 4 if it had low liquidity. \\nThus 2.5 percent < r3 < 4.5 percent.\\n2. C is correct. The sum of the real risk-free interest rate and the inflation premium \\nis the nominal risk-free rate.\\n3. C is correct. US Treasury bonds are highly liquid, whereas the bonds of small \\nissuers trade infrequently and the interest rate includes a liquidity premium. \\nThis liquidity premium reflects the relatively high costs (including the impact on \\nprice) of selling a position.\\n4. C is correct, as shown in the following (where FV is future value and PV is pres-\\nent value):\\n FV = PV  ( 1 +  \\n r s  \\n_ \\nm  ) \\nmN\\n  \\n FV 6 = $75, 000  ( 1 +  0.07 \\n_ \\n4  ) \\n ( 4×6 )  \\n \\n FV6 = $113,733.21.\\n5. A is correct. The effective annual rate (EAR) when compounded daily is 4.08%.\\n EAR = (1 + Periodic interest rate)m– 1  \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n46\\n EAR = (1 + 0.04/365)365 – 1  \\n EAR = (1.0408) – 1 = 0.04081 ≈ 4.08%.\\n6. B is correct. The difference between continuous compounding and daily com-\\npounding is\\n €127,496.85 – €127,491.29 = €5.56, or ≈ €6, as shown in the following \\ncalculations.\\nWith continuous compounding, the investment earns (where PV is present value)\\n PV  e  r s N − PV = €1,000,000e0.03(4) – €1,000,000\\n = €1,127,496.85 – €1,000,000\\n = €127,496.85\\nWith daily compounding, the investment earns:\\n €1,000,000(1 + 0.03/365)365(4) – €1,000,000 = €1,127,491.29 – €1,000,000 = \\n€127,491.29.\\n7. \\ni. \\nDraw a time line.\\n0\\n1\\n2\\n19\\n20\\n$20,000 $20,000\\n$20,000 $20,000\\nX = FV\\nii. \\nIdentify the problem as the future value of an annuity.\\niii. Use the formula for the future value of an annuity.\\n \\n FV N = A  [  ( 1 + r ) N − 1 \\n_ \\nr \\n ]  \\n  \\n \\n= $20, 000  [  ( 1 + 0.07 ) 20 − 1 \\n \\n____________ \\n0.07 \\n ]    \\n \\n \\n= $819, 909.85\\n \\n \\n20\\n19\\n0\\n1\\n2\\n$20,000 $20,000\\n$20,000 $20,000\\nFV = $819,909.85\\niv. \\nAlternatively, use a financial calculator.\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n20\\n%i\\n7\\nPV\\nn/a (= 0)\\nFV compute\\nX\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n47\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nPMT\\n$20,000\\nEnter 20 for N, the number of periods. Enter 7 for the interest rate and \\n20,000 for the payment size. The present value is not needed, so enter 0. \\nCalculate the future value. Verify that you get $819,909.85 to make sure \\nyou have mastered your calculator’s keystrokes.\\nIn summary, if the couple sets aside $20,000 each year (starting next year), \\nthey will have $819,909.85 in 20 years if they earn 7 percent annually.\\n8. \\ni. \\nDraw a time line.\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\n$20,000\\n$20,000\\n$20,000\\nX = FV\\nii. \\nRecognize the problem as the future value of a delayed annuity. Delaying \\nthe payments requires two calculations.\\niii. Use the formula for the future value of an annuity (Equation 7).\\n FV N = A  [  ( 1 + r ) N − 1 \\n_ \\nr \\n ]   \\nto bring the three $20,000 payments to an equivalent lump sum of \\n$65,562.00 four years from today.\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n3\\n%i\\n9\\nPV\\nn/a (= 0)\\nFV compute\\nX\\nPMT\\n$20,000\\niv. \\nUse the formula for the future value of a lump sum (Equation 2), FVN = \\nPV(1 + r)N, to bring the single lump sum of $65,562.00 to an equivalent \\nlump sum of $77,894.21 six years from today.\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n2\\n%i\\n9\\nPV\\n$65,562.00\\nFV compute\\nX\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n48\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nPMT\\nn/a (= 0)\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\n$20,000\\n$20,000\\n$65,562.00\\n$77,894.21\\n$20,000\\nFV\\nFV\\nFV\\n6\\n4\\nX\\nIn summary, your client will have $77,894.21 in six years if she receives \\nthree yearly payments of $20,000 starting in Year 2 and can earn 9 percent \\nannually on her investments.\\n9. B is correct. To solve for the future value of unequal cash flows, compute the \\nfuture value of each payment as of Year 4 at the semiannual rate of 2%, and then \\nsum the individual future values, as follows:\\nYear\\nEnd of Year Deposits ($)\\nFactor\\nFuture Value ($)\\n1\\n4,000\\n(1.02)6\\n4,504.65\\n2\\n8,000\\n(1.02)4\\n8,659.46\\n3\\n7,000\\n(1.02)2\\n7,282.80\\n4\\n10,000\\n(1.02)0\\n10,000.00\\n\\xa0\\n\\xa0\\nSum =\\n30,446.91\\n10. \\ni. \\nDraw a time line.\\n5\\n4\\n3\\n2\\n1\\n0\\nX\\nPV\\nFV\\n$75,000\\nii. \\nIdentify the problem as the present value of a lump sum.\\niii. Use the formula for the present value of a lump sum.\\n \\nPV =  FV N  ( 1 + r ) −N \\n  \\n \\n= $75, 000  ( 1 + 0.06 ) −5   \\n \\n= $56, 044.36\\n \\n \\n5\\n4\\n3\\n2\\n1\\n0\\n$56,044.36\\n$75,000\\nPV\\nFV\\nIn summary, the father will need to invest $56,044.36 today in order to \\nhave $75,000 in five years if his investments earn 6 percent annually.\\n11. B is correct. The PV in Year 5 of a $50,000 lump sum paid in Year 20 is $27,763.23 \\n(where FV is future value):\\n PV = FVN(1 + r)–N\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n49\\n PV = $50,000(1 + 0.04)–15\\n PV = $27,763.23\\n12. B is correct because £97,531 represents the present value (PV) of £100,000 \\nreceived one year from today when today’s deposit earns a stated annual rate of \\n2.50% and interest compounds weekly, as shown in the following equation (where \\nFV is future value):\\n PV =  FV N  ( 1 +  \\n r s  \\n_ \\nm  ) \\n−mN\\n  \\n PV = £100, 000  ( 1 +  0.025 \\n_ \\n52  ) \\n−52\\n  \\n PV = £97,531.58.\\n13. \\ni. \\nDraw a time line for the 10 annual payments.\\nX\\nPV\\n2\\n9\\n10\\n1\\n0\\n$100,000\\n$100,000\\n$100,000\\n$100,000\\nii. \\nIdentify the problem as the present value of an annuity.\\niii. Use the formula for the present value of an annuity.\\n \\nPV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n  \\n \\n= $100, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1 + 0.05 ) 10    \\n \\n___________ \\n0.05 \\n ]  \\n  \\n \\n \\n= $772, 173 . 49\\n \\n \\n10\\n9\\n2\\n1\\n0\\nX\\n$100,000\\n$100,000\\n$100,000\\n$100,000\\nPV = $772,173.49\\niv. \\nAlternatively, use a financial calculator.\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n10\\n%i\\n5\\nPV compute\\nX\\nFV\\nn/a (= 0)\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n50\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nPMT\\n$100,000\\nIn summary, the present value of 10 payments of $100,000 is $772,173.49 \\nif the first payment is received in one year and the rate is 5 percent com-\\npounded annually. Your client should accept no less than this amount for \\nhis lump sum payment.\\n14. \\nA. To evaluate the first instrument, take the following steps:\\ni. \\nDraw a time line.\\n0\\n1\\n2\\n5\\n4\\n3\\n6\\n7\\n$20,000\\n$20,000\\n$20,000\\n$20,000\\n \\n PV 3 = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n  \\n \\n= $20, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1 + 0.08 ) 4    \\n_ \\n0.08 \\n ]  \\n  \\n \\n \\n= $66, 242 . 54\\n \\n \\n PV 0 =  \\n PV 3  \\n_ \\n ( 1 + r ) N   =  $66, 242 . 54 \\n_ \\n 1.08 3  \\n = $52, 585.46 \\nii. \\nYou should be willing to pay $52,585.46 for this instrument.\\nB. To evaluate the second instrument, take the following steps:\\ni. \\nDraw a time line.\\n4\\n3\\n2\\n1\\n0\\n$20,000\\n$20,000\\n$20,000\\n$20,000\\n$30,000\\n+10,000\\nThe time line shows that this instrument can be analyzed as an ordinary \\nannuity of $20,000 with four payments (valued in Step ii below) and a \\n$10,000 payment to be received at t = 4 (valued in Step iii below).\\n \\nPV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  \\n  \\n \\n= $20, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1 + 0.08 ) 4    \\n_ \\n0.08 \\n ]  \\n  \\n \\n \\n= $66, 242 . 54\\n \\n \\n PV =  \\n FV 4  \\n_ \\n ( 1 + r ) N   =  $10, 000 \\n_ \\n ( 1 + 0.08 ) 4   = $7, 350.30 \\nii. \\nTotal = $66,242.54 + $7,350.30 = $73,592.84\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n51\\nYou should be willing to pay $73,592.84 for this instrument.\\n15. \\ni. \\nDraw a time line.\\n4\\n5\\n6\\n3\\n2\\n1\\n0\\n$10,000\\n$10,000\\n$10,000\\n$10,000\\nX\\nPV\\nii. \\nRecognize the problem as a delayed annuity. Delaying the payments \\nrequires two calculations.\\niii. Use the formula for the present value of an annuity (Equation 11).\\n PV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]   \\nto bring the four payments of $10,000 back to a single equivalent lump \\nsum of $33,121.27 at t = 2. Note that we use t = 2 because the first annuity \\npayment is then one period away, giving an ordinary annuity.\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n4\\n%i\\n8\\nPV compute\\nX\\nPMT\\n$10,000\\niv. \\nThen use the formula for the present value of a lump sum (Equation 8), PV \\n= FVN(1 + r)−N, to bring back the single payment of $33,121.27 (at t = 2) to \\nan equivalent single payment of $28,396.15 (at t = 0).\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n2\\n%i\\n8\\nPV compute\\nX\\nFV\\n$33,121.27\\nPMT\\nn/a (= 0)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n$10,000\\n$10,000\\n$10,000\\n$10,000\\nX\\nPV\\n$28,396.15\\n$33,121.27\\nEquation\\nEquation\\n8\\n11\\nIn summary, you should set aside $28,396.15 today to cover four payments \\nof $10,000 starting in three years if your investments earn a rate of 8 per-\\ncent annually.\\n16. B is correct, as shown in the following calculation for an annuity (A) due:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n52\\n PV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]   ( 1 + r )  ,\\nwhere A = €300, r = 0.04, and N = 5.\\n PV = €300  [ \\n1 −  \\n1 \\n_ \\n ( 1 + .04 ) 5    \\n_ \\n.04 \\n ]   ( 1.04 )   \\n PV = €1,388.97, or ≈ €1,389.\\n17. B is correct.\\nThe present value of a 10-year annuity (A) due with payments of $2,000 at a 5% \\ndiscount rate is calculated as follows:\\n PV = A  [ \\n1 −  \\n1 \\n_ \\n ( 1 + r ) N    \\n_ \\nr \\n ]  + $2, 000 \\n PV = $2, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1 + 0.05 ) 9    \\n_ \\n0.05 \\n ]  + $2, 000 \\n PV = $16,215.64.\\nAlternatively, the PV of a 10-year annuity due is simply the PV of the ordinary \\nannuity multiplied by 1.05:\\n PV = $15,443.47 × 1.05\\n PV = $16,215.64.\\n18. B is correct. First, find the present value (PV) of an ordinary annuity in Year 17 \\nthat represents the tuition costs:\\n $50, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1 + 0.06 ) 4    \\n_ \\n0.06 \\n ]   \\n = $50,000 × 3.4651\\n = $173,255.28.\\nThen, find the PV of the annuity in today’s dollars (where FV is future value):\\n PV 0 =  \\nFV \\n_ \\n ( 1 + 0.06 ) 17   \\n PV 0 =  $173, 255.28 \\n_ \\n ( 1 + 0.06 ) 17   \\n PV0 = $64,340.85 ≈ $64,341.\\n19. B is correct, as shown in the following table.\\nYear\\nCash Flow \\n(€)\\nFormula \\nCF × (1 + r)t\\nPV at \\nYear 0\\n1\\n100,000\\n100,000(1.12)–1 =\\n89,285.71\\n2\\n150,000\\n150,000(1.12)–2 =\\n119,579.08\\n5\\n–10,000\\n–10,000(1.12)–5 =\\n–5,674.27\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n53\\nYear\\nCash Flow \\n(€)\\nFormula \\nCF × (1 + r)t\\nPV at \\nYear 0\\n\\xa0\\n\\xa0\\n\\xa0\\n203,190.52\\n20. B is correct. The value of the perpetuity one year from now is calculated as:\\nPV = A/r, where PV is present value, A is annuity, and r is expressed as a quarter-\\nly required rate of return because the payments are quarterly.\\n PV = $2.00/(0.06/4)\\n PV = $133.33.\\nThe value today is (where FV is future value)\\n PV = FVN(1 + r)–N\\n PV = $133.33(1 + 0.015)–4\\n PV = $125.62 ≈ $126.\\n21. C is correct. As shown below, the present value (PV) of a £2,000 per month \\nperpetuity is worth approximately £400,000 at a 6% annual rate compounded \\nmonthly. Thus, the present value of the annuity (A) is worth more than the lump \\nsum offers.\\n A = £2,000\\n r = (6%/12) = 0.005\\n PV = (A/r)\\n PV = (£2,000/0.005)\\n PV = £400,000\\n22. A is correct. The effective annual rate (EAR) is calculated as follows:\\n EAR = (1 + Periodic interest rate)m – 1  \\n EAR = (1 + 0.03/365)365 – 1  \\n EAR= (1.03045) – 1 = 0.030453 ≈ 3.0453%.\\nSolving for N on a financial calculator results in (where FV is future value and PV \\nis present value):\\n (1 + 0.030453)N = FVN/PV = ¥1,000,000/¥250,000\\n = 46.21 years, which multiplied by 12 to convert to months results in 554.5, or ≈ \\n555 months.\\n23. C is correct, as shown in the following (where FV is future value and PV is pres-\\nent value):\\nIf:\\n FV N = PV  ( 1 +  \\n r s  \\n_ \\nm  ) \\nmN\\n ,\\nThen:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n54\\n ( \\n FV N  \\n_ \\nPV  ) \\n 1 _ \\nmN  \\n − 1 =  \\n r s  \\n_ \\nm  \\n ( 800, 000 \\n_ \\n500, 000  ) \\n 1 \\n_ \\n2×6  \\n − 1 =  \\n r s  \\n_ \\n2  \\n rs = 0.07988 (rounded to 8.0%).\\n24. \\ni. \\nDraw a time line.\\n0\\n1\\n2\\n17\\nX\\n18\\n19\\n20\\n21\\n$20,000\\n$20,000\\n$20,000\\n$20,000\\n(   )\\nX\\n(   )\\nX\\n(   )\\nii. \\nRecognize that you need to equate the values of two annuities.\\niii. Equate the value of the four $20,000 payments to a single payment in \\nPeriod 17 using the formula for the present value of an annuity (Equation \\n11), with r = 0.05. The present value of the college costs as of t = 17 is \\n$70,919.\\n PV = $20, 000  [ \\n1 −  \\n1 \\n_ \\n ( 1.05 ) 4    \\n_ \\n0.05 \\n ]  = $70, 919 \\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n4\\n%i\\n5\\nPV compute\\nX\\nFV\\nn/a (= 0)\\nPMT\\n$20,000\\niv. \\nEquate the value of the 17 investments of X to the amount calculated in \\nStep iii, college costs as of t = 17, using the formula for the future value of \\nan annuity (Equation 7). Then solve for X.\\n $70, 919 =   [  ( 1.05 ) 17 − 1 \\n_ \\n0.05 \\n ]  = 25.840366X  \\n \\n \\n \\nX = $2, 744.50\\n \\n \\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n17\\n%i\\n5\\nPV\\nn/a (= 0)\\nFV\\n$70,919\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n55\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nPMT compute\\nX\\n0\\n1\\n2\\n17\\nX\\n18\\n19\\n20\\n21\\n$20,000\\n$70,919\\n$70,919\\n$20,000\\n$20,000\\n$20,000\\nX\\n(   )\\nX\\n(   )\\nX\\n(   )\\nEquation 7\\nEquation11\\n25.8404\\n=\\nIn summary, your client will have to save $2,744.50 each year if she starts \\nnext year and makes 17 payments into a savings account paying 5 percent \\nannually.\\n25. \\ni. \\nDraw a time line.\\n0\\n1\\n2\\n17\\n18\\n19\\n20\\n21\\nC$7,000\\nYear 1\\nYear 2\\nYear 3\\nYear 4\\npayment\\npayment\\npayment\\npayment\\nii. \\nRecognize that the payments in Years 18, 19, 20, and 21 are the future val-\\nues of a lump sum of C$7,000 in Year 0.\\niii. With r = 5%, use the formula for the future value of a lump sum (Equation \\n2), FVN = PV (1 + r)N, four times to find the payments. These future values \\nare shown on the time line below.\\nEquation 2\\n0\\n1\\n2\\n17\\n18\\n19\\n20\\n21\\nC$7,000\\nC$17,689\\nC$18,573\\nC$19,502\\nC$16,846\\nYear 1\\nYear 2\\nYear 3\\nYear 4\\npayment\\npayment\\npayment\\npayment\\niv. \\nUsing the formula for the present value of a lump sum (r = 6%), equate \\nthe four college payments to single payments as of t = 17 and add them \\ntogether. C$16,846(1.06)−1 + C$17,689(1.06)−2 + C$18,573(1.06)−3 + \\nC$19,502(1.06)−4 = C$62,677\\nv. \\nEquate the sum of C$62,677 at t = 17 to the 17 payments of X, using the \\nformula for the future value of an annuity (Equation 7). Then solve for X.\\n C$62, 677 = X  [  ( 1.06 ) 17 − 1 \\n_ \\n0.06 \\n ]  = 28.21288X  \\n \\n \\n \\nX = C$2, 221.58\\n \\n \\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nN\\n17\\n%i\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 1 \\nThe Time Value of Money\\n56\\nNotation Used  \\non Most Calculators \\nNumerical Value \\nfor This Problem\\nPV\\nn/a (= 0)\\nFV\\nC$62,677\\nPMT compute\\nX\\n0\\n1\\n2\\n17\\nX\\n18\\n19\\n20\\n21\\nC$62,677\\nX\\n(   )\\nX\\n(   )\\nX\\n(   )\\nEquation 7\\n28.21288\\n=\\nIn summary, the couple will need to put aside C$2,221.58 each year if they \\nstart next year and make 17 equal payments.\\n26. B is correct, calculated as follows (where A is annuity and PV is present value):\\n \\nA =   ( PV of annuity )  \\u200a/\\u200a  \\n⎡\\n⎣ ⎢ \\n \\n1 −  \\n1 \\n_ \\n ( 1 +  r s \\u200a/\\u200am ) \\nmN \\n   \\n \\n____________ \\n r s \\u200a/\\u200am \\n \\n⎤\\n⎦ ⎥ \\n  \\n  \\n \\n \\n \\n=   ( £200, 000 )  \\u200a/\\u200a  \\n⎡\\n⎣ ⎢ \\n \\n1 −  \\n1 \\n_ \\n ( 1 +  r s \\u200a/\\u200am ) \\nmN \\n   \\n \\n____________ \\n r s \\u200a/\\u200am \\n \\n⎤\\n⎦ ⎥ \\n    \\n \\n \\n  ( £200, 000 )  \\u200a/\\u200a  [ \\n1 −  \\n1 \\n______________ \\n \\n ( 1 + 0.06\\u200a/\\u200a12 ) 12  ( 5 )      \\n \\n_______________ \\n0.06\\u200a/\\u200a12 \\n ]  \\n  \\n \\n \\n=   ( £200, 000 )  \\u200a/\\u200a51.72556\\n  \\n \\n= £3, 866.56\\n \\n \\n27. A is correct. To solve for an annuity (A) payment, when the future value (FV), \\ninterest rate, and number of periods is known, use the following equation:\\n FV = A  [\\n \\n ( 1 +  \\n r s  \\n_ \\nm  ) \\nmN\\n  − 1 \\n___________ \\n r _ \\nm   \\n ]\\n   \\n £25, 000 = A  [ \\n ( 1 +  0.06 \\n_ \\n4  ) \\n4×10\\n  − 1 \\n______________ \\n 0.06 \\n_ \\n4   \\n ]   \\n A = £460.68\\n28. B is correct, as the following cash flows show:\\n€20,000 initial deposit\\n€700\\n€700\\n€700\\n€700\\nannual\\ninterest payments\\n(which earn 2.0%/year)\\n+\\n€20,000 (return of principal)\\nThe four annual interest payments are based on the CD’s 3.5% annual rate.\\nThe first payment grows at 2.0% compounded monthly for three years (where FV \\nis future value):\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n57\\n FV N = €700  ( 1 +  0.02 \\n_ \\n12  ) \\n3×12\\n  \\n FVN = 743.25\\nThe second payment grows at 2.0% compounded monthly for two years:\\n FV N = €700  ( 1 +  0.02 \\n_ \\n12  ) \\n2×12\\n  \\n FVN = 728.54\\nThe third payment grows at 2.0% compounded monthly for one year:\\n FV N = €700  ( 1 +  0.02 \\n_ \\n12  ) \\n1×12\\n  \\n FVN = 714.13\\nThe fourth payment is paid at the end of Year 4. Its future value is €700.\\nThe sum of all future value payments is as follows:\\n€20,000.00\\n\\xa0\\nCD\\n€743.25\\n\\xa0\\nFirst payment’s FV\\n€728.54\\n\\xa0\\nSecond payment’s FV\\n€714.13\\n\\xa0\\nThird payment’s FV\\n€700.00\\n\\xa0\\nFourth payment’s FV\\n€22,885.92\\n\\xa0\\nTotal FV\\n© CFA Institute. For candidate use only. Not for distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\nOrganizing, Visualizing, \\nand Describing Data\\nby Pamela Peterson Drake, PhD, CFA, and Jian Wu, PhD.\\nPamela Peterson Drake, PhD, CFA, is at James Madison University (USA). Jian Wu, PhD, \\nis at State Street (USA). \\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\nidentify and compare data types\\ndescribe how data are organized for quantitative analysis\\ninterpret frequency and related distributions\\ninterpret a contingency table\\ndescribe ways that data may be visualized and evaluate uses of \\nspecific visualizations\\ndescribe how to select among visualization types\\ncalculate and interpret measures of central tendency\\nevaluate alternative definitions of mean to address an investment \\nproblem\\ncalculate quantiles and interpret related visualizations\\ncalculate and interpret measures of dispersion\\ncalculate and interpret target downside deviation\\ninterpret skewness\\ninterpret kurtosis\\ninterpret correlation between two variables\\nINTRODUCTION\\nData have always been a key input for securities analysis and investment management, \\nbut the acceleration in the availability and the quantity of data has also been driving \\nthe rapid evolution of the investment industry. With the rise of big data and machine \\n1\\nL E A R N I N G  M O D U L E\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]},\n",
       "  {'title': 'Learning Module 2\\tOrganizing, Visualizing, and Describing Data',\n",
       "   'page_number': 69,\n",
       "   'content': 'Organizing, Visualizing, \\nand Describing Data\\nby Pamela Peterson Drake, PhD, CFA, and Jian Wu, PhD.\\nPamela Peterson Drake, PhD, CFA, is at James Madison University (USA). Jian Wu, PhD, \\nis at State Street (USA). \\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\nidentify and compare data types\\ndescribe how data are organized for quantitative analysis\\ninterpret frequency and related distributions\\ninterpret a contingency table\\ndescribe ways that data may be visualized and evaluate uses of \\nspecific visualizations\\ndescribe how to select among visualization types\\ncalculate and interpret measures of central tendency\\nevaluate alternative definitions of mean to address an investment \\nproblem\\ncalculate quantiles and interpret related visualizations\\ncalculate and interpret measures of dispersion\\ncalculate and interpret target downside deviation\\ninterpret skewness\\ninterpret kurtosis\\ninterpret correlation between two variables\\n',\n",
       "   'children': [{'title': 'Introduction',\n",
       "     'page_number': 69,\n",
       "     'content': '',\n",
       "     'children': []},\n",
       "    {'title': 'Data Types',\n",
       "     'page_number': 70,\n",
       "     'content': 'data types, including continuous and discrete numerical \\ndata, nominal and ordinal categorical data, and structured versus unstructured data. \\nOrganizing data into arrays and data tables and summarizing data in frequency dis-\\ntributions and contingency tables are discussed in Section 3. Section 4 introduces the \\nimportant topic of data visualization using a range of charts and graphics to summa-\\nrize, explore, and better understand data. Section 5 covers the key measures of central \\ntendency, including several variants of mean that are especially useful in investments. \\nQuantiles and their investment applications are the focus of Section 6. Key measures \\nof dispersion are discussed in Section 7. The shape of data distributions—specifically, \\nskewness and kurtosis—are covered in Sections 8 and 9, respectively. Section 10 pro-\\nvides a graphical introduction to covariance and correlation between two variables. \\nThe reading concludes with a Summary.\\nDATA TYPES\\nidentify and compare data types\\ndescribe how data are organized for quantitative analysis\\nData can be defined as a collection of numberpanel datas, characters, words, and \\ntext—as well as images, audio, and video—in a raw or organized format to represent \\nfacts or information. To choose the appropriate statistical methods for summarizing \\nand analyzing data and to select suitable charts for visualizing data, we need to dis-\\ntinguish among different data types. We will discuss data types under three different \\nperspectives of classifications: ',\n",
       "     'children': [{'title': 'Numerical versus Categorical Data',\n",
       "       'page_number': 71,\n",
       "       'content': 'Numerical versus Categorical Data\\nFrom a statistical perspective, data can be classified into two basic groups: numerical \\ndata and categorical data.\\nNumerical Data\\nNumerical data are values that represent measured or counted quantities as a number \\nand are also called quantitative data. Numerical (quantitative) data can be split into \\ntwo types: continuous data and discrete data.\\nContinuous data are data that can be measured and can take on any numerical \\nvalue in a specified range of values. For example, the future value of a lump-sum \\ninvestment measures the amount of money to be received after a certain period of \\ntime bearing an interest rate. The future value could take on a range of values depend-\\ning on the time period and interest rate. Another common example of continuous \\ndata is the price returns of a stock that measures price change over a given period in \\npercentage terms.\\nDiscrete data are numerical values that result from a counting process. So, \\npractically speaking, the data are limited to a finite number of values. For example, \\nthe frequency of discrete compounding, m, counts the number of times that interest \\nis accrued and paid out in a given year. The frequency could be monthly (m = 12), \\nquarterly (m = 4), semi-yearly (m = 2), or yearly (m = 1).\\nCategorical Data\\nCategorical data (also called qualitative data) are values that describe a quality \\nor characteristic of a group of observations and therefore can be used as labels to \\ndivide a dataset into groups to summarize and visualize. Usually they can take only \\na limited number of values that are mutually exclusive. Examples of categorical data \\nfor classifying companies include bankrupt vs. not bankrupt and dividends increased \\nvs. no dividend action.\\nNominal data are categorical values that are not amenable to being organized \\nin a logical order. An example of nominal data is the classification of publicly \\nlisted stocks into 11 sectors, as shown in Exhibit 1, that are defined by the Global \\nIndustry Classification Standard (GICS). GICS, developed by Morgan Stanley Capital \\nInternational (MSCI) and Standard & Poor’s (S&P), is a four-tiered, hierarchical indus-\\ntry classification system consisting of 11 sectors, 24 industry groups, 69 industries, \\nand 158 sub-industries. Each sector is defined by a unique text label, as shown in the \\ncolumn named “Sector.”\\nExhibit 1: Equity Sector Classification by GICS\\nSector \\n(Text Label) \\nCode \\n(Numerical Label)\\nEnergy\\n10\\nMaterials\\n15\\nIndustrials\\n20\\nConsumer Discretionary\\n25\\nConsumer Staples\\n30\\nHealth Care\\n35\\nFinancials\\n40\\nInformation Technology\\n45\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n62\\nSector \\n(Text Label) \\nCode \\n(Numerical Label)\\nCommunication Services\\n50\\nUtilities\\n55\\nReal Estate\\n60\\nSource: S&P Global Market Intelligence.\\nText labels are a common format to represent nominal data, but nominal data can also \\nbe coded with numerical labels. As shown below, the column named “Code” contains \\na corresponding GICS code of each sector as a numerical value. However, the nominal \\ndata in numerical format do not indicate ranking, and any arithmetic operations on \\nnominal data are not meaningful. In this example, the energy sector with the code 10 \\ndoes not represent a lower or higher rank than the real estate sector with the code 60. \\nOften, financial models, such as regression models, require input data to be numerical; \\nso, nominal data in the input dataset must be coded numerically before applying an \\nalgorithm (that is, a process for problem solving) for performing the analysis. This \\nwould be mainly to identify the category (here, sector) in the model.\\nOrdinal data are categorical values that can be logically ordered or ranked. For \\nexample, the Morningstar and Standard & Poor’s star ratings for investment funds \\nare ordinal data in which one star represents a group of funds judged to have had \\nrelatively the worst performance, with two, three, four, and five stars representing \\ngroups with increasingly better performance or quality as evaluated by those firms.\\nOrdinal data may also involve numbers to identify categories. For example, in \\nranking growth-oriented investment funds based on their five-year cumulative returns, \\nwe might assign the number 1 to the top performing 10% of funds, the number 2 to \\nnext best performing 10% of funds, and so on; the number 10 represents the bottom \\nperforming 10% of funds. Despite the fact that categories represented by ordinal \\ndata can be ranked higher or lower compared to each other, they do not necessarily \\nestablish a numerical difference between each category. Importantly, such investment \\nfund ranking tells us nothing about the difference in performance between funds \\nranked 1 and 2 compared with the difference in performance between funds ranked \\n3 and 4 or 9 and 10.\\nHaving discussed different data types from a statistical perspective, it is import-\\nant to note that at first glance, identifying data types may seem straightforward. In \\nsome situations, where categorical data are coded in numerical format, they should \\nbe distinguished from numerical data. A sound rule of thumb: Meaningful arithmetic \\noperations can be performed on numerical data but not on categorical data.\\nEXAMPLE 1\\nIdentifying Data Types (I)\\nIdentify the data type for each of the following kinds of investment-related \\ninformation:\\n1. Number of coupon payments for a corporate bond. As background, a corpo-\\nrate bond is a contractual obligation between an issuing corporation (i.e., \\nborrower) and bondholders (i.e., lenders) in which the issuer agrees to pay \\ninterest—in the form of fixed coupon payments—on specified dates, typi-\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Types\\n63\\ncally semi-annually, over the life of the bond (i.e., to its maturity date) and to \\nrepay principal (i.e., the amount borrowed) at maturity.\\nSolution to 1\\nNumber of coupon payments are discrete data. For example, a newly-issued \\n5-year corporate bond paying interest semi-annually (quarterly) will make \\n10 (20) coupon payments during its life. In this case, coupon payments are \\nlimited to a finite number of values; so, they are discrete.\\n2. Cash dividends per share paid by a public company. Note that cash divi-\\ndends are a distribution paid to shareholders based on the number of shares \\nowned.\\nSolution to 2\\nCash dividends per share are continuous data since they can take on any \\nnon-negative values.\\n3. Credit ratings for corporate bond issues. As background, credit ratings \\ngauge the bond issuer’s ability to meet the promised payments on the bond. \\nBond rating agencies typically assign bond issues to discrete categories \\nthat are in descending order of credit quality (i.e., increasing probability of \\nnon-payment or default).\\nSolution to 3\\nCredit ratings are ordinal data. A rating places a bond issue in a category, \\nand the categories are ordered with respect to the expected probability of \\ndefault. But arithmetic operations cannot be done on credit ratings, and the \\ndifference in the expected probability of default between categories of highly \\nrated bonds, for example, is not necessarily equal to that between categories \\nof lowly rated bonds.\\n4. Hedge fund classification types. Note that hedge funds are investment ve-\\nhicles that are relatively unconstrained in their use of debt, derivatives, and \\nlong and short investment strategies. Hedge fund classification types group \\nhedge funds by the kind of investment strategy they pursue.\\nSolution to 4\\nHedge fund classification types are nominal data. Each type groups together \\nhedge funds with similar investment strategies. In contrast to credit ratings \\nfor bonds, however, hedge fund classification schemes do not involve a \\nranking. Thus, such classification schemes are not ordinal data.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Cross-Sectional versus Time-Series versus Panel Data',\n",
       "       'page_number': 73,\n",
       "       'content': 'Cross-Sectional versus Time-Series versus Panel Data\\nAnother data classification standard is based on how data are collected, and it cate-\\ngorizes data into three types: cross-sectional, time series, and panel.\\nPrior to the description of the data types, we need to explain two data-related \\nterminologies: variable and observation. A variable is a characteristic or quantity that \\ncan be measured, counted, or categorized and is subject to change. A variable can also \\nbe called a field, an attribute, or a feature. For example, stock price, market capital-\\nization, dividend and dividend yield, earnings per share (EPS), and price-to-earnings \\nratio (P/E) are basic data variables for the financial analysis of a public company. An \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n64\\nobservation is the value of a specific variable collected at a point in time or over a \\nspecified period of time. For example, last year DEF, Inc. recorded EPS of $7.50. This \\nvalue represented a 15% annual increase.\\nCross-sectional data are a list of the observations of a specific variable from \\nmultiple observational units at a given point in time. The observational units can be \\nindividuals, groups, companies, trading markets, regions, etc. For example, January \\ninflation rates (i.e., the variable) for each of the euro-area countries (i.e., the observa-\\ntional units) in the European Union for a given year constitute cross-sectional data.\\nTime-series data are a sequence of observations for a single observational unit \\nof a specific variable collected over time and at discrete and typically equally spaced \\nintervals of time, such as daily, weekly, monthly, annually, or quarterly. For example, \\nthe daily closing prices (i.e., the variable) of a particular stock recorded for a given \\nmonth constitute time-series data.\\nPanel data are a mix of time-series and cross-sectional data that are frequently \\nused in financial analysis and modeling. Panel data consist of observations through \\ntime on one or more variables for multiple observational units. The observations in \\npanel data are usually organized in a matrix format called a data table. Exhibit 2 is \\nan example of panel data showing quarterly earnings per share (i.e., the variable) for \\nthree companies (i.e., the observational units) in a given year by quarter. Each column \\nis a time series of data that represents the quarterly EPS observations from Q1 to Q4 \\nof a specific company, and each row is cross-sectional data that represent the EPS of \\nall three companies of a particular quarter.\\nExhibit 2: Earnings per Share in Euros of Three Eurozone Companies in a \\nGiven Year\\nTime Period\\nCompany A\\nCompany B\\nCompany C\\nQ1\\n13.53\\n0.84\\n−0.34\\nQ2\\n4.36\\n0.96\\n0.08\\nQ3\\n13.16\\n0.79\\n−2.72\\nQ4\\n12.95\\n0.19\\n0.09\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Structured versus Unstructured Data',\n",
       "       'page_number': 74,\n",
       "       'content': 'Structured versus Unstructured Data\\nCategorizing data into structured and unstructured types is based on whether or not \\nthe data are in a highly organized form.\\nStructured data are highly organized in a pre-defined manner, usually with \\nrepeating patterns. The typical forms of structured data are one-dimensional arrays, \\nsuch as a time series of a single variable, or two-dimensional data tables, where each \\ncolumn represents a variable or an observation unit and each row contains a set of \\nvalues for the same columns. Structured data are relatively easy to enter, store, query, \\nand analyze without much manual processing. Typical examples of structured com-\\npany financial data are:\\n■ \\nMarket data: data issued by stock exchanges, such as intra-day and daily \\nclosing stock prices and trading volumes.\\n■ \\nFundamental data: data contained in financial statements, such as earnings \\nper share, price to earnings ratio, dividend yield, and return on equity.\\n■ \\nAnalytical data: data derived from analytics, such as cash flow projections or \\nforecasted earnings growth.\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Types\\n65\\nUnstructured data, in contrast, are data that do not follow any conventionally \\norganized forms. Some common types of unstructured data are text—such as financial \\nnews, posts in social media, and company filings with regulators—and also audio/\\nvideo, such as managements’ earnings calls and presentations to analysts.\\nUnstructured data are a relatively new classification driven by the rise of alterna-\\ntive data (i.e., data generated from unconventional sources, like electronic devices, \\nsocial media, sensor networks, and satellites, but also by companies in the normal \\ncourse of business) and its growing adoption in the financial industry. Unstructured \\ndata are typically alternative data as they are usually collected from unconventional \\nsources. By indicating the source from which the data are generated, such data can \\nbe classified into three groups:\\n■ \\nProduced by individuals (i.e., via social media posts, web searches, etc.);\\n■ \\nGenerated by business processes (i.e., via credit card transactions, corporate \\nregulatory filings, etc.); and\\n■ \\nGenerated by sensors (i.e., via satellite imagery, foot traffic by mobile \\ndevices, etc.).\\nUnstructured data may offer new market insights not normally contained in data \\nfrom traditional sources and may provide potential sources of returns for investment \\nprocesses. Unlike structured data, however, utilizing unstructured data in investment \\nanalysis is challenging. Typically, financial models are able to take only structured data \\nas inputs; therefore, unstructured data must first be transformed into structured data \\nthat models can process.\\nExhibit 3 shows an excerpt from Form 10-Q (Quarterly Report) filed by Company \\nXYZ with the US Securities and Exchange Commission (SEC) for the fiscal quarter \\nended 31 March 20XX. The form is an unstructured mix of text and tables, so it can-\\nnot be directly used by computers as input to financial models. The SEC has utilized \\neXtensible Business Reporting Language (XBRL) to structure such data. The data \\nextracted from the XBRL submission can be organized into five tab-delimited TXT \\nformat files that contain information about the submission, including taxonomy tags \\n(i.e., financial statement items), dates, units of measure (uom), values (i.e., for the tag \\nitems), and more—making it readable by computer. Exhibit 4 shows an excerpt from \\none of the now structured data tables downloaded from the SEC’s EDGAR (Electronic \\nData Gathering, Analysis, and Retrieval) database.\\nExhibit 3: Excerpt from 10-Q of Company XYZ for Fiscal Quarter Ended 31 \\nMarch 20XX\\nCompany XYZ \\nForm 10-Q \\nFiscal Quarter Ended 31 March 20XX \\nTable of Contents\\nPart I\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nPage\\nItem 1\\nFinancial Statements\\n1\\nItem 2\\nManagement’s Discussion and Analysis of Financial \\nCondition and Results of Operations\\n21\\nItem 3\\nQuantitative and Qualitative Disclosures About Market Risk\\n32\\nItem 4\\nControls and Procedures\\n32\\nPart II\\n\\xa0\\n\\xa0\\nItem 1\\nLegal Proceedings\\n33\\nItem 1A\\nRisk Factors\\n33\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n66\\nItem 2\\nUnregistered Sales of Equity Securities and Use of Proceeds\\n43\\nItem 3\\nDefaults Upon Senior Securities\\n43\\nItem 4\\nMine Safety Disclosures\\n43\\nItem 5\\nOther Information\\n43\\nItem 6\\nExhibits\\n44\\nCondensed Consolidated Statements of Operations (Unaudited) \\n(in millions, except number of shares, which are reflected in thousands and per share \\namounts)\\n\\xa0\\n31 March 20XX\\nNet sales:\\n\\xa0\\n Products\\n$46,565\\n Services\\n11,450\\n  Total net sales\\n58,015\\n\\xa0\\n\\xa0\\nCost of sales:\\n\\xa0\\n Products\\n32,047\\n Services\\n4,147\\n  Total cost of sales\\n36,194\\n   Gross margin\\n21,821\\n\\xa0\\n\\xa0\\nOperating expenses:\\n\\xa0\\n Research and development\\n3,948\\n Selling, general and administrative\\n4,458\\n  Total operating expenses\\n8,406\\n\\xa0\\n\\xa0\\nOperating income\\n13,415\\nOther income/(expense), net\\n378\\nIncome before provision for income taxes\\n13,793\\nProvision for income taxes\\n2,232\\nNet income\\n$11,561\\nSource: EDGAR.\\nExhibit 4: Structured Data Extracted from Form 10-Q of Company XYZ for Fiscal Quarter Ended 31 March \\n20XX\\nadsh\\ntag\\nddate\\nuom\\nvalue\\n0000320193-19-000066\\nRevenueFromContractWithCustomerExcludingAssessedTax\\n20XX0331\\nUSD\\n$58,015,000,000\\n0000320193-19-000066\\nGrossProfit\\n20XX0331\\nUSD\\n$21,821,000,000\\n0000320193-19-000066\\nOperatingExpenses\\n20XX0331\\nUSD\\n$8,406,000,000\\n0000320193-19-000066\\nOperatingIncomeLoss\\n20XX0331\\nUSD\\n$13,415,000,000\\n0000320193-19-000066\\nNetIncomeLoss\\n20XX0331\\nUSD\\n$11,561,000,000\\nSource: EDGAR.\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Types\\n67\\nEXAMPLE 2\\nIdentifying Data Types (II)\\n1. Which of the following is most likely to be structured data?\\nA. Social media posts where consumers are commenting on what they \\nthink of a company’s new product.\\nB. Daily closing prices during the past month for all companies listed on \\nJapan’s Nikkei 225 stock index.\\nC. Audio and video of a CFO explaining her company’s latest earnings \\nannouncement to securities analysts.\\nSolution to 1\\nB is correct as daily closing prices constitute structured data. A is incorrect \\nas social media posts are unstructured data. C is incorrect as audio and \\nvideo are unstructured data.\\n2. Which of the following statements describing panel data is most accurate?\\nA. It is a sequence of observations for a single observational unit of a \\nspecific variable collected over time at discrete and equally spaced \\nintervals.\\nB. It is a list of observations of a specific variable from multiple observa-\\ntional units at a given point in time.\\nC. It is a mix of time-series and cross-sectional data that are frequently \\nused in financial analysis and modeling.\\nSolution to 2\\nC is correct as it most accurately describes panel data. A is incorrect as it \\ndescribes time-series data. B is incorrect as it describes cross-sectional data.\\n3. Which of the following data series is least likely to be sortable by values?\\nA. Daily trading volumes for stocks listed on the Shanghai Stock \\nExchange.\\nB. EPS for a given year for technology companies included in the S&P \\n500 Index.\\nC. Dates of first default on bond payments for a group of bankrupt \\nEuropean manufacturing companies.\\nSolution to 3\\nC is correct as dates are ordinal data that can be sorted by chronological \\norder but not by value. A and B are incorrect as both daily trading volumes \\nand earnings per share (EPS) are numerical data, so they can be sorted by \\nvalues.\\n4. Which of the following best describes a time series?\\nA. Daily stock prices of the XYZ stock over a 60-month period.\\nB. Returns on four-star rated Morningstar investment funds at the end of \\nthe most recent month.\\nC. Stock prices for all stocks in the FTSE100 on 31 December of the most \\nrecent calendar year.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n68\\nSolution to 4\\nA is correct since a time series is a sequence of observations of a spe-\\ncific variable (XYZ stock price) collected over time (60 months) and at \\ndiscrete intervals of time (daily). B and C are both incorrect as they are \\ncross-sectional data.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Data Summarization',\n",
       "       'page_number': 78,\n",
       "       'content': 'Learning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n68\\nSolution to 4\\nA is correct since a time series is a sequence of observations of a spe-\\ncific variable (XYZ stock price) collected over time (60 months) and at \\ndiscrete intervals of time (daily). B and C are both incorrect as they are \\ncross-sectional data.\\nData Summarization\\nGiven the wide variety of possible formats of raw data, which are data available \\nin their original form as collected, such data typically cannot be used by humans \\nor computers to directly extract information and insights. Organizing data into a \\none-dimensional array or a two-dimensional array is typically the first step in data \\nanalytics and modeling. In this section, we will illustrate the construction of these \\ntypical data organization formats. We will also introduce two useful tools that can \\nefficiently summarize one-variable and two-variable data: frequency distributions \\nand contingency tables, respectively. Both of them can give us a quick snapshot of \\nthe data and allow us to find patterns in the data and associations between variables.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Organizing Data for Quantitative Analysis',\n",
       "     'page_number': 78,\n",
       "     'content': 'ORGANIZING DATA FOR QUANTITATIVE ANALYSIS\\ndescribe how data are organized for quantitative analysis\\nQuantitative analysis and modeling typically require input data to be in a clean and \\nformatted form, so raw data are usually not suitable for use directly by analysts. \\nDepending upon the number of variables, raw data can be organized into two typ-\\nical formats for quantitative analysis: one-dimensional arrays and two-dimensional \\nrectangular arrays.\\nA one-dimensional array is the simplest format for representing a collection of \\ndata of the same data type, so it is suitable for representing a single variable. Exhibit \\n5 is an example of a one-dimensional array that shows the closing price for the first \\n10 trading days for ABC Inc. stock after the company went public. Closing prices are \\ntime-series data collected at daily intervals, so it is natural to organize them into a \\ntime-ordered sequence. The time-series format also facilitates future data updates \\nto the existing dataset. In this case, closing prices for future trading sessions can be \\neasily added to the end of the array with no alteration of previously formatted data.\\nMore importantly, in contrast to compiling the data randomly in an unorganized \\nmanner, organizing such data by its time-series nature preserves valuable information \\nbeyond the basic descriptive statistics that summarize central tendency and spread \\nvariation in the data’s distribution. For example, by simply plotting the data against time, \\nwe can learn whether the data demonstrate any increasing or decreasing trends over \\ntime or whether the time series repeats certain patterns in a systematic way over time.\\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nOrganizing Data for Quantitative Analysis\\n69\\nExhibit 5: One-Dimensional Array: Daily Closing Price of \\nABC Inc. Stock\\nObservation by Day\\nStock Price ($)\\n1\\n57.21\\n2\\n58.26\\n3\\n58.64\\n4\\n56.19\\n5\\n54.78\\n6\\n54.26\\n7\\n56.88\\n8\\n54.74\\n9\\n52.42\\n10\\n50.14\\nA two-dimensional rectangular array (also called a data table) is one of the most \\npopular forms for organizing data for processing by computers or for presenting data \\nvisually for consumption by humans. Similar to the structure in an Excel spreadsheet, \\na data table is comprised of columns and rows to hold multiple variables and multiple \\nobservations, respectively. When a data table is used to organize the data of one single \\nobservational unit (i.e., a single company), each column represents a different variable \\n(feature or attribute) of that observational unit, and each row holds an observation \\nfor the different variables; successive rows represent the observations for succes-\\nsive time periods. In other words, observations of each variable are a time-series \\nsequence that is sorted in either ascending or descending time order. Consequently, \\nobservations of different variables must be sorted and aligned to the same time scale. \\nExample 3 shows how to organize a raw dataset for a company collected online into \\na machine-readable data table.\\nEXAMPLE 3\\nOrganizing a Company’s Raw Data into a Data Table\\n1. Suppose you are conducting a valuation analysis of ABC Inc., which has \\nbeen listed on the stock exchange for two years. The metrics to be used in \\nyour valuation include revenue, earnings per share (EPS), and dividends \\npaid per share (DPS). You have retrieved the last two years of ABC’s quar-\\nterly data from the exchange’s website, which is shown in Exhibit 6. The data \\navailable online are pre-organized into a tabular format, where each column \\nrepresents a fiscal year and each row represents a particular quarter with \\nvalues of the three measures clustered together.\\n \\nExhibit 6: Metrics of ABC Inc. Retrieved Online\\n \\n \\nFiscal Quarter \\nYear 1 \\n(Fiscal Year)\\nYear 2 \\n(Fiscal Year)\\nMarch\\n\\xa0\\n\\xa0\\n Revenue\\n$3,784(M)\\n$4,097(M)\\n EPS\\n1.37\\n−0.34\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n70\\nFiscal Quarter \\nYear 1 \\n(Fiscal Year)\\nYear 2 \\n(Fiscal Year)\\n DPS\\nN/A\\nN/A\\nJune\\n\\xa0\\n\\xa0\\n Revenue\\n$4,236(M)\\n$5,905(M)\\n EPS\\n1.78\\n3.89\\n DPS\\nN/A\\n0.25\\nSeptember\\n\\xa0\\n\\xa0\\n Revenue\\n$4,187(M)\\n$4,997(M)\\n EPS\\n−3.38\\n−2.88\\n DPS\\nN/A\\n0.25\\nDecember\\n\\xa0\\n\\xa0\\n Revenue\\n$3,889(M)\\n$4,389(M)\\n EPS\\n−8.66\\n−3.98\\n DPS\\nN/A\\n0.25\\n \\nUse the data to construct a two-dimensional rectangular array (i.e., data \\ntable) with the columns representing the metrics for valuation and the ob-\\nservations arranged in a time-series sequence.\\nSolution:\\nTo construct a two-dimensional rectangular array, we first need to deter-\\nmine the data table structure. The columns have been specified to represent \\nthe three valuation metrics (i.e., variables): revenue, EPS and DPS. The rows \\nshould be the observations for each variable in a time ordered sequence. In \\nthis example, the data for the valuation measures will be organized in the \\nsame quarterly intervals as the raw data retrieved online, starting from Q1 \\nYear 1 to Q4 Year 2. Then, the observations from the original table can be \\nplaced accordingly into the data table by variable name and by filing quarter. \\nExhibit 7 shows the raw data reorganized in the two-dimensional rectangu-\\nlar array (by date and associated valuation metric), which can now be used \\nin financial analysis and is readable by a computer.\\nIt is worth pointing out that in case of missing values while organizing data, \\nhow to handle them depends largely on why the data are missing. In this \\nexample, dividends (DPS) in the first five quarters are missing because ABC \\nInc. did not authorize (and pay) any dividends. So, filling the dividend col-\\numn with zeros is appropriate. If revenue, EPS, and DPS of a given quarter \\nare missing due to particular data source issues, however, these missing \\nvalues cannot be simply replaced with zeros; this action would result in \\nincorrect interpretation. Instead, the missing values might be replaced with \\nthe latest available data or with interpolated values, depending on how the \\ndata will be consumed or modeled.\\n \\nExhibit 7: Data Table for ABC Inc.\\n \\n \\n\\xa0\\nRevenue ($ Million)\\nEPS ($)\\nDPS ($)\\nQ1 Year 1\\n3,784\\n1.37\\n0\\nQ2 Year 1\\n4,236\\n1.78\\n0\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Summarizing Data Using Frequency Distributions',\n",
       "     'page_number': 81,\n",
       "     'content': 'Summarizing Data Using Frequency Distributions\\n71\\n\\xa0\\nRevenue ($ Million)\\nEPS ($)\\nDPS ($)\\nQ3 Year 1\\n4,187\\n−3.38\\n0\\nQ4 Year 1\\n3,889\\n−8.66\\n0\\nQ1 Year 2\\n4,097\\n−0.34\\n0\\nQ2 Year 2\\n5,905\\n3.89\\n0.25\\nQ3 Year 2\\n4,997\\n−2.88\\n0.25\\nQ4 Year 2\\n4,389\\n−3.98\\n0.25\\n \\nSUMMARIZING DATA USING FREQUENCY \\nDISTRIBUTIONS\\ninterpret frequency and related distributions\\nWe now discuss various tabular formats for describing data based on the count of \\nobservations. These tables are a necessary step toward building a true visualization of \\na dataset. Later, we shall see how bar charts, tree-maps, and heat maps, among other \\ngraphic tools, are used to visualize important properties of a dataset.\\nA frequency distribution (also called a one-way table) is a tabular display of \\ndata constructed either by counting the observations of a variable by distinct values \\nor groups or by tallying the values of a numerical variable into a set of numerically \\nordered bins. It is an important tool for initially summarizing data by groups or bins \\nfor easier interpretation.\\nConstructing a frequency distribution of a categorical variable is relatively straight-\\nforward and can be stated in the following two basic steps:\\n1. Count the number of observations for each unique value of the variable.\\n2. Construct a table listing each unique value and the corresponding counts, \\nand then sort the records by number of counts in descending or ascending \\norder to facilitate the display.\\nExhibit 8 shows a frequency distribution of a portfolio’s stock holdings by sectors \\n(the variables), which are defined by GICS. The portfolio contains a total of 479 stocks \\nthat have been individually classified into 11 GICS sectors (first column). The stocks \\nare counted by sector and are summarized in the second column, absolute frequency. \\nThe absolute frequency, or simply the raw frequency, is the actual number of obser-\\nvations counted for each unique value of the variable (i.e., each sector). Often it is \\ndesirable to express the frequencies in terms of percentages, so we also show the rel-\\native frequency (in the third column), which is calculated as the absolute frequency \\nof each unique value of the variable divided by the total number of observations. The \\nrelative frequency provides a normalized measure of the distribution of the data, \\nallowing comparisons between datasets with different numbers of total observations.\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n72\\nExhibit 8: Frequency Distribution for a Portfolio by Sector\\nSector \\n(Variable)\\nAbsolute  \\nFrequency\\nRelative  \\nFrequency\\nIndustrials\\n73\\n15.2%\\nInformation Technology\\n69\\n14.4%\\nFinancials\\n67\\n14.0%\\nConsumer Discretionary\\n62\\n12.9%\\nHealth Care\\n54\\n11.3%\\nConsumer Staples\\n33\\n6.9%\\nReal Estate\\n30\\n6.3%\\nEnergy\\n29\\n6.1%\\nUtilities\\n26\\n5.4%\\nMaterials\\n26\\n5.4%\\nCommunication Services\\n10\\n2.1%\\nTotal\\n479\\n100.0%\\nA frequency distribution table provides a snapshot of the data, and it facilitates finding \\npatterns. Examining the distribution of absolute frequency in Exhibit 8, we see that \\nthe largest number of stocks (73), accounting for 15.2% of the stocks in the portfolio, \\nare held in companies in the industrials sector. The sector with the least number of \\nstocks (10) is communication services, which represents just 2.1% of the stocks in \\nthe portfolio.\\nIt is also easy to see that the top four sectors (i.e., industrials, information tech-\\nnology, financials, and consumer discretionary) have very similar relative frequencies, \\nbetween 15.2% and 12.9%. Similar relative frequencies, between 6.9% and 5.4%, are also \\nseen among several other sectors. Note that the absolute frequencies add up to the \\ntotal number of stocks in the portfolio (479), and the sum of the relative frequencies \\nshould be equal to 100%.\\nFrequency distributions also help in the analysis of large amounts of numerical \\ndata. The procedure for summarizing numerical data is a bit more involved than that \\nfor summarizing categorical data because it requires creating non-overlapping bins \\n(also called intervals or buckets) and then counting the observations falling into each \\nbin. One procedure for constructing a frequency distribution for numerical data can \\nbe stated as follows:\\n1. Sort the data in ascending order.\\n2. Calculate the range of the data, defined as Range = Maximum value − \\nMinimum value.\\n3. Decide on the number of bins (k) in the frequency distribution.\\n4. Determine bin width as Range/k.\\n5. Determine the first bin by adding the bin width to the minimum value. \\nThen, determine the remaining bins by successively adding the bin width to \\nthe prior bin’s end point and stopping after reaching a bin that includes the \\nmaximum value.\\n6. Determine the number of observations falling into each bin by counting the \\nnumber of observations whose values are equal to or exceed the bin mini-\\nmum value yet are less than the bin’s maximum value. The exception is in \\n© CFA Institute. For candidate use only. Not for distribution.\\nSummarizing Data Using Frequency Distributions\\n73\\nthe last bin, where the maximum value is equal to the last bin’s maximum, \\nand therefore, the observation with the maximum value is included in this \\nbin’s count.\\n7. Construct a table of the bins listed from smallest to largest that shows the \\nnumber of observations falling into each bin.\\nIn Step 4, when rounding the bin width, round up (rather than down) to ensure \\nthat the final bin includes the maximum value of the data.\\nThese seven steps are basic guidelines for constructing frequency distributions. In \\npractice, however, we may want to refine the above basic procedure. For example, we \\nmay want the bins to begin and end with whole numbers for ease of interpretation. \\nAnother practical refinement that promotes interpretation is to start the first bin at \\nthe nearest whole number below the minimum value.\\nAs this procedure implies, a frequency distribution groups data into a set of bins, \\nwhere each bin is defined by a unique set of values (i.e., beginning and ending points). \\nEach observation falls into only one bin, and the total number of bins covers all the \\nvalues represented in the data. The frequency distribution is the list of the bins together \\nwith the corresponding measures of frequency.\\nTo illustrate the basic procedure, suppose we have 12 observations sorted in \\nascending order (Step 1):\\n−4.57, −4.04, −1.64, 0.28, 1.34, 2.35, 2.38, 4.28, 4.42, 4.68, 7.16, and 11.43.\\nThe minimum observation is −4.57, and the maximum observation is +11.43. So, \\nthe range is +11.43 − (−4.57) = 16 (Step 2).\\nIf we set k = 4 (Step 3), then the bin width is 16/4 = 4 (Step 4).\\nExhibit 9 shows the repeated addition of the bin width of 4 to determine the end-\\npoint for each of the bins (Step 5).\\nExhibit 9: Determining Endpoints of the Bins\\n−4.57\\n+\\n4.0\\n=\\n−0.57\\n−0.57\\n+\\n4.0\\n=\\n3.43\\n3.43\\n+\\n4.0\\n=\\n7.43\\n7.40\\n+\\n4.0\\n=\\n11.43\\nThus, the bins are [−4.57 to −0.57), [−0.57 to 3.43), [3.43 to 7.43), and [7.43 to 11.43], \\nwhere the notation [−4.57 to −0.57) indicates −4.57 ≤ observation < −0.57. The \\nparentheses indicate that the endpoints are not included in the bins, and the square \\nbrackets indicate that the beginning points and the last endpoint are included in the \\nbin. Exhibit 10 summarizes Steps 5 through 7.\\nExhibit 10: Frequency Distribution\\nBin\\n\\xa0\\n\\xa0\\n\\xa0\\nAbsolute Frequency\\nA\\n−4.57\\n≤ observation <\\n−0.57\\n3\\nB\\n−0.57\\n≤ observation <\\n3.43\\n4\\nC\\n3.43\\n≤ observation <\\n7.43\\n4\\nD\\n7.43\\n≤ observation ≤\\n11.43\\n1\\nNote that the bins do not overlap, so each observation can be placed uniquely into \\none bin, and the last bin includes the maximum value.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n74\\nWe turn to these issues in discussing the construction of frequency distributions \\nfor daily returns of the fictitious Euro-Asia-Africa (EAA) Equity Index. The dataset \\nof daily returns of the EAA Equity Index spans a five-year period and consists of \\n1,258 observations with a minimum value of −4.1% and a maximum value of 5.0%. \\nThus, the range of the data is 5% − (−4.1%) = 9.1%, approximately. [The mean daily \\nreturn—mean as a measure of central tendency will be discussed shortly—is 0.04%.]\\nThe decision on the number of bins (k) into which we should group the observa-\\ntions often involves inspecting the data and exercising judgment. How much detail \\nshould we include? If we use too few bins, we will summarize too much and may lose \\npertinent characteristics. Conversely, if we use too many bins, we may not summarize \\nenough and may introduce unnecessary noise.\\nWe can establish an appropriate value for k by evaluating the usefulness of the \\nresulting bin width. A large number of empty bins may indicate that we are attempting \\nto over-organize the data to present too much detail. Starting with a relatively small \\nbin width, we can see whether or not the bins are mostly empty and whether or not \\nthe value of k associated with that bin width is too large. If the bins are mostly empty, \\nimplying that k is too large, we can consider increasingly larger bins (i.e., smaller values \\nof k) until we have a frequency distribution that effectively summarizes the distribution.\\nSuppose that for ease of interpretation we want to use a bin width stated in whole \\nrather than fractional percentages. In the case of the daily EAA Equity Index returns, \\na 1% bin width would be associated with 9.1/1 = 9.1 bins, which can be rounded up to \\nk = 10 bins. That number of bins will cover a range of 1% × 10 = 10%. By constructing \\nthe frequency distribution in this manner, we will also have bins that end and begin \\nat a value of 0%, thereby allowing us to count the negative and positive returns in the \\ndata. Without too much work, we have found an effective way to summarize the data.\\nExhibit 11 shows the frequency distribution for the daily returns of the EAA Equity \\nIndex using return bins of 1%, where the first bin includes returns from −5.0% to −4.0% \\n(exclusive, meaning < −4%) and the last bin includes daily returns from 4.0% to 5.0% \\n(inclusive, meaning ≤ 5%). Note that to facilitate interpretation, the first bin starts at \\nthe nearest whole number below the minimum value (so, at −5.0%).\\nExhibit 11 includes two other useful ways to present the data (which can be com-\\nputed in a straightforward manner once we have established the absolute and relative \\nfrequency distributions): the cumulative absolute frequency and the cumulative relative \\nfrequency. The cumulative absolute frequency cumulates (meaning, adds up) the \\nabsolute frequencies as we move from the first bin to the last bin. Similarly, the cumu-\\nlative relative frequency is a sequence of partial sums of the relative frequencies. For \\nthe last bin, the cumulative absolute frequency will equal the number observations in \\nthe dataset (1,258), and the cumulative relative frequency will equal 100%.\\nExhibit 11: Frequency Distribution for Daily Returns of EAA Equity Index\\nReturn  \\nBin \\n(%)\\nAbsolute \\nFrequency\\nRelative \\nFrequency \\n(%)\\nCumulative \\nAbsolute \\nFrequency\\nCumulative \\nRelative \\nFrequency (%)\\n−5.0 to −4.0\\n1\\n0.08\\n1\\n0.08\\n−4.0 to −3.0\\n7\\n0.56\\n8\\n0.64\\n−3.0 to −2.0\\n23\\n1.83\\n31\\n2.46\\n−2.0 to −1.0\\n77\\n6.12\\n108\\n8.59\\n−1.0 to 0.0\\n470\\n37.36\\n578\\n45.95\\n0.0 to 1.0\\n555\\n44.12\\n1,133\\n90.06\\n1.0 to 2.0\\n110\\n8.74\\n1,243\\n98.81\\n© CFA Institute. For candidate use only. Not for distribution.\\nSummarizing Data Using Frequency Distributions\\n75\\nReturn  \\nBin \\n(%)\\nAbsolute \\nFrequency\\nRelative \\nFrequency \\n(%)\\nCumulative \\nAbsolute \\nFrequency\\nCumulative \\nRelative \\nFrequency (%)\\n2.0 to 3.0\\n13\\n1.03\\n1,256\\n99.84\\n3.0 to 4.0\\n1\\n0.08\\n1,257\\n99.92\\n4.0 to 5.0\\n1\\n0.08\\n1,258\\n100.00\\nAs Exhibit 11 shows, the absolute frequencies vary widely, ranging from 1 to 555. The \\nbin encompassing returns between 0% and 1% has the most observations (555), and \\nthe corresponding relative frequency tells us these observations account for 44.12% of \\nthe total number of observations. The frequency distribution gives us a sense of not \\nonly where most of the observations lie but also whether the distribution is evenly \\nspread. It is easy to see that the vast majority of observations (37.36% + 44.12% = \\n81.48%) lie in the middle two bins spanning −1% to 1%. We can also see that not many \\nobservations are greater than 3% or less than −4%. Moreover, as there are bins with 0% \\nas ending or beginning points, we are able to count positive and negative returns in \\nthe data. Looking at the cumulative relative frequency in the last column, we see that \\nthe bin of −1% to 0% shows a cumulative relative frequency of 45.95%. This indicates \\nthat 45.95% of the observations lie below the daily return of 0% and that 54.05% of \\nthe observations are positive daily returns.\\nIt is worth noting that other than being summarized in tables, frequency distri-\\nbutions also can be effectively represented in visuals, which will be discussed shortly \\nin the section on data visualization.\\nEXAMPLE 4\\nConstructing a Frequency Distribution of Country Index \\nReturns\\n1. Suppose we have the annual equity index returns of a given year for 18 dif-\\nferent countries, as shown in Exhibit 12, and we are asked to summarize the \\ndata.\\n \\nExhibit 12: Annual Equity Index Returns for 18 Countries\\n \\n \\nMarket\\nIndex Return (%)\\nCountry A\\n7.7\\nCountry B\\n8.5\\nCountry C\\n9.1\\nCountry D\\n5.5\\nCountry E\\n7.1\\nCountry F\\n9.9\\nCountry G\\n6.2\\nCountry H\\n6.8\\nCountry I\\n7.5\\nCountry J\\n8.9\\nCountry K\\n7.4\\nCountry L\\n8.6\\nCountry M\\n9.6\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n76\\nMarket\\nIndex Return (%)\\nCountry N\\n7.7\\nCountry O\\n6.8\\nCountry P\\n6.1\\nCountry Q\\n8.8\\nCountry R\\n7.9\\n \\nConstruct a frequency distribution table from these data and state some key \\nfindings from the summarized data.\\nSolution:\\nThe first step in constructing a frequency distribution table is to sort the \\nreturn data in ascending order:\\n \\nMarket \\nIndex Return (%)\\nCountry D\\n5.5\\nCountry P\\n6.1\\nCountry G\\n6.2\\nCountry H\\n6.8\\nCountry O\\n6.8\\nCountry E\\n7.1\\nCountry K\\n7.4\\nCountry I\\n7.5\\nCountry A\\n7.7\\nCountry N\\n7.7\\nCountry R\\n7.9\\nCountry B\\n8.5\\nCountry L\\n8.6\\nCountry Q\\n8.8\\nCountry J\\n8.9\\nCountry C\\n9.1\\nCountry M\\n9.6\\nCountry F\\n9.9\\n \\nThe second step is to calculate the range of the data, which is 9.9% − 5.5% = \\n4.4%.\\nThe third step is to decide on the number of bins. Here, we will use k = 5.\\nThe fourth step is to determine the bin width. Here, it is 4.4%/5 = 0.88%, \\nwhich we will round up to 1.0%.\\nThe fifth step is to determine the bins, which are as follows:\\n5.0% + 1.0% = 6.0%\\n6.0% + 1.0% = 7.0%\\n7.0% + 1.0% = 8.0%\\n8.0% + 1.0% = 9.0%\\n9.0% + 1.0% = 10.0%\\nFor ease of interpretation, the first bin is set to begin with the nearest whole \\nnumber (5.0%) below the minimum value (5.5%) of the data series.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Summarizing Data Using a Contingency Table',\n",
       "     'page_number': 87,\n",
       "     'content': 'Summarizing Data Using a Contingency Table\\n77\\nThe sixth step requires counting the return observations falling into each \\nbin, and the seventh (last) step is use these results to construct the final \\nfrequency distribution table.\\nExhibit 13 presents the frequency distribution table, which summarizes \\nthe data in Exhibit 12 into five bins spanning 5% to 10%. Note that with 18 \\ncountries, the relative frequency for one observation is calculated as 1/18 = \\n5.56%.\\n \\nExhibit 13: Frequency Distribution of Equity Index Returns\\n \\n \\nReturn \\nBin (%)\\nAbsolute \\nFrequency\\nRelative \\nFrequency (%)\\nCumulative  \\nAbsolute  \\nFrequency\\nCumulative \\nRelative  \\nFrequency (%)\\n5.0 to 6.0\\n1\\n5.56\\n1\\n5.56\\n6.0 to 7.0\\n4\\n22.22\\n5\\n27.78\\n7.0 to 8.0\\n6\\n33.33\\n11\\n61.11\\n8.0 to 9.0\\n4\\n22.22\\n15\\n83.33\\n9.0 to 10.0\\n3\\n16.67\\n18\\n100.00\\n \\nAs Exhibit 13 shows, there is substantial variation in these equity index re-\\nturns. One-third of the observations fall in the 7.0 to 8.0% bin, making it the \\nbin with the most observations. Both the 6.0 to 7.0% bin and the 8.0 to 9.0% \\nbin hold four observations each, accounting for 22.22% of the total number \\nof the observations, respectively. The two remaining bins have fewer obser-\\nvations, one or three observations, respectively.\\nSUMMARIZING DATA USING A CONTINGENCY TABLE\\ninterpret a contingency table\\nWe have shown that the frequency distribution table is a powerful tool to summarize \\ndata for one variable. How can we summarize data for two variables simultaneously? \\nA contingency table provides a solution to this question.\\nA contingency table is a tabular format that displays the frequency distributions \\nof two or more categorical variables simultaneously and is used for finding patterns \\nbetween the variables. A contingency table for two categorical variables is also known \\nas a two-way table. Contingency tables are constructed by listing all the levels (i.e., \\ncategories) of one variable as rows and all the levels of the other variable as columns \\nin the table. A contingency table having R levels of one variable in rows and C levels \\nof the other variable in columns is referred to as an R × C table. Note that each vari-\\nable in a contingency table must have a finite number of levels, which can be either \\nordered (ordinal data) or unordered (nominal data). Importantly, the data displayed \\nin the cells of the contingency table can be either a frequency (count) or a relative \\nfrequency (percentage) based on either overall total, row totals, or column totals.\\nExhibit 14 presents a 5 × 3 contingency table that summarizes the number of \\nstocks (i.e., frequency) in a particular portfolio of 1,000 stocks by two variables, sec-\\ntor and company market capitalization. Sector has five levels, with each one being a \\nGICS-defined sector. Market capitalization (commonly referred to as “market cap”) is \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n78\\ndefined for a company as the number of shares outstanding times the price per share. \\nThe stocks in this portfolio are categorized by three levels of market capitalization: \\nlarge cap, more than $10 billion; mid cap, $10 billion to $2 billion; and small cap, less \\nthan $2 billion.\\nExhibit 14: Portfolio Frequencies by Sector and Market Capitalization\\n\\xa0\\nMarket Capitalization Variable \\n(3 Levels)\\n\\xa0\\nSector Variable (5 Levels)\\nSmall\\nMid\\nLarge\\nTotal\\nCommunication Services\\n55 \\n35 \\n 20\\n 110\\nConsumer Staples\\n 50\\n 30\\n 30\\n110 \\nEnergy\\n 175\\n 95\\n 20\\n 290\\nHealth Care\\n275 \\n 105\\n 55\\n 435\\nUtilities\\n20 \\n 25\\n 10\\n 55\\nTotal\\n 575\\n 290\\n 135\\n 1,000\\nThe entries in the cells of the contingency table show the number of stocks of each \\nsector with a given level of market cap. For example, there are 275 small-cap health \\ncare stocks, making it the portfolio’s largest subgroup in terms of frequency. These \\ndata are also called joint frequencies because you are joining one variable from the \\nrow (i.e., sector) and the other variable from the column (i.e., market cap) to count \\nobservations. The joint frequencies are then added across rows and across columns, \\nand these corresponding sums are called marginal frequencies. For example, the \\nmarginal frequency of health care stocks in the portfolio is the sum of the joint fre-\\nquencies across all three levels of market cap, so 435 (= 275 + 105 + 55). Similarly, \\nadding the joint frequencies of small-cap stocks across all five sectors gives the marginal \\nfrequency of small-cap stocks of 575 (= 55 + 50 + 175 + 275 + 20).\\nClearly, health care stocks and small-cap stocks have the largest marginal frequen-\\ncies among sector and market cap, respectively, in this portfolio. Note the marginal \\nfrequencies represent the frequency distribution for each variable. Finally, the marginal \\nfrequencies for each variable must sum to the total number of stocks (overall total) \\nin the portfolio—here, 1,000 (shown in the lower right cell).\\nSimilar to the one-way frequency distribution table, we can express frequency in \\npercentage terms as relative frequency by using one of three options. We can divide \\nthe joint frequencies by: a) the total count; b) the marginal frequency on a row; or c) \\nthe marginal frequency on a column.\\nExhibit 15 shows the contingency table using relative frequencies based on total \\ncount. It is readily apparent that small-cap health care and energy stocks comprise the \\nlargest portions of the total portfolio, at 27.5% (= 275/1,000) and 17.5% (= 175/1,000), \\nrespectively, followed by mid-cap health care and energy stocks, at 10.5% and 9.5%, \\nrespectively. Together, these two sectors make up nearly three-quarters of the portfolio \\n(43.5% + 29.0% = 72.5%).\\n© CFA Institute. For candidate use only. Not for distribution.\\nSummarizing Data Using a Contingency Table\\n79\\nExhibit 15: Relative Frequencies as Percentage of Total\\n\\xa0\\nMarket Capitalization Variable \\n(3 Levels)\\n\\xa0\\nSector Variable (5 Levels)\\nSmall\\nMid\\nLarge\\nTotal\\nCommunication Services\\n5.5% \\n3.5% \\n 2.0%\\n 11.0%\\nConsumer Staples\\n 5.0%\\n 3.0%\\n 3.0%\\n11.0% \\nEnergy\\n 17.5%\\n 9.5%\\n 2.0%\\n 29.0%\\nHealth Care\\n27.5%\\n 10.5%\\n 5.5%\\n 43.5%\\nUtilities\\n2.0% \\n 2.5%\\n 1.0%\\n 5.5%\\nTotal\\n 57.5%\\n 29.0%\\n 13.5%\\n 100%\\nExhibit 16 shows relative frequencies based on marginal frequencies of market cap \\n(i.e., columns). From this perspective, it is clear that the health care and energy sectors \\ndominate the other sectors at each level of market capitalization: 78.3% (= 275/575 + \\n175/575), 69.0% (= 105/290 + 95/290), and 55.6% (= 55/135 + 20/135), for small, mid, \\nand large caps, respectively. Note that there may be a small rounding error difference \\nbetween these results and the numbers shown in Exhibit 15.\\nExhibit 16: Relative Frequencies: Sector as Percentage of Market Cap\\n\\xa0\\nMarket Capitalization Variable \\n(3 Levels)\\n\\xa0\\nSector Variable (5 Levels)\\nSmall\\nMid\\nLarge\\nTotal\\nCommunication Services\\n9.6% \\n12.1% \\n14.8%\\n11.0%\\nConsumer Staples\\n8.7%\\n10.3%\\n22.2%\\n11.0%\\nEnergy\\n30.4%\\n32.8%\\n14.8%\\n29.0%\\nHealth Care\\n47.8%\\n36.2%\\n40.7%\\n43.5%\\nUtilities\\n3.5% \\n8.6%\\n7.4%\\n5.5%\\nTotal\\n 100.0%\\n100.0%\\n100.0%\\n100.0%\\nIn conclusion, the findings from these contingency tables using frequencies and \\nrelative frequencies indicate that in terms of the number of stocks, the portfolio can \\nbe generally described as a small- to mid-cap-oriented health care and energy sector \\nportfolio that also includes stocks of several other defensive sectors.\\nAs an analytical tool, contingency tables can be used in different applications. One \\napplication is for evaluating the performance of a classification model (in this case, \\nthe contingency table is called a confusion matrix). Suppose we have a model for \\nclassifying companies into two groups: those that default on their bond payments and \\nthose that do not default. The confusion matrix for displaying the model’s results will \\nbe a 2 × 2 table showing the frequency of actual defaults versus the model’s predicted \\nfrequency of defaults. Exhibit 17 shows such a confusion matrix for a sample of 2,000 \\nnon-investment-grade bonds. Using company characteristics and other inputs, the \\nmodel correctly predicts 300 cases of bond defaults and 1,650 cases of no defaults.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n80\\nExhibit 17: Confusion Matrix for Bond Default Prediction Model\\nPredicted\\nActual Default\\n\\xa0\\nDefault\\nYes\\nNo\\nTotal\\nYes\\n300\\n40\\n340\\nNo\\n10\\n1,650\\n1,660\\nTotal\\n310\\n1,690\\n2,000\\nWe can also observe that this classification model incorrectly predicts default in 40 \\ncases where no default actually occurred and also incorrectly predicts no default in \\n10 cases where default actually did occur. Later in the CFA Program curriculum you \\nwill learn how to construct a confusion matrix, how to calculate related model per-\\nformance metrics, and how to use them to evaluate and tune a classification model.\\nAnother application of contingency tables is to investigate potential association \\nbetween two categorical variables. For example, revisiting Exhibit 14, one may ask \\nwhether the distribution of stocks by sectors is independent of the levels of market \\ncapitalization? Given the dominance of small-cap and mid-cap health care and energy \\nstocks, the answer is likely, no.\\nOne way to test for a potential association between categorical variables is to per-\\nform a chi-square test of independence. Essentially, the procedure involves using \\nthe marginal frequencies in the contingency table to construct a table with expected \\nvalues of the observations. The actual values and expected values are used to derive \\nthe chi-square test statistic. This test statistic is then compared to a value from the \\nchi-square distribution for a given level of significance. If the test statistic is greater \\nthan the chi-square distribution value, then there is evidence to reject the claim of \\nindependence, implying a significant association exists between the categorical vari-\\nables. The following example describes how a contingency table is used to set up this \\ntest of independence.\\nEXAMPLE 5\\nContingency Tables and Association between Two \\nCategorical Variables\\nSuppose we randomly pick 315 investment funds and classify them two ways: \\nby fund style, either a growth fund or a value fund; and by risk level, either \\nlow risk or high risk. Growth funds primarily invest in stocks whose earnings \\nare expected to grow at a faster rate than earnings for the broad stock market. \\nValue funds primarily invest in stocks that appear to be undervalued relative to \\ntheir fundamental values. Risk here refers to volatility in the return of a given \\ninvestment fund, so low (high) volatility implies low (high) risk. The data are \\nsummarized in a 2 × 2 contingency table shown in Exhibit 18.\\n \\nExhibit 18: Contingency Table by Investment Fund Style and Risk \\nLevel\\n \\n \\n\\xa0\\nLow Risk\\nHigh Risk\\nGrowth\\n73\\n26\\nValue\\n183\\n33\\n© CFA Institute. For candidate use only. Not for distribution.\\nSummarizing Data Using a Contingency Table\\n81\\n \\n1. Calculate the number of growth funds and number of value funds out of the \\ntotal funds.\\nSolution to 1\\nThe task is to calculate the marginal frequencies by fund style, which is done \\nby adding joint frequencies across the rows. Therefore, the marginal fre-\\nquency for growth is 73 + 26 = 99, and the marginal frequency for value is \\n183 + 33 = 216.\\n2. Calculate the number of low-risk and high-risk funds out of the total funds.\\nSolution to 2\\nThe task is to calculate the marginal frequencies by fund risk, which is done \\nby adding joint frequencies down the columns. Therefore, the marginal fre-\\nquency for low risk is 73 + 183 = 256, and the marginal frequency for high \\nrisk is 26 + 33 = 59.\\n3. Describe how the contingency table is used to set up a test for independence \\nbetween fund style and risk level.\\nSolution to 3\\nBased on the procedure mentioned for conducting a chi-square test of inde-\\npendence, we would perform the following three steps.\\nStep 1: Add the marginal frequencies and overall total to the contingency ta-\\nble. We have also included the relative frequency table for observed values.\\n \\nExhibit 19: Observed Marginal Frequencies and Relative \\nFrequencies\\n \\n \\nObserved Values\\n\\xa0\\nObserved Values\\n\\xa0\\nLow \\nRisk\\nHigh \\nRisk\\n\\xa0\\n\\xa0\\n\\xa0\\nLow \\nRisk\\nHigh \\nRisk\\n\\xa0\\nGrowth\\n73\\n26\\n99\\n\\xa0 Growth\\n74%\\n26%\\n100%\\nValue\\n183\\n33\\n216\\n\\xa0 Value\\n85%\\n15%\\n100%\\n\\xa0\\n256\\n59\\n315\\n\\xa0 \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\nStep 2: Use the marginal frequencies in the contingency table to construct \\na table with expected values of the observations. To determine expected \\nvalues for each cell, multiply the respective row total by the respective \\ncolumn total, then divide by the overall total. So, for celli,j (in ith row and jth \\ncolumn):\\n Expected Valuei,j = (Total Row i × Total Column j)/Overall Total \\n(1)\\nFor example,\\n Expected value for Growth/Low Risk is: (99 × 256)/ 315 = 80.46; and\\n Expected value for Value/High Risk is: (216 × 59) / 315 = 40.46.\\nThe table of expected values (and accompanying relative frequency table) \\nare:\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n82\\nExhibit 20: Expected Marginal Frequencies and Relative \\nFrequencies\\n \\n \\nObserved Values\\n\\xa0\\nObserved Values\\n\\xa0\\nLow \\nRisk\\nHigh \\nRisk\\n\\xa0\\n\\xa0\\n\\xa0\\nLow \\nRisk\\nHigh \\nRisk\\n\\xa0\\nGrowth\\n80.457\\n18.543\\n99\\n\\xa0 Growth\\n81%\\n19%\\n100%\\nValue\\n175.543\\n40.457\\n216\\n\\xa0 Value\\n81%\\n19%\\n100%\\n\\xa0\\n256\\n59\\n315\\n\\xa0 \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\nStep 3: Use the actual values and the expected values of observation counts \\nto derive the chi-square test statistic, which is then compared to a value \\nfrom the chi-square distribution for a given level of significance. If the test \\nstatistic is greater than the chi-square distribution value, then there is evi-\\ndence of a significant association between the categorical variables.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Data Visualization',\n",
       "     'page_number': 92,\n",
       "     'content': 'Learning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n82\\nExhibit 20: Expected Marginal Frequencies and Relative \\nFrequencies\\n \\n \\nObserved Values\\n\\xa0\\nObserved Values\\n\\xa0\\nLow \\nRisk\\nHigh \\nRisk\\n\\xa0\\n\\xa0\\n\\xa0\\nLow \\nRisk\\nHigh \\nRisk\\n\\xa0\\nGrowth\\n80.457\\n18.543\\n99\\n\\xa0 Growth\\n81%\\n19%\\n100%\\nValue\\n175.543\\n40.457\\n216\\n\\xa0 Value\\n81%\\n19%\\n100%\\n\\xa0\\n256\\n59\\n315\\n\\xa0 \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\nStep 3: Use the actual values and the expected values of observation counts \\nto derive the chi-square test statistic, which is then compared to a value \\nfrom the chi-square distribution for a given level of significance. If the test \\nstatistic is greater than the chi-square distribution value, then there is evi-\\ndence of a significant association between the categorical variables.\\nDATA VISUALIZATION\\ndescribe ways that data may be visualized and evaluate uses of \\nspecific visualizations\\ndescribe how to select among visualization types\\nVisualization is the presentation of data in a pictorial or graphical format for the \\npurpose of increasing understanding and for gaining insights into the data. As has \\nbeen said, “a picture is worth a thousand words.” In this section, we discuss a variety \\nof charts that are useful for understanding distributions, making comparisons, and \\nexploring potential relationships among data. Specifically, we will cover visualizing \\nfrequency distributions of numerical and categorical data by using plots that represent \\nmulti-dimensional data for discovering relationships and by interpreting visuals that \\ndisplay unstructured data.\\n',\n",
       "     'children': [{'title': 'Histogram and Frequency Polygon',\n",
       "       'page_number': 92,\n",
       "       'content': 'Histogram and Frequency Polygon\\nA histogram is a chart that presents the distribution of numerical data by using the \\nheight of a bar or column to represent the absolute frequency of each bin or interval \\nin the distribution.\\nTo construct a histogram from a continuous variable, we first need to split the \\ndata into bins and summarize the data into a frequency distribution table, such as \\nthe one we constructed in Exhibit 11. In a histogram, the y-axis generally represents \\nthe absolute frequency or the relative frequency in percentage terms, while the x-axis \\nusually represents the bins of the variable. Using the frequency distribution table in \\nExhibit 11, we plot the histogram of daily returns of the EAA Equity Index, as shown \\nin Exhibit 21. The bars are of equal width, representing the bin width of 1% for each \\nreturn interval. The bars are usually drawn with no spaces in between, but small gaps \\ncan also be added between adjacent bars to increase readability, as in this exhibit. In \\nthis case, the height of each bar represents the absolute frequency for each return \\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n83\\nbin. A quick glance can tell us that the return bin 0% to 1% (exclusive) has the highest \\nfrequency, with more than 500 observations (555, to be exact), and it is represented \\nby the tallest bar in the histogram.\\nAn advantage of the histogram is that it can effectively present a large amount of \\nnumerical data that has been grouped into a frequency distribution and can allow a \\nquick inspection of the shape, center, and spread of the distribution to better under-\\nstand it. For example, in Exhibit 21, despite the histogram of daily EAA Equity Index \\nreturns appearing bell-shaped and roughly symmetrical, most bars to the right side \\nof the origin (i.e., zero) are taller than those on the left side, indicating that more \\nobservations lie in the bins in positive territory. Remember that in the earlier dis-\\ncussion of this return distribution, it was noted that 54.1% of the observations are \\npositive daily returns.\\nAs mentioned, histograms can also be created with relative frequencies—the choice \\nof using absolute versus relative frequency depends on the question being answered. \\nAn absolute frequency histogram best answers the question of how many items are \\nin each bin, while a relative frequency histogram gives the proportion or percentage \\nof the total observations in each bin.\\nExhibit 21: Histogram Overlaid with Frequency Polygon for Daily Returns of \\nEAA Equity Index\\nFrequency\\n500\\n400\\n300\\n200\\n100\\n0\\n–2\\n–5\\n–4\\n–3\\n–1\\n0\\n2\\n3\\n4\\n1\\n5\\nIndex Return (%)\\nAnother graphical tool for displaying frequency distributions is the frequency poly-\\ngon. To construct a frequency polygon, we plot the midpoint of each return bin on \\nthe x-axis and the absolute frequency for that bin on the y-axis. We then connect \\nneighboring points with a straight line. Exhibit 21 shows the frequency polygon \\nthat overlays the histogram. In the graph, for example, the return interval 1% to 2% \\n(exclusive) has a frequency of 110, so we plot the return-interval midpoint of 0.5% \\n(which is 1.50% on the x-axis) and a frequency of 110 (on the y-axis). Importantly, \\nthe frequency polygon can quickly convey a visual understanding of the distribution \\nsince it displays frequency as an area under the curve.\\nAnother form for visualizing frequency distributions is the cumulative frequency \\ndistribution chart. Such a chart can plot either the cumulative absolute frequency or \\nthe cumulative relative frequency on the y-axis against the upper limit of the interval. \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n84\\nThe cumulative frequency distribution chart allows us to see the number or the per-\\ncentage of the observations that lie below a certain value. To construct the cumulative \\nfrequency distribution, we graph the returns in the fourth (i.e., Cumulative Absolute \\nFrequency) or fifth (i.e., Cumulative Relative Frequency) column of Exhibit 11 against \\nthe upper limit of each return interval.\\nExhibit 22 presents the graph of the cumulative absolute frequency distribution \\nfor the daily returns on the EAA Equity Index. Notice that the cumulative distribution \\ntends to flatten out when returns are extremely negative or extremely positive because \\nthe frequencies in these bins are quite small. The steep slope in the middle of Exhibit \\n22 reflects the fact that most of the observations—[(470 + 555)/1,258], or 81.5%—lie \\nin the neighborhood of −1.0% to 1.0%.\\nExhibit 22: Cumulative Absolute Frequency Distribution of Daily Returns of \\nEAA Equity Index\\nCumulative Frequency\\n1,200\\n1,000\\n800\\n600\\n400\\n200\\n0\\n–2\\n–4\\n–3\\n–1\\n0\\n2\\n3\\n5\\n1\\n4\\nIndex Return (%)\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Bar Chart',\n",
       "       'page_number': 94,\n",
       "       'content': 'Bar Chart\\nAs we have demonstrated, the histogram is an efficient graphical tool to present the \\nfrequency distribution of numerical data. The frequency distribution of categorical \\ndata can be plotted in a similar type of graph called a bar chart. In a bar chart, each \\nbar represents a distinct category, with the bar’s height proportional to the frequency \\nof the corresponding category.\\nSimilar to plotting a histogram, the construction of a bar chart with one categorical \\nvariable first requires a frequency distribution table summarized from the variable. \\nNote that the bars can be plotted vertically or horizontally. In a vertical bar chart, \\nthe y-axis still represents the absolute frequency or the relative frequency. Different \\nfrom the histogram, however, is that the x-axis in a bar chart represents the mutually \\nexclusive categories to be compared rather than bins that group numerical data.\\nFor example, using the marginal frequencies for the five GICS sectors shown in \\nthe last column in Exhibit 14, we plot a horizontal bar chart in Exhibit 23 to show \\nthe frequency of stocks by sector in the portfolio. The bars are of equal width to \\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n85\\nrepresent each sector, and sufficient space should be between adjacent bars to separate \\nthem from each other. Because this is a horizontal bar chart—in this case, the x-axis \\nshows the absolute frequency and the y-axis represents the sectors—the length of \\neach bar represents the absolute frequency of each sector. Since sectors are nominal \\ndata with no logical ordering, the bars representing sectors may be arranged in any \\norder. However, in the particular case where the categories in a bar chart are ordered \\nby frequency in descending order and the chart includes a line displaying cumulative \\nrelative frequency, then it is called a Pareto Chart. The chart is often used to highlight \\ndominant categories or the most important groups.\\nBar charts provide a snapshot to show the comparison between categories of data. \\nAs shown in Exhibit 23, the sector in which the portfolio holds most stocks is the \\nhealth care sector, with 435 stocks, followed by the energy sector, with 290 stocks. \\nThe sector in which the portfolio has the least number of stocks is utilities, with 55 \\nstocks. To compare categories more accurately, in some cases we may add the fre-\\nquency count to the right end of each bar (or the top end of each bar in the case of \\na vertical bar chart).\\nExhibit 23: Frequency by Sector for Stocks in a Portfolio\\nSector\\nCommunication Services\\nConsumer Staples\\nEnergy\\nHealth Care\\nUtilities\\n0\\n100\\n400\\n300\\n200\\nFrequency\\nThe bar chart shown in Exhibit 23 can present the frequency distribution of only one \\ncategorical variable. In the case of two categorical variables, we need an enhanced \\nversion of the bar chart, called a grouped bar chart (also known as a clustered bar \\nchart), to show joint frequencies. Using the joint frequencies by sector and by level \\nof market capitalization given in Exhibit 14, for example, we show how a grouped bar \\nchart is constructed in Exhibit 24. While the y-axis still represents the same categor-\\nical variable (the distinct GICS sectors as in Exhibit 23), in Exhibit 24 three bars are \\nclustered side-by-side within the same sector to represent the three respective levels \\nof market capitalization. The bars within each cluster should be colored differently to \\ndistinguish between them, but the color schemes for the sub-groups must be iden-\\ntical across the sector clusters, as shown by the legend at the upper right of Exhibit \\n24. Additionally, the bars in each sector cluster must always be placed in the same \\norder throughout the chart. It is easy to see that the small-cap heath care stocks are \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n86\\nthe sub-group with the highest frequency (275), and we can also see that small-cap \\nstocks are the largest sub-group within each sector—except for utilities, where mid \\ncap is the largest.\\nExhibit 24: Frequency by Sector and Level of Market Capitalization for \\nStocks in a Portfolio\\nSector\\nCommunication Services\\nConsumer Staples\\nEnergy\\nHealth Care\\nUtilities\\n0\\n100\\n250\\n200\\n150\\n50\\nFrequency\\nSmall Cap\\nMid Cap\\nLarge Cap\\nAn alternative form for presenting the joint frequency distribution of two categorical \\nvariables is a stacked bar chart. In the vertical version of a stacked bar chart, the \\nbars representing the sub-groups are placed on top of each other to form a single bar. \\nEach subsection of the bar is shown in a different color to represent the contribution \\nof each sub-group, and the overall height of the stacked bar represents the marginal \\nfrequency for the category. Exhibit 24 can be replotted in a stacked bar chart, as \\nshown in Exhibit 25.\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n87\\nExhibit 25: Frequency by Sector and Level of Market Capitalization in a \\nStacked Bar Chart\\n400\\n300\\n200\\n100\\n0\\nCommunication\\nServices\\nHealh\\nCare\\nConsumer\\nStaples\\nEnergy\\nUtilities\\nSector\\nSmall Cap\\nMid Cap\\nLarge Cap\\nWe have shown that the frequency distribution of categorical data can be clearly and \\nefficiently presented by using a bar chart. However, it is worth noting that applica-\\ntions of bar charts may be extended to more general cases when categorical data are \\nassociated with numerical data. For example, suppose we want to show a company’s \\nquarterly profits over the past one year. In this case, we can plot a vertical bar chart \\nwhere each bar represents one of the four quarters in a time order and its height \\nindicates the value of profits for that quarter.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Tree-Map',\n",
       "       'page_number': 97,\n",
       "       'content': 'Tree-Map\\nIn addition to bar charts and grouped bar charts, another graphical tool for displaying \\ncategorical data is a tree-map. It consists of a set of colored rectangles to represent \\ndistinct groups, and the area of each rectangle is proportional to the value of the \\ncorresponding group. For example, referring back to the marginal frequencies by \\nGICS sector in Exhibit 14, we plot a tree-map in Exhibit 26 to represent the frequency \\ndistribution by sector for stocks in the portfolio. The tree-map clearly shows that \\nhealth care is the sector with the largest number of stocks in the portfolio, which is \\nrepresented by the rectangle with the largest area.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n88\\nExhibit 26: Tree-Map for Frequency Distribution by Sector in a Portfolio\\nHealth Care\\nSmall (275)\\nMid (105)\\nLarge (55)\\nSmall (50)\\nLarge\\n(30)\\nMid\\n(30)\\nMid (25)\\nSmall (20)\\n(10)\\nSmall (175)\\nMid (95)\\nLarge\\n(20)\\nSmall (55)\\nMid (35)\\nLarge (20)\\nEnergy\\nConsumer Staples\\nUtilites\\nComm.\\nServices\\ngraL\\ne\\nNote that this example also depicts one more categorical variable (i.e., level of market \\ncapitalization). The tree-map can represent data with additional dimensions by display-\\ning a set of nested rectangles. To show the joint frequencies of sub-groups by sector \\nand level of market capitalization, as given in Exhibit 14, we can split each existing \\nrectangle for sector into three sub-rectangles to represent small-cap, mid-cap, and \\nlarge-cap stocks, respectively. In this case, the area of each nested rectangle would \\nbe proportional to the number of stocks in each market capitalization sub-group. \\nThe exhibit clearly shows that small-cap health care is the sub-group with the largest \\nnumber of stocks. It is worth noting a caveat for using tree-maps: Tree-maps become \\ndifficult to read if the hierarchy involves more than three levels.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Word Cloud',\n",
       "       'page_number': 98,\n",
       "       'content': 'Word Cloud\\nSo far, we have shown how to visualize the frequency distribution of numerical data \\nor categorical data. However, can we find a chart to depict the frequency of unstruc-\\ntured data—particularly, textual data? A word cloud (also known as tag cloud) is a \\nvisual device for representing textual data. A word cloud consists of words extracted \\nfrom a source of textual data, with the size of each distinct word being proportional \\nto the frequency with which it appears in the given text. Note that common words \\n(e.g., “a,” “it,” “the”) are generally stripped out to focus on key words that convey the \\nmost meaningful information. This format allows us to quickly perceive the most \\nfrequent terms among the given text to provide information about the nature of the \\ntext, including topic and whether or not the text conveys positive or negative news. \\nMoreover, words conveying different sentiment may be displayed in different colors. \\nFor example, “profit” typically indicates positive sentiment so might be displayed in \\ngreen, while “loss” typically indicates negative sentiment and may be shown in red.\\nExhibit 27 is an excerpt from the Management’s Discussion and Analysis (MDA) \\nsection of the 10-Q filing for QXR Inc. for the quarter ended 31 March 20XX. Taking \\nthis text, we can create a word cloud, as shown in Exhibit 28. A quick glance at the word \\ncloud tells us that the following words stand out (i.e., they were used most frequently \\nin the MDA text): “billion,” “revenue,” “year,” “income,” “growth,” and “financial.” Note \\nthat specific words, such as “income” and “growth,” typically convey positive sentiment, \\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n89\\nas contrasted with such words as “loss” and “decline,” which typically convey negative \\nsentiment. In conclusion, word clouds are a useful tool for visualizing textual data that \\ncan facilitate understanding the topic of the text as well as the sentiment it may convey.\\nExhibit 27: Excerpt of MDA Section in Form 10-Q of QXR Inc. for Quarter Ended \\n31 March 20XX\\nMANAGEMENT’S DISCUSSION AND ANALYSIS OF \\nFINANCIAL CONDITION AND RESULTS OF OPERATIONS\\nPlease read the following discussion and analysis of our financial condition and \\nresults of operations together with our consolidated financial statements and \\nrelated notes included under Part I, Item 1 of this Quarterly Report on Form 10-Q\\nExecutive Overview of Results\\nBelow are our key financial results for the three months ended March 31, 20XX \\n(consolidated unless otherwise noted):\\n■ \\nRevenues of $36.3 billion and revenue growth of 17% year over year, \\nconstant currency revenue growth of 19% year over year.\\n■ \\nMajor segment revenues of $36.2 billion with revenue growth of 17% \\nyear over year and other segments’ revenues of $170 million with reve-\\nnue growth of 13% year over year.\\n■ \\nRevenues from the United States, EMEA, APAC, and Other Americas \\nwere $16.5 billion, $11.8 billion, $6.1 billion, and $1.9 billion, \\nrespectively.\\n■ \\nCost of revenues was $16.0 billion, consisting of TAC of $6.9 billion \\nand other cost of revenues of $9.2 billion. Our TAC as a percentage of \\nadvertising revenues were 22%.\\n■ \\nOperating expenses (excluding cost of revenues) were $13.7 billion, \\nincluding the EC AFS fine of $1.7 billion.\\n■ \\nIncome from operations was $6.6 billion\\n■ \\nOther income (expense), net, was $1.5 billion.\\n■ \\nEffective tax rate was 18%\\n■ \\nNet income was $6.7 billion with diluted net income per share of \\n$9.50.\\n■ \\nOperating cash flow was $12.0 billion.\\n■ \\nCapital expenditures were $4.6 billion.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n90\\nExhibit 28: Word Cloud Visualizing Excerpted Text in MDA Section in Form \\n10-Q of QXR Inc.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Line Chart',\n",
       "       'page_number': 100,\n",
       "       'content': 'Line Chart\\nA line chart is a type of graph used to visualize ordered observations. Often a line \\nchart is used to display the change of data series over time. Note that the frequency \\npolygon in Exhibit 21 and the cumulative frequency distribution chart in Exhibit \\n22 are also line charts but used particularly in those instances for representing data \\nfrequency distributions.\\nConstructing a line chart is relatively straightforward: We first plot all the data \\npoints against horizontal and vertical axes and then connect the points by straight \\nline segments. For example, to show the 10-day daily closing prices of ABC Inc. stock \\npresented in Exhibit 5, we first construct a chart with the x-axis representing time (in \\ndays) and the y-axis representing stock price (in dollars). Next, plot each closing price \\nas points against both axes, and then use straight line segments to join the points \\ntogether, as shown in Exhibit 29.\\nAn important benefit of a line chart is that it facilitates showing changes in the \\ndata and underlying trends in a clear and concise way. This helps to understand the \\ncurrent data and also helps with forecasting the data series. In Exhibit 29, for example, \\nit is easy to spot the price changes over the first 10 trading days since ABC’s initial \\npublic offering (IPO). We see that the stock price peaked on Day 3 and then traded \\nlower. Following a partial recovery on Day 7, it declined steeply to around $50 on Day \\n10. In contrast, although the one-dimensional data array table in Exhibit 5 displays \\nthe same values as the line chart, the data table by itself does not provide a quick \\nsnapshot of changes in the data or facilitate understanding underlying trends. This is \\nwhy line charts are helpful for visualization, particularly in cases of large amounts of \\ndata (i.e., hundreds, or even thousands, of data points).\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n91\\nExhibit 29: Daily Closing Prices of ABC Inc.’s Stock and Its Sector Index\\nPrice ($)\\n58\\n56\\n54\\n52\\n50\\nSector Index\\n6,380\\n6,340\\n6,200\\n6,260\\n6,240\\n6,280\\n6,320\\n6,360\\n1\\n10\\n4\\n2\\n3\\n5\\n6\\n8\\n9\\n7\\nDay\\nPrice ($)\\nSector Index\\nA line chart is also capable of accommodating more than one set of data points, which \\nis especially helpful for making comparisons. We can add a line to represent each \\ngroup of data (e.g., a competitor’s stock price or a sector index), and each line would \\nhave a distinct color or line pattern identified in a legend. For example, Exhibit 29 also \\nincludes a plot of ABC’s sector index (i.e., the sector index for which ABC stock is a \\nmember, like health care or energy) over the same period. The sector index is displayed \\nwith its own distinct color to facilitate comparison. Note also that because the sector \\nindex has a different range (approximately 6,230 to 6,390) than ABCs’ stock ($50 to \\n$59 per share), we need a secondary y-axis to correctly display the sector index, which \\nis on the right-hand side of the exhibit.\\nThis comparison can help us understand whether ABC’s stock price movement \\nover the period is due to potential mispricing of its share issuance or instead due to \\nindustry-specific factors that also affect its competitors’ stock prices. The comparison \\nshows that over the period, the sector index moved in a nearly opposite trend versus \\nABC’s stock price movement. This indicates that the steep decline in ABC’s stock price \\nis less likely attributable to sector-specific factors and more likely due to potential \\nover-pricing of its IPO or to other company-specific factors.\\nWhen an observational unit (here, ABC Inc.) has more than two features (or \\nvariables) of interest, it would be useful to show the multi-dimensional data all in \\none chart to gain insights from a more holistic view. How can we add an additional \\ndimension to a two-dimensional line chart? We can replace the data points with \\nvarying-sized bubbles to represent a third dimension of the data. Moreover, these \\nbubbles may even be color-coded to present additional information. This version of \\na line chart is called a bubble line chart.\\nExhibit 7, for example, presented three types of quarterly data for ABC Inc. for \\nuse in a valuation analysis. We would like to plot two of them, revenue and earnings \\nper share (EPS), over the two-year period. As shown in Exhibit 30, with the x-axis \\nrepresenting time (i.e., quarters) and the y-axis representing revenue in millions of \\ndollars, we can plot the revenue data points against both axes to form a typical line \\nchart. Next, each marker representing a revenue data point is replaced by a circular \\nbubble with its size proportional to the magnitude of the EPS in the corresponding \\nquarter. Moreover, the bubbles are colored in a binary scheme with green representing \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n92\\nprofits and red representing losses. In this way, the bubble line chart reflects the \\nchanges for both revenue and EPS simultaneously, and it also shows whether the EPS \\nrepresents a profit or a loss.\\nExhibit 30: Quarterly Revenue and EPS of ABC Incorporated\\nRevenue ($M)\\n6,000\\n6,500\\n5,000\\n4,500\\n4,000\\n3,500\\nQ1 Year 1Q2 Year 1\\nQ1 Year 2Q2 Year 2Q3 Year 2Q4 Year 2\\nQ3 Year 1 Q4 Year 1\\nRevenue\\nEPS Profit\\nEPS Loss\\n$1.37\\n$1.78\\n−$3.38\\n−$8.66\\n−$0.34\\n$3.89\\n−$2.88\\n−$3.98\\nAs depicted, ABC’s earning were quite volatile during its initial two years as a public \\ncompany. Earnings started off as a profit of $1.37/share but finished the first year \\nwith a big loss of −$8.66/share, during which time revenue experienced only small \\nfluctuations. Furthermore, while revenues and earnings both subsequently recovered \\nsharply—peaking in Q2 of Year 2—revenues then declined, and the company returned \\nto significant losses (−3.98/share) by the end of Year 2.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Scatter Plot',\n",
       "       'page_number': 102,\n",
       "       'content': 'Scatter Plot\\nA scatter plot is a type of graph for visualizing the joint variation in two numerical \\nvariables. It is a useful tool for displaying and understanding potential relationships \\nbetween the variables.\\nA scatter plot is constructed with the x-axis representing one variable and the \\ny-axis representing the other variable. It uses dots to indicate the values of the two \\nvariables for a particular point in time, which are plotted against the corresponding \\naxes. Suppose an analyst is investigating potential relationships between sector index \\nreturns and returns for the broad market, such as the S&P 500 Index. Specifically, he \\nor she is interested in the relative performance of two sectors, information technology \\n(IT) and utilities, compared to the market index over a specific five-year period. The \\nanalyst has obtained the sector and market index returns for each month over the \\nfive years under investigation and plotted the data points in the scatter plots, shown \\nin Exhibit 31 for IT versus the S&P 500 returns and in Exhibit 32 for utilities versus \\nthe S&P 500 returns.\\nDespite their relatively straightforward construction, scatter plots convey lots of \\nvaluable information. First, it is important to inspect for any potential association \\nbetween the two variables. The pattern of the scatter plot may indicate no apparent \\nrelationship, a linear association, or a non-linear relationship. A scatter plot with \\nrandomly distributed data points would indicate no clear association between the \\ntwo variables. However, if the data points seem to align along a straight line, then \\nthere may exist a significant relationship among the variables. A positive (negative) \\nslope for the line of data points indicates a positive (negative) association, meaning \\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n93\\nthe variables move in the same (opposite) direction. Furthermore, the strength of the \\nassociation can be determined by how closely the data points are clustered around \\nthe line. Tight (loose) clustering signals a potentially stronger (weaker) relationship.\\nExhibit 31: Scatter Plot of Information Technology Sector Index Return vs. \\nS&P 500 Index Return\\nInformation Technology\\n10.0\\n7.5\\n5.0\\n2.5\\n0\\n–2.5\\n–5.0\\n–7.5\\n0\\n2.5\\n–7.5 –5.0–2.5\\n5.0\\n7.5\\nS&P 500\\nExhibit 32: Scatter Plot of Utilities Sector Index Return vs. S&P 500 Index \\nReturn\\nInformation Technology\\n8\\n6\\n4\\n2\\n0\\n–2\\n–4\\n–6\\n0\\n2.5\\n–7.5 –5.0–2.5\\n5.0\\n7.5\\nS&P 500\\nExamining Exhibit 31, we can see the returns of the IT sector are highly positively \\nassociated with S&P 500 Index returns because the data points are tightly clustered \\nalong a positively sloped line. Exhibit 32 tells a different story for relative perfor-\\nmance of the utilities sector and S&P 500 index returns: The data points appear to \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n94\\nbe distributed in no discernable pattern, indicating no clear relationship among these \\nvariables. Second, observing the data points located toward the ends of each axis, \\nwhich represent the maximum or minimum values, provides a quick sense of the data \\nrange. Third, assuming that a relationship among the variables is apparent, inspecting \\nthe scatter plot can help to spot extreme values (i.e., outliers). For example, an outlier \\ndata point is readily detected in Exhibit 31, as indicated by the arrow. As you will learn \\nlater in the CFA Program curriculum, finding these extreme values and handling them \\nwith appropriate measures is an important part of the financial modeling process.\\nScatter plots are a powerful tool for finding patterns between two variables, for \\nassessing data range, and for spotting extreme values. In practice, however, there are \\nsituations where we need to inspect for pairwise associations among many variables—\\nfor example, when conducting feature selection from dozens of variables to build a \\npredictive model.\\nA scatter plot matrix is a useful tool for organizing scatter plots between pairs \\nof variables, making it easy to inspect all pairwise relationships in one combined \\nvisual. For example, suppose the analyst would like to extend his or her investigation \\nby adding another sector index. He or she can use a scatter plot matrix, as shown in \\nExhibit 33, which now incorporates four variables, including index returns for the \\nS&P 500 and for three sectors: IT, utilities, and financials.\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n95\\nExhibit 33: Pairwise Scatter Plot Matrix\\nS&P 500\\n–5.0\\n50\\n20\\n10\\n–10\\n10\\n0\\nInformation Technology\\n–5.0\\n50\\n40\\n20\\n–10\\n10\\n0\\nUtilities\\n7.5\\n2.5\\n5.0\\n0\\n–5.0\\n–2.5\\n–7.5\\n–10\\n10\\n0\\nFinancials\\n15\\n5\\n10\\n–50\\n–10\\n–10\\n10\\n0\\nS&P 500\\nS&P 500\\n–5.0\\n50\\n20\\n10\\n–10\\n0\\n10\\nInformation Technology\\n–5.0\\n50\\n40\\n20\\n–10\\n0\\n0\\nUtilities\\n7.5\\n2.5\\n5.0\\n0\\n–5.0\\n–2.5\\n–7.5\\n–10\\n0\\n10\\nFinancials\\n15\\n5\\n10\\n–50\\n–10\\n–10\\n0\\n10\\nInformation Technology\\nS&P 500\\n–5.0\\n50\\n20\\n10\\n0\\n–5\\n5\\nInformation Technology\\n–5.0\\n50\\n40\\n20\\n0\\n–5\\n5\\nUtilities\\n7.5\\n2.5\\n5.0\\n0\\n–5.0\\n–2.5\\n–7.5\\n0\\n–5\\n5\\nFinancials\\n15\\n5\\n10\\n–50\\n–10\\n0\\n–5\\n5\\nUtilities\\nS&P 500\\n–5.0\\n50\\n20\\n10\\n0\\n–10\\n10\\nInformation Technology\\n–5.0\\n50\\n40\\n20\\n0\\n–10\\n10\\nUtilities\\n7.5\\n2.5\\n5.0\\n0\\n–5.0\\n–2.5\\n–7.5\\n0\\n–10\\n10\\nFinancials\\n15\\n5\\n10\\n–50\\n–10\\n0\\n–10\\n10\\nFinancials\\nS&P 500\\nS&P 500 \\nS&P 500\\nInformation Technology\\nInformation Technology\\nInformation Technology\\nUtilities\\nUtilities\\nUtilities\\nFinancials\\nFinancials\\nFinancials\\nThe scatter plot matrix contains each combination of bivariate scatter plot (i.e., S&P \\n500 vs. each sector, IT vs. utilities, IT vs. financials, and financials vs. utilities) as well \\nas univariate frequency distribution histograms for each variable plotted along the \\ndiagonal. In this way, the scatter plot matrix provides a concise visual summary of each \\nvariable and of potential relationships among them. Importantly, the construction of \\nthe scatter plot matrix is typically a built-in function in most major statistical software \\npackages, so it is relatively easy to implement. It is worth pointing out that the upper \\ntriangle of the matrix is the mirror image of the lower triangle, so the compact form \\nof the scatter plot matrix that uses only the lower triangle is also appropriate.\\nWith the addition of the financial sector, the bottom panel of Exhibit 33 reveals the \\nfollowing additional information, which can support sector allocation in the portfolio \\nconstruction process:\\n■ \\nStrong positive relationship between returns of financial and S&P 500;\\n■ \\nPositive relationship between returns of financial and IT; and\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n96\\n■ \\nNo clear relationship between returns of financial and utilities.\\nIt is important to note that despite their usefulness, scatter plots and scatter plot \\nmatrixes should not be considered as a substitute for robust statistical tests; rather, \\nthey should be used alongside such tests for best results.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Heat Map',\n",
       "       'page_number': 106,\n",
       "       'content': 'Heat Map\\nA heat map is a type of graphic that organizes and summarizes data in a tabular \\nformat and represents them using a color spectrum. For example, given a portfolio, \\nwe can create a contingency table that summarizes the joint frequencies of the stock \\nholdings by sector and by level of market capitalization, as in Exhibit 34.\\nExhibit 34: Frequencies by Sector and Market Capitalization in Heat Map\\n21\\n36\\n99\\n4\\n81\\n43\\n81\\n95\\n8\\n37\\n83\\n45\\n29\\n18\\n58\\n80\\n60\\n40\\n20\\nCommunication Services\\nConsumer Staples\\nEnergy\\nHealth Care\\nUtilities\\nMid Cap\\nLarge Cap\\nSmall Cap\\nCells in the chart are color-coded to differentiate high values from low values by \\nusing the color scheme defined in the color spectrum on the right side of the chart. \\nAs shown by the heat map, this portfolio has the largest exposure (in terms of num-\\nber of stocks) to small- and mid-cap energy stocks. It has substantial exposures to \\nlarge-cap communications services, mid-cap consumer staples, and small-cap utilities; \\nhowever, exposure to the health care sector is limited. In sum, the heat map reveals \\nthis portfolio to be relatively well-diversified among sectors and market-cap levels. \\nBesides their use in displaying frequency distributions, heat maps are commonly used \\nfor visualizing the degree of correlation among different variables.\\nEXAMPLE 6\\nEvaluating Data Visuals\\n1. You have a cumulative absolute frequency distribution graph (similar to the \\none in Exhibit 22) of daily returns over a five-year period for an index of \\nAsian equity markets.\\nInterpret the meaning of the slope of such a graph.\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n97\\nSolution 1\\nThe slope of the graph of a cumulative absolute frequency distribution \\nreflects the change in the number of observations between two adjacent re-\\nturn bins. A steep (flat) slope indicates a large (small) change in the frequen-\\ncy of observations between adjacent return bins.\\n2. You are creating a word cloud for a visual representation of text on a compa-\\nny’s quarterly earnings announcements over the past three years. The word \\ncloud uses font size to indicate word frequency. This particular company \\nhas experienced both quarterly profits and losses during the period under \\ninvestigation.\\nDescribe how the word cloud might be used to convey information besides \\nword frequency.\\nSolution 2\\nColor can add an additional dimension to the information conveyed in \\nthe word cloud. For example, red can be used for “losses” and other words \\nconveying negative sentiment, and green can be used for “profit” and other \\nwords indicative of positive sentiment.\\n3. You are examining a scatter plot of monthly stock returns, similar to the one \\nin Exhibit 31, for two technology companies: one is a hardware manufac-\\nturer, and the other is a software developer. The scatter plot shows a strong \\npositive association among their returns.\\nDescribe what other information the scatter plot can provide.\\nSolution 3\\nBesides the sign and degree of association of the stocks’ returns, the scatter \\nplot can provide a visual representation of whether the association is linear \\nor non-linear, the maximum and minimum values for the return observa-\\ntions, and an indication of which observations may have extreme values (i.e., \\nare potential outliers).\\n4. You are reading a vertical bar chart displaying the sales of a company over \\nthe past five years. The sales of the first four years seem nearly flat as the \\ncorresponding bars are nearly the same height, but the bar representing \\nthe sales of the most recent year is approximately three times as high as the \\nother bars.\\nExplain whether we can conclude that the sales of the fifth year tripled com-\\npared to sales in the earlier years.\\nSolution 4\\nTypically, the heights of bars in a vertical bar chart are proportional to the \\nvalues that they represent. However, if the graph is using a truncated y-axis \\n(i.e., one that does not start at zero), then values are not accurately repre-\\nsented by the height of bars. Therefore, we need to examine the y-axis of the \\nbar chart before concluding that sales in the fifth year were triple the sales of \\nthe prior years.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n98\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Guide to Selecting among Visualization Types',\n",
       "       'page_number': 108,\n",
       "       'content': 'Guide to Selecting among Visualization Types\\nWe have introduced and discussed a variety of different visualization types that are \\nregularly used in investment practice. When it comes to selecting a chart for visualizing \\ndata, the intended purpose is the key consideration: Is it for exploring and/or presenting \\ndistributions or relationships, or is it for making comparisons? Given your intended \\npurpose, the best selection is typically the simplest visual that conveys the message \\nor achieves the specific goal. Exhibit 35 presents a flow chart for facilitating selection \\namong the visualization types we have discussed. Finally, note that some visualization \\ntypes, such as bar chart and heat map, may be suitable for several different purposes.\\nExhibit 35: Flow Chart of Selecting Visualization Types\\n• Scatter Plot\\n (Two Variables)\\n• Scatter Plot Matrix\\n (Multiple Variables)\\n• (Multiple Variables)\\n• Heat Map\\n• Bar Chart\\n• Tree Map\\n• Heat Map\\n• Bar Chart\\n• Tree Map\\n• Heat Map\\n• Word Cloud\\n• Histogram\\n• Frequency\\n Polygon\\n• Cumulative\\n Distribution\\n Chart\\nWhat to\\nexplore or\\npresent?\\nRelationship\\nComparison\\nAmong\\nCategories\\nOver Time\\nDistribution\\nNumerical\\nData\\nCategorical\\nData\\nUnstructured\\nData\\n• Line Chart\\n (Two Variables)\\n• Bubble Line Chart\\n (Three Variables)\\nData visualization is a powerful tool to show data and gain insights into data. However, \\nwe need to be cautious that a graph could be misleading if data are mispresented or \\nthe graph is poorly constructed. There are numerous different ways that may lead to \\na misleading graph. We list four typical pitfalls here that analysts should avoid.\\nFirst, an improper chart type is selected to present data, which would hinder the \\naccurate interpretation of data. For example, to investigate the correlation between \\ntwo data series, we can construct a scatter plot to visualize the joint variation between \\ntwo variables. In contrast, plotting the two data series separately in a line chart would \\nmake it rather difficult to examine the relationship.\\nSecond, data are selectively plotted in favor of the conclusion an analyst intends \\nto draw. For example, data presented for an overly short time period may appear to \\nshow a trend that is actually noise—that is, variation within the data’s normal range \\nif examining the data over a longer time period. So, presenting data for too short a \\ntime window may mistakenly point to a non-existing trend.\\nThird, data are improperly plotted in a truncated graph that has a y-axis that \\ndoes not start at zero. In some situations, the truncated graph can create the false \\nimpression of significant differences when there is actually only a small difference. \\nFor example, suppose a vertical bar chart is used to compare annual revenues of two \\ncompanies, one with $9 billion and the other with $10 billion. If the y-axis starts at \\n$8 billion, then the bar heights would inaccurately imply that the latter company’s \\nrevenue is twice the former company’s revenue.\\n© CFA Institute. For candidate use only. Not for distribution.\\nData Visualization\\n99\\nLast, but not least, is the improper scaling of axes. For example, given a line chart, \\nsetting a higher than necessary maximum on the y-axis tends to compress the graph \\ninto an area close to the x-axis. This causes the graph to appear to be less steep and \\nless volatile than if it was properly plotted. In sum, analysts need to avoid these misuses \\nof visualization when charting data and must ensure the ethical use of data visuals.\\nEXAMPLE 7\\nSelecting Visualization Types\\n1. A portfolio manager plans to buy several stocks traded on a small emerging \\nmarket exchange but is concerned whether the market can provide sufficient \\nliquidity to support her purchase order size. As the first step, she wants to \\nanalyze the daily trading volumes of one of these stocks over the past five \\nyears.\\nExplain which type of chart can best provide a quick view of trading volume \\nfor the given period.\\nSolution to 1\\nThe five-year history of daily trading volumes contains a large amount of \\nnumerical data. Therefore, a histogram is the best chart for grouping these \\ndata into frequency distribution bins and for showing a quick snapshot of \\nthe shape, center, and spread of the data’s distribution.\\n2. An analyst is building a model to predict stock market downturns. Accord-\\ning to the academic literature and his practitioner knowledge and expertise, \\nhe has selected 10 variables as potential predictors. Before continuing to \\nconstruct the model, the analyst would like to get a sense of how closely \\nthese variables are associated with the broad stock market index and wheth-\\ner any pair of variables are associated with each other.\\nDescribe the most appropriate visual to select for this purpose.\\nSolution to 2\\nTo inspect for a potential relationship between two variables, a scatter plot \\nis a good choice. But with 10 variables, plotting individual scatter plots is \\nnot an efficient approach. Instead, utilizing a scatter plot matrix would give \\nthe analyst a good overview in one comprehensive visual of all the pairwise \\nassociations between the variables.\\n3. Central Bank members meet regularly to assess the economy and decide on \\nany interest rate changes. Minutes of their meetings are published on the \\nCentral Bank’s website. A quantitative researcher wants to analyze the meet-\\ning minutes for use in building a model to predict future economic growth.\\nExplain which type of chart is most appropriate for creating an overview of \\nthe meeting minutes.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n100\\nSolution to 3\\nSince the meeting minutes consist of textual data, a word cloud would be the \\nmost suitable tool to visualize the textual data and facilitate the researcher’s \\nunderstanding of the topic of the text as well as the sentiment, positive or \\nnegative, it may convey.\\n4. A private investor wants to add a stock to her portfolio, so she asks her \\nfinancial adviser to compare the three-year financial performances (by quar-\\nter) of two companies. One company experienced consistent revenue and \\nearnings growth, while the other experienced volatile revenue and earnings \\ngrowth, including quarterly losses.\\nDescribe the chart the adviser should use to best show these performance \\ndifferences.\\nSolution to 4\\nThe best chart for making this comparison would be a bubble line chart \\nusing two different color lines to represent the quarterly revenues for each \\ncompany. The bubble sizes would then indicate the magnitude of each \\ncompany’s quarterly earnings, with green bubbles signifying profits and red \\nbubbles indicating losses.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Measures of Central Tendency',\n",
       "     'page_number': 110,\n",
       "     'content': 'MEASURES OF CENTRAL TENDENCY\\ncalculate and interpret measures of central tendency\\nevaluate alternative definitions of mean to address an investment \\nproblem\\nSo far, we have discussed methods we can use to organize and present data so that \\nthey are more understandable. The frequency distribution of an asset return series, \\nfor example, reveals much about the nature of the risks that investors may encounter \\nin a particular asset. Although frequency distributions, histograms, and contingency \\ntables provide a convenient way to summarize a series of observations, these methods \\nare just a first step toward describing the data. In this section, we discuss the use of \\nquantitative measures that explain characteristics of data. Our focus is on measures \\nof central tendency and other measures of location. A measure of central tendency \\nspecifies where the data are centered. Measures of central tendency are probably more \\nwidely used than any other statistical measure because they can be computed and \\napplied relatively easily. Measures of location include not only measures of central \\ntendency but other measures that illustrate the location or distribution of data.\\nIn the following subsections, we explain the common measures of central tendency—\\n',\n",
       "     'children': [{'title': 'The Arithmetic Mean',\n",
       "       'page_number': 111,\n",
       "       'content': 'The Arithmetic Mean\\nAnalysts and portfolio managers often want one number that describes a representative \\npossible outcome of an investment decision. The arithmetic mean is one of the most \\nfrequently used measures of the center of data.\\nDefinition of Arithmetic Mean. The arithmetic mean is the sum of the \\nvalues of the observations divided by the number of observations.\\nThe Sample Mean\\nThe sample mean is the arithmetic mean or arithmetic average computed for a sam-\\nple. As you will see, we use the terms “mean” and “average” interchangeably. Often, \\nwe cannot observe every member of a population; instead, we observe a subset or \\nsample of the population.\\nSample Mean Formula. The sample mean or average,   _\\n \\nX  (read “X-bar”), is \\nthe arithmetic mean value of a sample:\\n \\n_\\n \\nX  =  \\n ∑ \\ni=1\\n  \\nn\\n   X i   \\n_ \\nn  , \\n(2)\\nwhere n is the number of observations in the sample.\\nEquation 2 tells us to sum the values of the observations (Xi) and divide the sum \\nby the number of observations. For example, if a sample of market capitalizations for \\nsix publicly traded Australian companies contains the values (in AUD billions) 35, 30, \\n22, 18, 15, and 12, the sample mean market cap is 132/6 = A$22 billion. As previously \\nnoted, the sample mean is a statistic (that is, a descriptive measure of a sample).\\nMeans can be computed for individual units or over time. For instance, the sample \\nmight be the return on equity (ROE) in a given year for a sample of 25 companies in \\nthe FTSE Eurotop 100, an index of Europe’s 100 largest companies. In this case, we \\ncalculate the mean ROE in that year as an average across 25 individual units. When \\nwe examine the characteristics of some units at a specific point in time (such as ROE \\nfor the FTSE Eurotop 100), we are examining cross-sectional data; the mean of these \\nobservations is the cross-sectional mean. If the sample consists of the historical \\nmonthly returns on the FTSE Eurotop 100 for the past five years, however, then we \\nhave time-series data; the mean of these observations is the time-series mean. We \\nwill examine specialized statistical methods related to the behavior of time series in \\nthe reading on time-series analysis.\\nExcept in cases of large datasets with many observations, we should not expect any \\nof the actual observations to equal the mean; sample means provide only a summary \\nof the data being analyzed. Also, although in some cases the number of values below \\nthe mean is quite close to the number of values above the mean, this need not be \\nthe case. As an analyst, you will often need to find a few numbers that describe the \\ncharacteristics of the distribution, and we will consider more later. The mean is gen-\\nerally the statistic that you use as a measure of the typical outcome for a distribution. \\nYou can then use the mean to compare the performance of two different markets. \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n102\\nFor example, you might be interested in comparing the stock market performance of \\ninvestments in Asia Pacific with investments in Europe. You can use the mean returns \\nin these markets to compare investment results.\\nEXAMPLE 8\\nCalculating a Cross-Sectional Mean\\n1. Suppose we want to examine the performance of a sample of selected stock \\nindexes from 11 different countries. The 52-week percentage change is re-\\nported in Exhibit 36 for Year 1, Year 2, and Year 3 for the sample of indexes.\\n \\nExhibit 36: Annual Returns for Years 1 to 3 for Selected \\nCountries’ Stock Indexes\\n \\n \\n\\xa0\\n52-Week Return (%)\\nIndex \\nYear 1\\nYear 2\\nYear 3\\nCountry A\\n−15.6\\n−5.4\\n6.1\\nCountry B\\n7.8\\n6.3\\n−1.5\\nCountry C\\n5.3\\n1.2\\n3.5\\nCountry D\\n−2.4\\n−3.1\\n6.2\\nCountry E\\n−4.0\\n−3.0\\n3.0\\nCountry F\\n5.4\\n5.2\\n−1.0\\nCountry G\\n12.7\\n6.7\\n−1.2\\nCountry H\\n3.5\\n4.3\\n3.4\\nCountry I\\n6.2\\n7.8\\n3.2\\nCountry J\\n8.1\\n4.1\\n−0.9\\nCountry K\\n11.5\\n3.4\\n1.2\\n \\nCountry\\nB\\nA\\nC\\nD\\nF\\nE\\nG\\nH\\nI\\nJ\\nK\\n–20\\n15\\n–10\\n–15\\n–5\\n10\\n5\\n0\\nAnnual Return (%)\\nYear 3\\nYear 2\\nYear 1\\nUsing the data provided, calculate the sample mean return for the 11 index-\\nes for each year.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Central Tendency\\n103\\nSolution:\\nFor Year 3, the calculation applies Equation 2 to the returns for Year 3: (6.1 \\n− 1.5 + 3.5 + 6.2 + 3.0 − 1.0 − 1.2 + 3.4 + 3.2 − 0.9 + 1.2)/11 = 22.0/11 = 2.0%. \\nUsing a similar calculation, the sample mean is 3.5% for Year 1 and 2.5% for \\nYear 2.\\nProperties of the Arithmetic Mean\\nThe arithmetic mean can be likened to the center of gravity of an object. Exhibit 37 \\nexpresses this analogy graphically by plotting nine hypothetical observations on a \\nbar. The nine observations are 2, 4, 4, 6, 10, 10, 12, 12, and 12; the arithmetic mean is \\n72/9 = 8. The observations are plotted on the bar with various heights based on their \\nfrequency (that is, 2 is one unit high, 4 is two units high, and so on). When the bar is \\nplaced on a fulcrum, it balances only when the fulcrum is located at the point on the \\nscale that corresponds to the arithmetic mean.\\nExhibit 37: Center of Gravity Analogy for the Arithmetic Mean\\n1\\n3\\n2\\n5\\n6\\n9\\n11\\n10\\n12\\n7\\n4\\nFulcrum\\nAs analysts, we often use the mean return as a measure of the typical outcome for an \\nasset. As in Example 8, however, some outcomes are above the mean and some are \\nbelow it. We can calculate the distance between the mean and each outcome, which is \\nthe deviation. Mathematically, it is always true that the sum of the deviations around \\nthe mean equals 0. We can see this by using the definition of the arithmetic mean \\nshown in Equation 2, multiplying both sides of the equation by n:  n _\\n \\nX  =  ∑ \\ni=1\\n  \\nn\\n   X i  . The \\nsum of the deviations from the mean is calculated as follows:\\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  )  =  ∑ \\ni=1\\n  \\nn\\n   X i  −  ∑ \\ni=1\\n  \\nn\\n   \\n_\\n \\nX   =  ∑ \\ni=1\\n  \\nn\\n   X i  − n \\n_\\n \\nX  = 0 .\\nDeviations from the arithmetic mean are important information because they indicate \\nrisk. The concept of deviations around the mean forms the foundation for the more \\ncomplex concepts of variance, skewness, and kurtosis, which we will discuss later.\\nA property and potential drawback of the arithmetic mean is its sensitivity to \\nextreme values, or outliers. Because all observations are used to compute the mean and \\nare given equal weight (i.e., importance), the arithmetic mean can be pulled sharply \\nupward or downward by extremely large or small observations, respectively. For \\nexample, suppose we compute the arithmetic mean of the following seven numbers: \\n1, 2, 3, 4, 5, 6, and 1,000. The mean is 1,021/7 = 145.86, or approximately 146. Because \\nthe magnitude of the mean, 146, is so much larger than most of the observations (the \\nfirst six), we might question how well it represents the location of the data. Perhaps \\nthe most common approach in such cases is to report ',\n",
       "       'children': []},\n",
       "      {'title': 'The Median',\n",
       "       'page_number': 115,\n",
       "       'content': 'The Median\\nA second important measure of central tendency is the median.\\nDefinition of Median. The median is the value of the middle item of a \\nset of items that has been sorted into ascending or descending order. In an \\nodd-numbered sample of n items, the median is the value of the item that \\noccupies the (n + 1)/2 position. In an even-numbered sample, we define the \\nmedian as the mean of the values of items occupying the n/2 and (n + 2)/2 \\npositions (the two middle items).\\nSuppose we have a return on assets (in %) for each of three companies: 0.0, 2.0, \\nand 2.1. With an odd number of observations (n = 3), the median occupies the (n + \\n1)/2 = 4/2 = 2nd position. The median is 2.0%. The value of 2.0% is the “middlemost” \\nobservation: One lies above it, and one lies below it. Whether we use the calculation \\nfor an even- or odd-numbered sample, an equal number of observations lie above \\nand below the median. A distribution has only one median.\\nA potential advantage of the median is that, unlike the mean, extreme values do \\nnot affect it. For example, if a sample consists of the observations of 1, 2, 3, 4, 5, 6 and \\n1,000, the median is 4. The median is not influenced by the extremely large outcome \\nof 1,000. In other words, the median is affected less by outliers than the mean and \\ntherefore is useful in describing data that follow a distribution that is not symmetric, \\nsuch as revenues.\\nThe median, however, does not use all the information about the size of the obser-\\nvations; it focuses only on the relative position of the ranked observations. Calculating \\nthe median may also be more complex. To do so, we need to order the observations \\nfrom smallest to largest, determine whether the sample size is even or odd, and then \\non that basis, apply one of two calculations. Mathematicians express this disadvantage \\nby saying that the median is less mathematically tractable than the mean.\\nWe use the data from Exhibit 36 to demonstrate finding the median, reproduced \\nin Exhibit 39 in ascending order of the return for Year 3, with the ranked position \\nfrom 1 (lowest) to 11 (highest) indicated. Because this sample has 11 observations, \\nthe median is the value in the sorted array that occupies the (11 + 1)/2 = 6th position. \\nCountry E’s index occupies the sixth position and is the median. The arithmetic mean \\nfor Year 3 for this sample of indexes is 2.0%, whereas the median is 3.0.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n106\\nExhibit 39: Returns on Selected Country Stock Indexes for \\nYear 3 in Ascending Order\\nIndex \\nYear 3 \\nReturn (%)\\n\\xa0\\nPosition\\nCountry B\\n−1.5\\n\\xa0\\n1\\nCountry G\\n−1.2\\n\\xa0\\n2\\nCountry F\\n−1.0\\n\\xa0\\n3\\nCountry J\\n−0.9\\n\\xa0\\n4\\nCountry K\\n1.2\\n\\xa0\\n5\\nCountry E\\n3.0\\n←\\n6\\nCountry I\\n3.2\\n\\xa0\\n7\\nCountry H\\n3.4\\n\\xa0\\n8\\nCountry C\\n3.5\\n\\xa0\\n9\\nCountry A\\n6.1\\n\\xa0\\n10\\nCountry D\\n6.2\\n\\xa0\\n11\\nCountry\\nA\\nD\\nC\\nH\\nE\\nI\\nK\\nJ\\nF\\nG\\nB\\n–2\\n8\\n2\\n6\\n4\\n0\\nReturn (%)\\nMedian\\nIf a sample has an even number of observations, the median is the mean of the two \\nvalues in the middle. For example, if our sample in Exhibit 39 had 12 indexes instead \\nof 11, the median would be the mean of the values in the sorted array that occupy \\nthe sixth and the seventh positions.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'The Mode',\n",
       "       'page_number': 116,\n",
       "       'content': 'The Mode\\nThe third important measure of central tendency is the mode.\\nDefinition of Mode. The mode is the most frequently occurring value in a \\ndistribution.\\nA distribution can have more than one mode, or even no mode. When a distribu-\\ntion has a single value that is most frequently occurring, the distribution is said to be \\nunimodal. If a distribution has two most frequently occurring values, then it has two \\nmodes and is called bimodal. If the distribution has three most frequently occurring \\nvalues, then it is trimodal. When all the values in a dataset are different, the distri-\\nbution has no mode because no value occurs more frequently than any other value.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Central Tendency\\n107\\nStock return data and other data from continuous distributions may not have a \\nmodal outcome. When such data are grouped into bins, however, we often find an \\ninterval (possibly more than one) with the highest frequency: the modal interval (or \\nintervals). Consider the frequency distribution of the daily returns for the EAA Equity \\nIndex over five years that we looked at in Exhibit 11. A histogram for the frequency \\ndistribution of these daily returns is shown in Exhibit 40. The modal interval always \\nhas the highest bar in the histogram; in this case, the modal interval is 0.0 to 0.9%, \\nand this interval has 493 observations out of a total of 1,258 observations.\\nNotice that this histogram in Exhibit 40 looks slightly different from the one in \\nExhibit 11, since this one has 11 bins and follows the seven-step procedure exactly. \\nThus, the bin width is 0.828 [= (5.00 − −4.11)/11], and the first bin begins at the \\nminimum value of −4.11%. It was noted previously that for ease of interpretation, in \\npractice bin width is often rounded up to the nearest whole number; the first bin can \\nstart at the nearest whole number below the minimum value. These refinements and \\nthe use of 10 bins were incorporated into the histogram in Exhibit 11, which has a \\nmodal interval of 0.0% to 1.0%.\\nExhibit 40: Histogram of Daily Returns on the EAA Equity Index\\nNumber of Observations\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\n–1.6 to\\n–0.8\\n–4.1 to\\n–3.3\\n–3.3 to\\n–2.5\\n–2.5 to\\n–1.6\\n–0.8 to\\n0\\n0.9 to\\n1.7\\n2.5 to\\n3.3\\n3.3 to\\n4.2\\n4.2 to\\n5.0\\n0 to\\n0.9\\n1.7 to\\n2.5\\nDaily Return Range (%)\\n5\\n7\\n37\\n94\\n470\\n493\\n122\\n27\\n1\\n1\\n1\\nThe mode is the only measure of central tendency that can be used with nominal \\ndata. For example, when we categorize investment funds into different styles and \\nassign a number to each style, the mode of these categorized data is the most frequent \\ninvestment fund style.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Other Concepts of Mean',\n",
       "       'page_number': 117,\n",
       "       'content': 'Other Concepts of Mean\\nEarlier we explained the arithmetic mean, which is a fundamental concept for describ-\\ning the central tendency of data. An advantage of the arithmetic mean over two other \\nmeasures of central tendency, the median and mode, is that the mean uses all the \\ninformation about the size of the observations. The mean is also relatively easy to \\nwork with mathematically.\\nHowever, other concepts of mean are very important in investments. In the fol-\\nlowing sections, we discuss such concepts.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n108\\nThe Weighted Mean\\nThe concept of weighted mean arises repeatedly in portfolio analysis. In the arithmetic \\nmean, all sample observations are equally weighted by the factor 1/n. In working with \\nportfolios, we often need the more general concept of weighted mean to allow for \\ndifferent (i.e., unequal) weights on different observations.\\nTo illustrate the weighted mean concept, an investment manager with $100 million \\nto invest might allocate $70 million to equities and $30 million to bonds. The portfolio, \\ntherefore, has a weight of 0.70 on stocks and 0.30 on bonds. How do we calculate the \\nreturn on this portfolio? The portfolio’s return clearly involves an averaging of the \\nreturns on the stock and bond investments. The mean that we compute, however, \\nmust reflect the fact that stocks have a 70% weight in the portfolio and bonds have a \\n30% weight. The way to reflect this weighting is to multiply the return on the stock \\ninvestment by 0.70 and the return on the bond investment by 0.30, then sum the two \\nresults. This sum is an example of a weighted mean. It would be incorrect to take an \\narithmetic mean of the return on the stock and bond investments, equally weighting \\nthe returns on the two asset classes.\\nWeighted Mean Formula. The weighted mean  _\\n \\nX  w   (read “X-bar sub-w”), \\nfor a set of observations X1, X2, …, Xn with corresponding weights of w1, w2, \\n…, wn, is computed as:\\n \\n_\\n \\nX  w =  ∑ \\ni=1\\n  \\nn\\n   w i  X i  , \\n(3)\\nwhere the sum of the weights equals 1; that is,   ∑ \\ni\\n   w i  = 1 .\\nIn the context of portfolios, a positive weight represents an asset held long and a \\nnegative weight represents an asset held short.\\nThe formula for the weighted mean can be compared to the formula for the arith-\\nmetic mean. For a set of observations X1, X2, …, Xn, let the weights w1, w2, …, wn all \\nequal 1/n. Under this assumption, the formula for the weighted mean is   ( 1 / n )  ∑ \\ni=1\\n  \\nn\\n   X i  . \\nThis is the formula for the arithmetic mean. Therefore, the arithmetic mean is a special \\ncase of the weighted mean in which all the weights are equal.\\nEXAMPLE 9\\nCalculating a Weighted Mean\\n1. Using the country index data shown in Exhibit 36, consider a portfolio \\nthat consists of three funds that track three countries’ indexes: County C, \\nCountry G, and Country K. The portfolio weights and index returns are as \\nfollows:\\n \\nIndex Tracked by Fund\\nAllocation \\n(%)\\nAnnual Return (%)\\nYear 1\\nYear 2\\nYear 3\\nCountry C\\n25%\\n5.3\\n1.2\\n3.5\\nCountry G\\n45%\\n12.7\\n6.7\\n−1.2\\nCountry K\\n30%\\n11.5\\n3.4\\n1.2\\n \\nUsing the information provided, calculate the returns on the portfolio for \\neach year.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Central Tendency\\n109\\nSolution\\nConverting the percentage asset allocation to decimal form, we find the \\nmean return as the weighted average of the funds’ returns. We have:\\n \\nMean portfolio return for Year 1\\n= 0.25 (5.3) + 0.45 (12.7) + 0.30(11.5)\\n\\xa0\\n= 10.50%\\nMean portfolio return for Year 2\\n= 0.25 (1.2) + 0.45 (6.7) + 0.30 (3.4)\\n\\xa0\\n= 4.34%\\nMean portfolio return for Year 3\\n= 0.25 (3.5) + 0.45 (−1.2) + 0.30 (1.2)\\n\\xa0\\n= 0.70%\\n \\nThis example illustrates the general principle that a portfolio return is a weighted \\nsum. Specifically, a portfolio’s return is the weighted average of the returns on the \\nassets in the portfolio; the weight applied to each asset’s return is the fraction of the \\nportfolio invested in that asset.\\nMarket indexes are computed as weighted averages. For market-capitalization \\nweighted indexes, such as the CAC-40 in France, the TOPIX in Japan, or the S&P \\n500 in the United States, each included stock receives a weight corresponding to its \\nmarket value divided by the total market value of all stocks in the index.\\nOur illustrations of weighted mean use past data, but they might just as well use \\nforward-looking data. When we take a weighted average of forward-looking data, the \\nweighted mean is the expected value. Suppose we make one forecast for the year-end \\nlevel of the S&P 500 assuming economic expansion and another forecast for the \\nyear-end level of the S&P 500 assuming economic contraction. If we multiply the first \\nforecast by the probability of expansion and the second forecast by the probability of \\ncontraction and then add these weighted forecasts, we are calculating the expected \\nvalue of the S&P 500 at year-end. If we take a weighted average of possible future \\nreturns on the S&P 500, where the weights are the probabilities, we are computing the \\nS&P 500’s expected return. The probabilities must sum to 1, satisfying the condition \\non the weights in the expression for weighted mean, Equation 3.\\nThe Geometric Mean\\nThe geometric mean is most frequently used to average rates of change over time or \\nto compute the growth rate of a variable. In investments, we frequently use the geo-\\nmetric mean to either average a time series of rates of return on an asset or a portfolio \\nor to compute the growth rate of a financial variable, such as earnings or sales. The \\ngeometric mean is defined by the following formula.\\nGeometric Mean Formula. The geometric mean,  _\\n \\nX  G , of a set of observa-\\ntions X1, X2, …, Xn is:\\n \\n_\\n \\nX  G =  \\nn √ \\n___________ \\n \\n X 1  X 2  X 3 … X n  with Xi ≥ 0 for i = 1, 2, …, n. \\n(4)\\nEquation 4 has a solution, and the geometric mean exists only if the product under \\nthe square root sign is non-negative. Therefore, we must impose the restriction that all \\nthe observations Xi are greater than or equal to zero. We can solve for the geometric \\nmean directly with any calculator that has an exponentiation key (on most calculators, \\nyx). We can also solve for the geometric mean using natural logarithms. Equation 4 \\ncan also be stated as\\n ln  \\n_\\n \\nX  G =  1 _ \\nn  ln  ( X 1  X 2  X 3 … X n )  ,\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n110\\nor, because the logarithm of a product of terms is equal to the sum of the loga-\\nrithms of each of the terms, as\\n ln  \\n_\\n \\nX  G =  \\n ∑ \\ni=1\\n  \\nn\\n  ln  X i   \\n_ \\nn \\n .\\nWhen we have computed  ln  _\\n \\nX  G , then   _\\n \\nX  G =  e ln _\\n \\nX  G  (on most calculators, the key \\nfor this step is ex).\\nRisky assets can have negative returns up to −100% (if their price falls to zero), so \\nwe must take some care in defining the relevant variables to average in computing a \\ngeometric mean. We cannot just use the product of the returns for the sample and \\nthen take the nth root because the returns for any period could be negative. We must \\nrecast the returns to make them positive. We do this by adding 1.0 to the returns \\nexpressed as decimals, where Rt represents the return in period t. The term (1 + Rt) \\nrepresents the year-ending value relative to an initial unit of investment at the begin-\\nning of the year. As long as we use (1 + Rt), the observations will never be negative \\nbecause the biggest negative return is −100%. The result is the geometric mean of 1 \\n+ Rt; by then subtracting 1.0 from this result, we obtain the geometric mean of the \\nindividual returns Rt.\\nAn equation that summarizes the calculation of the geometric mean return, RG, is \\na slightly modified version of Equation 4 in which Xi represents “1 + return in decimal \\nform.” Because geometric mean returns use time series, we use a subscript t indexing \\ntime as well. We calculate one plus the geometric mean return as:\\n 1 +  R G =  \\nT √ \\n__________________________ \\n \\n \\n  ( 1 +  R 1 )   ( 1 +  R 2 )  …  ( 1 +  R T )   .\\nWe can represent this more compactly as:\\n 1 +  R G =  [ ∏ \\nt=1\\n  \\nT\\n   ( 1 +  R t )  ] \\n 1 _ \\nT  \\n ,\\nwhere the capital Greek letter ‘pi,’ Π, denotes the arithmetical operation of multi-\\nplication of the T terms. Once we subtract one, this becomes the formula for the \\ngeometric mean return.\\nFor example, the returns on Country B’s index are given in Exhibit 36 as 7.8, 6.3, \\nand −1.5%. Putting the returns into decimal form and adding 1.0 produces 1.078, \\n1.063, and 0.985. Using Equation 4, we have   \\n3 √ \\n_____________________ \\n \\n  ( 1.078 )   ( 1.063 )   ( 0.985 )   =  \\n3 √ _ \\n1.128725  \\n= 1.041189. This number is 1 plus the geometric mean rate of return. Subtracting 1.0 \\nfrom this result, we have 1.041189 − 1.0 = 0.041189, or approximately 4.12%. This is \\nlower than the arithmetic mean for County B’s index of 4.2%.\\nGeometric Mean Return Formula. Given a time series of holding period \\nreturns Rt, t = 1, 2, …, T, the geometric mean return over the time period \\nspanned by the returns R1 through RT is:\\n R G =  [ ∏ \\nt=1\\n  \\nT\\n   ( 1 +  R t )  ] \\n 1 _ \\nT  \\n − 1 . \\n(5)\\nWe can use Equation 5 to solve for the geometric mean return for any return data \\nseries. Geometric mean returns are also referred to as compound returns. If the returns \\nbeing averaged in Equation 5 have a monthly frequency, for example, we may call the \\ngeometric mean monthly return the compound monthly return. The next example \\nillustrates the computation of the geometric mean while contrasting the geometric \\nand arithmetic means.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Central Tendency\\n111\\nEXAMPLE 10\\nGeometric and Arithmetic Mean Returns\\n1. Using the data in Exhibit 36, calculate the arithmetic mean and the geomet-\\nric mean returns over the three years for each of the three stock indexes: \\nthose of Country D, Country E, and Country F.\\nSolution\\nThe arithmetic mean returns calculations are:\\n \\n\\xa0\\nAnnual Return (%)\\nSum \\n ∑ \\ni=1\\n  \\n3\\n   R i   \\nArithmetic \\nMean\\nYear 1\\nYear 2\\nYear 3\\nCountry D\\n−2.4\\n−3.1\\n6.2\\n0.7\\n0.233\\nCountry E\\n−4.0\\n−3.0\\n3.0\\n−4.0\\n−1.333\\nCountry F\\n5.4\\n5.2\\n−1.0\\n9.6\\n3.200\\nGeometric mean returns calculations are:\\n \\n\\xa0\\n1 + Return in Decimal \\nForm \\n(1 + Rt)\\nProduct \\nT t ∏ \\n   ( 1 +  R t )   \\n3rd root \\n [ ∏ \\n3 t\\n   ( 1 +  R t )  ] \\n3   1 _ \\n \\nGeometric \\nmean \\nreturn (%)\\nYear 1\\nYear 2\\nYear 3\\nCountry D\\n0.976\\n0.969\\n1.062\\n1.00438\\n1.00146\\n0.146\\nCountry E\\n0.960\\n0.970\\n1.030\\n0.95914\\n0.98619\\n−1.381\\nCountry F\\n1.054\\n1.052\\n0.990\\n1.09772\\n1.03157\\n3.157\\n \\nIn Example 10, the geometric mean return is less than the arithmetic mean return \\nfor each country’s index returns. In fact, the geometric mean is always less than or \\nequal to the arithmetic mean. The only time that the two means will be equal is when \\nthere is no variability in the observations—that is, when all the observations in the \\nseries are the same.\\nIn general, the difference between the arithmetic and geometric means increases \\nwith the variability within the sample; the more disperse the observations, the greater \\nthe difference between the arithmetic and geometric means. Casual inspection of the \\nreturns in Exhibit 36 and the associated graph of means suggests a greater variability for \\nCountry A’s index relative to the other indexes, and this is confirmed with the greater \\ndeviation of the geometric mean return (−5.38%) from the arithmetic mean return \\n(−4.97%), as we show in Exhibit 41. How should the analyst interpret these results?\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n112\\nExhibit 41: Arithmetic and Geometric Mean Returns for Country Stock \\nIndexes: Years 1 to 3\\nCountry\\nB\\nA\\nC\\nD\\nF\\nE\\nG\\nH\\nI\\nJ\\nK\\n6\\n8\\n–4\\n–2\\n4\\n6\\n2\\n0\\nMean Return (%)\\nGeometric Mean\\nArithmetic Average\\nThe geometric mean return represents the growth rate or compound rate of return \\non an investment. One unit of currency invested in a fund tracking the Country B \\nindex at the beginning of Year 1 would have grown to (1.078)(1.063)(0.985) = 1.128725 \\nunits of currency, which is equal to 1 plus the geometric mean return compounded \\nover three periods: [1 + 0.041189]3 = 1.128725, confirming that the geometric mean \\nis the compound rate of return. With its focus on the profitability of an investment \\nover a multi-period horizon, the geometric mean is of key interest to investors. The \\narithmetic mean return, focusing on average single-period performance, is also of \\ninterest. Both arithmetic and geometric means have a role to play in investment \\nmanagement, and both are often reported for return series.\\nFor reporting historical returns, the geometric mean has considerable appeal \\nbecause it is the rate of growth or return we would have to earn each year to match \\nthe actual, cumulative investment performance. Suppose we purchased a stock for \\n€100 and two years later it was worth €100, with an intervening year at €200. The \\ngeometric mean of 0% is clearly the compound rate of growth during the two years, \\nwhich we can confirm by compounding the returns: [(1 + 1.00)(1 − 0.50)]1/2 − 1 = \\n0%. Specifically, the ending amount is the beginning amount times (1 + RG)2. The \\ngeometric mean is an excellent measure of past performance.\\nThe arithmetic mean, which is [100% + −50%]/2 = 25% in the above example, \\ncan distort our assessment of historical performance. As we noted previously, the \\narithmetic mean is always greater than or equal to the geometric mean. If we want to \\nestimate the average return over a one-period horizon, we should use the arithmetic \\nmean because the arithmetic mean is the average of one-period returns. If we want \\nto estimate the average returns over more than one period, however, we should use \\nthe geometric mean of returns because the geometric mean captures how the total \\nreturns are linked over time. In a forward-looking context, a financial analyst calcu-\\nlating expected risk premiums may find that the weighted mean is appropriate, with \\nthe probabilities of the possible outcomes used as the weights.\\nDispersion in cash flows or returns causes the arithmetic mean to be larger than \\nthe geometric mean. The more dispersion in the sample of returns, the more diver-\\ngence exists between the arithmetic and geometric means. If there is zero variance in \\na sample of observations, the geometric and arithmetic return are equal.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Central Tendency\\n113\\nThe Harmonic Mean\\nThe arithmetic mean, the weighted mean, and the geometric mean are the most \\nfrequently used concepts of mean in investments. A fourth concept, the harmonic \\nmean,     _\\n \\nX    H   , is another measure of central tendency. The harmonic mean is appropriate \\nin cases in which the variable is a rate or a ratio. The terminology “harmonic” arises \\nfrom its use of a type of series involving reciprocals known as a harmonic series.\\nHarmonic Mean Formula. The harmonic mean of a set of observations X1, \\nX2, …, Xn is:\\n \\n_\\n \\nX  H =  \\nn \\n_ \\n ∑ \\ni=1\\n  \\nn\\n   ( 1\\u200a/\\u200a X i )  \\n   with Xi > 0 for i = 1, 2, …, n. \\n(6)\\nThe harmonic mean is the value obtained by summing the reciprocals of the \\nobservations—terms of the form 1/Xi—then averaging that sum by dividing it by the \\nnumber of observations n, and, finally, taking the reciprocal of the average.\\nThe harmonic mean may be viewed as a special type of weighted mean in which an \\nobservation’s weight is inversely proportional to its magnitude. For example, if there \\nis a sample of observations of 1, 2, 3, 4, 5, 6, and 1,000, the harmonic mean is 2.8560. \\nCompared to the arithmetic mean of 145.8571, we see the influence of the outlier (the \\n1,000) to be much less than in the case of the arithmetic mean. So, the harmonic mean \\nis quite useful as a measure of central tendency in the presence of outliers.\\nThe harmonic mean is used most often when the data consist of rates and ratios, \\nsuch as P/Es. Suppose three peer companies have P/Es of 45, 15, and 15. The arithmetic \\nmean is 25, but the harmonic mean, which gives less weight to the P/E of 45, is 19.3.\\nEXAMPLE 11\\nHarmonic Mean Returns and the Returns on Selected \\nCountry Stock Indexes\\nUsing data in Exhibit 36, calculate the harmonic mean return over the 2016–2018 \\nperiod for three stock indexes: Country D, Country E, and Country F.\\n \\nCalculating the Harmonic Mean for the Indexes\\n \\n \\nIndex\\nInverse of 1 + Return, or \\n \\n1 \\n_ \\n ( 1 +  X i )     \\nwhere Xi is the return \\nin decimal form\\nn i ∑ \\n  1 /  X i   \\n \\nn \\n_ \\nn i ∑ \\n  1 /  X i   \\n  \\nHarmonic \\nMean (%)\\nYear 1\\nYear 2\\nYear 3\\nCountry D\\n1.02459\\n1.03199\\n0.94162\\n2.99820\\n1.00060\\n0.05999\\nCountry E\\n1.04167\\n1.03093\\n0.97087\\n3.04347\\n0.98572\\n−1.42825\\nCountry F\\n0.94877\\n0.95057\\n1.01010\\n2.90944\\n1.03113\\n3.11270\\n \\nComparing the three types of means, we see the arithmetic mean is higher \\nthan the geometric mean return, and the geometric mean return is higher than \\nthe harmonic mean return. We can see the differences in these means in the \\nfollowing graph:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n114\\nHarmonic, Geometric, and Arithmetic Means of Selected \\nCountry Indexes\\nCountry\\nD\\nE\\nF\\n–2\\n4\\n–1\\n2\\n1\\n3\\nMean Return (%)\\nHarmonic\\nGeometric Mean\\nArithmetic\\n0\\n0.233\\n0.146\\n0.060\\n−1.333\\n−1.381\\n−1.428\\n3.200\\n3.157\\n3.113\\nThe harmonic mean is a relatively specialized concept of the mean that is appro-\\npriate for averaging ratios (“amount per unit”) when the ratios are repeatedly applied \\nto a fixed quantity to yield a variable number of units. The concept is best explained \\nthrough an illustration. A well-known application arises in the investment strategy \\nknown as cost averaging, which involves the periodic investment of a fixed amount \\nof money. In this application, the ratios we are averaging are prices per share at \\ndifferent purchase dates, and we are applying those prices to a constant amount of \\nmoney to yield a variable number of shares. An illustration of the harmonic mean to \\ncost averaging is provided in Example 12.\\nEXAMPLE 12\\nCost Averaging and the Harmonic Mean\\n1. Suppose an investor purchases €1,000 of a security each month for n = 2 \\nmonths. The share prices are €10 and €15 at the two purchase dates. What is \\nthe average price paid for the security?\\nPurchase in the first month = €1,000/€10 = 100 shares\\nPurchase in the second month = €1,000/€15 = 66.67 shares\\nThe purchases are 166.67 shares in total, and the price paid per share is \\n€2,000/166.67 = €12.\\nThe average price paid is in fact the harmonic mean of the asset’s prices at \\nthe purchase dates. Using Equation 6, the harmonic mean price is 2/[(1/10) \\n+ (1/15)] = €12. The value €12 is less than the arithmetic mean purchase \\nprice (€10 + €15)/2 = €12.5.\\nSolution:\\nHowever, we could find the correct value of €12 using the weight-\\ned mean formula, where the weights on the purchase prices equal the \\nshares purchased at a given price as a proportion of the total shares pur-\\nchased. In our example, the calculation would be (100/166.67)€10.00 + \\n(66.67/166.67)€15.00 = €12. If we had invested varying amounts of money at \\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Central Tendency\\n115\\neach date, we could not use the harmonic mean formula. We could, howev-\\ner, still use the weighted mean formula.\\nSince they use the same data but involve different progressions in their respective \\ncalculations (that is, arithmetic, geometric, and harmonic progressions) the arithmetic, \\ngeometric, and harmonic means are mathematically related to one another. While we \\nwill not go into the proof of this relationship, the basic result follows:\\n Arithmetic mean × Harmonic mean = Geometric mean2.\\nHowever, the key question is: Which mean to use in what circumstances?\\nEXAMPLE 13\\nCalculating the Arithmetic, Geometric, and Harmonic \\nMeans for P/Es\\nEach year in December, a securities analyst selects her 10 favorite stocks for the \\nnext year. Exhibit 42 gives the P/E, the ratio of share price to projected earnings \\nper share (EPS), for her top-10 stock picks for the next year.\\n \\nExhibit 42: Analyst’s 10 Favorite Stocks for Next Year\\n \\n \\nStock\\nP/E\\nStock 1\\n22.29\\nStock 2\\n15.54\\nStock 3\\n9.38\\nStock 4\\n15.12\\nStock 5\\n10.72\\nStock 6\\n14.57\\nStock 7\\n7.20\\nStock 8\\n7.97\\nStock 9\\n10.34\\nStock 10\\n8.35\\n \\nFor these 10 stocks,\\n1. Calculate the arithmetic mean P/E.\\nSolution to 1:\\nThe arithmetic mean is 121.48/10 = 12.1480.\\n2. Calculate the geometric mean P/E.\\nSolution to 2:\\nThe geometric mean is  e 24.3613/10 = 11.4287.\\n3. Calculate the harmonic mean P/E.\\nSolution to 3:\\nThe harmonic mean is 10/0.9247 = 10.8142.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n116\\nA mathematical fact concerning the harmonic, geometric, and arithmetic means is \\nthat unless all the observations in a dataset have the same value, the harmonic mean \\nis less than the geometric mean, which, in turn, is less than the arithmetic mean. The \\nchoice of which mean to use depends on many factors, as we describe in Exhibit 43:\\n■ \\nAre there outliers that we want to include?\\n■ \\nIs the distribution symmetric?\\n■ \\nIs there compounding?\\n■ \\nAre there extreme outliers?\\nExhibit 43: Deciding Which Central Tendency Measure to Use\\nInclude all\\nvalues,\\nincluding\\noutliers?\\nCollect Sample\\nCompounding?\\nExtreme\\noutliers?\\nYes\\nYes\\nYes\\nGeometric Mean\\nArithmetic Mean\\nHarmonic mean,\\nTrimmed mean,\\nWinsorized mean\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Quantiles',\n",
       "     'page_number': 126,\n",
       "     'content': 'QUANTILES\\ncalculate quantiles and interpret related visualizations\\nHaving discussed measures of central tendency, we now examine an approach to \\ndescribing the location of data that involves identifying values at or below which \\nspecified proportions of the data lie. For example, establishing that 25, 50, and 75% \\nof the annual returns on a portfolio are at or below the values −0.05, 0.16, and 0.25, \\nrespectively, provides concise information about the distribution of portfolio returns. \\nStatisticians use the word quantile (or fractile) as the most general term for a value at \\nor below which a stated fraction of the data lies. In the following section, we describe \\nthe most commonly used quantiles—',\n",
       "     'children': [{'title': 'Quartiles, Quintiles, Deciles, and Percentiles',\n",
       "       'page_number': 127,\n",
       "       'content': 'Quartiles, Quintiles, Deciles, and Percentiles\\nWe know that the median divides a distribution of data in half. We can define other \\ndividing lines that split the distribution into smaller sizes. Quartiles divide the dis-\\ntribution into quarters, quintiles into fifths, deciles into tenths, and percentiles \\ninto hundredths. Given a set of observations, the yth percentile is the value at or \\nbelow which y% of observations lie. Percentiles are used frequently, and the other \\nmeasures can be defined with respect to them. For example, the first quartile (Q1) \\ndivides a distribution such that 25% of the observations lie at or below it; therefore, \\nthe first quartile is also the 25th percentile. The second quartile (Q2) represents the \\n50th percentile, and the third quartile (Q3) represents the 75th percentile (i.e., 75%of \\nthe observations lie at or below it). The interquartile range (IQR) is the difference \\nbetween the third quartile and the first quartile, or IQR = Q3 − Q1.\\nWhen dealing with actual data, we often find that we need to approximate the \\nvalue of a percentile. For example, if we are interested in the value of the 75th percen-\\ntile, we may find that no observation divides the sample such that exactly 75% of the \\nobservations lie at or below that value. The following procedure, however, can help us \\ndetermine or estimate a percentile. The procedure involves first locating the position \\nof the percentile within the set of observations and then determining (or estimating) \\nthe value associated with that position.\\nLet Py be the value at or below which y% of the distribution lies, or the yth per-\\ncentile. (For example, P18 is the point at or below which 18% of the observations \\nlie; this implies that 100 − 18 = 82% of the observations are greater than P18.) The \\nformula for the position (or location) of a percentile in an array with n entries sorted \\nin ascending order is:\\n L y =   ( n + 1 )  \\ny _ \\n100  , \\n(7)\\nwhere y is the percentage point at which we are dividing the distribution, and Ly \\nis the location (L) of the percentile (Py) in the array sorted in ascending order. The \\nvalue of Ly may or may not be a whole number. In general, as the sample size increases, \\nthe percentile location calculation becomes more accurate; in small samples it may \\nbe quite approximate.\\nTo summarize:\\n■ \\nWhen the location, Ly, is a whole number, the location corresponds to an \\nactual observation. For example, if we are determining the third quartile \\n(Q3) in a sample of size n = 11, then Ly would be L75 = (11 + 1)(75/100) = \\n9, and the third quartile would be P75 = X9, where Xi is defined as the value \\nof the observation in the ith (i = L75, so 9th), position of the data sorted in \\nascending order.\\n■ \\nWhen Ly is not a whole number or integer, Ly lies between the two closest \\ninteger numbers (one above and one below), and we use linear interpola-\\ntion between those two places to determine Py. Interpolation means esti-\\nmating an unknown value on the basis of two known values that surround \\nit (i.e., lie above and below it); the term “linear” refers to a straight-line \\nestimate.\\nExample 14 illustrates the calculation of various quantiles for the daily return on \\nthe EAA Equity Index.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n118\\nEXAMPLE 14\\nPercentiles, Quintiles, and Quartiles for the EAA Equity \\nIndex\\nUsing the daily returns on the fictitious EAA Equity Index over five years and \\nranking them by return, from lowest to highest daily return, we show the return \\nbins from 1 (the lowest 5%) to 20 (the highest 5%) as follows:\\n \\nExhibit 44: EAA Equity Index Daily Returns Grouped by Size of \\nReturn\\n \\n \\nBin\\nCumulative \\nPercentage \\nof Sample \\nTrading Days \\n(%)\\nDaily Return (%) Between*\\nNumber of \\nObservations\\nLower Bound\\nUpper Bound\\n1\\n5\\n−4.108\\n−1.416\\n63\\n2\\n10\\n−1.416\\n−0.876\\n63\\n3\\n15\\n−0.876\\n−0.629\\n63\\n4\\n20\\n−0.629\\n−0.432\\n63\\n5\\n25\\n−0.432\\n−0.293\\n63\\n6\\n30\\n−0.293\\n−0.193\\n63\\n7\\n35\\n−0.193\\n−0.124\\n62\\n8\\n40\\n−0.124\\n−0.070\\n63\\n9\\n45\\n−0.070\\n−0.007\\n63\\n10\\n50\\n−0.007\\n0.044\\n63\\n11\\n55\\n0.044\\n0.108\\n63\\n12\\n60\\n0.108\\n0.173\\n63\\n13\\n65\\n0.173\\n0.247\\n63\\n14\\n70\\n0.247\\n0.343\\n62\\n15\\n75\\n0.343\\n0.460\\n63\\n16\\n80\\n0.460\\n0.575\\n63\\n17\\n85\\n0.575\\n0.738\\n63\\n18\\n90\\n0.738\\n0.991\\n63\\n19\\n95\\n0.991\\n1.304\\n63\\n20\\n100\\n1.304\\n5.001\\n63\\n \\nNote that because of the continuous nature of returns, it is not likely for a \\nreturn to fall on the boundary for any bin other than the minimum (Bin = 1) \\nand maximum (Bin = 20).\\n1. Identify the 10th and 90th percentiles.\\nSolution to 1\\nThe 10th and 90th percentiles correspond to the bins or ranked returns that \\ninclude 10% and 90% of the daily returns, respectively. The 10th percentile \\ncorresponds to the return of −0.876% (and includes returns of that much \\nand lower), and the 90th percentile corresponds to the return of 0.991% (and \\nlower).\\n© CFA Institute. For candidate use only. Not for distribution.\\nQuantiles\\n119\\n2. Identify the first, second, and third quintiles.\\nSolution to 2\\nThe first quintile corresponds to the lowest 20% of the ranked data, or \\n−0.432% (and lower).\\nThe second quintile corresponds to the lowest 40% of the ranked data, or \\n−0.070% (and lower).\\nThe third quintile corresponds to the lowest 60% of the ranked data, or \\n0.173% (and lower).\\n3. Identify the first and third quartiles.\\nSolution to 3\\nThe first quartile corresponds to the lowest 25% of the ranked data, or \\n−0.293% (and lower).\\nThe third quartile corresponds to the lowest 75% of the ranked data, or \\n0.460% (and lower).\\n4. Identify the median.\\nSolution to 4\\nThe median is the return for which 50% of the data lies on either side, which \\nis 0.044%, the highest daily return in the 10th bin out of 20.\\n5. Calculate the interquartile range.\\nSolution to 5\\nThe interquartile range is the difference between the third and first quar-\\ntiles, 0.460% and −0.293%, or 0.753%.\\nOne way to visualize the dispersion of data across quartiles is to use a diagram, \\nsuch as a box and whisker chart. A box and whisker plot consists of a “box” with \\n“whiskers” connected to the box, as shown in Exhibit 45. The “box” represents the \\nlower bound of the second quartile and the upper bound of the third quartile, with \\nthe median or arithmetic average noted as a measure of central tendency of the entire \\ndistribution. The whiskers are the lines that run from the box and are bounded by the \\n“fences,” which represent the lowest and highest values of the distribution.\\nExhibit 45: Box and Whisker Plot\\nInterquartile\\nRange\\nHighest Value\\n×\\nUpper Boundary for Q3\\nMedian\\nArithmetic Average\\nLowest Boundary for Q2\\nLowest Value\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n120\\nThere are several variations for box and whisker displays. For example, for ease in \\ndetecting potential outliers, the fences of the whiskers may be a function of the inter-\\nquartile range instead of the highest and lowest values like that in Exhibit 45.\\nIn Exhibit 45, visually, the interquartile range is the height of the box and the \\nfences are set at extremes. But another form of box and whisker plot typically uses \\n1.5 times the interquartile range for the fences. Thus, the upper fence is 1.5 times the \\ninterquartile range added to the upper bound of Q3, and the lower fence is 1.5 times \\nthe interquartile range subtracted from the lower bound of Q2. Observations beyond \\nthe fences (i.e., outliers) may also be displayed.\\nWe can see the role of outliers in such a box and whisker plot using the EAA Equity \\nIndex daily returns, as shown in Exhibit 46. Referring back to Exhibit 44 (Example \\n13), we know:\\n■ \\nThe maximum and minimum values of the distribution are 5.001 and \\n−4.108, respectively, while the median (50th percentile) value is 0.044.\\n■ \\nThe interquartile range is 0.753 [= 0.460 − (−0.293)], and when multiplied by \\n1.5 and added to the Q3 upper bound of 0.460 gives an upper fence of 1.589 \\n[= (1.5 × 0.753) + 0.460].\\n■ \\nThe lower fence is determined in a similar manner, using the Q2 lower \\nbound, to be −1.422 [= −(1.5 × 0.753) + (−0.293)].\\nAs noted, any observation above (below) the upper (lower) fence is deemed to \\nbe an outlier.\\nExhibit 46: Box and Whisker Chart for EAA Equity Index Daily Returns\\nDaily Return (%)\\n6\\n5\\n4\\n3\\n2\\n1\\n–1\\n0\\n–2\\n–3\\n–4\\n–5\\nMaximum of 5.001%\\n(Q3 Upper \\nBound)\\n0.460%\\n(Q2 Lower\\nBound)\\n–0.293%\\n1.589% (Upper Fence)\\nMedian of\\n0.044%\\n–1.422%\\n(Lower Fence)\\nMinimum of –4.108%\\nEXAMPLE 15\\nQuantiles\\nConsider the results of an analysis focusing on the market capitalizations of a \\nsample of 100 firms:\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nQuantiles\\n121\\nBin\\nCumulative \\nPercentage of \\nSample (%)\\nMarket Capitalization  \\n(in billions of €)\\nNumber of \\nObservations\\nLower Bound\\nUpper Bound\\n1\\n5\\n0.28\\n15.45\\n5\\n2\\n10\\n15.45\\n21.22\\n5\\n3\\n15\\n21.22\\n29.37\\n5\\n4\\n20\\n29.37\\n32.57\\n5\\n5\\n25\\n32.57\\n34.72\\n5\\n6\\n30\\n34.72\\n37.58\\n5\\n7\\n35\\n37.58\\n39.90\\n5\\n8\\n40\\n39.90\\n41.57\\n5\\n9\\n45\\n41.57\\n44.86\\n5\\n10\\n50\\n44.86\\n46.88\\n5\\n11\\n55\\n46.88\\n49.40\\n5\\n12\\n60\\n49.40\\n51.27\\n5\\n13\\n65\\n51.27\\n53.58\\n5\\n14\\n70\\n53.58\\n56.66\\n5\\n15\\n75\\n56.66\\n58.34\\n5\\n16\\n80\\n58.34\\n63.10\\n5\\n17\\n85\\n63.10\\n67.06\\n5\\n18\\n90\\n67.06\\n73.00\\n5\\n19\\n95\\n73.00\\n81.62\\n5\\n20\\n100\\n81.62\\n96.85\\n5\\n \\nUsing this information, answer the following five questions.\\n1. The tenth percentile corresponds to observations in bins:\\nA. 2.\\nB. 1 and 2.\\nC. 19 and 20.\\nSolution to 1\\nB is correct because the tenth percentile corresponds to the lowest 10% of \\nthe observations in the sample, which are in bins 1 and 2.\\n2. The second quintile corresponds to observations in bins:\\nA. 8\\nB. 5, 6, 7, and 8.\\nC. 6, 7, 8, 9, and 10.\\nSolution to 2\\nB is correct because the second quintile corresponds to the second 20% of \\nobservations. The first 20% consists of bins 1 through 4. The second 20% of \\nobservations consists of bins 5 through 8.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n122\\n3. The fourth quartile corresponds to observations in bins:\\nA. 17.\\nB. 17, 18, 19, and 20.\\nC. 16, 17, 18, 19, and 20.\\nSolution to 3\\nC is correct because a quartile consists of 25% of the data, and the last 25% \\nof the 20 bins are 16 through 20.\\n4. The median is closest to:\\nA. 44.86.\\nB. 46.88.\\nC. 49.40.\\nSolution to 4\\nB is correct because this is the center of the 20 bins. The market capitaliza-\\ntion of 46.88 is the highest value of the 10th bin and the lowest value of the \\n11th bin.\\n5. The interquartile range is closest to:\\nA. 20.76.\\nB. 23.62.\\nC. 25.52.\\nSolution to 5\\nB is correct because the interquartile range is the difference between the \\nlowest value in the second quartile and the highest value in the third quar-\\ntile. The lowest value of the second quartile is 34.72, and the highest value of \\nthe third quartile is 58.34. Therefore, the interquartile range is 58.34 − 34.72 \\n= 23.62.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Quantiles in Investment Practice',\n",
       "       'page_number': 132,\n",
       "       'content': 'Quantiles in Investment Practice\\nIn this section, we briefly discuss the use of quantiles in investments. Quantiles are \\nused in portfolio performance evaluation as well as in investment strategy develop-\\nment and research.\\nInvestment analysts use quantiles every day to rank performance—for example, \\nthe performance of portfolios. The performance of investment managers is often \\ncharacterized in terms of the percentile or quartile in which they fall relative to the \\nperformance of their peer group of managers. The Morningstar investment fund star \\nrankings, for example, associate the number of stars with percentiles of performance \\nrelative to similar-style investment funds.\\nAnother key use of quantiles is in investment research. For example, analysts often \\nrefer to the set of companies with returns falling below the 10th percentile cutoff point \\nas the bottom return decile. Dividing data into quantiles based on some characteristic \\nallows analysts to evaluate the impact of that characteristic on a quantity of interest. \\nFor instance, empirical finance studies commonly rank companies based on the mar-\\nket value of their equity and then sort them into deciles. The first decile contains the \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Measures of Dispersion',\n",
       "     'page_number': 133,\n",
       "     'content': 'Measures of Dispersion\\n123\\nportfolio of those companies with the smallest market values, and the tenth decile \\ncontains those companies with the largest market values. Ranking companies by decile \\nallows analysts to compare the performance of small companies with large ones.\\nMEASURES OF DISPERSION\\ncalculate and interpret measures of dispersion\\nFew would disagree with the importance of expected return or mean return in invest-\\nments: The mean return tells us where returns, and investment results, are centered. \\nTo more completely understand an investment, however, we also need to know how \\nreturns are dispersed around the mean. Dispersion is the variability around the central \\ntendency. If mean return addresses reward, then dispersion addresses risk.\\nIn this section, we examine the most common measures of dispersion: range, \\nmean absolute deviation, variance, and standard deviation. These are all measures of \\nabsolute dispersion. Absolute dispersion is the amount of variability present without \\ncomparison to any reference point or benchmark.\\nThese measures are used throughout investment practice. The variance or standard \\ndeviation of return is often used as a measure of risk pioneered by Nobel laureate \\nHarry Markowitz. Other measures of dispersion, mean absolute deviation and range, \\nare also useful in analyzing data.\\n',\n",
       "     'children': [{'title': 'The Range',\n",
       "       'page_number': 133,\n",
       "       'content': 'The Range\\nWe encountered range earlier when we discussed the construction of frequency dis-\\ntributions. It is the simplest of all the measures of dispersion.\\nDefinition of Range. The range is the difference between the maximum \\nand minimum values in a dataset:\\n Range = Maximum value − Minimum value.   \\n(8)\\nAs an illustration of range, consider Exhibit 35, our example of annual returns \\nfor countries’ stock indexes. The range of returns for Year 1 is the difference between \\nthe returns of Country G’s index and Country A’s index, or 12.7 − (−15.6) = 28.3%. \\nThe range of returns for Year 3 is the difference between the returns for the County \\nD index and the Country B index, or 6.2 − (−1.5) = 7.7%.\\nAn alternative definition of range specifically reports the maximum and minimum \\nvalues. This alternative definition provides more information than does the range as \\ndefined in Equation 8. In other words, in the above-mentioned case for Year 1, the \\nrange is reported as “from 12.7% to −15.6%.”\\nOne advantage of the range is ease of computation. A disadvantage is that the \\nrange uses only two pieces of information from the distribution. It cannot tell us how \\nthe data are distributed (that is, the shape of the distribution). Because the range is \\nthe difference between the maximum and minimum returns, it can reflect extremely \\nlarge or small outcomes that may not be representative of the distribution.\\n9\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n124\\n',\n",
       "       'children': []},\n",
       "      {'title': 'The Mean Absolute Deviation',\n",
       "       'page_number': 134,\n",
       "       'content': 'The Mean Absolute Deviation\\nMeasures of dispersion can be computed using all the observations in the distribution \\nrather than just the highest and lowest. But how should we measure dispersion? Our \\nprevious discussion on properties of the arithmetic mean introduced the notion of \\ndistance or deviation from the mean    ( X i −  _\\n \\nX  )  as a fundamental piece of information \\nused in statistics. We could compute measures of dispersion as the arithmetic average \\nof the deviations around the mean, but we would encounter a problem: The deviations \\naround the mean always sum to 0. If we computed the mean of the deviations, the \\nresult would also equal 0. Therefore, we need to find a way to address the problem of \\nnegative deviations canceling out positive deviations.\\nOne solution is to examine the absolute deviations around the mean as in the mean \\nabsolute deviation. This is also known as the average absolute deviation.\\nMean Absolute Deviation Formula. The mean absolute deviation (MAD) \\nfor a sample is:\\n MAD =  \\n ∑ \\ni=1\\n  \\nn\\n   | X i −  \\n_\\nX  |    \\n_ \\nn \\n , \\n(9)\\n where  \\n_\\n \\nX  is the sample mean, n is the number of observations in the sample, and \\nthe | | indicate the absolute value of what is contained within these bars.\\nIn calculating MAD, we ignore the signs of the deviations around the mean. For \\nexample, if Xi = −11.0 and   _\\n \\nX  = 4.5, the absolute value of the difference is |−11.0 − \\n4.5| = |−15.5| = 15.5. The mean absolute deviation uses all of the observations in the \\nsample and is thus superior to the range as a measure of dispersion. One technical \\ndrawback of MAD is that it is difficult to manipulate mathematically compared with \\nthe next measure we will introduce, sample variance. Example 16 illustrates the use \\nof the range and the mean absolute deviation in evaluating risk.\\nEXAMPLE 16\\nMean Absolute Deviation for Selected Countries’ Stock \\nIndex Returns\\n1. Using the country stock index returns in Exhibit 35, calculate the mean \\nabsolute deviation of the index returns for each year. Note the sample mean \\nreturns ( _\\n \\nX  ) are 3.5%, 2.5%, and 2.0% for Years 1, 2, and 3, respectively.\\nSolution\\n\\xa0\\nAbsolute Value of Deviation from the Mean \\n  | X i −  \\n_\\n \\nX  |   \\n\\xa0\\nYear 1\\nYear 2\\nYear 3\\nCountry A\\n19.1\\n7.9\\n4.1\\nCountry B\\n4.3\\n3.8\\n3.5\\nCountry C\\n1.8\\n1.3\\n1.5\\nCountry D\\n5.9\\n5.6\\n4.2\\nCountry E\\n7.5\\n5.5\\n1.0\\nCountry F\\n1.9\\n2.7\\n3.0\\nCountry G\\n9.2\\n4.2\\n3.2\\nCountry H\\n0.0\\n1.8\\n1.4\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Dispersion\\n125\\n\\xa0\\nAbsolute Value of Deviation from the Mean \\n  | X i −  \\n_\\n \\nX  |   \\n\\xa0\\nYear 1\\nYear 2\\nYear 3\\nCountry I\\n2.7\\n5.3\\n1.2\\nCountry J\\n4.6\\n1.6\\n2.9\\nCountry K\\n8.0\\n0.9\\n0.8\\nSum\\n65.0\\n40.6\\n26.8\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nMAD\\n5.91\\n3.69\\n2.44\\n \\nFor Year 3, for example, the sum of the absolute deviations from the arith-\\nmetic mean ( _\\n \\nX  = 2.0) is 26.8. We divide this by 11, with the resulting MAD \\nof 2.44.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Sample Variance and Sample Standard Deviation',\n",
       "       'page_number': 135,\n",
       "       'content': 'Sample Variance and Sample Standard Deviation\\nThe mean absolute deviation addressed the issue that the sum of deviations from the \\nmean equals zero by taking the absolute value of the deviations. A second approach \\nto the treatment of deviations is to square them. The variance and standard deviation, \\nwhich are based on squared deviations, are the two most widely used measures of \\ndispersion. Variance is defined as the average of the squared deviations around the \\nmean. Standard deviation is the positive square root of the variance. The following \\ndiscussion addresses the calculation and use of variance and standard deviation.\\nSample Variance\\nIn investments, we often do not know the mean of a population of interest, usually \\nbecause we cannot practically identify or take measurements from each member of \\nthe population. We then estimate the population mean using the mean from a sample \\ndrawn from the population, and we calculate a sample variance or standard deviation.\\nSample Variance Formula. The sample variance, s2, is:\\n s 2 =  \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  ) \\n_ 2   \\nn − 1 \\n , \\n(10)\\nwhere   _\\n \\nX  is the sample mean and n is the number of observations in the \\nsample.\\nGiven knowledge of the sample mean, we can use Equation 10 to calculate the sum \\nof the squared differences from the mean, taking account of all n items in the sample, \\nand then to find the mean squared difference by dividing the sum by n − 1. Whether \\na difference from the mean is positive or negative, squaring that difference results in \\na positive number. Thus, variance takes care of the problem of negative deviations \\nfrom the mean canceling out positive deviations by the operation of squaring those \\ndeviations.\\nFor the sample variance, by dividing by the sample size minus 1 (or n − 1) rather \\nthan n, we improve the statistical properties of the sample variance. In statistical terms, \\nthe sample variance defined in Equation 10 is an unbiased estimator of the population \\nvariance (a concept covered later in the curriculum on sampling). The quantity n − 1 is \\nalso known as the number of degrees of freedom in estimating the population variance. \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n126\\nTo estimate the population variance with s2, we must first calculate the sample mean, \\nwhich itself is an estimated parameter. Therefore, once we have computed the sample \\nmean, there are only n − 1 independent pieces of information from the sample; that \\nis, if you know the sample mean and n − 1 of the observations, you could calculate \\nthe missing sample observation.\\nSample Standard Deviation\\nBecause the variance is measured in squared units, we need a way to return to the \\noriginal units. We can solve this problem by using standard deviation, the square root \\nof the variance. Standard deviation is more easily interpreted than the variance because \\nstandard deviation is expressed in the same unit of measurement as the observations. \\nBy taking the square root, we return the values to the original unit of measurement. \\nSuppose we have a sample with values in euros. Interpreting the standard deviation \\nin euros is easier than interpreting the variance in squared euros.\\nSample Standard Deviation Formula. The sample standard deviation, s, \\nis:\\n s =  √ \\n_\\n \\n \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  ) \\n_ 2   \\nn − 1 \\n  , \\n(11)\\nwhere   _\\n \\nX  is the sample mean and n is the number of observations in the \\nsample.\\nTo calculate the sample standard deviation, we first compute the sample variance. \\nWe then take the square root of the sample variance. The steps for computing the \\nsample variance and the standard deviation are provided in Exhibit 47.\\nExhibit 47: Steps to Calculate Sample Standard Deviation and Variance\\nStep\\nDescription\\nNotation\\n1\\nCalculate the sample mean\\n _\\n \\nX  \\n2\\nCalculate the deviations from the sample mean\\n  ( X i −  _\\n \\nX  )   \\n3\\nCalculate each observation’s squared deviation from the \\nsample mean\\n ( X i −  _\\n \\nX  ) 2  \\n4\\nSum the squared deviations from the mean\\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  _\\n \\nX  ) 2  \\n5\\nDivide the sum of squared deviations from the mean by \\nn − 1. This is the variance (s2).\\n \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  _\\n \\nX  ) 2   \\n_ \\nn − 1 \\n \\n6\\nTake the square root of the sum of the squared deviations \\ndivided by n − 1. This is the standard deviation (s).\\n √ \\n_\\n \\n \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  _\\n \\nX  ) 2   \\n_ \\nn − 1 \\n  \\nWe illustrate the process of calculating the sample variance and standard deviation \\nin Example 17 using the returns of the selected country stock indexes presented in \\nExhibit 35.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMeasures of Dispersion\\n127\\nEXAMPLE 17\\nCalculating Sample Variance and Standard Deviation for \\nReturns on Selected Country Stock Indexes\\n1. Using the sample information on country stock indexes in Exhibit 35, \\ncalculate the sample variance and standard deviation of the sample of index \\nreturns for Year 3.\\nSolution\\n \\nIndex\\nSample \\nObservation\\nDeviation from the \\nSample Mean\\nSquared \\nDeviation\\nCountry A\\n6.1\\n4.1\\n16.810\\nCountry B\\n−1.5\\n−3.5\\n12.250\\nCountry C\\n3.5\\n1.5\\n2.250\\nCountry D\\n6.2\\n4.2\\n17.640\\nCountry E\\n3.0\\n1.0\\n1.000\\nCountry F\\n−1.0\\n−3.0\\n9.000\\nCountry G\\n−1.2\\n−3.2\\n10.240\\nCountry H\\n3.4\\n1.4\\n1.960\\nCountry I\\n3.2\\n1.2\\n1.440\\nCountry J\\n−0.9\\n−2.9\\n8.410\\nCountry K\\n1.2\\n−0.8\\n0.640\\nSum\\n22.0\\n0.0\\n81.640\\n \\nSample variance = 81.640/10 = 8.164\\nSample standard deviation =  √ _ \\n8.164   = 2.857\\nIn addition to looking at the cross-sectional standard deviation as we did in Example \\n17, we could also calculate the standard deviation of a given country’s returns across \\ntime (that is, the three years). Consider Country F, which has an arithmetic mean \\nreturn of 3.2%. The sample standard deviation is calculated as:\\n \\n √ \\n________________________________________ \\n \\n \\n \\n  ( 0.054 − 0.032 ) 2 +  ( 0.052 − 0.032 ) 2 +  ( −\\u200a0.01 − 0.032 ) 2  \\n \\n \\n \\n_______________________________________ \\n \\n2 \\n  \\n  \\n \\n \\n \\n \\n=  √ \\n________________________ \\n \\n \\n 0.000484 + 0.000400 + 0.001764 \\n \\n \\n_______________________ \\n \\n2 \\n    \\n \\n \\n=  √ _ \\n0.001324  \\n  \\n \\n= 3.6387 % .\\n \\n \\nBecause the standard deviation is a measure of dispersion about the arithmetic \\nmean, we usually present the arithmetic mean and standard deviation together when \\nsummarizing data. When we are dealing with data that represent a time series of \\npercentage changes, presenting the geometric mean—representing the compound \\nrate of growth—is also very helpful.\\nDispersion and the Relationship between the Arithmetic and the Geometric Means\\nWe can use the sample standard deviation to help us understand the gap between the \\narithmetic mean and the geometric mean. The relation between the arithmetic mean   \\n( _\\n \\nX  )  and geometric mean    ( _\\n \\nX  G )  is:\\n \\n_\\n \\nX  G ≈  \\n_\\n \\nX  −   s 2  \\n_ \\n2  .\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n128\\nIn other words, the larger the variance of the sample, the wider the difference between \\nthe geometric mean and the arithmetic mean.\\nUsing the data for Country F from Example 8, the geometric mean return is 3.1566%, \\nthe arithmetic mean return is 3.2%, and the factor s2/2 is 0.001324/2 = 0.0662%:\\n 3.1566% ≈ 3.2% − 0.0662%\\n 3.1566% ≈ 3.1338%.\\nThis relation informs us that the more disperse or volatile the returns, the larger the \\ngap between the geometric mean return and the arithmetic mean return.\\nDOWNSIDE DEVIATION AND COEFFICIENT OF \\nVARIATION\\ncalculate and interpret target downside deviation\\nAn asset’s variance or standard deviation of returns is often interpreted as a measure \\nof the asset’s risk. Variance and standard deviation of returns take account of returns \\nabove and below the mean, or upside and downside risks, respectively. However, \\ninvestors are typically concerned only with downside risk—for example, returns \\nbelow the mean or below some specified minimum target return. As a result, analysts \\nhave developed measures of downside risk.\\nIn practice, we may be concerned with values of return (or another variable) below \\nsome level other than the mean. For example, if our return objective is 6.0% annually \\n(our minimum acceptable return), then we may be concerned particularly with returns \\nbelow 6.0% a year. The 6.0% is the target. The target downside deviation, also referred \\nto as the target semideviation, is a measure of dispersion of the observations (here, \\nreturns) below the target. To calculate a sample target semideviation, we first specify \\nthe target. After identifying observations below the target, we find the sum of the \\nsquared negative deviations from the target, divide that sum by the total number of \\nobservations in the sample minus 1, and, finally, take the square root.\\nSample Target Semideviation Formula. The target semideviation, sTarget, is:\\n sTarget =  √\\n \\n_______________\\n \\n \\n \\n∑ \\nfor all X i ≤B\\n  \\n n\\n  \\n ( X i − B ) \\n_ 2  \\nn − 1    , \\n(12)\\nwhere B is the target and n is the total number of sample observations. We illus-\\ntrate this in Example 18.\\nEXAMPLE 18\\nCalculating Target Downside Deviation\\nSuppose the monthly returns on a portfolio are as shown:\\n \\nMonthly Portfolio Returns\\n \\n \\nMonth\\nReturn (%)\\nJanuary\\n5\\nFebruary\\n3\\n10\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Downside Deviation and Coefficient of Variation',\n",
       "     'page_number': 138,\n",
       "     'content': 'Downside Deviation and ',\n",
       "     'children': [{'title': 'Coefficient of Variation',\n",
       "       'page_number': 141,\n",
       "       'content': 'Coefficient of Variation\\n131\\n \\nThe target semideviation with 2% target =  √ \\n_\\n 53  \\n_ \\n11   = 2.195%.\\n3. Compare the standard deviation, the target downside deviation if the target \\nis 2%, and the target downside deviation if the target is 3%.\\nSolution to 3\\nThe standard deviation is based on the deviation from the mean, which is \\n2.25%. The standard deviation includes all deviations from the mean, not \\njust those below it. This results in a sample standard deviation of 2.958%.\\nConsidering just the four observations below the 2% target, the target \\nsemideviation is 2.195%. It is less than the sample standard deviation since \\ntarget semideviation captures only the downside risk (i.e., deviations below \\nthe target). Considering target semideviation with a 3% target, there are now \\nfive observations below 3%, so the target semideviation is higher, at 2.763%.\\nCoefficient of Variation\\nWe noted earlier that the standard deviation is more easily interpreted than variance \\nbecause standard deviation uses the same units of measurement as the observations. \\nWe may sometimes find it difficult to interpret what standard deviation means in terms \\nof the relative degree of variability of different sets of data, however, either because \\nthe datasets have markedly different means or because the datasets have different \\nunits of measurement. In this section, we explain a measure of relative dispersion, \\nthe coefficient of variation that can be useful in such situations. Relative dispersion \\nis the amount of dispersion relative to a reference value or benchmark.\\nThe coefficient of variation is helpful in such situations as that just described (i.e., \\ndatasets with markedly different means or different units of measurement).\\nCoefficient of Variation Formula. The coefficient of variation, CV, is the \\nratio of the standard deviation of a set of observations to their mean value:\\n CV = s\\u200a/\\u200a \\n_\\nX  ,  \\n(13)\\nwhere s is the sample standard deviation and   _\\n \\nX  is the sample mean.\\nWhen the observations are returns, for example, the coefficient of variation mea-\\nsures the amount of risk (standard deviation) per unit of reward (mean return). An \\nissue that may arise, especially when dealing with returns, is that if   _\\n \\nX  is negative, the \\nstatistic is meaningless.\\nThe CV may be stated as a multiple (e.g., 2 times) or as a percentage (e.g., 200%). \\nExpressing the magnitude of variation among observations relative to their average \\nsize, the coefficient of variation permits direct comparisons of dispersion across \\ndifferent datasets. Reflecting the correction for scale, the coefficient of variation is a \\nscale-free measure (that is, it has no units of measurement).\\nWe illustrate the usefulness of coefficient of variation for comparing datasets with \\nmarkedly different standard deviations using two hypothetical samples of companies \\nin Example 20.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n132\\nEXAMPLE 20\\nCoefficient of Variation of Returns on Assets\\nSuppose an analyst collects the return on assets (in percentage terms) for ten \\ncompanies for each of two industries:\\n \\nCompany \\nIndustry A\\nIndustry B\\n1\\n−5\\n−10\\n2\\n−3\\n−9\\n3\\n−1\\n−7\\n4\\n2\\n−3\\n5\\n4\\n1\\n6\\n6\\n3\\n7\\n7\\n5\\n8\\n9\\n18\\n9\\n10\\n20\\n10\\n11\\n22\\n \\nThese data can be represented graphically as the following:\\n–5\\n–3\\n–1\\n2\\n4\\n6 7\\n9 10 11\\n–10–9\\n–7\\n–3\\n1\\n3\\n5\\n18 20\\n22\\nIndustry A\\nIndustry B\\n1. Calculate the average return on assets (ROA) for each industry.\\nSolution to 1\\nThe arithmetic mean for both industries is the sum divided by 10, or 40/10 \\n= 4%.\\n2. Calculate the standard deviation of ROA for each industry.\\nSolution to 2\\nThe standard deviation using Equation 11 for Industry A is 5.60, and for \\nIndustry B the standard deviation is 12.12.\\n3. Calculate the coefficient of variation of ROA for each industry.\\nSolution to 3\\nThe coefficient of variation for Industry A = 5.60/4 = 1.40.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'The Shape of the Distributions',\n",
       "     'page_number': 143,\n",
       "     'content': 'The Shape of the Distributions\\n133\\nThe coefficient of variation for Industry B = 12.12/4 = 3.03.\\nThough the two industries have the same arithmetic mean ROA, the dis-\\npersion is different—with Industry B’s returns on assets being much more \\ndisperse than those of Industry A. The coefficients of variation for these two \\nindustries reflects this, with Industry B having a larger coefficient of varia-\\ntion. The interpretation is that the risk per unit of mean return is more than \\ntwo times (2.16 = 3.03/1.40) greater for Industry B compared to Industry A.\\nTHE SHAPE OF THE DISTRIBUTIONS\\ninterpret skewness\\ninterpret kurtosis\\nMean and variance may not adequately describe an investment’s distribution of returns. \\nIn calculations of variance, for example, the deviations around the mean are squared, \\nso we do not know whether large deviations are likely to be positive or negative. \\nWe need to go beyond measures of central tendency and dispersion to reveal other \\nimportant characteristics of the distribution. One important characteristic of interest \\nto analysts is the degree of symmetry in return distributions.\\nIf a return distribution is symmetrical about its mean, each side of the distribution \\nis a mirror image of the other. Thus, equal loss and gain intervals exhibit the same \\nfrequencies. If the mean is zero, for example, then losses from −5% to −3% occur with \\nabout the same frequency as gains from 3% to 5%.\\nOne of the most important distributions is the normal distribution, depicted \\nin Exhibit 48. This symmetrical, bell-shaped distribution plays a central role in the \\nmean–variance model of portfolio selection; it is also used extensively in financial risk \\nmanagement. The normal distribution has the following characteristics:\\n■ \\nIts mean, median, and mode are equal.\\n■ \\nIt is completely described by two parameters—its mean and variance (or \\nstandard deviation).\\nBut with any distribution other than a normal distribution, more information than \\nthe mean and variance is needed to characterize its shape.\\n11\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n134\\nExhibit 48: The Normal Distribution\\nDensity of Probability\\n–5\\n5\\n–1\\n0\\n–4\\n–3\\n–2\\n2\\n3\\n4\\n1\\nStandard Deviation\\nA distribution that is not symmetrical is skewed. A return distribution with positive \\nskew has frequent small losses and a few extreme gains. A return distribution with \\nnegative skew has frequent small gains and a few extreme losses. Exhibit 49 shows \\ncontinuous positively and negatively skewed distributions. The continuous positively \\nskewed distribution shown has a long tail on its right side; the continuous negatively \\nskewed distribution shown has a long tail on its left side.\\nFor a continuous positively skewed unimodal distribution, the mode is less than the \\nmedian, which is less than the mean. For the continuous negatively skewed unimodal \\ndistribution, the mean is less than the median, which is less than the mode. For a given \\nexpected return and standard deviation, investors should be attracted by a positive \\nskew because the mean return lies above the median. Relative to the mean return, \\npositive skew amounts to limited, though frequent, downside returns compared with \\nsomewhat unlimited, but less frequent, upside returns.\\n© CFA Institute. For candidate use only. Not for distribution.\\nThe Shape of the Distributions\\n135\\nExhibit 49: Properties of Skewed Distributions\\nDensity of Probability\\nA. Positively Skewed\\nMode Median\\nMean\\nDensity of Probability\\nB. Negatively Skewed\\nMedianMode\\nMean\\nSkewness is the name given to a statistical measure of skew. (The word “skewness” is \\nalso sometimes used interchangeably for “skew.”) Like variance, skewness is computed \\nusing each observation’s deviation from its mean. Skewness (sometimes referred \\nto as relative skewness) is computed as the average cubed deviation from the mean \\nstandardized by dividing by the standard deviation cubed to make the measure free \\nof scale. A symmetric distribution has skewness of 0, a positively skewed distribution \\nhas positive skewness, and a negatively skewed distribution has negative skewness, \\nas given by this measure.\\nWe can illustrate the principle behind the measure by focusing on the numera-\\ntor. Cubing, unlike squaring, preserves the sign of the deviations from the mean. If \\na distribution is positively skewed with a mean greater than its median, then more \\nthan half of the deviations from the mean are negative and less than half are positive. \\nHowever, for the sum of the cubed deviations to be positive, the losses must be small \\nand likely and the gains less likely but more extreme. Therefore, if skewness is positive, \\nthe average magnitude of positive deviations is larger than the average magnitude of \\nnegative deviations.\\nThe approximation for computing sample skewness when n is large (100 or \\nmore) is:\\n Skewness ≈   ( 1 _ \\nn  )  \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  ) \\n_ 3   \\n s 3  \\n .\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n136\\nA simple example illustrates that a symmetrical distribution has a skewness measure \\nequal to 0. Suppose we have the following data: 1, 2, 3, 4, 5, 6, 7, 8, and 9. The mean \\noutcome is 5, and the deviations are −4, −3, −2, −1, 0, 1, 2, 3, and 4. Cubing the devi-\\nations yields −64, −27, −8, −1, 0, 1, 8, 27, and 64, with a sum of 0. The numerator of \\nskewness (and so skewness itself) is thus equal to 0, supporting our claim.\\nAs you will learn as the CFA Program curriculum unfolds, different investment \\nstrategies may tend to introduce different types and amounts of skewness into returns.\\n',\n",
       "     'children': [{'title': 'The Shape of the Distributions: Kurtosis',\n",
       "       'page_number': 146,\n",
       "       'content': 'The Shape of the Distributions: Kurtosis\\nIn the previous section, we discussed how to determine whether a return distribution \\ndeviates from a normal distribution because of skewness. Another way in which a \\nreturn distribution might differ from a normal distribution is its relative tendency \\nto generate large deviations from the mean. Most investors would perceive a greater \\nchance of extremely large deviations from the mean as increasing risk.\\nKurtosis is a measure of the combined weight of the tails of a distribution relative \\nto the rest of the distribution—that is, the proportion of the total probability that is \\noutside of, say, 2.5 standard deviations of the mean. A distribution that has fatter tails \\nthan the normal distribution is referred to as leptokurtic or fat-tailed; a distribution \\nthat has thinner tails than the normal distribution is referred to as being platykurtic or \\nthin-tailed; and a distribution similar to the normal distribution as concerns relative \\nweight in the tails is called mesokurtic. A fat-tailed (thin-tailed) distribution tends \\nto generate more-frequent (less-frequent) extremely large deviations from the mean \\nthan the normal distribution.\\nExhibit 50 illustrates a fat-tailed distribution. It has fatter tails than the normal \\ndistribution. By construction, the fat-tailed and normal distributions in this exhibit \\nhave the same mean, standard deviation, and skewness. Note that this fat-tailed dis-\\ntribution is more likely than the normal distribution to generate observations in the \\ntail regions defined by the intersection of graphs near a standard deviation of about \\n±2.5. This fat-tailed distribution is also more likely to generate observations that are \\nnear the mean, defined here as the region ±1 standard deviation around the mean. \\nIn compensation, to have probabilities sum to 1, this distribution generates fewer \\nobservations in the regions between the central region and the two tail regions.\\nExhibit 50: Fat-Tailed Distribution Compared to the Normal Distribution\\nDensity of Probability\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0\\nNormal Distribution\\nFat-Tailed Distribution\\n–5\\n5\\n–1\\n0\\n–4\\n–3\\n–2\\n2\\n3\\n4\\n1\\nStandard Deviation\\n© CFA Institute. For candidate use only. Not for distribution.\\nThe Shape of the Distributions\\n137\\nThe calculation for kurtosis involves finding the average of deviations from the mean \\nraised to the fourth power and then standardizing that average by dividing by the \\nstandard deviation raised to the fourth power. A normal distribution has kurtosis of \\n3.0, so a fat-tailed distribution has a kurtosis of above 3 and a thin-tailed distribution \\nof below 3.0.\\nExcess kurtosis is the kurtosis relative to the normal distribution. For a large sam-\\nple size (n = 100 or more), sample excess kurtosis (KE) is approximately as follows:\\n K E ≈   \\n⎡\\n⎣ ⎢ \\n  ( 1 _ \\nn  )  \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  ) \\n_ 4   \\n s 4  \\n \\n⎤\\n⎦ ⎥ \\n  − 3 .\\nAs with skewness, this measure is free of scale. Many statistical packages report esti-\\nmates of sample excess kurtosis, labeling this as simply “kurtosis.”\\nExcess kurtosis thus characterizes kurtosis relative to the normal distribution. A \\nnormal distribution has excess kurtosis equal to 0. A fat-tailed distribution has excess \\nkurtosis greater than 0, and a thin-tailed distribution has excess kurtosis less than 0. A \\nreturn distribution with positive excess kurtosis—a fat-tailed return distribution—has \\nmore frequent extremely large deviations from the mean than a normal distribution.\\nSummarizing:\\nIf kurtosis is …\\nthen excess \\nkurtosis is …\\nTherefore, the \\ndistribution is …\\nAnd we refer to \\nthe distribution as \\nbeing …\\nabove 3.0\\nabove 0.\\nfatter-tailed than the \\nnormal distribution.\\nfat-tailed \\n(leptokurtic).\\nequal to 3.0\\nequal to 0.\\nsimilar in tails to the nor-\\nmal distribution.\\nmesokurtic.\\nless than 3.0\\nless than 0.\\nthinner-tailed than the \\nnormal distribution.\\nthin-tailed \\n(platykurtic).\\nMost equity return series have been found to be fat-tailed. If a return distribution is \\nfat-tailed and we use statistical models that do not account for the distribution, then \\nwe will underestimate the likelihood of very bad or very good outcomes. Using the \\ndata on the daily returns of the fictitious EAA Equity Index, we see the skewness and \\nkurtosis of these returns in Exhibit 51.\\nExhibit 51: Skewness and Kurtosis of EAA Equity Index Daily Returns\\n\\xa0\\nDaily Return (%)\\nArithmetic mean\\n0.0347\\nStandard deviation\\n0.8341\\n\\xa0\\n\\xa0\\n\\xa0\\nMeasure of Symmetry\\nSkewness\\n−0.4260\\nExcess kurtosis\\n3.7962\\nWe can see this graphically, comparing the distribution of the daily returns with \\na normal distribution with the same mean and standard deviation:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n138\\nNumber of Observations\\nNormal Distribution\\n–5\\n5\\n–1\\n0\\n–4\\n–3\\n–2\\n2\\n3\\n4\\n1\\nStandard Deviation\\nEAA Daily Returns\\nUsing both the statistics and the graph, we see the following:\\n■ \\nThe distribution is negatively skewed, as indicated by the negative cal-\\nculated skewness of −0.4260 and the influence of observations below \\nthe mean of 0.0347%.\\n■ \\nThe highest frequency of returns occurs within the −0.5 to 0.0 stan-\\ndard deviations from the mean (i.e., negatively skewed).\\n■ \\nThe distribution is fat-tailed, as indicated by the positive excess kur-\\ntosis of 3.7962. We can see fat tails, a concentration of returns around \\nthe mean, and fewer observations in the regions between the central \\nregion and the two-tail regions.\\nEXAMPLE 21\\nInterpreting Skewness and Kurtosis\\nConsider the daily trading volume for a stock for one year, as shown in the graph \\nbelow. In addition to the count of observations within each bin or interval, the \\nnumber of observations anticipated based on a normal distribution (given the \\nsample arithmetic average and standard deviation) is provided in the chart as \\nwell. The average trading volume per day for this stock in this year is 8.6 million \\nshares, and the standard deviation is 4.9 million shares.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Correlation between Two Variables',\n",
       "     'page_number': 149,\n",
       "     'content': 'Correlation between Two Variables\\n139\\nHistogram of Daily Trading Volume for a Stock for One Year\\n \\n3.1\\nto\\n4.6\\n4.6\\nto\\n6.1\\n6.1\\nto\\n7.7\\n7.7\\nto\\n9.2\\n9.2\\nto\\n10.7\\n10.7\\nto\\n12.3\\n12.3\\nto\\n13.8\\n13.8\\nto\\n15.3\\n15.3\\nto\\n16.8\\n16.8\\nto\\n18.4\\n18.4\\nto\\n19.9\\n19.9\\nto\\n21.4\\n21.4\\nto\\n23.0\\n23.0\\nto\\n24.5\\n24.5\\nto\\n26.0\\n26.0\\nto\\n27.5\\n27.5\\nto\\n29.1\\n29.1\\nto\\n30.6\\n30.6\\nto\\n32.1\\n32.1\\nto\\n33.7\\nNumber of Trading Days\\n70\\n60\\n50\\n40\\n30\\n20\\n10\\n0\\nTrading Volume Range of Shares (millions)\\nBased on the Normal Distribution\\nBased on the Sample\\n1. Describe whether or not this distribution is skewed. If so, what could ac-\\ncount for this situation?\\nSolution to 1\\nThe distribution appears to be skewed to the right, or positively skewed. \\nThis is likely due to: (1) no possible negative trading volume on a given trad-\\ning day, so the distribution is truncated at zero; and (2) greater-than-typical \\ntrading occurring relatively infrequently, such as when there are \\ncompany-specific announcements.\\nThe actual skewness for this distribution is 2.1090, which supports this \\ninterpretation.\\n2. Describe whether or not this distribution displays kurtosis. How would you \\nmake this determination?\\nSolution to 2\\nThe distribution appears to have excess kurtosis, with a right-side fat tail and \\nwith maximum shares traded in the 4.6 to 6.1 million range, exceeding what \\nis expected if the distribution was normally distributed. There are also fewer \\nobservations than expected between the central region and the tail.\\nThe actual excess kurtosis for this distribution is 5.2151, which supports this \\ninterpretation.\\nCORRELATION BETWEEN TWO VARIABLES\\ninterpret correlation between two variables\\n12\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n140\\nNow that we have some understanding of sample variance and standard deviation, we \\ncan more formally consider the concept of correlation between two random variables \\nthat we previously explored visually in the scatter plots in Section 4. Correlation is a \\nmeasure of the linear relationship between two random variables.\\nThe first step is to consider how two variables vary together, their covariance.\\nDefinition of Sample Covariance. The sample covariance (sXY) is a mea-\\nsure of how two variables in a sample move together:\\n s XY =  \\n ∑ \\ni=1\\n  \\nn\\n    ( X i −  \\n_\\n \\nX  )   ( Y i −  \\n_\\n \\n Y  )    \\n________________ \\nn − 1 \\n . \\n(14)\\nEquation 14 indicates that the sample covariance is the average value of the product \\nof the deviations of observations on two random variables (Xi and Yi) from their sample \\nmeans. If the random variables are returns, the units would be returns squared. Also, \\nnote the use of n − 1 in the denominator, which ensures that the sample covariance \\nis an unbiased estimate of population covariance.\\nStated simply, covariance is a measure of the joint variability of two random vari-\\nables. If the random variables vary in the same direction—for example, X tends to be \\nabove its mean when Y is above its mean, and X tends to be below its mean when Y is \\nbelow its mean—then their covariance is positive. If the variables vary in the opposite \\ndirection relative to their respective means, then their covariance is negative.\\nBy itself, the size of the covariance measure is difficult to interpret as it is not \\nnormalized and so depends on the magnitude of the variables. This brings us to the \\nnormalized version of covariance, which is the correlation coefficient.\\nDefinition of Sample Correlation Coefficient. The sample correlation \\ncoefficient is a standardized measure of how two variables in a sample \\nmove together. The sample correlation coefficient (rXY) is the ratio of the \\nsample covariance to the product of the two variables’ standard deviations:\\n r XY =  \\n s XY  \\n_ \\n s X  s Y   . \\n(15)\\nImportantly, the correlation coefficient expresses the strength of the linear rela-\\ntionship between the two random variables.\\n',\n",
       "     'children': [{'title': 'Properties of Correlation',\n",
       "       'page_number': 150,\n",
       "       'content': 'Properties of Correlation\\nWe now discuss the correlation coefficient, or simply correlation, and its properties \\nin more detail, as follows:\\n1. Correlation ranges from −1 and +1 for two random variables, X and Y:\\n −1 ≤ rXY ≤ +1.\\n2. A correlation of 0 (uncorrelated variables) indicates an absence of any linear \\n(that is, straight-line) relationship between the variables.\\n3. A positive correlation close to +1 indicates a strong positive linear relation-\\nship. A correlation of 1 indicates a perfect linear relationship.\\n4. A negative correlation close to −1 indicates a strong negative (that is, \\ninverse) linear relationship. A correlation of −1 indicates a perfect inverse \\nlinear relationship.\\n© CFA Institute. For candidate use only. Not for distribution.\\nCorrelation between Two Variables\\n141\\nWe will make use of scatter plots, similar to those used previously in our discussion \\nof data visualization, to illustrate correlation. In contrast to the correlation coefficient, \\nwhich expresses the relationship between two data series using a single number, a \\nscatter plot depicts the relationship graphically. Therefore, scatter plots are a very \\nuseful tool for the sensible interpretation of a correlation coefficient.\\nExhibit 52 shows examples of scatter plots. Panel A shows the scatter plot of \\ntwo variables with a correlation of +1. Note that all the points on the scatter plot in \\nPanel A lie on a straight line with a positive slope. Whenever variable X increases by \\none unit, variable Y increases by two units. Because all of the points in the graph lie \\non a straight line, an increase of one unit in X is associated with exactly a two-unit \\nincrease in Y, regardless of the level of X. Even if the slope of the line were different \\n(but positive), the correlation between the two variables would still be +1 as long as \\nall the points lie on that straight line. Panel B shows a scatter plot for two variables \\nwith a correlation coefficient of −1. Once again, the plotted observations all fall on a \\nstraight line. In this graph, however, the line has a negative slope. As X increases by \\none unit, Y decreases by two units, regardless of the initial value of X.\\nExhibit 52: Scatter Plots Showing Various Degrees of Correlation\\nVariable Y\\nA. Variables With a Correlation of +1\\n35\\n25\\n30\\n20\\n15\\n10\\n5\\n0\\n0\\n20\\n15\\n5\\n10\\nVariable X\\nVariable Y\\nB. Variables With a Correlation of –1\\n20\\n10\\n15\\n5\\n0\\n–5\\n–10\\n–15\\n0\\n20\\n10\\nVariable X\\nVariable Y\\nC. Variables With a Correlation of 0\\n12\\n8\\n10\\n6\\n4\\n2\\n0\\n0\\n20\\n10\\nVariable X\\nVariable Y\\nD. Variables With a Strong \\nNonlinear Association\\n60\\n40\\n50\\n30\\n20\\n5\\n0\\n0\\n20\\n10\\nVariable X\\nPanel C shows a scatter plot of two variables with a correlation of 0; they have no \\nlinear relation. This graph shows that the value of variable X tells us nothing about \\nthe value of variable Y. Panel D shows a scatter plot of two variables that have a \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n142\\nnon-linear relationship. Because the correlation coefficient is a measure of the linear \\nassociation between two variables, it would not be appropriate to use the correlation \\ncoefficient in this case.\\nExample 22 is meant to reinforce your understanding of how to interpret covari-\\nance and correlation.\\nEXAMPLE 22\\nInterpreting the Correlation Coefficient\\nConsider the statistics for the returns over twelve months for three funds, A, \\nB, and C, shown in Exhibit 53.\\n \\nExhibit 53\\n \\n \\n\\xa0\\nFund A\\nFund B\\nFund C\\nArithmetic average\\n2.9333\\n3.2250\\n2.6250\\nStandard deviation\\n2.4945\\n2.4091\\n3.6668\\n \\nThe covariances are represented in the upper-triangle (shaded area) of the \\nmatrix shown in Exhibit 54.\\n \\nExhibit 54\\n \\n \\n\\xa0\\nFund A\\nFund B\\nFund C\\nFund A\\n6.2224\\n5.7318\\n−3.6682\\nFund B\\n\\xa0\\n5.8039\\n−2.3125\\nFund C\\n\\xa0\\n\\xa0\\n13.4457\\n \\nThe covariance of Fund A and Fund B returns, for example, is 5.7318.\\nWhy show just the upper-triangle of this matrix? Because the covariance of \\nFund A and Fund B returns is the same as the covariance of Fund B and Fund \\nA returns.\\nThe diagonal of the matrix in Exhibit 54 is the variance of each fund’s return. \\nFor example, the variance of Fund A returns is 6.2224, but the covariance of \\nFund A and Fund B returns is 5.7138.\\nThe correlations among the funds’ returns are given in Exhibit 55, where \\nthe correlations are reported in the upper-triangle (shaded area) of the matrix. \\nNote that the correlation of a fund’s returns with itself is +1, so the diagonal in \\nthe correlation matrix consists of 1.000.\\n \\nExhibit 55\\n \\n \\n\\xa0\\nFund A\\nFund B\\nFund C\\nFund A\\n1.0000\\n0.9538\\n−0.4010\\nFund B\\n\\xa0\\n1.0000\\n−0.2618\\nFund C\\n\\xa0\\n\\xa0\\n1.0000\\n© CFA Institute. For candidate use only. Not for distribution.\\nCorrelation between Two Variables\\n143\\n \\n1. Interpret the correlation between Fund A’s returns and Fund B’s returns.\\nSolution to 1\\nThe correlation of Fund A and Fund B returns is 0.9538, which is positive \\nand close to 1.0. This means that when returns of Fund A tend to be above \\ntheir mean, Fund B’s returns also tend to be above their mean. Graphically, \\nwe would observe a positive, but not perfect, linear relationship between the \\nreturns for the two funds.\\n2. Interpret the correlation between Fund A’s returns and Fund C’s returns.\\nSolution to 2\\nThe correlation of Fund A’s returns and Fund C’s returns is −0.4010, which \\nindicates that when Fund A’s returns are above their mean, Fund B’s returns \\ntend to be below their mean. This implies a negative slope when graphing \\nthe returns of these two funds, but it would not be a perfect inverse relation-\\nship.\\n3. Describe the relationship of the covariance of these returns and the correla-\\ntion of returns.\\nSolution to 3\\nThere are two negative correlations: Fund A returns with Fund C returns, \\nand Fund B returns with Fund C returns. What determines the sign of the \\ncorrelation is the sign of the covariance, which in each of these cases is \\nnegative. When the covariance between fund returns is positive, such as \\nbetween Fund A and Fund B returns, the correlation is positive. This follows \\nfrom the fact that the correlation coefficient is the ratio of the covariance of \\nthe two funds’ returns to the product of their standard deviations.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Limitations of Correlation Analysis',\n",
       "       'page_number': 153,\n",
       "       'content': 'Limitations of Correlation Analysis\\nExhibit 52 illustrates that correlation measures the linear association between two \\nvariables, but it may not always be reliable. Two variables can have a strong nonlinear \\nrelation and still have a very low correlation. For example, the relation Y = (X − 4)2 is a \\nnonlinear relation contrasted to the linear relation Y = 2X − 4. The nonlinear relation \\nbetween variables X and Y is shown in Panel D. Below a level of 4 for X, Y increases \\nwith decreasing values of X. When X is 4 or greater, however, Y increases whenever X \\nincreases. Even though these two variables are perfectly associated, there is no linear \\nassociation between them (hence, no meaningful correlation).\\nCorrelation may also be an unreliable measure when outliers are present in one \\nor both of the variables. As we have seen, outliers are small numbers of observations \\nat either extreme (small or large) of a sample. The correlation may be quite sensitive \\nto outliers. In such a situation, we should consider whether it makes sense to exclude \\nthose outlier observations and whether they are noise or news. As a general rule, we \\nmust determine whether a computed sample correlation changes greatly by removing \\noutliers. We must also use judgment to determine whether those outliers contain \\ninformation about the two variables’ relationship (and should thus be included in the \\ncorrelation analysis) or contain no information (and should thus be excluded). If they \\nare to be excluded from the correlation analysis, as we have seen previously, outlier \\nobservations can be handled by trimming or winsorizing the dataset.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n144\\nImportantly, keep in mind that correlation does not imply causation. Even if two \\nvariables are highly correlated, one does not necessarily cause the other in the sense \\nthat certain values of one variable bring about the occurrence of certain values of \\nthe other.\\nMoreover, with visualizations too, including scatter plots, we must be on guard \\nagainst unconsciously making judgments about causal relationships that may or may \\nnot be supported by the data.\\nThe term spurious correlation has been used to refer to: 1) correlation between \\ntwo variables that reflects chance relationships in a particular dataset; 2) correlation \\ninduced by a calculation that mixes each of two variables with a third variable; and \\n3) correlation between two variables arising not from a direct relation between them \\nbut from their relation to a third variable.\\nAs an example of the chance relationship, consider the monthly US retail sales of \\nbeer, wine, and liquor and the atmospheric carbon dioxide levels from 2000–2018. \\nThe correlation is 0.824, indicating that there is a positive relation between the two. \\nHowever, there is no reason to suspect that the levels of atmospheric carbon dioxide \\nare related to the retail sales of beer, wine, and liquor.\\nAs an example of the second kind of spurious correlation, two variables that are \\nuncorrelated may be correlated if divided by a third variable. For example, consider a \\ncross-sectional sample of companies’ dividends and total assets. While there may be \\na low correlation between these two variables, dividing each by market capitalization \\nmay increase the correlation.\\nAs an example of the third kind of spurious correlation, height may be positively \\ncorrelated with the extent of a person’s vocabulary, but the underlying relationships \\nare between age and height and between age and vocabulary.\\nInvestment professionals must be cautious in basing investment strategies on high \\ncorrelations. Spurious correlations may suggest investment strategies that appear \\nprofitable but actually would not be, if implemented.\\nA further issue is that correlation does not tell the whole story about the data. \\nConsider Anscombe’s Quartet, discussed in Exhibit 56, where very dissimilar graphs \\ncan be developed with variables that have the same mean, same standard deviation, \\nand same correlation.\\nExhibit 56: Anscombe’s Quartet\\nFrancis Anscombe, a British statistician, developed datasets that illustrate why \\njust looking at ',\n",
       "       'children': []}]},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 156,\n",
       "     'content': 'SUMMARY\\nIn this reading, we have presented tools and techniques for organizing, visualizing, \\nand describing data that permit us to convert raw data into useful information for \\ninvestment analysis.\\n■ \\nData can be defined as a collection of numbers, characters, words, and \\ntext—as well as images, audio, and video—in a raw or organized format to \\nrepresent facts or information.\\n■ \\nFrom a statistical perspective, data can be classified as numerical data and \\ncategorical data. Numerical data (also called quantitative data) are values \\nthat represent measured or counted quantities as a number. Categorical data \\n(also called qualitative data) are values that describe a quality or character-\\nistic of a group of observations and usually take only a limited number of \\nvalues that are mutually exclusive.\\n© CFA Institute. For candidate use only. Not for distribution.\\nCorrelation between Two Variables\\n147\\n■ \\nNumerical data can be further split into two types: continuous data and dis-\\ncrete data. Continuous data can be measured and can take on any numerical \\nvalue in a specified range of values. Discrete data are numerical values that \\nresult from a counting process and therefore are limited to a finite number \\nof values.\\n■ \\nCategorical data can be further classified into two types: nominal data and \\nordinal data. Nominal data are categorical values that are not amenable to \\nbeing organized in a logical order, while ordinal data are categorical values \\nthat can be logically ordered or ranked.\\n■ \\nBased on how they are collected, data can be categorized into three types: \\ncross-sectional, time series, and panel. Time-series data are a sequence of \\nobservations for a single observational unit on a specific variable collected \\nover time and at discrete and typically equally spaced intervals of time. \\nCross-sectional data are a list of the observations of a specific variable from \\nmultiple observational units at a given point in time. Panel data are a mix of \\ntime-series and cross-sectional data that consists of observations through \\ntime on one or more variables for multiple observational units.\\n■ \\nBased on whether or not data are in a highly organized form, they can \\nbe classified into structured and unstructured types. Structured data are \\nhighly organized in a pre-defined manner, usually with repeating patterns. \\nUnstructured data do not follow any conventionally organized forms; they \\nare typically alternative data as they are usually collected from unconven-\\ntional sources.\\n■ \\nRaw data are typically organized into either a one-dimensional array or a \\ntwo-dimensional rectangular array (also called a data table) for quantitative \\nanalysis.\\n■ \\nA frequency distribution is a tabular display of data constructed either by \\ncounting the observations of a variable by distinct values or groups or by \\ntallying the values of a numerical variable into a set of numerically ordered \\nbins. Frequency distributions permit us to evaluate how data are distributed.\\n■ \\nThe relative frequency of observations in a bin (interval or bucket) is the \\nnumber of observations in the bin divided by the total number of observa-\\ntions. The cumulative relative frequency cumulates (adds up) the relative \\nfrequencies as we move from the first bin to the last, thus giving the fraction \\nof the observations that are less than the upper limit of each bin.\\n■ \\nA contingency table is a tabular format that displays the frequency distribu-\\ntions of two or more categorical variables simultaneously. One application \\nof contingency tables is for evaluating the performance of a classification \\nmodel (using a confusion matrix). Another application of contingency tables \\nis to investigate a potential association between two categorical variables by \\nperforming a chi-square test of independence.\\n■ \\nVisualization is the presentation of data in a pictorial or graphical format for \\nthe purpose of increasing understanding and for gaining insights into the \\ndata.\\n■ \\nA histogram is a bar chart of data that have been grouped into a frequency \\ndistribution. A frequency polygon is a graph of frequency distributions \\nobtained by drawing straight lines joining successive midpoints of bars rep-\\nresenting the class frequencies.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n148\\n■ \\nA bar chart is used to plot the frequency distribution of categorical data, \\nwith each bar representing a distinct category and the bar’s height (or \\nlength) proportional to the frequency of the corresponding category. \\nGrouped bar charts or stacked bar charts can present the frequency distri-\\nbution of multiple categorical variables simultaneously.\\n■ \\nA tree-map is a graphical tool to display categorical data. It consists of a set \\nof colored rectangles to represent distinct groups, and the area of each rect-\\nangle is proportional to the value of the corresponding group. Additional \\ndimensions of categorical data can be displayed by nested rectangles.\\n■ \\nA word cloud is a visual device for representing textual data, with the size \\nof each distinct word being proportional to the frequency with which it \\nappears in the given text.\\n■ \\nA line chart is a type of graph used to visualize ordered observations and \\noften to display the change of data series over time. A bubble line chart is a \\nspecial type of line chart that uses varying-sized bubbles as data points to \\nrepresent an additional dimension of data.\\n■ \\nA scatter plot is a type of graph for visualizing the joint variation in two \\nnumerical variables. It is constructed by drawing dots to indicate the values \\nof the two variables plotted against the corresponding axes. A scatter plot \\nmatrix organizes scatter plots between pairs of variables into a matrix for-\\nmat to inspect all pairwise relationships between more than two variables in \\none combined visual.\\n■ \\nA heat map is a type of graphic that organizes and summarizes data in a \\ntabular format and represents it using a color spectrum. It is often used in \\ndisplaying frequency distributions or visualizing the degree of correlation \\namong different variables.\\n■ \\nThe key consideration when selecting among chart types is the intended \\npurpose of visualizing data (i.e., whether it is for exploring/presenting distri-\\nbutions or relationships or for making comparisons).\\n■ \\nA population is defined as all members of a specified group. A sample is a \\nsubset of a population.\\n■ \\nA parameter is any descriptive measure of a population. A sample statis-\\ntic (statistic, for short) is a quantity computed from or used to describe a \\nsample.\\n■ \\nSample statistics—such as measures of central tendency, measures of disper-\\nsion, skewness, and kurtosis—help with investment analysis, particularly in \\nmaking probabilistic statements about returns.\\n■ \\nMeasures of central tendency specify where data are centered and include \\nthe mean, median, and mode (i.e., the most frequently occurring value).\\n■ \\nThe arithmetic mean is the sum of the observations divided by the number \\nof observations. It is the most frequently used measure of central tendency.\\n■ \\nThe median is the value of the middle item (or the mean of the values of \\nthe two middle items) when the items in a set are sorted into ascending or \\ndescending order. The median is not influenced by extreme values and is \\nmost useful in the case of skewed distributions.\\n■ \\nThe mode is the most frequently observed value and is the only measure of \\ncentral tendency that can be used with nominal data. A distribution may \\nbe unimodal (one mode), bimodal (two modes), trimodal (three modes), or \\nhave even more modes.\\n© CFA Institute. For candidate use only. Not for distribution.\\nCorrelation between Two Variables\\n149\\n■ \\nA portfolio’s return is a weighted mean return computed from the returns \\non the individual assets, where the weight applied to each asset’s return is \\nthe fraction of the portfolio invested in that asset.\\n■ \\nThe geometric mean,   _\\n \\nX  G , of a set of observations X1, X2, …, Xn, is   _\\n \\nX  G =  \\nn √ \\n____________ \\n \\n X 1  X 2  X 3 … X n  , with Xi ≥ 0 for i = 1, 2, …, n. The geometric mean is espe-\\ncially important in reporting compound growth rates for time-series data. \\nThe geometric mean will always be less than an arithmetic mean whenever \\nthere is variance in the observations.\\n■ \\nThe harmonic mean,   _\\n \\nX  H , is a type of weighted mean in which an observa-\\ntion’s weight is inversely proportional to its magnitude.\\n■ \\nQuantiles—such as the median, quartiles, quintiles, deciles, and \\npercentiles—are location parameters that divide a distribution into halves, \\nquarters, fifths, tenths, and hundredths, respectively.\\n■ \\nA box and whiskers plot illustrates the interquartile range (the “box”) as \\nwell as a range outside of the box that is based on the interquartile range, \\nindicated by the “whiskers.”\\n■ \\nDispersion measures—such as the range, mean absolute deviation (MAD), \\nvariance, standard deviation, target downside deviation, and coefficient of \\nvariation—describe the variability of outcomes around the arithmetic mean.\\n■ \\nThe range is the difference between the maximum value and the minimum \\nvalue of the dataset. The range has only a limited usefulness because it uses \\ninformation from only two observations.\\n■ \\nThe MAD for a sample is the average of the absolute deviations of observa-\\ntions from the mean,   \\n ∑ \\ni=1\\n  \\nn\\n     | X i −  _\\nX  |     \\n_ \\nn \\n , where   _\\n \\nX  is the sample mean and n is the \\nnumber of observations in the sample.\\n■ \\nThe variance is the average of the squared deviations around the mean, and \\nthe standard deviation is the positive square root of variance. In computing \\nsample variance (s2) and sample standard deviation (s), the average squared \\ndeviation is computed using a divisor equal to the sample size minus 1.\\n■ \\nThe target downside deviation, or target semideviation, is a measure of the \\nrisk of being below a given target. It is calculated as the square root of the \\naverage squared deviations from the target, but it includes only those obser-\\nvations below the target (B), or   √\\n \\n_______________ \\n \\n \\n∑ \\nfor\\xa0all X i ≤B\\n  \\n n\\n   ( X i − B ) 2  \\n_ \\nn − 1    .\\n■ \\nThe coefficient of variation, CV, is the ratio of the standard deviation of a \\nset of observations to their mean value. By expressing the magnitude of \\nvariation among observations relative to their average size, the CV permits \\ndirect comparisons of dispersion across different datasets. Reflecting the \\ncorrection for scale, the CV is a scale-free measure (i.e., it has no units of \\nmeasurement).\\n■ \\nSkew or skewness describes the degree to which a distribution is asymmet-\\nric about its mean. A return distribution with positive skewness has fre-\\nquent small losses and a few extreme gains compared to a normal distribu-\\ntion. A return distribution with negative skewness has frequent small gains \\nand a few extreme losses compared to a normal distribution. Zero skewness \\nindicates a symmetric distribution of returns.\\n■ \\nKurtosis measures the combined weight of the tails of a distribution rela-\\ntive to the rest of the distribution. A distribution with fatter tails than the \\nnormal distribution is referred to as fat-tailed (leptokurtic); a distribution \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n150\\nwith thinner tails than the normal distribution is referred to as thin-tailed \\n(platykurtic). Excess kurtosis is kurtosis minus 3, since 3 is the value of kur-\\ntosis for all normal distributions.\\n■ \\nThe correlation coefficient is a statistic that measures the association \\nbetween two variables. It is the ratio of covariance to the product of the two \\nvariables’ standard deviations. A positive correlation coefficient indicates \\nthat the two variables tend to move together, whereas a negative coeffi-\\ncient indicates that the two variables tend to move in opposite directions. \\nCorrelation does not imply causation, simply association. Issues that arise \\nin evaluating correlation include the presence of outliers and spurious \\ncorrelation.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 161,\n",
       "     'content': 'Practice Problems\\n151\\nPRACTICE PROBLEMS\\n1. Published ratings on stocks ranging from 1 (strong sell) to 5 (strong buy) are \\nexamples of which measurement scale?\\nA. Ordinal\\nB. Continuous\\nC. Nominal\\n2. Data values that are categorical and not amenable to being organized in a logical \\norder are most likely to be characterized as:\\nA. ordinal data.\\nB. discrete data.\\nC. nominal data.\\n3. Which of the following data types would be classified as being categorical?\\nA. Discrete\\nB. Nominal\\nC. Continuous\\n4. A fixed-income analyst uses a proprietary model to estimate bankruptcy proba-\\nbilities for a group of firms. The model generates probabilities that can take any \\nvalue between 0 and 1. The resulting set of estimated probabilities would most \\nlikely be characterized as:\\nA. ordinal data.\\nB. discrete data.\\nC. continuous data.\\n5. An analyst uses a software program to analyze unstructured data—specifically, \\nmanagement’s earnings call transcript for one of the companies in her research \\ncoverage. The program scans the words in each sentence of the transcript and \\nthen classifies the sentences as having negative, neutral, or positive sentiment. \\nThe resulting set of sentiment data would most likely be characterized as:\\nA. ordinal data.\\nB. discrete data.\\nC. nominal data.\\nThe following information relates to questions \\n6-7\\nAn equity analyst gathers total returns for three country equity indexes over the \\npast four years. The data are presented below.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n152\\nTime Period\\nIndex A\\nIndex B\\nIndex C\\nYear t–3\\n15.56%\\n11.84%\\n−4.34%\\nYear t–2\\n−4.12%\\n−6.96%\\n9.32%\\nYear t–1\\n11.19%\\n10.29%\\n−12.72%\\nYear t\\n8.98%\\n6.32%\\n21.44%\\n\\xa0\\n6. Each individual column of data in the table can be best characterized as:\\nA. panel data.\\nB. time-series data.\\nC. cross-sectional data.\\n7. Each individual row of data in the table can be best characterized as:\\nA. panel data.\\nB. time-series data.\\nC. cross-sectional data.\\n8. A two-dimensional rectangular array would be most suitable for organizing a \\ncollection of raw:\\nA. panel data.\\nB. time-series data.\\nC. cross-sectional data.\\n9. In a frequency distribution, the absolute frequency measure:\\nA. represents the percentages of each unique value of the variable.\\nB. represents the actual number of observations counted for each unique value \\nof the variable.\\nC. allows for comparisons between datasets with different numbers of total \\nobservations.\\n10. An investment fund has the return frequency distribution shown in the following \\nexhibit.\\nReturn Interval (%)\\nAbsolute Frequency\\n−10.0 to −7.0\\n3\\n−7.0 to −4.0\\n7\\n−4.0 to −1.0\\n10\\n−1.0 to +2.0\\n12\\n+2.0 to +5.0\\n23\\n+5.0 to +8.0\\n5\\nWhich of the following statements is correct?\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n153\\nA. The relative frequency of the bin “−1.0 to +2.0” is 20%.\\nB. The relative frequency of the bin “+2.0 to +5.0” is 23%.\\nC. The cumulative relative frequency of the bin “+5.0 to +8.0” is 91.7%.\\n11. An analyst is using the data in the following exhibit to prepare a statistical report.\\nPortfolio’s Deviations from Benchmark Return for a 12-Year Period (%)\\nYear 1\\n2.48\\n\\xa0\\nYear 7\\n−9.19\\nYear 2\\n−2.59\\n\\xa0\\nYear 8\\n−5.11\\nYear 3\\n9.47\\n\\xa0\\nYear 9\\n1.33\\nYear 4\\n−0.55\\n\\xa0\\nYear 10\\n6.84\\nYear 5\\n−1.69\\n\\xa0\\nYear 11\\n3.04\\nYear 6\\n−0.89\\n\\xa0\\nYear 12\\n4.72\\nThe cumulative relative frequency for the bin −1.71% ≤ x < 2.03% is closest to:\\nA. 0.250.\\nB. 0.333.\\nC. 0.583.\\nThe following information relates to questions \\n12-13\\nA fixed-income portfolio manager creates a contingency table of the number of \\nbonds held in her portfolio by sector and bond rating. The contingency table is \\npresented here:\\n\\xa0\\nBond Rating\\nSector\\nA\\nAA\\nAAA\\nCommunication Services\\n25\\n32\\n27\\nConsumer Staples\\n30\\n25\\n25\\nEnergy\\n100\\n85\\n30\\nHealth Care\\n200\\n100\\n63\\nUtilities\\n22\\n28\\n14\\n12. The marginal frequency of energy sector bonds is closest to:\\nA. 27.\\nB. 85.\\nC. 215.\\n13. The relative frequency of AA rated energy bonds, based on the total count, is \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n154\\nclosest to:\\nA. 10.5%.\\nB. 31.5%.\\nC. 39.5%.\\n14. The following is a frequency polygon of monthly exchange rate changes in the US \\ndollar/Japanese yen spot exchange rate for a four-year period. A positive change \\nrepresents yen appreciation (the yen buys more dollars), and a negative change \\nrepresents yen depreciation (the yen buys fewer dollars).\\nExhibit 1: Monthly Changes in the US Dollar/Japanese Yen Spot Exchange \\nRate\\n–3\\n–5\\n3\\n1\\n–1\\nFrequency\\n20\\n15\\n10\\n5\\n0\\nReturn Interval Midpoint (%)\\nBased on the chart, yen appreciation:\\nA. occurred more than 50% of the time.\\nB. was less frequent than yen depreciation.\\nC. in the 0.0 to 2.0 interval occurred 20% of the time.\\n15. A bar chart that orders categories by frequency in descending order and includes \\na line displaying cumulative relative frequency is referred to as a:\\nA. Pareto Chart.\\nB. grouped bar chart.\\nC. frequency polygon.\\n16. Which visualization tool works best to represent unstructured, textual data?\\nA. Tree-Map\\nB. Scatter plot\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n155\\nC. Word cloud\\n17. A tree-map is best suited to illustrate:\\nA. underlying trends over time.\\nB. joint variations in two variables.\\nC. value differences of categorical groups.\\n18. A line chart with two variables—for example, revenues and earnings per share—\\nis best suited for visualizing:\\nA. the joint variation in the variables.\\nB. underlying trends in the variables over time.\\nC. the degree of correlation between the variables.\\n19. A heat map is best suited for visualizing the:\\nA. frequency of textual data.\\nB. degree of correlation between different variables.\\nC. shape, center, and spread of the distribution of numerical data.\\n20. Which valuation tool is recommended to be used if the goal is to make compari-\\nsons of three or more variables over time?\\nA. Heat map\\nB. Bubble line chart\\nC. Scatter plot matrix\\nThe following information relates to questions \\n21-22\\nThe following histogram shows a distribution of the S&P 500 Index annual re-\\nturns for a 50-year period:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n156\\n–37\\nto\\n–32\\n–32\\nto\\n–27\\n–27\\nto\\n–22\\n–22\\nto\\n–17\\n–17\\nto\\n–12\\n–12\\nto\\n–7\\n–7\\nto\\n–2\\n–2\\nto\\n3\\nto3\\n8\\nto8\\n13\\n13\\nto\\n18\\n18\\nto\\n23\\n23\\nto\\n28\\n28\\nto\\n33\\n33\\nto\\n38\\nFrequency\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\n0\\nReturn Intervals (%)\\n\\xa0\\n21. The bin containing the median return is:\\nA. 3% to 8%.\\nB. 8% to 13%.\\nC. 13% to 18%.\\n22. Based on the previous histogram, the distribution is best described as being:\\nA. unimodal.\\nB. bimodal.\\nC. trimodal.\\n23. The annual returns for three portfolios are shown in the following exhibit. Portfo-\\nlios P and R were created in Year 1, Portfolio Q in Year 2.\\n\\xa0\\nAnnual Portfolio Returns (%)\\n\\xa0\\nYear 1\\nYear 2\\nYear 3\\nYear 4\\nYear 5\\nPortfolio P\\n−3.0\\n4.0\\n5.0\\n3.0\\n7.0\\nPortfolio Q\\n−3.0\\n6.0\\n4.0\\n8.0\\nPortfolio R\\n1.0\\n−1.0\\n4.0\\n4.0\\n3.0\\nThe median annual return from portfolio creation to Year 5 for:\\nA. Portfolio P is 4.5%.\\nB. Portfolio Q is 4.0%.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n157\\nC. Portfolio R is higher than its arithmetic mean annual return.\\n24. At the beginning of Year X, an investor allocated his retirement savings in the \\nasset classes shown in the following exhibit and earned a return for Year X as also \\nshown.\\nAsset Class\\nAsset Allocation \\n(%)\\nAsset Class Return for Year X (%)\\nLarge-cap US equities\\n20.0\\n8.0\\nSmall-cap US equities\\n40.0\\n12.0\\nEmerging market equities\\n25.0\\n−3.0\\nHigh-yield bonds\\n15.0\\n4.0\\nThe portfolio return for Year X is closest to:\\nA. 5.1%.\\nB. 5.3%.\\nC. 6.3%.\\n25. The following exhibit shows the annual returns for Fund Y.\\n\\xa0\\nFund Y (%)\\nYear 1\\n19.5\\nYear 2\\n−1.9\\nYear 3\\n19.7\\nYear 4\\n35.0\\nYear 5\\n5.7\\nThe geometric mean return for Fund Y is closest to:\\nA. 14.9%.\\nB. 15.6%.\\nC. 19.5%.\\n26. A portfolio manager invests €5,000 annually in a security for four years at the \\nprices shown in the following exhibit.\\n\\xa0\\nPurchase Price of Security (€ per unit)\\nYear 1\\n62.00\\nYear 2\\n76.00\\nYear 3\\n84.00\\nYear 4\\n90.00\\nThe average price is best represented as the:\\nA. harmonic mean of €76.48.\\nB. geometric mean of €77.26.\\nC. arithmetic average of €78.00.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n158\\n27. When analyzing investment returns, which of the following statements is cor-\\nrect?\\nA. The geometric mean will exceed the arithmetic mean for a series with \\nnon-zero variance.\\nB. The geometric mean measures an investment’s compound rate of growth \\nover multiple periods.\\nC. The arithmetic mean measures an investment’s terminal value over multiple \\nperiods.\\nThe following information relates to questions \\n28-32\\nA fund had the following experience over the past 10 years:\\nYear\\nReturn\\n1\\n4.5%\\n2\\n6.0%\\n3\\n1.5%\\n4\\n−2.0%\\n5\\n0.0%\\n6\\n4.5%\\n7\\n3.5%\\n8\\n2.5%\\n9\\n5.5%\\n10\\n4.0%\\n\\xa0\\n28. The arithmetic mean return over the 10 years is closest to:\\nA. 2.97%.\\nB. 3.00%.\\nC. 3.33%.\\n29. The geometric mean return over the 10 years is closest to:\\nA. 2.94%.\\nB. 2.97%.\\nC. 3.00%.\\n30. The harmonic mean return over the 10 years is closest to:\\nA. 2.94%.\\nB. 2.97%.\\nC. 3.00%.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n159\\n31. The standard deviation of the 10 years of returns is closest to:\\nA. 2.40%.\\nB. 2.53%.\\nC. 7.58%.\\n32. The target semideviation of the returns over the 10 years if the target is 2% is \\nclosest to:\\nA. 1.42%.\\nB. 1.50%.\\nC. 2.01%.\\nThe following information relates to questions \\n33-34\\n180\\n160\\n140\\n120\\n100\\n80\\n60\\n40\\n154.45\\n51.51\\n114.25\\n100.49\\n79.74\\n33. The median is closest to:\\nA. 34.51.\\nB. 100.49.\\nC. 102.98.\\n34. The interquartile range is closest to:\\nA. 13.76.\\nB. 25.74.\\nC. 34.51.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n160\\nThe following information relates to questions \\n35-36\\nThe following exhibit shows the annual MSCI World Index total returns for a \\n10-year period.\\nYear 1\\n15.25%\\n\\xa0\\nYear 6\\n30.79%\\nYear 2\\n10.02%\\n\\xa0\\nYear 7\\n12.34%\\nYear 3\\n20.65%\\n\\xa0\\nYear 8\\n−5.02%\\nYear 4\\n9.57%\\n\\xa0\\nYear 9\\n16.54%\\nYear 5\\n−40.33%\\n\\xa0\\nYear 10\\n27.37%\\n\\xa0\\n35. The fourth quintile return for the MSCI World Index is closest to:\\nA. 20.65%.\\nB. 26.03%.\\nC. 27.37%.\\n36. For Year 6–Year 10, the mean absolute deviation of the MSCI World Index total \\nreturns is closest to:\\nA. 10.20%.\\nB. 12.74%.\\nC. 16.40%.\\n37. Annual returns and summary statistics for three funds are listed in the following \\nexhibit:\\n\\xa0\\nAnnual Returns (%)\\nYear\\nFund ABC\\nFund XYZ\\nFund PQR\\nYear 1\\n−20.0\\n−33.0\\n−14.0\\nYear 2\\n23.0\\n−12.0\\n−18.0\\nYear 3\\n−14.0\\n−12.0\\n6.0\\nYear 4\\n5.0\\n−8.0\\n−2.0\\nYear 5\\n−14.0\\n11.0\\n3.0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nMean\\n−4.0\\n−10.8\\n−5.0\\nStandard deviation\\n17.8\\n15.6\\n10.5\\nThe fund with the highest absolute dispersion is:\\nA. Fund PQR if the measure of dispersion is the range.\\nB. Fund XYZ if the measure of dispersion is the variance.\\nC. Fund ABC if the measure of dispersion is the mean absolute deviation.\\n38. The average return for Portfolio A over the past twelve months is 3%, with a stan-\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n161\\ndard deviation of 4%. The average return for Portfolio B over this same period \\nis also 3%, but with a standard deviation of 6%. The geometric mean return of \\nPortfolio A is 2.85%. The geometric mean return of Portfolio B is:\\nA. less than 2.85%.\\nB. equal to 2.85%.\\nC. greater than 2.85%.\\n39. The mean monthly return and the standard deviation for three industry sectors \\nare shown in the following exhibit.\\nSector\\nMean Monthly Return (%)\\nStandard Deviation of Return \\n(%)\\nUtilities (UTIL)\\n2.10\\n1.23\\nMaterials (MATR)\\n1.25\\n1.35\\nIndustrials (INDU)\\n3.01\\n1.52\\nBased on the coefficient of variation, the riskiest sector is:\\nA. utilities.\\nB. materials.\\nC. industrials.\\nThe following information relates to questions \\n40-42\\nAn analyst examined a cross-section of annual returns for 252 stocks and calcu-\\nlated the following statistics:\\nArithmetic Average \\n9.986%\\nGeometric Mean\\n9.909%\\nVariance\\n0.001723\\nSkewness\\n0.704\\nExcess Kurtosis\\n0.503\\n40. The coefficient of variation is closest to:\\nA. 0.02.\\nB. 0.42.\\nC. 2.41.\\n41. This distribution is best described as:\\nA. negatively skewed.\\nB. having no skewness.\\nC. positively skewed.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n162\\n42. Compared to the normal distribution, this sample’s distribution is best described \\nas having tails of the distribution with:\\nA. less probability than the normal distribution.\\nB. the same probability as the normal distribution.\\nC. more probability than the normal distribution.\\n43. An analyst calculated the excess kurtosis of a stock’s returns as −0.75. From this \\ninformation, we conclude that the distribution of returns is:\\nA. normally distributed.\\nB. thin-tailed compared to the normal distribution.\\nC. fat-tailed compared to the normal distribution.\\n44. A correlation of 0.34 between two variables, X and Y, is best described as:\\nA. changes in X causing changes in Y.\\nB. a positive association between X and Y.\\nC. a curvilinear relationship between X and Y.\\n45. Which of the following is a potential problem with interpreting a correlation \\ncoefficient?\\nA. Outliers\\nB. Spurious correlation\\nC. Both outliers and spurious correlation\\nThe following information relates to questions \\n46-47\\nAn analyst is evaluating the tendency of returns on the portfolio of stocks she \\nmanages to move along with bond and real estate indexes. She gathered monthly \\ndata on returns and the indexes:\\n\\xa0\\nReturns (%)\\n\\xa0\\nPortfolio Returns\\nBond Index \\nReturns\\nReal Estate Index \\nReturns\\nArithmetic average\\n5.5\\n3.2\\n7.8\\nStandard deviation\\n8.2\\n3.4\\n10.3\\n\\xa0\\nPortfolio Returns and \\nBond Index Returns\\nPortfolio Returns and Real \\nEstate Index Returns\\nCovariance\\n18.9\\n−55.9\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n163\\n46. Without calculating the correlation coefficient, the correlation of the portfolio \\nreturns and the bond index returns is:\\nA. negative.\\nB. zero.\\nC. positive.\\n47. Without calculating the correlation coefficient, the correlation of the portfolio \\nreturns and the real estate index returns is:\\nA. negative.\\nB. zero.\\nC. positive.\\n48. Consider two variables, A and B. If variable A has a mean of −0.56, variable B \\nhas a mean of 0.23, and the covariance between the two variables is positive, the \\ncorrelation between these two variables is:\\nA. negative.\\nB. zero.\\nC. positive.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n164\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 174,\n",
       "     'content': 'SOLUTIONS\\n1. A is correct. Ordinal scales sort data into categories that are ordered with respect \\nto some characteristic and may involve numbers to identify categories but do not \\nassure that the differences between scale values are equal. The buy rating scale \\nindicates that a stock ranked 5 is expected to perform better than a stock ranked \\n4, but it tells us nothing about the performance difference between stocks ranked \\n4 and 5 compared with the performance difference between stocks ranked 1 and \\n2, and so on.\\n2. C is correct. Nominal data are categorical values that are not amenable to being \\norganized in a logical order. A is incorrect because ordinal data are categorical \\ndata that can be logically ordered or ranked. B is incorrect because discrete data \\nare numerical values that result from a counting process; thus, they can be or-\\ndered in various ways, such as from highest to lowest value.\\n3. B is correct. Categorical data (or qualitative data) are values that describe a quali-\\nty or characteristic of a group of observations and therefore can be used as labels \\nto divide a dataset into groups to summarize and visualize. The two types of \\ncategorical data are nominal data and ordinal data. Nominal data are categorical \\nvalues that are not amenable to being organized in a logical order, while ordinal \\ndata are categorical values that can be logically ordered or ranked. A is incorrect \\nbecause discrete data would be classified as numerical data (not categorical data). \\nC is incorrect because continuous data would be classified as numerical data (not \\ncategorical data).\\n4. C is correct. Continuous data are data that can be measured and can take on \\nany numerical value in a specified range of values. In this case, the analyst is \\nestimating bankruptcy probabilities, which can take on any value between 0 and \\n1. Therefore, the set of bankruptcy probabilities estimated by the analyst would \\nlikely be characterized as continuous data. A is incorrect because ordinal data \\nare categorical values that can be logically ordered or ranked. Therefore, the \\nset of bankruptcy probabilities would not be characterized as ordinal data. B is \\nincorrect because discrete data are numerical values that result from a counting \\nprocess, and therefore the data are limited to a finite number of values. The pro-\\nprietary model used can generate probabilities that can take any value between 0 \\nand 1; therefore, the set of bankruptcy probabilities would not be characterized \\nas discrete data.\\n5. A is correct. Ordinal data are categorical values that can be logically ordered or \\nranked. In this case, the classification of sentences in the earnings call transcript \\ninto three categories (negative, neutral, or positive) describes ordinal data, as the \\ndata can be logically ordered from positive to negative. B is incorrect because \\ndiscrete data are numerical values that result from a counting process. In this \\ncase, the analyst is categorizing sentences (i.e., unstructured data) from the earn-\\nings call transcript as having negative, neutral, or positive sentiment. Thus, these \\ncategorical data do not represent discrete data. C is incorrect because nominal \\ndata are categorical values that are not amenable to being organized in a logical \\norder. In this case, the classification of unstructured data (i.e., sentences from \\nthe earnings call transcript) into three categories (negative, neutral, or positive) \\ndescribes ordinal (not nominal) data, as the data can be logically ordered from \\npositive to negative.\\n6. B is correct. Time-series data are a sequence of observations of a specific variable \\ncollected over time and at discrete and typically equally spaced intervals of time, \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n165\\nsuch as daily, weekly, monthly, annually, and quarterly. In this case, each column \\nis a time series of data that represents annual total return (the specific variable) \\nfor a given country index, and it is measured annually (the discrete interval of \\ntime). A is incorrect because panel data consist of observations through time \\non one or more variables for multiple observational units. The entire table of \\ndata is an example of panel data showing annual total returns (the variable) for \\nthree country indexes (the observational units) by year. C is incorrect because \\ncross-sectional data are a list of the observations of a specific variable from multi-\\nple observational units at a given point in time. Each row (not column) of data in \\nthe table represents cross-sectional data.\\n7. C is correct. Cross-sectional data are observations of a specific variable from \\nmultiple observational units at a given point in time. Each row of data in the table \\nrepresents cross-sectional data. The specific variable is annual total return, the \\nmultiple observational units are the three countries’ indexes, and the given point \\nin time is the time period indicated by the particular row. A is incorrect because \\npanel data consist of observations through time on one or more variables for \\nmultiple observational units. The entire table of data is an example of panel data \\nshowing annual total returns (the variable) for three country indexes (the obser-\\nvational units) by year. B is incorrect because time-series data are a sequence of \\nobservations of a specific variable collected over time and at discrete and typical-\\nly equally spaced intervals of time, such as daily, weekly, monthly, annually, and \\nquarterly. In this case, each column (not row) is a time series of data that rep-\\nresents annual total return (the specific variable) for a given country index, and it \\nis measured annually (the discrete interval of time).\\n8. A is correct. Panel data consist of observations through time on one or more \\nvariables for multiple observational units. A two-dimensional rectangular array, \\nor data table, would be suitable here as it is comprised of columns to hold the \\nvariable(s) for the observational units and rows to hold the observations through \\ntime. B is incorrect because a one-dimensional (not a two-dimensional rect-\\nangular) array would be most suitable for organizing a collection of data of the \\nsame data type, such as the time-series data from a single variable. C is incorrect \\nbecause a one-dimensional (not a two-dimensional rectangular) array would \\nbe most suitable for organizing a collection of data of the same data type, such \\nas the same variable for multiple observational units at a given point in time \\n(cross-sectional data).\\n9. B is correct. In a frequency distribution, the absolute frequency, or simply the \\nraw frequency, is the actual number of observations counted for each unique \\nvalue of the variable. A is incorrect because the relative frequency, which is cal-\\nculated as the absolute frequency of each unique value of the variable divided by \\nthe total number of observations, presents the absolute frequencies in terms of \\npercentages. C is incorrect because the relative (not absolute) frequency provides \\na normalized measure of the distribution of the data, allowing comparisons be-\\ntween datasets with different numbers of total observations.\\n10. A is correct. The relative frequency is the absolute frequency of each bin divided \\nby the total number of observations. Here, the relative frequency is calculated as: \\n(12/60) × 100 = 20%. B is incorrect because the relative frequency of this bin is \\n(23/60) × 100 = 38.33%. C is incorrect because the cumulative relative frequency \\nof the last bin must equal 100%.\\n11. C is correct. The cumulative relative frequency of a bin identifies the fraction of \\nobservations that are less than the upper limit of the given bin. It is determined \\nby summing the relative frequencies from the lowest bin up to and including the \\ngiven bin. The following exhibit shows the relative frequencies for all the bins of \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n166\\nthe data from the previous exhibit:\\nLower Limit \\n(%)\\nUpper Limit \\n(%)\\nAbsolute \\nFrequency\\nRelative \\nFrequency\\nCumulative Relative \\nFrequency\\n−9.19 ≤\\n< −5.45\\n1\\n0.083\\n0.083\\n−5.45 ≤\\n< −1.71\\n2\\n0.167\\n0.250\\n−1.71 ≤\\n< 2.03\\n4\\n0.333\\n0.583\\n2.03 ≤\\n< 5.77\\n3\\n0.250\\n0.833\\n5.77 ≤\\n≤ 9.47\\n2\\n0.167\\n1.000\\nThe bin −1.71% ≤ x < 2.03% has a cumulative relative frequency of 0.583.\\n12. C is correct. The marginal frequency of energy sector bonds in the portfolio is \\nthe sum of the joint frequencies across all three levels of bond rating, so 100 \\n+ 85 + 30 = 215. A is incorrect because 27 is the relative frequency for energy \\nsector bonds based on the total count of 806 bonds, so 215/806 = 26.7%, not the \\nmarginal frequency. B is incorrect because 85 is the joint frequency for AA rated \\nenergy sector bonds, not the marginal frequency.\\n13. A is correct. The relative frequency for any value in the table based on the total \\ncount is calculated by dividing that value by the total count. Therefore, the rela-\\ntive frequency for AA rated energy bonds is calculated as 85/806 = 10.5%.\\nB is incorrect because 31.5% is the relative frequency for AA rated energy bonds, \\ncalculated based on the marginal frequency for all AA rated bonds, so 85/(32 + \\n25 + 85 + 100 + 28), not based on total bond counts. C is incorrect because 39.5% \\nis the relative frequency for AA rated energy bonds, calculated based on the \\nmarginal frequency for all energy bonds, so 85/(100 + 85 + 30), not based on total \\nbond counts.\\n14. A is correct. Twenty observations lie in the interval “0.0 to 2.0,” and six observa-\\ntions lie in the “2.0 to 4.0” interval. Together, they represent 26/48, or 54.17%, of \\nall observations, which is more than 50%.\\n15. A is correct. A bar chart that orders categories by frequency in descending order \\nand includes a line displaying cumulative relative frequency is called a Pareto \\nChart. A Pareto Chart is used to highlight dominant categories or the most im-\\nportant groups. B is incorrect because a grouped bar chart or clustered bar chart \\nis used to present the frequency distribution of two categorical variables. C is \\nincorrect because a frequency polygon is used to display frequency distributions.\\n16. C is correct. A word cloud, or tag cloud, is a visual device for representing \\nunstructured, textual data. It consists of words extracted from text with the size \\nof each word being proportional to the frequency with which it appears in the \\ngiven text. A is incorrect because a tree-map is a graphical tool for displaying \\nand comparing categorical data, not for visualizing unstructured, textual data. B \\nis incorrect because a scatter plot is used to visualize the joint variation in two \\nnumerical variables, not for visualizing unstructured, textual data.\\n17. C is correct. A tree-map is a graphical tool used to display and compare categor-\\nical data. It consists of a set of colored rectangles to represent distinct groups, \\nand the area of each rectangle is proportional to the value of the corresponding \\ngroup. A is incorrect because a line chart, not a tree-map, is used to display the \\nchange in a data series over time. B is incorrect because a scatter plot, not a \\ntree-map, is used to visualize the joint variation in two numerical variables.\\n18. B is correct. An important benefit of a line chart is that it facilitates showing \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n167\\nchanges in the data and underlying trends in a clear and concise way. Often a line \\nchart is used to display the changes in data series over time. A is incorrect be-\\ncause a scatter plot, not a line chart, is used to visualize the joint variation in two \\nnumerical variables. C is incorrect because a heat map, not a line chart, is used to \\nvisualize the values of joint frequencies among categorical variables.\\n19. B is correct. A heat map is commonly used for visualizing the degree of cor-\\nrelation between different variables. A is incorrect because a word cloud, or tag \\ncloud, not a heat map, is a visual device for representing textual data with the size \\nof each distinct word being proportional to the frequency with which it appears \\nin the given text. C is incorrect because a histogram, not a heat map, depicts the \\nshape, center, and spread of the distribution of numerical data.\\n20. B is correct. A bubble line chart is a version of a line chart where data points \\nare replaced with varying-sized bubbles to represent a third dimension of the \\ndata. A line chart is very effective at visualizing trends in three or more variables \\nover time. A is incorrect because a heat map differentiates high values from low \\nvalues and reflects the correlation between variables but does not help in making \\ncomparisons of variables over time. C is incorrect because a scatterplot matrix is \\na useful tool for organizing scatterplots between pairs of variables, making it easy \\nto inspect all pairwise relationships in one combined visual. However, it does not \\nhelp in making comparisons of these variables over time.\\n21. C is correct. Because 50 data points are in the histogram, the median return \\nwould be the mean of the 50/2 = 25th and (50 + 2)/2 = 26th positions. The sum of \\nthe return bin frequencies to the left of the 13% to 18% interval is 24. As a result, \\nthe 25th and 26th returns will fall in the 13% to 18% interval.\\n22. C is correct. The mode of a distribution with data grouped in intervals is the \\ninterval with the highest frequency. The three intervals of 3% to 8%, 18% to 23%, \\nand 28% to 33% all have a high frequency of 7.\\n23. C is correct. The median of Portfolio R is 0.8% higher than the mean for Portfolio \\nR.\\n24. C is correct. The portfolio return must be calculated as the weighted mean re-\\nturn, where the weights are the allocations in each asset class:\\n (0.20 × 8%) + (0.40 × 12%) + (0.25 × −3%) + (0.15 × 4%) = 6.25%, or ≈ 6.3%.\\n25. A is correct. The geometric mean return for Fund Y is found as follows:\\nFund Y = [(1 + 0.195) × (1 − 0.019) × (1 + 0.197) × (1 + 0.350) × (1 + 0.057)]\\n(1/5) − 1\\n= 14.9%.\\n26. A is correct. The harmonic mean is appropriate for determining the average price \\nper unit. It is calculated by summing the reciprocals of the prices, then averaging \\nthat sum by dividing by the number of prices, then taking the reciprocal of the \\naverage:\\n 4/[(1/62.00) + (1/76.00) + (1/84.00) + (1/90.00)] = €76.48.\\n27. B is correct. The geometric mean compounds the periodic returns of every \\nperiod, giving the investor a more accurate measure of the terminal value of an \\ninvestment.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n168\\n28. B is correct. The sum of the returns is 30.0%, so the arithmetic mean is 30.0%/10 \\n= 3.0%.\\n29. B is correct.\\nYear\\nReturn\\n1+ Return\\n1\\n4.5%\\n1.045\\n2\\n6.0%\\n1.060\\n3\\n1.5%\\n1.015\\n4\\n−2.0%\\n0.980\\n5\\n0.0%\\n1.000\\n6\\n4.5%\\n1.045\\n7\\n3.5%\\n1.035\\n8\\n2.5%\\n1.025\\n9\\n5.5%\\n1.055\\n10\\n4.0%\\n1.040\\nThe product of the 1 + Return is 1.3402338.\\nTherefore,  _\\n \\nX  G =  \\n10 √ _ \\n1.3402338  − 1  = 2.9717%.\\n30. A is correct.\\nYear\\nReturn\\n1+ Return\\n1/(1+Return)\\n1\\n4.5%\\n1.045\\n0.957\\n2\\n6.0%\\n1.060\\n0.943\\n3\\n1.5%\\n1.015\\n0.985\\n4\\n−2.0%\\n0.980\\n1.020\\n5\\n0.0%\\n1.000\\n1.000\\n6\\n4.5%\\n1.045\\n0.957\\n7\\n3.5%\\n1.035\\n0.966\\n8\\n2.5%\\n1.025\\n0.976\\n9\\n5.5%\\n1.055\\n0.948\\n10\\n4.0%\\n1.040\\n0.962\\nSum\\n\\xa0\\n\\xa0\\n9.714\\nThe harmonic mean return = (n/Sum of reciprocals) − 1 = (10 / 9.714) − 1.\\nThe harmonic mean return = 2.9442%.\\n31. B is correct.\\nYear\\nReturn\\nDeviation\\nDeviation Squared\\n1\\n4.5%\\n0.0150\\n0.000225\\n2\\n6.0%\\n0.0300\\n0.000900\\n3\\n1.5%\\n−0.0150\\n0.000225\\n4\\n−2.0%\\n−0.0500\\n0.002500\\n5\\n0.0%\\n−0.0300\\n0.000900\\n6\\n4.5%\\n0.0150\\n0.000225\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n169\\nYear\\nReturn\\nDeviation\\nDeviation Squared\\n7\\n3.5%\\n0.0050\\n0.000025\\n8\\n2.5%\\n−0.0050\\n0.000025\\n9\\n5.5%\\n0.0250\\n0.000625\\n10\\n4.0%\\n0.0100\\n0.000100\\nSum\\n\\xa0\\n0.0000\\n0.005750\\nThe standard deviation is the square root of the sum of the squared deviations \\ndivided by n − 1:\\n s =  √ _ \\n 0.005750 \\n_ \\n9 \\n  = 2.5276%.\\n32. B is correct.\\nYear\\nReturn\\nDeviation Squared \\nbelow Target of 2%\\n1\\n4.5%\\n\\xa0\\n2\\n6.0%\\n\\xa0\\n3\\n1.5%\\n0.000025\\n4\\n−2.0%\\n0.001600\\n5\\n0.0%\\n0.000400\\n6\\n4.5%\\n\\xa0\\n7\\n3.5%\\n\\xa0\\n8\\n2.5%\\n\\xa0\\n9\\n5.5%\\n\\xa0\\n10\\n4.0%\\n\\xa0\\nSum\\n\\xa0\\n0.002025\\nThe target semi-deviation is the square root of the sum of the squared deviations \\nfrom the target, divided by n − 1:\\nsTarget =  √ _ \\n 0.002025 \\n_ \\n9 \\n  = 1.5%.\\n33. B is correct. The median is indicated within the box, which is the 100.49 in this \\ndiagram.\\n34. C is correct. The interquartile range is the difference between 114.25 and 79.74, \\nwhich is 34.51.\\n35. B is correct. Quintiles divide a distribution into fifths, with the fourth quintile \\noccurring at the point at which 80% of the observations lie below it. The fourth \\nquintile is equivalent to the 80th percentile. To find the yth percentile (Py), \\nwe first must determine its location. The formula for the location (Ly) of a yth \\npercentile in an array with n entries sorted in ascending order is Ly = (n + 1) × \\n(y/100). In this case, n = 10 and y = 80%, so\\n L80 = (10 + 1) × (80/100) = 11 × 0.8 = 8.8.\\nWith the data arranged in ascending order (−40.33%, −5.02%, 9.57%, 10.02%, \\n12.34%, 15.25%, 16.54%, 20.65%, 27.37%, and 30.79%), the 8.8th position would \\nbe between the 8th and 9th entries, 20.65% and 27.37%, respectively. Using linear \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 2 \\nOrganizing, Visualizing, and Describing Data\\n170\\ninterpolation, P80 = X8 + (Ly − 8) × (X9 − X8),\\nP80 = 20.65 + (8.8 − 8) × (27.37 − 20.65)\\n= 20.65 + (0.8 × 6.72) = 20.65 + 5.38\\n= 26.03%.\\n36. A is correct. The formula for mean absolute deviation (MAD) is\\n MAD =  \\n ∑ \\ni=1\\n  \\nn\\n   | X i −  \\n_\\nX  |    \\n_ \\nn \\n .\\nColumn 1: Sum annual returns and divide by n to find the arithmetic mean    ( _\\n \\nX  )  \\nof 16.40%.\\nColumn 2: Calculate the absolute value of the difference between each year’s \\nreturn and the mean from Column 1. Sum the results and divide by n to find the \\nMAD.\\nThese calculations are shown in the following exhibit:\\n\\xa0\\nColumn 1\\n\\xa0\\n\\xa0\\nColumn 2\\nYear\\nReturn\\n\\xa0\\n\\xa0\\n  | X i −  \\n_\\n \\nX  |   \\nYear 6\\n30.79%\\n\\xa0\\n\\xa0\\n14.39%\\nYear 7\\n12.34%\\n\\xa0\\n\\xa0\\n4.06%\\nYear 8\\n−5.02%\\n\\xa0\\n\\xa0\\n21.42%\\nYear 9\\n16.54%\\n\\xa0\\n\\xa0\\n0.14%\\nYear 10\\n27.37%\\n\\xa0\\n\\xa0\\n10.97%\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSum:\\n82.02%\\n\\xa0\\nSum:\\n50.98%\\nn:\\n5\\n\\xa0\\nn:\\n5\\n _\\nX  : \\n16.40%\\n\\xa0\\nMAD:\\n10.20%\\n37. C is correct. The mean absolute deviation (MAD) of Fund ABC’s returns is great-\\ner than the MAD of both of the other funds.\\n MAD =  \\n ∑ \\ni=1\\n  \\nn\\n   | X i −  \\n_\\nX  |    \\n_ \\nn \\n , where  \\n_\\n \\nX  is the arithmetic mean of the series.\\nMAD for Fund ABC =\\n   |  − 20 −   ( −\\u200a4 )   |  +   | 23 −   ( −\\u200a4 )   |  +   |  − 14 −   ( −\\u200a4 )   |  +   | 5 −   ( −\\u200a4 )   |  +   |  − 14 −   ( −\\u200a4 )   |   \\n \\n \\n \\n \\n \\n_________________________________________________________ \\n \\n \\n5 \\n = 14.4% .\\nMAD for Fund XYZ =\\n   |  − 33 −   ( −\\u200a10.8 )   |  +   |  − 12 −   ( −\\u200a10.8 )   |  +   |  − 12 −   ( −\\u200a10.8 )   |  +   |  − 8 −   ( −\\u200a10.8 )   |  +   | 11 −   ( −\\u200a10.8 )  |   \\n \\n \\n \\n \\n \\n \\n______________________________________________________________________ \\n \\n \\n \\n5 \\n \\n= 9.8% .\\nMAD for Fund PQR =\\n   |  − 14 −   ( −\\u200a5 )  |   +   |  − 18 −   ( −\\u200a5 )   |  +   | 6 −   ( −\\u200a5 )   |  +   |  − 2 −   ( −\\u200a5 )   |  +   | 3 −   ( −\\u200a5 )   |   \\n \\n \\n \\n \\n \\n________________________________________________________ \\n \\n \\n5 \\n = 8.8% .\\nA and B are incorrect because the range and variance of the three funds are as \\nfollows:\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n171\\n\\xa0\\nFund ABC\\nFund XYZ\\nFund PQR\\nRange\\n43%\\n44%\\n24%\\nVariance\\n317\\n243\\n110\\nThe numbers shown for variance are understood to be in “percent squared” terms \\nso that when taking the square root, the result is standard deviation in percentage \\nterms. Alternatively, by expressing standard deviation and variance in decimal \\nform, one can avoid the issue of units. In decimal form, the variances for Fund \\nABC, Fund XYZ, and Fund PQR are 0.0317, 0.0243, and 0.0110, respectively.\\n38. A is correct. The more disperse a distribution, the greater the difference between \\nthe arithmetic mean and the geometric mean.\\n39. B is correct. The coefficient of variation (CV) is the ratio of the standard devia-\\ntion to the mean, where a higher CV implies greater risk per unit of return.\\n CV UTIL =  s _ \\n \\n_\\n \\nX    =  1.23% \\n_ \\n2.10%  = 0.59 .\\n CV MATR =  s _ \\n \\n_\\n \\nX    =  1.35% \\n_ \\n1.25%  = 1.08 .\\n CV INDU =  s _ \\n \\n_\\n \\nX    =  1.52% \\n_ \\n3.01%  = 0.51 .\\n40. B is correct. The coefficient of variation is the ratio of the standard deviation to \\nthe arithmetic average, or  √ _ \\n0.001723  / 0.09986 = 0.416.\\n41. C is correct. The skewness is positive, so it is right-skewed (positively skewed).\\n42. C is correct. The excess kurtosis is positive, indicating that the distribution is \\n“fat-tailed”; therefore, there is more probability in the tails of the distribution \\nrelative to the normal distribution.\\n43. B is correct. The distribution is thin-tailed relative to the normal distribution \\nbecause the excess kurtosis is less than zero.\\n44. B is correct. The correlation coefficient is positive, indicating that the two series \\nmove together.\\n45. C is correct. Both outliers and spurious correlation are potential problems with \\ninterpreting correlation coefficients.\\n46. C is correct. The correlation coefficient is positive because the covariation is posi-\\ntive.\\n47. A is correct. The correlation coefficient is negative because the covariation is neg-\\native.\\n48. C is correct. The correlation coefficient is positive because the covariance is pos-\\nitive. The fact that one or both variables have a negative mean does not affect the \\nsign of the correlation coefficient.\\n© CFA Institute. For candidate use only. Not for distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\nProbability Concepts\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity \\nManagement (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndefine a random variable, an outcome, and an event\\nidentify the two defining properties of probability, including \\nmutually exclusive and exhaustive events, and compare and contrast \\nempirical, subjective, and a priori probabilities\\ndescribe the probability of an event in terms of odds for and against \\nthe event\\ncalculate and interpret conditional probabilities\\ndemonstrate the application of the multiplication and addition rules \\nfor probability\\ncompare and contrast dependent and independent events\\ncalculate and interpret an unconditional probability using the total \\nprobability rule\\ncalculate and interpret the expected value, variance, and standard \\ndeviation of random variables\\nexplain the use of conditional expectation in investment applications\\ninterpret a probability tree and demonstrate its application to \\ninvestment problems\\ncalculate and interpret the expected value, variance, standard \\ndeviation, covariances, and correlations of portfolio returns\\ncalculate and interpret the covariances of portfolio returns using the \\njoint probability function\\ncalculate and interpret an updated probability using Bayes’ formula\\nidentify the most appropriate method to solve a particular \\ncounting problem and analyze counting problems using factorial, \\ncombination, and permutation concepts\\nL E A R N I N G  M O D U L E\\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]},\n",
       "  {'title': 'Learning Module 3\\tProbability Concepts',\n",
       "   'page_number': 183,\n",
       "   'content': 'Probability Concepts\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity \\nManagement (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndefine a random variable, an outcome, and an event\\nidentify the two defining properties of probability, including \\nmutually exclusive and exhaustive events, and compare and contrast \\nempirical, subjective, and a priori probabilities\\ndescribe the probability of an event in terms of odds for and against \\nthe event\\ncalculate and interpret conditional probabilities\\ndemonstrate the application of the multiplication and addition rules \\nfor probability\\ncompare and contrast dependent and independent events\\ncalculate and interpret an unconditional probability using the total \\nprobability rule\\ncalculate and interpret the expected value, variance, and standard \\ndeviation of random variables\\nexplain the use of conditional expectation in investment applications\\ninterpret a probability tree and demonstrate its application to \\ninvestment problems\\ncalculate and interpret the expected value, variance, standard \\ndeviation, covariances, and correlations of portfolio returns\\ncalculate and interpret the covariances of portfolio returns using the \\njoint probability function\\ncalculate and interpret an updated probability using Bayes’ formula\\nidentify the most appropriate method to solve a particular \\ncounting problem and analyze counting problems using factorial, \\ncombination, and permutation concepts\\nL E A R N I N G  M O D U L E\\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n174\\nPROBABILITY CONCEPTS AND ODDS RATIOS\\ndefine a random variable, an outcome, and an event\\nidentify the two defining properties of probability, including \\nmutually exclusive and exhaustive events, and compare and contrast \\nempirical, subjective, and a priori probabilities\\ndescribe the probability of an event in terms of odds for and against \\nthe event\\nInvestment decisions are made in a risky environment. The tools that allow us to make \\ndecisions with consistency and logic in this setting are based on probability concepts. \\nThis reading presents the essential probability tools needed to frame and address \\nmany real-world problems involving risk. These tools apply to a variety of issues, such \\nas predicting investment manager performance, forecasting financial variables, and \\npricing bonds so that they fairly compensate bondholders for default risk. Our focus \\nis practical. We explore the concepts that are most important to investment research \\nand practice. Among these are independence, as it relates to the predictability of \\nreturns and financial variables; expectation, as analysts continually look to the future \\nin their analyses and decisions; and variability, variance or dispersion around expec-\\ntation, as a risk concept important in investments. The reader will acquire specific \\nskills and competencies in using these probability concepts to understand risks and \\nreturns on investments.\\nProbability, Expected Value, and Variance\\nThe probability concepts and tools necessary for most of an analyst’s work are rel-\\natively few and straightforward but require thought to apply. This section presents \\nthe essentials for working with probability, expectation, and variance, drawing on \\nexamples from equity and fixed income analysis.\\nAn investor’s concerns center on returns. The return on a risky asset is an example \\nof a random variable.\\n■ \\nDefinition of Random Variable. A random variable is a quantity whose \\nfuture outcomes are uncertain.\\n■ \\nDefinition of Outcome. An outcome is a possible value of a random \\nvariable.\\nUsing Exhibit 1 as an example, a portfolio manager may have a return objective of \\n10% a year. The portfolio manager’s focus at the moment may be on the likelihood of \\nearning a return that is less than 10% over the next year. Ten percent is a particular \\nvalue or outcome of the random variable “portfolio return.” Although we may be con-\\ncerned about a single outcome, frequently our interest may be in a set of outcomes. \\nThe concept of “event” covers both.\\n■ \\nDefinition of Event. An event is a specified set of outcomes.\\n1\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "   'children': [{'title': 'Probability Concepts and Odds Ratios',\n",
       "     'page_number': 184,\n",
       "     'content': 'Learning Module 3 \\nProbability Concepts\\n174\\nPROBABILITY CONCEPTS AND ODDS RATIOS\\ndefine a random variable, an outcome, and an event\\nidentify the two defining properties of probability, including \\nmutually exclusive and exhaustive events, and compare and contrast \\nempirical, subjective, and a priori probabilities\\ndescribe the probability of an event in terms of odds for and against \\nthe event\\nInvestment decisions are made in a risky environment. The tools that allow us to make \\ndecisions with consistency and logic in this setting are based on probability concepts. \\nThis reading presents the essential probability tools needed to frame and address \\nmany real-world problems involving risk. These tools apply to a variety of issues, such \\nas predicting investment manager performance, forecasting financial variables, and \\npricing bonds so that they fairly compensate bondholders for default risk. Our focus \\nis practical. We explore the concepts that are most important to investment research \\nand practice. Among these are independence, as it relates to the predictability of \\nreturns and financial variables; expectation, as analysts continually look to the future \\nin their analyses and decisions; and variability, variance or dispersion around expec-\\ntation, as a risk concept important in investments. The reader will acquire specific \\nskills and competencies in using these probability concepts to understand risks and \\nreturns on investments.\\n',\n",
       "     'children': [{'title': 'Probability, Expected Value, and Variance',\n",
       "       'page_number': 184,\n",
       "       'content': 'Probability, Expected Value, and Variance\\nThe probability concepts and tools necessary for most of an analyst’s work are rel-\\natively few and straightforward but require thought to apply. This section presents \\nthe essentials for working with probability, expectation, and variance, drawing on \\nexamples from equity and fixed income analysis.\\nAn investor’s concerns center on returns. The return on a risky asset is an example \\nof a random variable.\\n■ \\nDefinition of Random Variable. A random variable is a quantity whose \\nfuture outcomes are uncertain.\\n■ \\nDefinition of Outcome. An outcome is a possible value of a random \\nvariable.\\nUsing Exhibit 1 as an example, a portfolio manager may have a return objective of \\n10% a year. The portfolio manager’s focus at the moment may be on the likelihood of \\nearning a return that is less than 10% over the next year. Ten percent is a particular \\nvalue or outcome of the random variable “portfolio return.” Although we may be con-\\ncerned about a single outcome, frequently our interest may be in a set of outcomes. \\nThe concept of “event” covers both.\\n■ \\nDefinition of Event. An event is a specified set of outcomes.\\n1\\n© CFA Institute. For candidate use only. Not for distribution.\\nProbability Concepts and Odds Ratios\\n175\\nExhibit 1: Visualizing Probability\\n–50\\n–50\\n50\\n50\\n10\\n10\\n0\\nPortfolio Return (%)\\nEvent B: Probability\\nPortfolio Earns < 10%\\nReturn: Area under\\nCurve to Left of\\nVertical Line\\nEvent C: Probability\\nPortfolio Earns > 10%\\nReturn: Area under\\nCurve to Right of\\nVertical Line\\nEvent A: Probability\\nPortfolio Earns 10% Return\\nAn event can be a single outcome—for example, the portfolio earns a return of (exactly) \\n10%. We can capture the portfolio manager’s concerns by defining another event as \\nthe portfolio earns a return below 10%. This second event, referring as it does to all \\npossible returns greater than or equal to −100% (the worst possible return, losing all \\nthe money in the portfolio) but less than 10%, contains an infinite number of outcomes. \\nTo save words, it is common to use a capital letter in italics to represent a defined \\nevent. We could define A = the portfolio earns a return of 10% and B = the portfolio \\nearns a return below 10%.\\nTo return to the portfolio manager’s concern, how likely is it that the portfolio \\nwill earn a return below 10%? The answer to this question is a probability: a number \\nbetween 0 and 1 that measures the chance that a stated event will occur. If the prob-\\nability is 0.65 that the portfolio earns a return below 10%, there is a 65% chance of \\nthat event happening. If an event is impossible, it has a probability of 0. If an event is \\ncertain to happen, it has a probability of 1. If an event is impossible or a sure thing, it \\nis not random at all. So, 0 and 1 bracket all the possible values of a probability.\\nTo reiterate, a probability can be thought of as the likelihood that something will \\nhappen. If it has a probability of 1, it is likely to happen 100% of the time, and if it \\nhas a probably of 0, it is likely to never happen. Some people think of probabilities as \\nakin to relative frequencies. If something is expected to happen 30 times out of 100, \\nthe probability is 0.30. The probability is the number of ways that an (equally likely) \\nevent can happen divided by the total number of possible outcomes.\\nProbability has two properties, which together constitute its definition.\\n■ \\nDefinition of Probability. The two defining properties of a probability are:\\n1. The probability of any event E is a number between 0 and 1: 0 ≤ P(E) ≤ 1.\\n2. The sum of the probabilities of any set of mutually exclusive and exhaus-\\ntive events equals 1.\\nP followed by parentheses stands for “the probability of (the event in parentheses),” \\nas in P(E) for “the probability of event E.” We can also think of P as a rule or function \\nthat assigns numerical values to events consistent with Properties 1 and 2.\\nIn the above definition, the term mutually exclusive means that only one event \\ncan occur at a time; exhaustive means that the events cover all possible outcomes. \\nReferring back to Exhibit 1, the events A = the portfolio earns a return of 10% and \\nB = the portfolio earns a return below 10% are mutually exclusive because A and B \\ncannot both occur at the same time. For example, a return of 8.1% means that B has \\noccurred and A has not occurred. Although events A and B are mutually exclusive, \\nthey are not exhaustive because they do not cover outcomes such as a return of 11%. \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n176\\nSuppose we define a third event: C = the portfolio earns a return above 10%. Clearly, \\nA, B, and C are mutually exclusive and exhaustive events. Each of P(A), P(B), and P(C) \\nis a number between 0 and 1, and P(A) + P(B) + P(C) = 1.\\nEarlier, to illustrate a concept, we assumed a probability of 0.65 for a portfolio \\nearning less than 10%, without justifying the particular assumption. We also talked \\nabout using assigned probabilities of outcomes to calculate the probability of events, \\nwithout explaining how such a probability distribution might be estimated. Making \\nactual financial decisions using inaccurate probabilities could have grave consequences. \\nHow, in practice, do we estimate probabilities? This topic is a field of study in itself, \\nbut there are three broad approaches to estimating probabilities. In investments, we \\noften estimate the probability of an event as a relative frequency of occurrence based \\non historical data. This method produces an empirical probability. For example, \\nsuppose you noted that 51 of the 60 stocks in a particular large-cap equity index pay \\ndividends. The empirical probability of the stocks in the index paying a dividend is \\nP(stock is dividend paying) = 51 / 60 = 0.85.\\nRelationships must be stable through time for empirical probabilities to be accu-\\nrate. We cannot calculate an empirical probability of an event not in the historical \\nrecord or a reliable empirical probability for a very rare event. In some cases, then, \\nwe may adjust an empirical probability to account for perceptions of changing rela-\\ntionships. In other cases, we have no empirical probability to use at all. We may also \\nmake a personal assessment of probability without reference to any particular data. \\nAnother type of probability is a subjective probability, one drawing on personal or \\nsubjective judgment. Subjective probabilities are of great importance in investments. \\nInvestors, in making buy and sell decisions that determine asset prices, often draw \\non subjective probabilities.\\nFor many well-defined problems, we can deduce probabilities by reasoning about \\nthe problem. The resulting probability is an a priori probability, one based on log-\\nical analysis rather than on observation or personal judgment. Because a priori and \\nempirical probabilities generally do not vary from person to person, they are often \\ngrouped as objective probabilities.\\nFor examples of the three types of probabilities, suppose you want to estimate the \\nprobability of flipping a coin and getting exactly two heads out of five flips. For the \\nempirical probability, you do the experiment 100 times (five flips each time) and find \\nthat you get two heads 33 times. The empirical probability would be 33/100 = 0.33. For \\na subjective judgement, you think the probability is somewhere between 0.25 and 0.50, \\nso you split the difference and choose 0.375. For the a priori probability, you assume \\nthat the binomial probability function (discussed later in the curriculum) applies, and \\nthe mathematical probability of two heads out of five flips is 0.3125.\\nAnother way of stating probabilities often encountered in investments is in terms \\nof odds—for instance, “the odds for E” or the “odds against E.” A probability is the \\nfraction of the time you expect an event to occur, and the odds for an event is the \\nprobability that an event will occur divided by the probability that the event will not \\noccur. Consider a football team that has a 0.25 probability of winning the World Cup, \\nand a 0.75 probability of losing. The odds for winning are 0.25/0.75 = 0.33 (and the \\nodds for losing are 0.75/0.25 = 3.0). If another team has a 0.80 probability of winning, \\nthe odds for winning would be 0.80/0.20 = 4.0. If, for a third team, the probability of \\nwinning was 0.50, the odds are even: odds = 0.50/0.50 = 1. If the probability is low, \\nthe odds are very close to the probability. For example, if the probability of winning \\nis 0.05, the odds for winning are 0.05/0.95 = 0.0526.\\n© CFA Institute. For candidate use only. Not for distribution.\\nProbability Concepts and Odds Ratios\\n177\\nEXAMPLE 1\\nOdds of Passing a Quantitative Methods Investment \\nCourse\\nTwo of your colleagues are taking a quantitative methods investment course.\\n1. If your first colleague has a 0.40 probability of passing, what are his odds for \\npassing?\\nSolution for 1:\\nThe odds are the probability of passing divided by the probability of not \\npassing. The odds are 0.40 / 0.60 = 2/3 ≈ 0.667.\\n2. If your second colleague has odds of passing of 4 to 1, what is the probability \\nof her passing?\\nSolution for 2: \\nThe odds = Probability (passing) / Probability (not passing). If Y = Probabili-\\nty of passing, then 4 = Y / (1 – Y). Solving for Y, we get 0.80 as the probabili-\\nty of passing.\\nWe interpret probabilities stated in terms of odds as follows:\\n■ \\nProbability Stated as Odds. Given a probability P(E),\\n1. Odds for E = P(E)/[1 − P(E)]. The odds for E are the probability of E \\ndivided by 1 minus the probability of E. Given odds for E of “a to b,” the \\nimplied probability of E is a/(a + b).\\nIn the example, the statement that your second colleague’s odds of passing \\nthe exam are 4 to 1 means that the probability of the event is 4/(4 + 1) = 4/5 \\n= 0.80. \\n2. Odds against E = [1 − P(E)]/P(E), the reciprocal of odds for E. Given \\nodds against E of “a to b,” the implied probability of E is b/(a + b).\\nIn the example, if the odds against your second colleague passing the exam \\nare 1 to 4, this means that the probability of the event is 1/(4 + 1) = 1/5 = \\n0.20. \\nTo further explain odds for an event, if P(E) = 1/8, the odds for E are (1/8)/(7/8) \\n= (1/8)(8/7) = 1/7, or “1 to 7.” For each occurrence of E, we expect seven cases of \\nnon-occurrence; out of eight cases in total, therefore, we expect E to happen once, \\nand the probability of E is 1/8. In wagering, it is common to speak in terms of the \\nodds against something, as in Statement 2. For odds of “15 to 1” against E (an implied \\nprobability of E of 1/16), a $1 wager on E, if successful, returns $15 in profits plus \\nthe $1 staked in the wager. We can calculate the bet’s anticipated profit as follows:\\nWin:\\nProbability = 1/16; Profit =$15\\nLoss:\\nProbability = 15/16; Profit = −$1\\nAnticipated profit = (1/16)($15) + (15/16)(−$1) = $0\\nWeighting each of the wager’s two outcomes by the respective probability of the out-\\ncome, if the odds (probabilities) are accurate, the anticipated profit of the bet is $0.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n178\\nEXAMPLE 2\\nProfiting from Inconsistent Probabilities\\n1. You are examining the common stock of two companies in the same indus-\\ntry in which an important antitrust decision will be announced next week. \\nThe first company, SmithCo Corporation, will benefit from a governmental \\ndecision that there is no antitrust obstacle related to a merger in which it is \\ninvolved. You believe that SmithCo’s share price reflects a 0.85 probability of \\nsuch a decision. A second company, Selbert Corporation, will equally benefit \\nfrom a “go ahead” ruling. Surprisingly, you believe Selbert stock reflects only \\na 0.50 probability of a favorable decision. Assuming your analysis is correct, \\nwhat investment strategy would profit from this pricing discrepancy?\\nConsider the logical possibilities. One is that the probability of 0.50 reflected \\nin Selbert’s share price is accurate. In that case, Selbert is fairly valued, but \\nSmithCo is overvalued, because its current share price overestimates the \\nprobability of a “go ahead” decision. The second possibility is that the prob-\\nability of 0.85 is accurate. In that case, SmithCo shares are fairly valued, but \\nSelbert shares, which build in a lower probability of a favorable decision, are \\nundervalued. You diagram the situation as shown in Exhibit 2.\\n \\nExhibit 2: Worksheet for Investment Problem\\n \\n \\n\\xa0\\nTrue Probability of a “Go Ahead” Decision\\n\\xa0\\n0.50\\n0.85\\nSmithCo\\nShares Overvalued\\nShares Fairly Valued\\nSelbert\\nShares Fairly Valued\\nShares Undervalued\\nStrategy\\nShort-Sell Smith /\\nSell Smith /\\n\\xa0\\nBuy Selbert\\nBuy Selbert\\n \\nThe 0.50 probability column shows that Selbert shares are a better value \\nthan SmithCo shares. Selbert shares are also a better value if a 0.85 prob-\\nability is accurate. Thus, SmithCo shares are overvalued relative to Selbert \\nshares.\\nYour investment actions depend on your confidence in your analysis and \\non any investment constraints you face (such as constraints on selling stock \\nshort). Selling short or shorting stock means selling borrowed shares in the \\nhope of repurchasing them later at a lower price. A conservative strategy \\nwould be to buy Selbert shares and reduce or eliminate any current position \\nin SmithCo. The most aggressive strategy is to short SmithCo stock (rela-\\ntively overvalued) and simultaneously buy the stock of Selbert (relatively \\nundervalued). The prices of SmithCo and Selbert shares reflect probabilities \\nthat are not consistent. According to one of the most important probability \\nresults for investments, the Dutch Book Theorem, inconsistent probabilities \\ncreate profit opportunities. In our example, investors’ buy and sell decisions \\nexploit the inconsistent probabilities to eliminate the profit opportunity and \\ninconsistency.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Conditional and Joint Probability',\n",
       "     'page_number': 189,\n",
       "     'content': 'Conditional and Joint Probability\\n179\\nCONDITIONAL AND JOINT PROBABILITY\\ncalculate and interpret conditional probabilities\\ndemonstrate the application of the multiplication and addition rules \\nfor probability\\ncompare and contrast dependent and independent events\\ncalculate and interpret an unconditional probability using the total \\nprobability rule\\nTo understand the meaning of a probability in investment contexts, we need to \\ndistinguish between two types of probability: unconditional and conditional. Both \\nunconditional and conditional probabilities satisfy the definition of probability stated \\nearlier, but they are calculated or estimated differently and have different interpreta-\\ntions. They provide answers to different questions.\\nThe probability in answer to the straightforward question “What is the proba-\\nbility of this event A?” is an unconditional probability, denoted P(A). Suppose the \\nquestion is “What is the probability that the stock earns a return above the risk-free \\nrate (event A)?” The answer is an unconditional probability that can be viewed as the \\nratio of two quantities. The numerator is the sum of the probabilities of stock returns \\nabove the risk-free rate. Suppose that sum is 0.70. The denominator is 1, the sum of \\nthe probabilities of all possible returns. The answer to the question is P(A) = 0.70.\\nContrast the question “What is the probability of A?” with the question “What \\nis the probability of A, given that B has occurred?” The probability in answer to this \\nlast question is a conditional probability, denoted P(A | B) (read: “the probability \\nof A given B”).\\nSuppose we want to know the probability that the stock earns a return above the \\nrisk-free rate (event A), given that the stock earns a positive return (event B). With the \\nwords “given that,” we are restricting returns to those larger than 0%—a new element \\nin contrast to the question that brought forth an unconditional probability. The con-\\nditional probability is calculated as the ratio of two quantities. The numerator is the \\nsum of the probabilities of stock returns above the risk-free rate; in this particular \\ncase, the numerator is the same as it was in the unconditional case, which we gave as \\n0.70. The denominator, however, changes from 1 to the sum of the probabilities for \\nall outcomes (returns) above 0%. Suppose that number is 0.80, a larger number than \\n0.70 because returns between 0 and the risk-free rate have some positive probability \\nof occurring. Then P(A | B) = 0.70/0.80 = 0.875. If we observe that the stock earns a \\npositive return, the probability of a return above the risk-free rate is greater than the \\nunconditional probability, which is the probability of the event given no other infor-\\nmation. To review, an unconditional probability is the probability of an event without \\nany restriction (i.e., a standalone probability). A conditional probability, in contrast, \\nis a probability of an event given that another event has occurred.\\nTo state an exact definition of conditional probability, we first need to introduce the \\nconcept of joint probability. Suppose we ask the question “What is the probability of \\nboth A and B happening?” The answer to this question is a joint probability, denoted \\nP(AB) (read: “the probability of A and B”). If we think of the probability of A and the \\nprobability of B as sets built of the outcomes of one or more random variables, the \\njoint probability of A and B is the sum of the probabilities of the outcomes they have in \\ncommon. For example, consider two events: the stock earns a return above the risk-free \\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n180\\nrate (A) and the stock earns a positive return (B). The outcomes of A are contained \\nwithin (a subset of) the outcomes of B, so P(AB) equals P(A). We can now state a \\nformal definition of conditional probability that provides a formula for calculating it.\\n■ \\nDefinition of Conditional Probability. The conditional probability of A given \\nthat B has occurred is equal to the joint probability of A and B divided by \\nthe probability of B (assumed not to equal 0).\\n P(A | B) = P(AB)/P(B), P(B) ≠ 0   \\n(1)\\nFor example, suppose B happens half the time, P(B) = 0.50, and A and B both \\nhappen 10% of the time, P(AB) = 0.10. What is the probability that A happens, given \\nthat B happens? That is P(A | B) = P(AB)/P(B) = 0.10 / 0.50 = 0.20. Sometimes we \\nknow the conditional probability P(A | B) and we want to know the joint probability \\nP(AB). We can obtain the joint probability from the following multiplication rule for \\nprobabilities, Equation 1 rearranged.\\n■ \\nMultiplication Rule for Probability. The joint probability of A and B can be \\nexpressed as\\n P(AB) = P(A | B)P(B)   \\n(2)\\nWith the same numbers above, if B happens 50% of the time, and the probability \\nof A given that B happens is 20%, the joint probability of A and B happening is P(AB) \\n= P(A | B)P(B) = 0.20 × 0.50 = 0.10.\\nEXAMPLE 3\\nConditional Probabilities and Predictability of Mutual \\nFund Performance (1)\\nAn analyst conducts a study of the returns of 200 mutual funds over a two-year \\nperiod. For each year, the total returns for the funds were ranked, and the top \\n50% of funds were labeled winners; the bottom 50% were labeled losers. Exhibit \\n3 shows the percentage of those funds that were winners in two consecutive \\nyears, winners in one year and then losers in the next year, losers then winners, \\nand finally losers in both years. The winner–winner entry, for example, shows \\nthat 66% of the first-year winner funds were also winners in the second year. The \\nfour entries in the table can be viewed as conditional probabilities.\\n \\nExhibit 3: Persistence of Returns: Conditional Probability for Year 2 \\nPerformance Given Year 1 Performance\\n \\n \\n\\xa0\\nYear 2 Winner\\nYear 2 Loser\\nYear 1 Winner\\n66%\\n34%\\nYear 1 Loser\\n34%\\n66%\\n \\nBased on the data in Exhibit 3, answer the following questions:\\n1. State the four events needed to define the four conditional probabilities.\\nSolution to 1:\\nThe four events needed to define the conditional probabilities are as follows:\\n \\nFund is a Year 1 winner\\nFund is a Year 1 loser\\n© CFA Institute. For candidate use only. Not for distribution.\\nConditional and Joint Probability\\n181\\nFund is a Year 2 loser\\nFund is a Year 2 winner\\n \\n2. State the four entries of the table as conditional probabilities using the form \\nP(this event | that event) = number.\\nSolution to 2:\\nFrom Row 1:\\n \\nP(fund is a Year 2 winner | fund is a Year 1 winner) = 0.66\\nP(fund is a Year 2 loser | fund is a Year 1 winner) = 0.34\\n \\nFrom Row 2:\\n \\nP(fund is a Year 2 winner | fund is a Year 1 loser) = 0.34\\nP(fund is a Year 2 loser | fund is a Year 1 loser) = 0.66\\n \\n3. Are the conditional probabilities in Question 2 empirical, a priori, or subjec-\\ntive probabilities?\\nSolution to 3:\\nThese probabilities are calculated from data, so they are empirical probabil-\\nities.\\n4. Using information in the table, calculate the probability of the event a fund \\nis a loser in both Year 1 and Year 2. (Note that because 50% of funds are \\ncategorized as losers in each year, the unconditional probability that a fund \\nis labeled a loser in either year is 0.5.)\\nSolution to 4:\\nThe estimated probability is 0.33. Let A represent the event that a fund is \\na Year 2 loser, and let B represent the event that the fund is a Year 1 loser. \\nTherefore, the event AB is the event that a fund is a loser in both Year 1 and \\nYear 2. From Exhibit 3, P(A | B) = 0.66 and P(B) = 0.50. Thus, using Equation \\n2, we find that\\n P(AB) = P(A | B)P(B) = 0.66(0.50) = 0.33\\nor a probability of 0.33. Note that Equation 2 states that the joint probabil-\\nity of A and B equals the probability of A given B times the probability of \\nB. Because P(AB) = P(BA), the expression P(AB) = P(BA) = P(B | A)P(A) is \\nequivalent to Equation 2.\\nWhen we have two events, A and B, that we are interested in, we often want to know \\nthe probability that either A or B occurs. Here the word “or” is inclusive, meaning that \\neither A or B occurs or that both A and B occur. Put another way, the probability of \\nA or B is the probability that at least one of the two events occurs. Such probabilities \\nare calculated using the addition rule for probabilities.\\n■ \\nAddition Rule for Probabilities. Given events A and B, the probability that \\nA or B occurs, or both occur, is equal to the probability that A occurs, plus \\nthe probability that B occurs, minus the probability that both A and B occur.\\n P(A or B) = P(A) + P(B) – P(AB)   \\n(3)\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n182\\nIf we think of the individual probabilities of A and B as sets built of outcomes of \\none or more random variables, the first step in calculating the probability of A or B \\nis to sum the probabilities of the outcomes in A to obtain P(A). If A and B share any \\noutcomes, then if we now added P(B) to P(A), we would count twice the probabilities \\nof those shared outcomes. So we add to P(A) the quantity [P(B) − P(AB)], which is the \\nprobability of outcomes in B net of the probability of any outcomes already counted \\nwhen we computed P(A). Exhibit 4 illustrates this process; we avoid double-counting \\nthe outcomes in the intersection of A and B by subtracting P(AB). As an example of \\nthe calculation, if P(A) = 0.50, P(B) = 0.40, and P(AB) = 0.20, then P(A or B) = 0.50 \\n+ 0.40 − 0.20 = 0.70. Only if the two events A and B were mutually exclusive, so that \\nP(AB) = 0, would it be correct to state that P(A or B) = P(A) + P(B).\\nExhibit 4: Addition Rule for Probabilities \\nA\\nA\\nand\\nB\\nB\\nExample 4 illustrates the relation between empirical frequencies and unconditional, \\nconditional, and joint probabilities as well as the multiplication and addition rules \\nfor probability.\\nEXAMPLE 4\\nFrequencies and Probability Concepts\\n1. Analysts often discuss the frequencies of events as well as their probabilities. \\nIn Exhibit 5, there are 150 cells, each representing one trading day. Out-\\ncome A, one of the 80 trading days when the stock market index increased, \\nis represented by the dark-shaded rectangle with 80 cells. Outcome B, one \\nthe 30 trading days when interest rates decreased, is represented by the light \\n-bordered rectangle with 30 cells. The overlap between these two rectangles, \\nwhen both events A and B occurred—the stock market index increased, \\nand interest rates decreased—happened 15 times and is represented by the \\nintermediate-shaded rectangle.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nConditional and Joint Probability\\n183\\nExhibit 5: Frequencies for Two Events\\n \\nNot \\nA\\nor\\nB\\nA\\nand\\nB\\nB\\nA\\n■ \\nThe frequency of A (stock market index increased) is 80 and has an \\nunconditional probability P(A) = 80/150 = 0.533.\\n■ \\nThe frequency of B (interest rates decreased) is 30 and has an uncon-\\nditional probability P(B) = 30/150 = 0.20.\\n■ \\nThe frequency of A and B (stock market index increased, and interest \\nrates decreased) is 15 and has a joint probability, P(AB) = 15/150 = \\n0.10.\\nThe frequency of A or B (stock market index increased or interest rates \\ndecreased is 95, and P(A or B) is 95/150 = 0.633. Using the addition rule for \\nprobabilities, the probability of A or B is P(A or B) = P(A) + P(B) – P(AB) = \\n80/150 + 30/150 – 15/150 = 95/150 = 0.633. The probability of not A or B \\n(stock market index did not increase or interest rates did not decrease) = 1 – \\n95/150 = 55/150 = 0.367.\\nThe conditional probability of A given B, P(A|B), stock market index in-\\ncreased given that interest rates decreased, was 15/30 = 0.50, which is also \\nP(A|B) = P(AB) / P(B) = (15/150) / (30/150) = 0.10 / 0.20 = 0.50\\nThe next example shows how much useful information can be obtained using the \\nprobability rules presented to this point.\\nEXAMPLE 5\\nProbability of a Limit Order Executing\\nYou have two buy limit orders outstanding on the same stock. A limit order to \\nbuy stock at a stated price is an order to buy at that price or lower. A number \\nof vendors, including an internet service that you use, supply the estimated \\nprobability that a limit order will be filled within a stated time horizon, given \\nthe current stock price and the price limit. One buy order (Order 1) was placed \\nat a price limit of $10. The probability that it will execute within one hour is \\n0.35. The second buy order (Order 2) was placed at a price limit of $9.75; it has \\na 0.25 probability of executing within the same one-hour time frame.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n184\\n1. What is the probability that either Order 1 or Order 2 will execute?\\nSolution to 1:\\nThe probability is 0.35. The two probabilities that are given are P(Order 1 ex-\\necutes) = 0.35 and P(Order 2 executes) = 0.25. Note that if Order 2 executes, \\nit is certain that Order 1 also executes because the price must pass through \\n$10 to reach $9.75. Thus,\\n P(Order 1 executes | Order 2 executes) = 1\\nand using the multiplication rule for probabilities,\\n P(Order 1 executes and Order 2 executes) = P(Order 1 executes |  \\nOrder 2 executes)P(Order 2 executes) = 1(0.25) = 0.25\\nTo answer the question, we use the addition rule for probabilities:\\n P(Order 1 executes or Order 2 executes) = P(Order 1 executes)  \\n+ P(Order 2 executes) − P(Order 1 executes and Order 2 executes)  \\n= 0.35 + 0.25 − 0.25 = 0.35\\nNote that the outcomes for which Order 2 executes are a subset of the \\noutcomes for which Order 1 executes. After you count the probability that \\nOrder 1 executes, you have counted the probability of the outcomes for \\nwhich Order 2 also executes. Therefore, the answer to the question is the \\nprobability that Order 1 executes, 0.35.\\n2. What is the probability that Order 2 executes, given that Order 1 executes?\\nSolution to 2:\\nIf the first order executes, the probability that the second order executes is \\n0.714. In the solution to Part 1, you found that P(Order 1 executes and Order \\n2 executes) = P(Order 1 executes | Order 2 executes)P(Order 2 executes) = \\n1(0.25) = 0.25. An equivalent way to state this joint probability is useful here:\\n P(Order 1 executes and Order 2 executes) = 0.25  \\n= P(Order 2 executes | Order 1 executes)P(Order 1 executes)\\nBecause P(Order 1 executes) = 0.35 was a given, you have one equation with \\none unknown:\\n 0.25 = P(Order 2 executes | Order 1 executes)(0.35) \\nYou conclude that P(Order 2 executes | Order 1 executes) = 0.25/0.35 = \\n0.714. You can also use Equation 1 to obtain this answer.\\nThe concepts of independence and dependence are of great interest to investment \\nanalysts. These concepts bear on such basic investment questions as which financial \\nvariables are useful for investment analysis, whether asset returns can be predicted, \\nand whether superior investment managers can be selected based on their past records.\\nTwo events are independent if the occurrence of one event does not affect the \\nprobability of occurrence of the other event.\\n■ \\nDefinition of Independent Events. Two events A and B are independent if \\nand only if P(A | B) = P(A) or, equivalently, P(B | A) = P(B).\\nThe logic of independence is clear: A and B are independent if the conditional \\nprobability of A given B, P(A | B), is the same as the unconditional probability of A, \\nP(A). Independence means that knowing B tells you nothing about A.\\n© CFA Institute. For candidate use only. Not for distribution.\\nConditional and Joint Probability\\n185\\nFor an example of independent events, suppose that event A is the bankruptcy of \\nCompany A, and event B is the bankruptcy of Company B. If the probability of bank-\\nruptcy of Company A is P(A) = 0.20, and the probability of bankruptcy of Company \\nA given that Company B goes bankrupt is the same, P(A | B) = 0.20, then event A is \\nindependent of event B.\\nWhen two events are not independent, they are dependent: The probability of \\noccurrence of one is related to the occurrence of the other. If we are trying to forecast \\none event, information about a dependent event may be useful, but information about \\nan independent event will not be useful. For example, suppose an announcement is \\nreleased that a biotech company will be acquired at an attractive price by another \\ncompany. If the prices of pharmaceutical companies increase as a result of this news, \\nthe companies’ stock prices are not independent of the biotech takeover announcement \\nevent. For a different example, if two events are mutually exclusive, then knowledge \\nthat one event has occurred gives us information that the other (mutually exclusive) \\nevent cannot occur.\\nWhen two events are independent, the multiplication rule for probabilities, \\nEquation 2, simplifies because P(A | B) in that equation then equals P(A).\\n■ \\nMultiplication Rule for Independent Events. When two events are inde-\\npendent, the joint probability of A and B equals the product of the individ-\\nual probabilities of A and B.\\n P(AB) = P(A)P(B)   \\n(4)\\nTherefore, if we are interested in two independent events with probabilities of 0.75 \\nand 0.50, respectively, the probability that both will occur is 0.375 = 0.75(0.50). The \\nmultiplication rule for independent events generalizes to more than two events; for \\nexample, if A, B, and C are independent events, then P(ABC) = P(A)P(B)P(C).\\nEXAMPLE 6\\nBankCorp’s Earnings per Share (1)\\nAs part of your work as a banking industry analyst, you build models for forecast-\\ning earnings per share of the banks you cover. Today you are studying BankCorp. \\nThe historical record shows that in 55% of recent quarters, BankCorp’s EPS has \\nincreased sequentially, and in 45% of quarters, EPS has decreased or remained \\nunchanged sequentially. At this point in your analysis, you are assuming that \\nchanges in sequential EPS are independent.\\nEarnings per share for 2Q:Year 1 (that is, EPS for the second quarter of Year \\n1) were larger than EPS for 1Q:Year 1.\\n1. What is the probability that 3Q:Year 1 EPS will be larger than 2Q:Year 1 EPS \\n(a positive change in sequential EPS)?\\nSolution to 1:\\nUnder the assumption of independence, the probability that 3Q:Year 1 EPS \\nwill be larger than 2Q:Year 1 EPS is the unconditional probability of positive \\nchange, 0.55. The fact that 2Q:Year 1 EPS was larger than 1Q:Year 1 EPS is \\nnot useful information, because the next change in EPS is independent of \\nthe prior change.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n186\\n2. What is the probability that EPS decreases or remains unchanged in the \\nnext two quarters?\\nSolution to 2:\\nAssuming independence, the probability is 0.2025 = 0.45(0.45). \\nThe following example illustrates how difficult it is to satisfy a set of independent \\ncriteria even when each criterion by itself is not necessarily stringent.\\nEXAMPLE 7\\nScreening Stocks for Investment\\nYou have developed a stock screen—a set of criteria for selecting stocks. Your \\ninvestment universe (the set of securities from which you make your choices) is \\n905 large- and medium-cap US equities, specifically all stocks that are members \\nof the S&P 500 and S&P 400 Indexes. Your criteria capture different aspects of \\nthe stock selection problem; you believe that the criteria are independent of \\neach other, to a close approximation.\\n \\nCriterion\\nNumber of stocks \\nmeeting criterion\\nFraction of stocks \\nmeeting criterion\\nFirst valuation criterion\\n556\\n0.614\\nSecond valuation criterion\\n489\\n0.540\\nAnalyst coverage criterion\\n600\\n0.663\\nProfitability criterion\\n490\\n0.541\\nFinancial strength criterion\\n313\\n0.346\\n \\nHow many stocks do you expect to pass your screen?\\nOnly 37 stocks out of 905 should pass through your screen. If you define five \\nevents—the stock passes the first valuation criterion, the stock passes the second \\nvaluation criterion, the stock passes the analyst coverage criterion, the company \\npasses the profitability criterion, the company passes the financial strength cri-\\nterion (say events A, B, C, D, and E, respectively)—then the probability that a \\nstock will pass all five criteria, under independence, is\\n P(ABCDE) = P(A)P(B)P(C)P(D)P(E) = (0.614)(0.540)(0.663)(0.541)(0.346) = \\n0.0411\\nAlthough only one of the five criteria is even moderately strict (the strictest lets \\n34.6% of stocks through), the probability that a stock can pass all five criteria is \\nonly 0.0411, or about 4%. If the criteria are independent, the size of the list of \\ncandidate investments is expected to be 0.0411(905) = 37 stocks.\\nAn area of intense interest to investment managers and their clients is whether \\nrecords of past performance are useful in identifying repeat winners and losers. The \\nfollowing example shows how this issue relates to the concept of independence.\\n© CFA Institute. For candidate use only. Not for distribution.\\nConditional and Joint Probability\\n187\\nEXAMPLE 8\\nConditional Probabilities and Predictability of Mutual \\nFund Performance (2)\\n1. The purpose of the mutual fund study introduced in Example 3 was to \\naddress the question of repeat mutual fund winners and losers. If the status \\nof a fund as a winner or a loser in one year is independent of whether it \\nis a winner in the next year, the practical value of performance ranking is \\nquestionable. Using the four events defined in Example 3 as building blocks, \\nwe can define the following events to address the issue of predictability of \\nmutual fund performance:\\n \\nFund is a Year 1 winner and fund is a Year 2 winner\\nFund is a Year 1 winner and fund is a Year 2 loser\\nFund is a Year 1 loser and fund is a Year 2 winner\\nFund is a Year 1 loser and fund is a Year 2 loser\\n \\nIn Part 4 of Example 3, you calculated that\\n P(fund is a Year 2 loser and fund is a Year 1 loser) = 0.33\\nIf the ranking in one year is independent of the ranking in the next year, \\nwhat will you expect P(fund is a Year 2 loser and fund is a Year 1 loser) to \\nbe? Interpret the empirical probability 0.33.\\nBy the multiplication rule for independent events, P(fund is a Year 2 loser \\nand fund is a Year 1 loser) = P(fund is a Year 2 loser)P(fund is a Year 1 loser). \\nBecause 50% of funds are categorized as losers in each year, the uncondi-\\ntional probability that a fund is labeled a loser in either year is 0.50. Thus \\nP(fund is a Year 2 loser)P(fund is a Year 1 loser) = 0.50(0.50) = 0.25. If the \\nstatus of a fund as a loser in one year is independent of whether it is a loser \\nin the prior year, we conclude that P(fund is a Year 2 loser and fund is a \\nYear 1 loser) = 0.25. This probability is a priori because it is obtained from \\nreasoning about the problem. You could also reason that the four events \\ndescribed above define categories and that if funds are randomly assigned \\nto the four categories, there is a 1/4 probability of fund is a Year 1 loser and \\nfund is a Year 2 loser. If the classifications in Year 1 and Year 2 were depen-\\ndent, then the assignment of funds to categories would not be random. The \\nempirical probability of 0.33 is above 0.25. Is this apparent predictability the \\nresult of chance? Further analysis would be necessary to determine wheth-\\ner these results would allow you to reject the hypothesis that investment \\nreturns are independent between Year 1 and Year 2.\\nIn many practical problems, we logically analyze a problem as follows: We for-\\nmulate scenarios that we think affect the likelihood of an event that interests us. We \\nthen estimate the probability of the event, given the scenario. When the scenarios \\n(conditioning events) are mutually exclusive and exhaustive, no possible outcomes \\nare left out. We can then analyze the event using the total probability rule. This rule \\nexplains the unconditional probability of the event in terms of probabilities conditional \\non the scenarios.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n188\\nThe total probability rule is stated below for two cases. Equation 5 gives the sim-\\nplest case, in which we have two scenarios. One new notation is introduced: If we \\nhave an event or scenario S, the event not-S, called the complement of S, is written \\nSC. Note that P(S) + P(SC) = 1, as either S or not-S must occur. Equation 6 states the \\nrule for the general case of n mutually exclusive and exhaustive events or scenarios.\\n■ \\nTotal Probability Rule.\\n P  ( A )  = P  ( AS )  + P  ( A  S C )    \\n \\n \\n= P  ( A | S  )  P  ( S )  + P  ( A |  S C  )  P  ( S C )   \\n(5)\\n \\nP  ( A )  = P  ( A  S 1 )  + P  ( A  S 2 )  + …\\u200a+\\u200aP  ( A  S n )  \\n  \\n \\n \\n \\n= P  ( A |  S 1  )  P  ( S 1 )  + P  ( A |  S 2  )  P  ( S 2 )  + …\\u200a+\\u200aP  ( A |  S n  )  P  ( S n )  \\n \\n(6)\\nwhere S1, S2, …, Sn are mutually exclusive and exhaustive scenarios or \\nevents.\\nEquation 6 states the following: The probability of any event [P(A)] can be expressed \\nas a weighted average of the probabilities of the event, given scenarios [terms such \\nP(A | S1)]; the weights applied to these conditional probabilities are the respective \\nprobabilities of the scenarios [terms such as P(S1) multiplying P(A | S1)], and the \\nscenarios must be mutually exclusive and exhaustive. Among other applications, this \\nrule is needed to understand Bayes’ formula, which we discuss later.\\nExhibit 6 is a visual representation of the total probability rule. Panel A illustrates \\nEquation 5 for the total probability rule when there are two scenarios (S and its \\ncomplement SC). For two scenarios, the probabilities of S and SC sum to 1, and the \\nprobability of A is a weighted average where the probability of A in each scenario is \\nweighted by the probability of each scenario. Panel B of Exhibit 6 illustrates Equation \\n6 for the total probability rule when there are n scenarios. The scenarios are mutually \\nexclusive and exhaustive, and the sum of the probabilities for the scenarios is 1. Like \\nthe two-scenario case, the probability of A given the n-scenarios is a weighted average \\nof the conditional probabilities of A in each scenario, using as weights the probability \\nof each scenario.\\n© CFA Institute. For candidate use only. Not for distribution.\\nConditional and Joint Probability\\n189\\nExhibit 6: The Total Probability Rule for Two Scenarios and for n Scenarios \\nΣ\\nA. Total Probability Rule for Two Scenarios (S and Sc) \\nP(S) + P(Sc) = 1 ;  P(A) = P(AS) + P(ASc) = P(A | S) P(S) + P(A | Sc) P(Sc)\\nS\\nA | S\\nA | Sc\\nSc\\nB.  Total Probability Rule for n Scenarios \\nS2\\nS1\\nS3\\n...\\n...\\nA | S2\\nA | S3\\nA | S1\\nA | Sn\\nP(Si) = 1\\ni=1\\nn\\nSn\\nS1, S2, … Sn, are mutually exclusive and exhaustive scenarios, such that         \\n        P(A) = P(AS1) + P(AS2) + … + P(ASn)\\n= P(A | S1) P(S1) + P(A | S2) P(S2) + … + P(A | Sn) P(Sn) \\nIn the next example, we use the total probability rule to develop a consistent set of \\nviews about BankCorp’s earnings per share.\\nEXAMPLE 9\\nBankCorp’s Earnings per Share (2)\\nYou are continuing your investigation into whether you can predict the direction \\nof changes in BankCorp’s quarterly EPS. You define four events:\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n190\\nEvent\\nProbability\\nA = Change in sequential EPS is positive next quarter\\n0.55\\nAC = Change in sequential EPS is 0 or negative next quarter\\n0.45\\nS = Change in sequential EPS is positive in the prior quarter\\n0.55\\nSC = Change in sequential EPS is 0 or negative in the prior quarter\\n0.45\\n \\nOn inspecting the data, you observe some persistence in EPS changes: \\nIncreases tend to be followed by increases, and decreases by decreases. The \\nfirst probability estimate you develop is P(change in sequential EPS is positive \\nnext quarter | change in sequential EPS is 0 or negative in the prior quarter) = \\nP(A | SC) = 0.40. The most recent quarter’s EPS (2Q:Year 1) is announced, and \\nthe change is a positive sequential change (the event S). You are interested in \\nforecasting EPS for 3Q:Year 1.\\n1. Write this statement in probability notation: “the probability that the change \\nin sequential EPS is positive next quarter, given that the change in sequential \\nEPS is positive the prior quarter.”\\nSolution to 1:\\nIn probability notation, this statement is written P(A | S).\\n2. Calculate the probability in Part 1. (Calculate the probability that is consis-\\ntent with your other probabilities or beliefs.)\\nSolution to 2:\\nThe probability is 0.673 that the change in sequential EPS is positive for 3Q:-\\nYear 1, given the positive change in sequential EPS for 2Q:Year 1, as shown \\nbelow.\\nAccording to Equation 5, P(A) = P(A | S)P(S) + P(A | SC)P(SC). The values of \\nthe probabilities needed to calculate P(A | S) are already known: P(A) = 0.55, \\nP(S) = 0.55, P(SC) = 0.45, and P(A | SC) = 0.40. Substituting into Equation 5,\\n 0.55 = P(A | S)(0.55) + 0.40(0.45) \\nSolving for the unknown, P(A | S) = [0.55 − 0.40(0.45)]/0.55 = 0.672727, or \\n0.673.\\nYou conclude that P(change in sequential EPS is positive next quarter | \\nchange in sequential EPS is positive the prior quarter) = 0.673. Any other \\nprobability is not consistent with your other estimated probabilities. Reflect-\\ning the persistence in EPS changes, this conditional probability of a positive \\nEPS change, 0.673, is greater than the unconditional probability of an EPS \\nincrease, 0.55.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Expected Value and Variance',\n",
       "     'page_number': 201,\n",
       "     'content': 'Expected Value and Variance\\n191\\nEXPECTED VALUE AND VARIANCE\\ncalculate and interpret the expected value, variance, and standard \\ndeviation of random variables\\nexplain the use of conditional expectation in investment applications\\ninterpret a probability tree and demonstrate its application to \\ninvestment problems\\nThe expected value of a random variable is an essential quantitative concept in invest-\\nments. Investors continually make use of expected values—in estimating the rewards \\nof alternative investments, in forecasting EPS and other corporate financial variables \\nand ratios, and in assessing any other factor that may affect their financial position. \\nThe expected value of a random variable is defined as follows:\\n■ \\nDefinition of Expected Value. The expected value of a random variable is \\nthe probability-weighted average of the possible outcomes of the random \\nvariable. For a random variable X, the expected value of X is denoted E(X).\\nExpected value (for example, expected stock return) looks either to the future, as \\na forecast, or to the “true” value of the mean (the population mean). We should dis-\\ntinguish expected value from the concepts of historical or sample mean. The sample \\nmean also summarizes in a single number a central value. However, the sample mean \\npresents a central value for a particular set of observations as an equally weighted \\naverage of those observations. In sum, the contrast is forecast versus historical, or \\npopulation versus sample.\\nEXAMPLE 10\\nBankCorp’s Earnings per Share (3)\\nYou continue with your analysis of BankCorp’s EPS. In Exhibit 7, you have \\nrecorded a probability distribution for BankCorp’s EPS for the current fiscal year.\\n \\nExhibit 7: Probability Distribution for BankCorp’s EPS\\n \\n \\nProbability\\nEPS ($)\\n0.15\\n2.60\\n0.45\\n2.45\\n0.24\\n2.20\\n0.16\\n2.00\\n1.00\\n\\xa0\\n \\nWhat is the expected value of BankCorp’s EPS for the current fiscal year?\\nFollowing the definition of expected value, list each outcome, weight it by its \\nprobability, and sum the terms.\\nE(EPS) = 0.15($2.60) + 0.45($2.45) + 0.24($2.20) + 0.16($2.00) = $2.3405\\nThe expected value of EPS is $2.34.\\nAn equation that summarizes your calculation in Example 10 is\\n E  ( X )  = P  ( X 1 )  X 1 + P  ( X 2 )  X 2 + …\\u200a+\\u200aP  ( X n )  X n =  ∑ \\ni=1\\n  \\nn\\n  P  ( X i )  X i  \\n(7)\\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n192\\nwhere Xi is one of n possible outcomes of the random variable X.\\nThe expected value is our forecast. Because we are discussing random quantities, \\nwe cannot count on an individual forecast being realized (although we hope that, on \\naverage, forecasts will be accurate). It is important, as a result, to measure the risk \\nwe face. Variance and standard deviation measure the dispersion of outcomes around \\nthe expected value or forecast.\\n■ \\nDefinition of Variance. The variance of a random variable is the expected \\nvalue (the probability-weighted average) of squared deviations from the \\nrandom variable’s expected value:\\n σ 2  ( X )  = E  { [ X − E  ( X )  ] 2 }  \\n(8)\\nThe two notations for variance are σ2(X) and Var(X).\\nVariance is a number greater than or equal to 0 because it is the sum of squared \\nterms. If variance is 0, there is no dispersion or risk. The outcome is certain, and the \\nquantity X is not random at all. Variance greater than 0 indicates dispersion of out-\\ncomes. Increasing variance indicates increasing dispersion, all else equal. Variance of X \\nis a quantity in the squared units of X. For example, if the random variable is return in \\npercent, variance of return is in units of percent squared. Standard deviation is easier \\nto interpret than variance because it is in the same units as the random variable. If the \\nrandom variable is return in percent, standard deviation of return is also in units of \\npercent. In the following example, when the variance of returns is stated as a percent \\nor amount of money, to conserve space, we may suppress showing the unit squared.\\n■ \\nDefinition of Standard Deviation. Standard deviation is the positive square \\nroot of variance.\\nThe best way to become familiar with these concepts is to work examples.\\nEXAMPLE 11\\nBankCorp’s Earnings per Share (4)\\nIn Example 10, you calculated the expected value of BankCorp’s EPS as $2.34, \\nwhich is your forecast. Using the probability distribution of EPS from Exhibit 6, \\nyou want to measure the dispersion around your forecast. What are the variance \\nand standard deviation of BankCorp’s EPS for the current fiscal year?\\nThe order of calculation is always expected value, then variance, then standard \\ndeviation. Expected value has already been calculated. Following the definition \\nof variance above, calculate the deviation of each outcome from the mean or \\nexpected value, square each deviation, weight (multiply) each squared deviation \\nby its probability of occurrence, and then sum these terms.\\n \\n σ 2  ( EPS )  = P  ( $2.60 )  [ $2.60 − E  ( EPS )  ] 2 + P  ( $2.45 )  [ $2.45 − E  ( EPS )  ] 2 \\n  \\n \\n \\n \\n \\n \\n \\n+\\u200a  P  ( $2.20 )  [ $2.20 − E  ( EPS )  ] 2 + P  ( $2.00 )  [ $2.00 − E  ( EPS )  ] 2 \\n  \\n \\n \\n \\n \\n \\n= 0.15  ( 2.60 − 2.34 ) 2 + 0.45  ( 2.45 − 2.34 ) 2   \\n \\n \\n \\n+\\u200a  0.24  ( 2.20 − 2.34 ) 2 + 0.16  ( 2.00 − 2.34 ) 2 \\n  \\n \\n \\n \\n= 0.01014 + 0.005445 + 0.004704 + 0.018496 = 0.038785\\n \\n \\nStandard deviation is the positive square root of 0.038785:\\n σ(EPS) = 0.0387851/2 = 0.196939, or approximately 0.20.\\nAn equation that summarizes your calculation of variance in Example 11 is\\n© CFA Institute. For candidate use only. Not for distribution.\\nExpected Value and Variance\\n193\\n \\n σ 2  ( X )  = P  ( X 1 )  [ X 1 − E  ( X )  ] \\n2 + P  ( X 2 )  [ X 2 − E  ( X )  ] \\n  2 \\n \\n \\n \\n \\n             + …\\u200a+\\u200aP  ( X n )  [ X n − E  ( X )  ] \\n2 =  ∑ \\ni = 1\\n  \\nn\\n  P  ( X i )  [ X i − E  ( X )  ] \\n 2  \\n(9)\\nwhere Xi is one of n possible outcomes of the random variable X.\\nIn investments, we make use of any relevant information available in making our \\nforecasts. When we refine our expectations or forecasts, we are typically making \\nadjustments based on new information or events; in these cases, we are using condi-\\ntional expected values. The expected value of a random variable X given an event \\nor scenario S is denoted E(X | S). Suppose the random variable X can take on any one \\nof n distinct outcomes X1, X2, …, Xn (these outcomes form a set of mutually exclusive \\nand exhaustive events). The expected value of X conditional on S is the first outcome, \\nX1, times the probability of the first outcome given S, P(X1 | S), plus the second out-\\ncome, X2, times the probability of the second outcome given S, P(X2 | S), and so forth.\\n E(X | S) = P(X1 | S)X1 + P(X2 | S)X2 + … + P(Xn | S)Xn   \\n(10)\\nWe will illustrate this equation shortly.\\nParallel to the total probability rule for stating unconditional probabilities in terms \\nof conditional probabilities, there is a principle for stating (unconditional) expected \\nvalues in terms of conditional expected values. This principle is the total probability \\nrule for expected value.\\n■ \\nTotal Probability Rule for Expected Value.\\n E(X) = E(X | S)P(S) + E(X | SC)P(SC)   \\n(11)\\n E(X) = E(X | S1)P(S1) + E(X | S2)P(S2) + … + E(X | Sn)P(Sn)   \\n(12)\\nwhere S1, S2, …, Sn are mutually exclusive and exhaustive scenarios or \\nevents.\\nThe general case, Equation 12, states that the expected value of X equals the \\nexpected value of X given Scenario 1, E(X | S1), times the probability of Scenario 1, \\nP(S1), plus the expected value of X given Scenario 2, E(X | S2), times the probability \\nof Scenario 2, P(S2), and so forth.\\nTo use this principle, we formulate mutually exclusive and exhaustive scenarios that \\nare useful for understanding the outcomes of the random variable. This approach was \\nemployed in developing the probability distribution of BankCorp’s EPS in Examples \\n10 and 11, as we now discuss.\\nThe earnings of BankCorp are interest rate sensitive, benefiting from a declining \\ninterest rate environment. Suppose there is a 0.60 probability that BankCorp will \\noperate in a declining interest rate environment in the current fiscal year and a 0.40 \\nprobability that it will operate in a stable interest rate environment (assessing the chance \\nof an increasing interest rate environment as negligible). If a declining interest rate \\nenvironment occurs, the probability that EPS will be $2.60 is estimated at 0.25, and the \\nprobability that EPS will be $2.45 is estimated at 0.75. Note that 0.60, the probability \\nof declining interest rate environment, times 0.25, the probability of $2.60 EPS given \\na declining interest rate environment, equals 0.15, the (unconditional) probability of \\n$2.60 given in the table in Exhibit 7. The probabilities are consistent. Also, 0.60(0.75) \\n= 0.45, the probability of $2.45 EPS given in Exhibit 7. The probability tree diagram \\nin Exhibit 8 shows the rest of the analysis.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n194\\nExhibit 8: BankCorp’s Forecasted EPS\\nE(EPS) = $2.34\\nProb. of declining\\ninterest rates = 0.60 \\nProb. of stable\\ninterest rates = 0.40\\nEPS = $2.60 with\\nProb = 0.15\\n0.25\\n0.75\\n0.60\\n0.40\\nEPS = $2.45 with\\nProb = 0.45\\nEPS = $2.20 with\\nProb = 0.24\\nEPS = $2.00 with\\nProb = 0.16\\nA declining interest rate environment points us to the node of the tree that branches \\noff into outcomes of $2.60 and $2.45. We can find expected EPS given a declining \\ninterest rate environment as follows, using Equation 10:\\n E(EPS | declining interest rate environment) = 0.25($2.60) + 0.75($2.45)\\n \\n= $2.4875\\nIf interest rates are stable,\\n E(EPS | stable interest rate environment) = 0.60($2.20) + 0.40($2.00)\\n \\n= $2.12\\nOnce we have the new piece of information that interest rates are stable, for example, \\nwe revise our original expectation of EPS from $2.34 downward to $2.12. Now using \\nthe total probability rule for expected value,\\n \\nE  ( EPS )  \\n  \\n= E  ( EPS | declining interest rate environment )  P(declining interest rateenvironment)  \\n \\n \\n \\n \\n \\n \\n  + E  ( EPS | stable interest rate environment )  P(stable interest rateenvironment)\\n \\n \\nSo, E(EPS) = $2.4875(0.60) + $2.12(0.40) = $2.3405 or about $2.34.\\nThis amount is identical to the estimate of the expected value of EPS calculated directly \\nfrom the probability distribution in Example 10. Just as our probabilities must be \\nconsistent, so must our expected values, unconditional and conditional; otherwise our \\ninvestment actions may create profit opportunities for other investors at our expense.\\nTo review, we first developed the factors or scenarios that influence the outcome \\nof the event of interest. After assigning probabilities to these scenarios, we formed \\nexpectations conditioned on the different scenarios. Then we worked backward to \\nformulate an expected value as of today. In the problem just worked, EPS was the \\nevent of interest, and the interest rate environment was the factor influencing EPS.\\nWe can also calculate the variance of EPS given each scenario: \\n σ2(EPS|declining interest rate environment)\\n = P($2.60|declining interest rate environment)\\n  × [$2.60 − E(EPS|declining interest rate environment)]2\\n  + P($2.45|declining interest rate environment)\\n© CFA Institute. For candidate use only. Not for distribution.\\nExpected Value and Variance\\n195\\n  × [$2.45 − E(EPS|declining interest rate environment)]2\\n = 0.25($2.60 − $2.4875)2 + 0.75($2.45 − $2.4875)2 = 0.004219\\n Similarly, σ2(EPS | stable interest rate environment) is found to be equal to\\n = 0.60($2.20 - $2.12)2 + 0.40($2.00 - $2.12)2 = 0.0096\\nThese are conditional variances, the variance of EPS given a declining interest rate \\nenvironment and the variance of EPS given a stable interest rate environment. The \\nrelationship between unconditional variance and conditional variance is a relatively \\nadvanced topic. The main points are 1) that variance, like expected value, has a con-\\nditional counterpart to the unconditional concept and 2) that we can use conditional \\nvariance to assess risk given a particular scenario.\\nEXAMPLE 12\\nBankCorp’s Earnings per Share (5)\\nContinuing with BankCorp, you focus now on BankCorp’s cost structure. One \\nmodel, a simple linear regression model, you are researching for BankCorp’s \\noperating costs is\\n ˆ \\nY  = a + bX \\nwhere   ˆ \\nY  is a forecast of operating costs in millions of dollars and X is the num-\\nber of branch offices.   ˆ \\nY  represents the expected value of Y given X, or E(Y | X). \\nYou interpret the intercept a as fixed costs and b as variable costs. You estimate \\nthe equation as\\n ˆ \\nY  = 12.5 + 0.65X \\nBankCorp currently has 66 branch offices, and the equation estimates oper-\\nating costs as 12.5 + 0.65(66) = $55.4 million. You have two scenarios for growth, \\npictured in the tree diagram in Exhibit 9.\\n \\nExhibit 9: BankCorp’s Forecasted Operating Costs\\n \\nExpected Op.\\nCosts = ?\\nHigh Growth\\nProbability = 0.80 \\nLow Growth\\nProbability = 0.20\\nBranches = 125\\nOp. Costs = ?\\nProb = ?\\nBranches = 100\\nOp. Costs = ?\\nProb = ?\\nBranches = 80\\nOp. Costs = ?\\nProb = ?\\nBranches = 70\\nOp. Costs = ?\\nProb = ?\\n0.50\\n0.50\\n0.85\\n0.15\\n1. Compute the forecasted operating costs given the different levels of oper-\\nating costs, using  ˆ \\nY  = 12.5 + 0.65X . State the probability of each level of \\nthe number of branch offices. These are the answers to the questions in the \\nterminal boxes of the tree diagram.\\nSolution to 1:\\nUsing  ˆ \\nY  = 12.5 + 0.65X, from top to bottom, we have\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n196\\n \\nOperating Costs\\nProbability\\n ˆ \\nY   = 12.5 + 0.65(125) = $93.75 million\\n0.80(0.50) = 0.40\\n ˆ \\nY   = 12.5 + 0.65(100) = $77.50 million\\n0.80(0.50) = 0.40\\n ˆ \\nY   = 12.5 + 0.65(80) = $64.50 million\\n0.20(0.85) = 0.17\\n ˆ \\nY   = 12.5 + 0.65(70) = $58.00 million\\n0.20(0.15) = 0.03\\n\\xa0\\nSum = 1.00\\n \\n2. Compute the expected value of operating costs under the high growth \\nscenario. Also calculate the expected value of operating costs under the low \\ngrowth scenario.\\nSolution to 2:\\nDollar amounts are in millions.\\n E  ( operating costs | high growth  )  = 0.50  ( $93.75 )  + 0.50  ( $77.50 )    \\n \\n \\n \\n \\n= $85.625  \\n \\n E  ( operating costs | low growth  )  = 0.85  ( $64.50 )  + 0.15  ( $58.00 )    \\n \\n \\n \\n \\n= $63.525  \\n \\n3. Answer the question in the initial box of the tree: What are BankCorp’s \\nexpected operating costs?\\nSolution to 3:\\nDollar amounts are in millions.\\n \\nE  ( operating costs )  = E  ( operating costs | high growth  )  P  ( high growth )  \\n  \\n \\n \\n \\n \\n \\n  \\u200a + E  ( operating costs | low growth  )  P  ( low growth )    \\n \\n \\n \\n \\n= $85.625  ( 0.80 )  + $63.525  ( 0.20 )  = $81.205\\n \\n \\nBankCorp’s expected operating costs are $81.205 million.\\nIn this section, we have treated random variables such as EPS as standalone quan-\\ntities. We have not explored how descriptors such as expected value and variance \\nof EPS may be functions of other random variables. Portfolio return is one random \\nvariable that is clearly a function of other random variables, the random returns on \\nthe individual securities in the portfolio. To analyze a portfolio’s expected return and \\nvariance of return, we must understand these quantities are a function of characteristics \\nof the individual securities’ returns. Looking at the variance of portfolio return, we \\nsee that the way individual security returns move together or covary is key. So, next \\nwe cover portfolio expected return, variance of return, and importantly, covariance \\nand correlation.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Portfolio Expected Return and Variance of Return',\n",
       "     'page_number': 207,\n",
       "     'content': 'Portfolio Expected Return and Variance of Return\\n197\\nPORTFOLIO EXPECTED RETURN AND VARIANCE OF \\nRETURN\\ncalculate and interpret the expected value, variance, standard \\ndeviation, covariances, and correlations of portfolio returns\\nModern portfolio theory makes frequent use of the idea that investment opportu-\\nnities can be evaluated using expected return as a measure of reward and variance \\nof return as a measure of risk. In this section, we will develop an understanding of \\nportfolio expected return and variance of return, which are functions of the returns \\non the individual portfolio holdings. To begin, the expected return on a portfolio is \\na weighted average of the expected returns on the securities in the portfolio, using \\nexactly the same weights. When we have estimated the expected returns on the indi-\\nvidual securities, we immediately have portfolio expected return.\\n■ \\nCalculation of Portfolio Expected Return. Given a portfolio with n secu-\\nrities, the expected return on the portfolio (E(Rp)) is a weighted average \\nof the expected returns (R1 to Rn) on the component securities using their \\nrespective weights (w1 to wn):\\n \\nE  ( R p )  = E  ( w 1  R 1 +  w 2  R 2 + …\\u200a+\\u200a w n  R n )  \\n  \\n \\n \\n \\n=  w 1 E  ( R 1 )  +  w 2 E  ( R 2 )  + …\\u200a+\\u200a w n E  ( R n )  \\n  \\n(13)\\nSuppose we have estimated expected returns on assets in the three-asset portfolio \\nshown in Exhibit 10.\\nExhibit 10: Weights and Expected Returns\\nAsset Class\\nWeight\\nExpected Return (%)\\nS&P 500\\n0.50\\n13\\nUS long-term corporate bonds\\n0.25\\n6\\nMSCI EAFE\\n0.25\\n15\\nWe calculate the expected return on the portfolio as 11.75%:\\n E  ( R p )  =  w 1 E  ( R 1 )  +  w 2 E  ( R 2 )  +  w 3 E  ( R 3 )    \\n \\n \\n \\n= 0.50  ( 13% )  + 0.25  ( 6% )  + 0.25  ( 15% )  = 11.75%\\n  \\nHere we are interested in portfolio variance of return as a measure of investment \\nrisk. Accordingly, portfolio variance is σ2(Rp) = E{[Rp − E(Rp)]2}, which is variance in \\na forward-looking sense. To implement this definition of portfolio variance, we use \\ninformation about the individual assets in the portfolio, but we also need the concept \\nof covariance. To avoid notational clutter, we write ERp for E(Rp).\\n■ \\nDefinition of Covariance. Given two random variables Ri and Rj, the covari-\\nance between Ri and Rj is\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n198\\n Cov  ( R i ,  R j )  = E  [  ( R i − E  R i )   ( R j − E  R j )  ]  \\n(14)\\nAlternative notations are σ(Ri,Rj) and σij. Equation 14 states that the cova-\\nriance between two random variables is the probability-weighted average \\nof the cross-products of each random variable’s deviation from its own \\nexpected value. The above measure is the population covariance and is \\nforward-looking. The sample covariance between two random variables Ri \\nand Rj, based on a sample of past data of size n is\\n Cov  ( R i ,  R j )  =  ∑ \\nn=1\\n  \\nn\\n    ( R i,t −  \\n_\\n \\nR  i )   ( R j,t −  \\n_\\n \\nR  j )  \\u200a/\\u200a  ( n − 1 )   \\n(15)\\nStart with the definition of variance for a three-asset portfolio and see how it \\ndecomposes into three variance terms and six covariance terms. Dispensing with the \\nderivation, the result is Equation 16:\\n \\n σ 2  ( R p )  = E  [ ( R p − E  R p ) \\n2\\n ]  \\n  \\n \\n \\n= E  { [ w 1  R 1 +  w 2  R 2 +  w 3  R 3 − E  ( w 1  R 1 +  w 2  R 2 +  w 3  R 3 )  ] \\n 2 }    \\n \\n \\n \\n \\n= E  { [ w 1  R 1 +  w 2  R 2 +  w 3  R 3 −  w 1 E  R 1 −  w 2 E  R 2 −  w 3 E  R 3 ] \\n2 }  \\n  \\n \\n \\n \\n \\n \\n  ( using Equation 13 )   \\n \\n \\n \\n=  w 1 2  σ 2  ( R 1 )  +  w 1  w 2 Cov  ( R 1 ,  R 2 )  +  w 1  w 3 Cov  ( R 1 ,  R 3 )  \\n  \\n \\n \\n \\n \\n   +  w 1  w 2 Cov  ( R 1 ,  R 2 )  +  w 2 2  σ 2  ( R 2 )  +  w 2  w 3 Cov  ( R 2 ,  R 3 )   \\n \\n \\n \\n \\n   +  w 1  w 3 Cov  ( R 1 ,  R 3 )  +  w 2  w 3 Cov  ( R 2 ,  R 3 )  +  w 3 2  σ 2  ( R 3 )  \\n  \\n(16)\\nNoting that the order of variables in covariance does not matter, for example, Cov(R2,R1) \\n= Cov(R1,R2), and that diagonal variance terms σ2(R1), σ2(R2), and σ2(R3) can be \\nexpressed as Cov(R1,R1), Cov(R2,R2), and Cov(R3,R3), respectively, the most compact \\nway to state Equation 16 is   σ 2  ( R p )  =  ∑ \\ni=1\\n  \\n3\\n   ∑ \\nj=1\\n  \\n3\\n   w i  w j   Cov   ( R i ,  R j )   . Moreover, this expres-\\nsion generalizes for a portfolio of any size n to\\n σ 2  ( R p )  =  ∑ \\ni=1\\n  \\nn\\n   ∑ \\nj=1\\n  \\nn\\n   w i  w j  Cov  ( R i ,  R j )  \\n(17)\\nWe see from Equation 16 that individual variances of return constitute part, but not \\nall, of portfolio variance. The three variances are actually outnumbered by the six \\ncovariance terms off the diagonal. If there are 20 assets, there are 20 variance terms \\nand 20(20) − 20 = 380 off-diagonal covariance terms. A first observation is that as \\nthe number of holdings increases, covariance becomes increasingly important, all \\nelse equal.\\nThe covariance terms capture how the co-movements of returns affect portfolio \\nvariance. From the definition of covariance, we can establish two essential observa-\\ntions about covariance.\\n1. We can interpret the sign of covariance as follows:\\nCovariance of returns is negative if, when the return on one asset is above \\nits expected value, the return on the other asset tends to be below its \\nexpected value (an average inverse relationship between returns).\\nCovariance of returns is 0 if returns on the assets are unrelated.\\nCovariance of returns is positive when the returns on both assets tend to be \\non the same side (above or below) their expected values at the same time \\n(an average positive relationship between returns).\\n© CFA Institute. For candidate use only. Not for distribution.\\nPortfolio Expected Return and Variance of Return\\n199\\n2. The covariance of a random variable with itself (own covariance) is its own \\nvariance: Cov(R,R) = E{[R − E(R)][R − E(R)]} = E{[R − E(R)]2} = σ2(R).\\nExhibit 11 summarizes the inputs for portfolio expected return and variance of return. \\nA complete list of the covariances constitutes all the statistical data needed to compute \\nportfolio variance of return as shown in the covariance matrix in Panel B.\\nExhibit 11: Inputs to Portfolio Expected Return and Variance\\nA. Inputs to Portfolio Expected Return\\nAsset\\nA\\nB\\nC\\n\\xa0\\nE(RA)\\nE(RB)\\nE(RC)\\nB. Covariance Matrix: The Inputs to Portfolio Variance of Return\\nAsset\\nA\\nB\\nC\\nA\\nCov(RA,RA)\\nCov(RA,RB)\\nCov(RA,RC)\\nB\\nCov(RB,RA)\\nCov(RB,RB)\\nCov(RB,RC)\\nC\\nCov(RC,RA)\\nCov(RC,RB)\\nCov(RC,RC)\\nWith three assets, the covariance matrix has 32 = 3 × 3 = 9 entries, but the diagonal \\nterms, the variances (bolded in Exhibit 11), are treated separately from the off-diagonal \\nterms. So there are 9 − 3 = 6 covariances, excluding variances. But Cov(RB,RA) = \\nCov(RA,RB), Cov(RC,RA) = Cov(RA,RC), and Cov(RC,RB) = Cov(RB,RC). The covari-\\nance matrix below the diagonal is the mirror image of the covariance matrix above \\nthe diagonal, so you only need to use one (i.e., either below or above the diagonal). \\nAs a result, there are only 6/2 = 3 distinct covariance terms to estimate. In general, \\nfor n securities, there are n(n − 1)/2 distinct covariances and n variances to estimate.\\nSuppose we have the covariance matrix shown in Exhibit 12. We will be working \\nin returns stated as percents, and the table entries are in units of percent squared (%2). \\nThe terms 38%2 and 400%2 are 0.0038 and 0.0400, respectively, stated as decimals; \\ncorrectly working in percents and decimals leads to identical answers.\\nExhibit 12: Covariance Matrix\\n\\xa0\\nS&P 500\\nUS Long-Term \\nCorporate Bonds\\nMSCI \\nEAFE\\nS&P 500\\n400\\n45\\n189\\nUS long-term corporate bonds\\n45\\n81\\n38\\nMSCI EAFE\\n189\\n38\\n441\\nTaking Equation 16 and grouping variance terms together produces the following:\\n \\n σ 2  ( R p )  =  w 1 2  σ 2  ( R 1 )  +  w 2 2  σ 2  ( R 2 )  +  w 3 2  σ 2  ( R 3 )  + 2  w 1  w 2 Cov  ( R 1 ,  R 2 )  \\n  \\n \\n \\n \\n \\n \\n \\n   + 2  w 1  w 3 Cov  ( R 1 ,  R 3 )  + 2  w 2  w 3 Cov  ( R 2 ,  R 3 )  \\n  \\n \\n \\n \\n=  ( 0.50 ) 2  ( 400 )  +  ( 0.25 ) 2  ( 81 )  +  ( 0.25 ) 2  ( 441 )    \\n \\n \\n \\n   + 2  ( 0.50 )   ( 0.25 )   ( 45 )  + 2  ( 0.50 )   ( 0.25 )   ( 189 )  \\n  \\n \\n \\n \\n   + 2  ( 0.25 )   ( 0.25 )   ( 38 )  \\n  \\n \\n= 100 + 5.0625 + 27.5625 + 11.25 + 47.25 + 4.75 = 195.875\\n \\n \\n(18)\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n200\\nThe variance is 195.875. Standard deviation of return is 195.8751/2 = 14%. To summa-\\nrize, the portfolio has an expected annual return of 11.75% and a standard deviation \\nof return of 14%.\\nLooking at the first three terms in the calculation above, their sum (100 + 5.0625 + \\n27.5625) is 132.625, the contribution of the individual variances to portfolio variance. \\nIf the returns on the three assets were independent, covariances would be 0 and the \\nstandard deviation of portfolio return would be 132.6251/2 = 11.52% as compared \\nto 14% before, so a less risky portfolio. If the covariance terms were negative, then a \\nnegative number would be added to 132.625, so portfolio variance and risk would be \\neven smaller, while expected return would not change. For the same expected port-\\nfolio return, the portfolio has less risk. This risk reduction is a diversification benefit, \\nmeaning a risk-reduction benefit from holding a portfolio of assets. The diversifica-\\ntion benefit increases with decreasing covariance. This observation is a key insight \\nof modern portfolio theory. This insight is even more intuitively stated when we can \\nuse the concept of correlation.\\n■ \\nDefinition of Correlation. The correlation between two random variables, Ri \\nand Rj, is defined as ρ(Ri,Rj) = Cov(Ri,Rj)/[σ(Ri)σ(Rj)]. Alternative notations \\nare Corr(Ri,Rj) and ρij.\\nThe above definition of correlation is forward-looking because it involves dividing the \\nforward-looking covariance by the product of forward-looking standard deviations. \\nFrequently, covariance is substituted out using the relationship Cov(Ri,Rj) = ρ(Ri,Rj)\\nσ(Ri)σ(Rj). Like covariance, the correlation coefficient is a measure of linear association. \\nHowever, the division in the definition makes correlation a pure number (without a \\nunit of measurement) and places bounds on its largest and smallest possible values, \\nwhich are +1 and –1, respectively.\\nIf two variables have a strong positive linear relation, then their correlation will be \\nclose to +1. If two variables have a strong negative linear relation, then their correlation \\nwill be close to –1. If two variables have a weak linear relation, then their correlation \\nwill be close to 0. Using the above definition, we can state a correlation matrix from \\ndata in the covariance matrix alone. Exhibit 13 shows the correlation matrix.\\nExhibit 13: Correlation Matrix of Returns\\n\\xa0\\nS&P 500\\nUS Long-Term \\nCorporate Bonds\\nMSCI EAFE\\nS&P 500\\n1.00\\n0.25\\n0.45\\nUS long-term corporate bonds\\n0.25\\n1.00\\n0.20\\nMSCI EAFE\\n0.45\\n0.20\\n1.00\\nFor example, the covariance between long-term bonds and MSCI EAFE is 38, from \\nExhibit 12. The standard deviation of long-term bond returns is 811/2 = 9%, that of \\nMSCI EAFE returns is 4411/2 = 21%, from diagonal terms in Exhibit 12. The correlation \\nρ(Rlong-term bonds, REAFE) is 38/[(9%)(21%)] = 0.201, rounded to 0.20. The correlation \\nof the S&P 500 with itself equals 1: The calculation is its own covariance divided by \\nits standard deviation squared.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPortfolio Expected Return and Variance of Return\\n201\\nEXAMPLE 13\\nPortfolio Expected Return and Variance of Return with \\nVarying Portfolio Weights\\nAnna Cintara is constructing different portfolios from the following two stocks:\\n \\nExhibit 14: Description of Two-Stock Portfolio\\n \\n \\n\\xa0\\nStock 1\\nStock 2\\nExpected return\\n4%\\n8%\\nStandard deviation\\n6%\\n15%\\nCurrent portfolio weights\\n0.40\\n0.60\\nCorrelation between returns\\n0.30\\n \\n1. Calculate the covariance between the returns on the two stocks.\\nSolution to 1:\\nThe correlation between two stock returns is ρ(Ri,Rj) = Cov(Ri,Rj)/[σ(Ri) \\nσ(Rj)], so the covariance is Cov(Ri,Rj) = ρ(Ri,Rj) σ(Ri) σ(Rj). For these two \\nstocks, the covariance is Cov(R1,R2) = ρ(R1,R2) σ(R1) σ(R2) = 0.30 (6) (15) = \\n27.\\n2. What is the portfolio expected return and standard deviation if Cintara puts \\n100% of her investment in Stock 1 (w1 = 1.00 and w2 = 0.00)? What is the \\nportfolio expected return and standard deviation if Cintara puts 100% of her \\ninvestment in Stock 2 (w1 = 0.00 and w2 = 1.00)?\\nSolution to 2:\\nIf the portfolio is 100% invested in Stock 1, the portfolio has an expected \\nreturn of 4% and a standard deviation of 6%. If the portfolio is 100% invested \\nin Stock 2, the portfolio has an expected return of 8% and a standard devia-\\ntion of 15%.\\n3. What are the portfolio expected return and standard deviation using the \\ncurrent portfolio weights?\\nSolution to 3:\\nFor the current 40/60 portfolio, the expected return is\\n E(Rp) = w1E(R1) + (1 − w1)E(R2) = 0.40(4%) + 0.60(8%) = 6.4%\\nThe portfolio variance and standard deviation are\\n \\n σ 2 ( R p ) =  w 1 2  σ 2 ( R 1 ) +\\u200a w 2 2  σ 2 ( R 2 ) +\\u200a2  w 1  w 2 Cov( R 1 ,  R 2 )\\n  \\n \\n \\n \\n \\n=  ( 0.40  ) 2  (36 ) +\\u200a ( 0.60  ) 2  (225 ) +\\u200a2(0.40 ) (0.60 ) (27)  \\n \\n \\n \\n= 5.76 + 81 + 12.94 = 99.72\\n \\n \\n σ( R p ) =  99.72 1/2 = 9.99% \\n4. Calculate the expected return and standard deviation of the portfolios when \\nw1 goes from 0.00 to 1.00 in 0.10 increments (and w2 = 1 – w1). Place the \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n202\\nresults (stock weights, portfolio expected return, and portfolio standard \\ndeviation) in a table, and then sketch a graph of the results with the standard \\ndeviation on the horizontal axis and expected return on the vertical axis.\\nSolution to 4:\\nThe portfolio expected returns, variances, and standard deviations for the \\ndifferent sets of portfolio weights are given in the following table. Three of \\nthe rows are already computed in the Solutions to 2 and 3, and the other \\nrows are computed using the same expected return, variance, and standard \\ndeviation formulas as in the Solution to 3:\\n \\nStock 1 \\nweight\\nStock 2 \\nweight\\nExpected \\nreturn (%)\\nVariance \\n(%2)\\nStandard \\ndeviation (%)\\n1.00\\n0.00\\n4.00\\n36.00\\n6.00\\n0.90\\n0.10\\n4.40\\n36.27\\n6.02\\n0.80\\n0.20\\n4.80\\n40.68\\n6.38\\n0.70\\n0.30\\n5.20\\n49.23\\n7.02\\n0.60\\n0.40\\n5.60\\n61.92\\n7.87\\n0.50\\n0.50\\n6.00\\n78.75\\n8.87\\n0.40\\n0.60\\n6.40\\n99.72\\n9.99\\n0.30\\n0.70\\n6.80\\n124.83\\n11.17\\n0.20\\n0.80\\n7.20\\n154.08\\n12.41\\n0.10\\n0.90\\n7.60\\n187.47\\n13.69\\n0.00\\n1.00\\n8.00\\n225.00\\n15.00\\n \\nThe graph of the expected return and standard deviation is\\nExpected Return (%)\\n9\\n7\\n8\\n123456\\n0\\n0\\n16\\n16\\n6\\n2\\n4\\n8\\n14\\n14\\n10\\n10\\n12\\n12\\nStandard Deviation (%)\\nCOVARIANCE GIVEN A JOINT PROBABILITY \\nFUNCTION\\ncalculate and interpret the covariances of portfolio returns using the \\njoint probability function\\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Covariance Given a Joint Probability Function',\n",
       "     'page_number': 212,\n",
       "     'content': 'Covariance Given a Joint Probability Function\\n203\\nHow do we estimate return covariance and correlation? Frequently, we make forecasts \\non the basis of historical covariance or use other methods such as a market model \\nregression based on historical return data. We can also calculate covariance using \\nthe joint probability function of the random variables, if that can be estimated. The \\njoint probability function of two random variables X and Y, denoted P(X,Y), gives the \\nprobability of joint occurrences of values of X and Y. For example, P(X=3, Y=2), is the \\nprobability that X equals 3 and Y equals 2.\\nSuppose that the joint probability function of the returns on BankCorp stock (RA) \\nand the returns on NewBank stock (RB) has the simple structure given in Exhibit 15.\\nExhibit 15: Joint Probability Function of BankCorp and NewBank Returns \\n(Entries Are Joint Probabilities)\\n\\xa0\\nRB = 20%\\nRB = 16%\\nRB = 10%\\nRA = 25%\\n0.20\\n0\\n0\\nRA = 12%\\n0\\n0.50\\n0\\nRA = 10%\\n0\\n0\\n0.30\\nThe expected return on BankCorp stock is 0.20(25%) + 0.50(12%) + 0.30(10%) = \\n14%. The expected return on NewBank stock is 0.20(20%) + 0.50(16%) + 0.30(10%) = \\n15%. The joint probability function above might reflect an analysis based on whether \\nbanking industry conditions are good, average, or poor. Exhibit 16 presents the cal-\\nculation of covariance.\\nExhibit 16: Covariance Calculations\\nBanking \\nIndustry \\nCondition\\nDeviations \\nBankCorp\\nDeviations \\nNewBank\\nProduct of \\nDeviations\\nProbability of \\nCondition\\nProbability-Weighted \\nProduct\\nGood\\n25−14\\n20−15\\n55\\n0.20\\n11\\nAverage\\n12−14\\n16−15\\n−2\\n0.50\\n−1\\nPoor\\n10−14\\n10−15\\n20\\n0.30\\n6\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCov(RA,RB) = 16\\nNote: Expected return for BankCorp is 14% and for NewBank, 15%.\\nThe first and second columns of numbers show, respectively, the deviations of BankCorp \\nand NewBank returns from their mean or expected value. The next column shows the \\nproduct of the deviations. For example, for good industry conditions, (25 − 14)(20 − 15) \\n= 11(5) = 55. Then, 55 is multiplied or weighted by 0.20, the probability that banking \\nindustry conditions are good: 55(0.20) = 11. The calculations for average and poor \\nbanking conditions follow the same pattern. Summing up these probability-weighted \\nproducts, we find Cov(RA,RB) = 16.\\nA formula for computing the covariance between random variables RA and RB is\\n Cov  ( R A ,  R B )  =  ∑ \\ni\\n   ∑ \\nj\\n  P  ( R A,i ,  R B,j )     ( R A,i − E  R A )   ( R B,j − E  R B )  \\n(19)\\nThe formula tells us to sum all possible deviation cross-products weighted by the \\nappropriate joint probability.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n204\\nNext, we take note of the fact that when two random variables are independent, \\ntheir joint probability function simplifies.\\n■ \\nDefinition of Independence for Random Variables. Two random variables X \\nand Y are independent if and only if P(X,Y) = P(X)P(Y).\\nFor example, given independence, P(3,2) = P(3)P(2). We multiply the individual \\nprobabilities to get the joint probabilities. Independence is a stronger property than \\nuncorrelatedness because correlation addresses only linear relationships. The follow-\\ning condition holds for independent random variables and, therefore, also holds for \\nuncorrelated random variables.\\n■ \\nMultiplication Rule for Expected Value of the Product of Uncorrelated \\nRandom Variables. The expected value of the product of uncorrelated ran-\\ndom variables is the product of their expected values.\\nE(XY) = E(X)E(Y) if X and Y are uncorrelated.\\nMany financial variables, such as revenue (price times quantity), are the product \\nof random quantities. When applicable, the above rule simplifies calculating expected \\nvalue of a product of random variables.\\nEXAMPLE 14\\nCovariances and Correlations of Security Returns\\n1. Isabel Vasquez is reviewing the correlations between four of the asset classes \\nin her company portfolio. In Exhibit 17, she plots 24 recent monthly returns \\nfor large-cap US stocks versus for large-cap world ex-US stocks (Panel 1) \\nand the 24 monthly returns for intermediate-term corporate bonds versus \\nlong-term corporate bonds (Panel 2). Vasquez presents the returns, varianc-\\nes, and covariances in decimal form instead of percentage form. Note the \\ndifferent ranges of their vertical axes (Return %).\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nCovariance Given a Joint Probability Function\\n205\\nExhibit 17: Monthly Returns for Four Asset Classes \\n \\nA. Equity Monthly Returns\\n0.15\\n0.15\\n0.10\\n0.10\\n0\\n0.05\\n0.05\\n–0.05\\n–0.05\\n–0.10\\n–0.10\\n–0.15\\n–0.15\\n0.15\\n0.15\\n0.10\\n0.10\\n0\\n0.05\\n0.05\\n–0.05\\n–0.05\\n–0.10\\n–0.10\\n–0.15\\n–0.15\\n1\\n15\\n15 17\\n17 19\\n19 21\\n21 23\\n23\\n3\\n5\\n7\\n9\\n1111\\n13\\n13\\nLarge-Cap US\\nLarge-Cap World Ex US\\nB. Corporate Bond Monthly Returns\\n1\\n15\\n15 17\\n17 19\\n19 21\\n21 23\\n23\\n3\\n5\\n7\\n9\\n1111\\n13\\n13\\nIntermediate Corp Bonds\\nLong-Term Corp Bonds\\nReturn (%)\\nReturn (%)\\nSelected data for the four asset classes are shown in Exhibit 18.\\n \\nExhibit 18: Selected Data for Four Asset Classes\\n \\n \\nAsset Classes\\nLarge-Cap \\nUS \\nEquities\\nWorld \\n(ex US) \\nEquities\\nIntermediate \\nCorp Bonds\\nLong-Term \\nCorp Bonds\\nVariance\\n0.001736\\n0.001488\\n0.000174\\n0.000699\\nStandard deviation\\n0.041668\\n0.038571\\n0.013180\\n0.026433\\nCovariance\\n0.001349\\n0.000318\\nCorrelation\\n0.87553\\n0.95133\\n \\nVasquez noted, as shown in Exhibit 18, that although the two equity classes \\nhad much greater variances and covariance than the two bond classes, the \\ncorrelation between the two equity classes was lower than the correlation \\nbetween the two bond classes. She also noted that although long-term \\nbonds were more volatile (higher variance) than intermediate-term bonds, \\nlong- and intermediate-term bond returns still had a high correlation.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n206\\n',\n",
       "     'children': []},\n",
       "    {'title': \"Bayes' Formula\",\n",
       "     'page_number': 216,\n",
       "     'content': \"Learning Module 3 \\nProbability Concepts\\n206\\nBAYES' FORMULA\\ncalculate and interpret an updated probability using \",\n",
       "     'children': [{'title': 'Bayes’ Formula',\n",
       "       'page_number': 216,\n",
       "       'content': \"Bayes’ formula\\nA topic that is often useful in solving investment problems is Bayes’ formula: what \\nprobability theory has to say about learning from experience.\\nBayes’ Formula\\nWhen we make decisions involving investments, we often start with viewpoints based \\non our experience and knowledge. These viewpoints may be changed or confirmed by \\nnew knowledge and observations. Bayes’ formula is a rational method for adjusting \\nour viewpoints as we confront new information. Bayes’ formula and related concepts \\nhave been applied in many business and investment decision-making contexts.\\nBayes’ formula makes use of Equation 6, the total probability rule. To review, that \\nrule expressed the probability of an event as a weighted average of the probabilities of \\nthe event, given a set of scenarios. Bayes’ formula works in reverse; more precisely, it \\nreverses the “given that” information. Bayes’ formula uses the occurrence of the event \\nto infer the probability of the scenario generating it. For that reason, Bayes’ formula \\nis sometimes called an inverse probability. In many applications, including those \\nillustrating its use in this section, an individual is updating his/her beliefs concerning \\nthe causes that may have produced a new observation.\\n■ \\nBayes’ Formula. Given a set of prior probabilities for an event of interest, \\nif you receive new information, the rule for updating your probability of the \\nevent is\\n \\nUpdated probability of event given the new information\\n  \\n \\n \\n \\n \\n=  \\nProbability of the new information given event \\n \\n \\n \\n___________________________________ \\n \\n \\n \\nUnconditional probability of the new information  × Prior probability of event  \\nIn probability notation, this formula can be written concisely as\\n P  ( Event  |  Information )  =  P  ( Information  |  Event )   \\n \\n_________________ \\n \\nP  ( Information )   \\n P  ( Event )  \\n(20)\\nConsider the following example using frequencies—which may be more straight-\\nforward initially than probabilities—for illustrating and understanding Bayes’ formula. \\nAssume a hypothetical large-cap stock index has 500 member firms, of which 100 are \\ntechnology firms, and 60 of these had returns of > 10%, and 40 had returns of ≤ 10%. \\nOf the 400 non-technology firms in the index, 100 had returns of > 10%, and 300 had \\nreturns of ≤ 10%. The tree map in Exhibit 19 is useful for visualizing this example, \\nwhich is summarized in the table in Exhibit 20.\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nBayes' Formula\\n207\\nExhibit 19: Tree Map for Visualizing Bayes’ Formula Using Frequencies\\nP (Tech | R > 10%) = 60/(60+100)\\nAll Firms\\nNon-Tech Firms\\nTech Firms\\nReturn > 10 %\\nReturn > 10 %\\nReturn ≤ 10%\\nReturn ≤ 10%\\n100\\n400\\n500\\n40\\n60\\n300\\n100\\nExhibit 20: Summary of Returns for Tech and Non-Tech Firms in \\nHypothetical Large-Cap Equity Index\\nRate of Return (R)\\nType of Firm in Stock Index\\n\\xa0\\nNon-Tech\\nTech\\nTotal\\nR > 10%\\n100\\n60\\n160\\nR ≤ 10%\\n300\\n40\\n340\\nTotal\\n400\\n100\\n500\\nWhat is the probability a firm is a tech firm given that it has a return of > 10% or \\nP(tech | R > 10%)? Looking at the frequencies in the tree map and in the table, we \\ncan see many empirical probabilities, such as the following:\\n■ \\nP(tech) = 100 / 500 = 0.20,\\n■ \\nP(non-tech) = 400 / 500 = 0.80,\\n■ \\nP(R > 10% | tech) = 60 / 100 = 0.60,\\n■ \\nP(R > 10% | non-tech) = 100 / 400 = 0.25,\\n■ \\nP(R > 10%) = 160 / 500 = 0.32, and, finally,\\n■ \\nP(tech | R > 10%) = 60/ 160 = 0.375. This probability is the answer to our \\ninitial question.\\nWithout looking at frequencies, let us use Bayes’ formula to find the probability \\nthat a firm has a return of > 10% and then the probability that a firm with a return of \\n> 10% is a tech firm, P(tech | R > 10%). First,\\n P(R > 10%) = P(R > 10% | tech)×P(tech) + P(R > 10% | non-tech)×P(non-tech)\\n = 0.60×0.20 + 0.25×0.80 = 0.32.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n208\\nNow we can implement the Bayes’ formula answer to our question:\\n P(tech | R > 10 %  ) =  P(R > 10% | tech ) × P(tech) \\n \\n____________________ \\n \\nP(R > 10 % ) \\n =  0.60 × 0.20 \\n_ \\n0.32 \\n = 0.375  \\nThe probability that a firm with a return of > 10% is a tech firm is 0.375, which is \\nimpressive because the probability that a firm is a tech firm (from the whole sample) \\nis only 0.20. In sum, it can be readily seen from the tree map and the underlying \\nfrequency data (Exhibits 19 and 20, respectively) or from the probabilities in Bayes’ \\nformula that there are 160 firms with R > 10%, and 60 of them are tech firms, so P(tech \\n| R > 10%) = 60/160 = .375.\\nUsers of Bayesian statistics do not consider probabilities (or likelihoods) to be \\nknown with certainty but that these should be subject to modification whenever new \\ninformation becomes available. Our beliefs or probabilities are continually updated \\nas new information arrives over time.\\nTo further illustrate Bayes’ formula, we work through an investment example that \\ncan be adapted to any actual problem. Suppose you are an investor in the stock of \\nDriveMed, Inc. Positive earnings surprises relative to consensus EPS estimates often \\nresult in positive stock returns, and negative surprises often have the opposite effect. \\nDriveMed is preparing to release last quarter’s EPS result, and you are interested in \\nwhich of these three events happened: last quarter’s EPS exceeded the consensus EPS \\nestimate, last quarter’s EPS exactly met the consensus EPS estimate, or last quarter’s \\nEPS fell short of the consensus EPS estimate. This list of the alternatives is mutually \\nexclusive and exhaustive.\\nOn the basis of your own research, you write down the following prior probabil-\\nities (or priors, for short) concerning these three events:\\n■ \\nP(EPS exceeded consensus) = 0.45\\n■ \\nP(EPS met consensus) = 0.30\\n■ \\nP(EPS fell short of consensus) = 0.25\\nThese probabilities are “prior” in the sense that they reflect only what you know \\nnow, before the arrival of any new information.\\nThe next day, DriveMed announces that it is expanding factory capacity in Singapore \\nand Ireland to meet increased sales demand. You assess this new information. The \\ndecision to expand capacity relates not only to current demand but probably also to \\nthe prior quarter’s sales demand. You know that sales demand is positively related to \\nEPS. So now it appears more likely that last quarter’s EPS will exceed the consensus.\\nThe question you have is, “In light of the new information, what is the updated \\nprobability that the prior quarter’s EPS exceeded the consensus estimate?”\\nBayes’ formula provides a rational method for accomplishing this updating. We \\ncan abbreviate the new information as DriveMed expands. The first step in applying \\nBayes’ formula is to calculate the probability of the new information (here: DriveMed \\nexpands), given a list of events or scenarios that may have generated it. The list of \\nevents should cover all possibilities, as it does here. Formulating these conditional \\nprobabilities is the key step in the updating process. Suppose your view, based on \\nresearch of DriveMed and its industry, is\\n P(DriveMed expands | EPS exceeded consensus) = 0.75\\n P(DriveMed expands | EPS met consensus) = 0.20\\n P(DriveMed expands | EPS fell short of consensus) = 0.05\\nConditional probabilities of an observation (here: DriveMed expands) are sometimes \\nreferred to as likelihoods. Again, likelihoods are required for updating the probability.\\n© CFA Institute. For candidate use only. Not for distribution.\\nBayes' Formula\\n209\\nNext, you combine these conditional probabilities or likelihoods with your prior \\nprobabilities to get the unconditional probability for DriveMed expanding, P(DriveMed \\nexpands), as follows:\\n \\nP  ( DriveMed expands )  \\n  \\n \\n= P  ( DriveMed expands | EPS exceeded consensus  )  \\n  \\n \\n \\n \\n \\n   ×  P  ( EPS exceeded consensus )  \\n  \\n \\n \\n+\\u200a P  ( DriveMed expands | EPS met consensus  )    \\n \\n \\n \\n   ×  P  ( EPS met consensus )    \\n \\n \\n+\\u200a P  ( DriveMed expands | EPS fell short of consensus  )  \\n  \\n \\n \\n \\n \\n   ×  P  ( EPS fell short of consensus )  \\n  \\n \\n \\n \\n= 0.75  ( 0.45 )  + 0.20  ( 0.30 )  + 0.05  ( 0.25 )  = 0.41,  or 41%\\n  \\nThis is Equation 6, the total probability rule, in action. Now you can answer your \\nquestion by applying Bayes’ formula:\\n \\nP  ( EPS exceeded consensus | DriveMed expands  )  \\n  \\n \\n \\n \\n=  \\nP  ( DriveMed expands | EPS exceeded consensus  )   \\n \\n \\n \\n__________________________________ \\n \\n \\nP  ( DriveMed expands )   \\n P  ( EPS exceeded consensus )    \\n \\n \\n \\n \\n \\n=   ( 0.75\\u200a/\\u200a0.41 )   ( 0.45 )  = 1.829268  ( 0.45 )  = 0.823171\\n \\n \\nPrior to DriveMed’s announcement, you thought the probability that DriveMed \\nwould beat consensus expectations was 45%. On the basis of your interpretation of the \\nannouncement, you update that probability to 82.3%. This updated probability is called \\nyour posterior probability because it reflects or comes after the new information.\\nThe Bayes’ calculation takes the prior probability, which was 45%, and multiplies \\nit by a ratio—the first term on the right-hand side of the equal sign. The denominator \\nof the ratio is the probability that DriveMed expands, as you view it without consider-\\ning (conditioning on) anything else. Therefore, this probability is unconditional. The \\nnumerator is the probability that DriveMed expands, if last quarter’s EPS actually \\nexceeded the consensus estimate. This last probability is larger than unconditional \\nprobability in the denominator, so the ratio (1.83 roughly) is greater than 1. As a result, \\nyour updated or posterior probability is larger than your prior probability. Thus, the \\nratio reflects the impact of the new information on your prior beliefs.\\nEXAMPLE 15\\nInferring Whether DriveMed’s EPS Met Consensus EPS\\nYou are still an investor in DriveMed stock. To review the givens, your prior \\nprobabilities are P(EPS exceeded consensus) = 0.45, P(EPS met consensus) = 0.30, \\nand P(EPS fell short of consensus) = 0.25. You also have the following conditional \\nprobabilities:\\n P(DriveMed expands | EPS exceeded consensus) = 0.75\\n P(DriveMed expands | EPS met consensus) = 0.20\\n P(DriveMed expands | EPS fell short of consensus) = 0.05\\nRecall that you updated your probability that last quarter’s EPS exceeded the \\nconsensus estimate from 45% to 82.3% after DriveMed announced it would \\nexpand. Now you want to update your other priors.\\nWhat is your estimate of the probability P(EPS exceeded consensus | DriveMed \\nexpands)?\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n210\\n1. Update your prior probability that DriveMed’s EPS met consensus.\\nSolution to 1:\\nThe probability is P(EPS met consensus | DriveMed expands) =\\n P  ( DriveMed expands | EPS met consensus  )   \\n \\n \\n \\n_______________________________ \\n \\n \\nP  ( DriveMed expands )   \\n P  ( EPS met consensus )   \\nThe probability P(DriveMed expands) is found by taking each of the \\nthree conditional probabilities in the statement of the problem, such as \\nP(DriveMed expands | EPS exceeded consensus); multiplying each one by the \\nprior probability of the conditioning event, such as P(EPS exceeded consen-\\nsus); then adding the three products. The calculation is unchanged from the \\nproblem in the text above: P(DriveMed expands) = 0.75(0.45) + 0.20(0.30) \\n+ 0.05(0.25) = 0.41, or 41%. The other probabilities needed, P(DriveMed \\nexpands | EPS met consensus) = 0.20 and P(EPS met consensus) = 0.30, are \\ngivens. So\\n P(EPS met consensus | DriveMed expands)\\n = [P(DriveMed expands | EPS met consensus)/P(DriveMed expands)]P(EPS \\nmet consensus)\\n = (0.20/0.41)(0.30) = 0.487805(0.30) = 0.146341\\nAfter taking account of the announcement on expansion, your updated \\nprobability that last quarter’s EPS for DriveMed just met consensus is 14.6% \\ncompared with your prior probability of 30%.\\n2. Update your prior probability that DriveMed’s EPS fell short of consensus.\\nSolution to 2:\\nP(DriveMed expands) was already calculated as 41%. Recall that P(DriveMed \\nexpands | EPS fell short of consensus) = 0.05 and P(EPS fell short of consen-\\nsus) = 0.25 are givens.\\n \\nP  ( EPS fell short of consensus | DriveMed expands  )  \\n  \\n \\n \\n \\n \\n=   [ P  ( DriveMed expands | EPS fell short of consensus  )   \\u200a/  \\n \\n \\n \\n \\n P  ( DriveMed expands )  ]  P  ( EPS fell short of consensus )  \\n  \\n \\n \\n \\n \\n=   ( 0.05\\u200a/\\u200a0.41 )   ( 0.25 )  = 0.121951  ( 0.25 )  = 0.030488\\n  \\nAs a result of the announcement, you have revised your probability that \\nDriveMed’s EPS fell short of consensus from 25% (your prior probability) to \\n3%.\\n3. Show that the three updated probabilities sum to 1. (Carry each probability \\nto four decimal places.)\\nSolution to 3:\\nThe sum of the three updated probabilities is\\n \\nP  ( EPS exceeded consensus | DriveMed expands  )  + P  ( EPS met consensus |   \\n  \\n \\n \\n \\n \\n \\n \\n DriveMed expands )  + P  ( EPS fell short of consensus | DriveMed expands  )    \\n \\n \\n \\n \\n \\n \\n= 0.8232 + 0.1463 + 0.0305 = 1.0000\\n \\n \\nThe three events (EPS exceeded consensus, EPS met consensus, EPS fell short \\nof consensus) are mutually exclusive and exhaustive: One of these events \\n© CFA Institute. For candidate use only. Not for distribution.\\nBayes' Formula\\n211\\nor statements must be true, so the conditional probabilities must sum to 1. \\nWhether we are talking about conditional or unconditional probabilities, \\nwhenever we have a complete set of distinct possible events or outcomes, \\nthe probabilities must sum to 1. This calculation serves to check your work.\\n4. Suppose, because of lack of prior beliefs about whether DriveMed would \\nmeet consensus, you updated on the basis of prior probabilities that all three \\npossibilities were equally likely: P(EPS exceeded consensus) = P(EPS met \\nconsensus) = P(EPS fell short of consensus) = 1/3.\\nSolution to 4:\\nUsing the probabilities given in the question,\\n \\nP  ( DriveMed expands )  \\n  \\n \\n= P  ( DriveMed expands | EPS exceeded consensus  )  \\n  \\n \\n \\n \\n \\n  P  ( EPS exceeded consensus )  + P  ( DriveMed expands |     \\n \\n \\n \\n \\n   EPS met consensus )  P  ( EPS met consensus )  + P  ( DriveMed expands |   \\n  \\n \\n \\n \\n \\n \\n   EPS fell short of consensus )  P  ( EPS fell short of consensus )  \\n  \\n \\n \\n \\n \\n= 0.75  ( 1\\u200a/\\u200a3 )  + 0.20  ( 1\\u200a/\\u200a3 )  + 0.05  ( 1\\u200a/\\u200a3 )  = 1\\u200a/\\u200a3\\n \\n \\nNot surprisingly, the probability of DriveMed expanding is 1/3 because the \\ndecision maker has no prior beliefs or views regarding how well EPS per-\\nformed relative to the consensus estimate.\\nNow we can use Bayes’ formula to find P(EPS exceeded consensus | \\nDriveMed expands) = [P(DriveMed expands | EPS exceeded consen-\\nsus)/P(DriveMed expands)] P(EPS exceeded consensus) = [(0.75/(1/3)](1/3) \\n= 0.75 or 75%. This probability is identical to your estimate of P(DriveMed \\nexpands | EPS exceeded consensus).\\nWhen the prior probabilities are equal, the probability of information given \\nan event equals the probability of the event given the information. When \\na decision maker has equal prior probabilities (called diffuse priors), the \\nprobability of an event is determined by the information.\\nExample 16 shows how Bayes’ formula is used in credit granting where the prob-\\nability of payment given credit information is higher than the probability of payment \\nwithout the information.\\nEXAMPLE 16\\nBayes’ Formula and the Probability of Payment\\n1. Jake Bronson is predicting the probability that consumer finance applicants \\ngranted credit will repay in a timely manner (i.e., their accounts will not \\nbecome “past due”). Using Bayes’ formula, he has structured the problem as\\n P  ( Event  |  Information )  =  P  ( Information  |  Event )   \\n \\n_________________ \\n \\nP  ( Information )   \\n P  ( Event )  ,\\nwhere the event (A) is “timely repayment” and the information (B) is having a \\n“good credit report.”\\nBronson estimates that the unconditional probability of receiving timely \\npayment, P(A), is 0.90 and that the unconditional probability of having a \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n212\\ngood credit report, P(B), is 0.80. The probability of having a good credit \\nreport given that borrowers paid on time, P(B | A), is 0.85.\\nWhat is the probability that applicants with good credit reports will repay in \\na timely manner?\\nA. 0.720\\nB. 0.944\\nC. 0.956\\nSolution:\\nThe correct answer is C. The probability of timely repayment given a good \\ncredit report, P(A | B), is\\n P(A | B ) =  P(B | A) \\n_ \\nP(B)  P(A ) =  0.85 \\n_ \\n0.80  × 0.90 = 0.956  \\n\",\n",
       "       'children': []}]},\n",
       "    {'title': 'Principles of Counting',\n",
       "     'page_number': 222,\n",
       "     'content': 'PRINCIPLES OF COUNTING\\nidentify the most appropriate method to solve a particular \\ncounting problem and analyze counting problems using factorial, \\ncombination, and permutation concepts\\nThe first step in addressing a question often involves determining the different logical \\npossibilities. We may also want to know the number of ways that each of these possi-\\nbilities can happen. In the back of our mind is often a question about probability. How \\nlikely is it that I will observe this particular possibility? Records of success and failure \\nare an example. For instance, the counting methods presented in this section have \\nbeen used to evaluate a market timer’s record. We can also use the methods in this \\nsection to calculate what we earlier called a priori probabilities. When we can assume \\nthat the possible outcomes of a random variable are equally likely, the probability of \\nan event equals the number of possible outcomes favorable for the event divided by \\nthe total number of outcomes.\\nIn counting, enumeration (counting the outcomes one by one) is of course the \\nmost basic resource. What we discuss in this section are shortcuts and principles. \\nWithout these shortcuts and principles, counting the total number of outcomes can \\nbe very difficult and prone to error. The first and basic principle of counting is the \\nmultiplication rule.\\n■ \\nMultiplication Rule for Counting. If one task can be done in n1 ways, and \\na second task, given the first, can be done in n2 ways, and a third task, given \\nthe first two tasks, can be done in n3 ways, and so on for k tasks, then the \\nnumber of ways the k tasks can be done is (n1)(n2)(n3) … (nk).\\nExhibit 21 illustrates the multiplication rule where, for example, we have three \\nsteps in an investment decision process. In the first step, stocks are classified two ways, \\nas domestic or foreign (represented by dark- and light-shaded circles, respectively). \\nIn the second step, stocks are assigned to one of four industries in our investment \\nuniverse: consumer, energy, financial, or technology (represented by four circles with \\nprogressively darker shades, respectively). In the third step, stocks are classified three \\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\nPrinciples of Counting\\n213\\nways by size: small-cap, mid-cap, and large-cap (represented by light-, medium-, and \\ndark-shaded circles, respectively). Because the first step can be done in two ways, the \\nsecond in four ways, and the third in three ways, using the multiplication rule, we can \\ncarry out the three steps in (2)(4)(3) = 24 different ways.\\nExhibit 21: Investment Decision Process Using Multiplication Rule: n1 = 2, \\nn2 = 4, n3 = 3\\n2nd Step\\n1st Step\\n3rd Step\\nStep 1: Domestic\\nStep 1: Foreign\\nStep 2: Financial\\nStep 2: Energy\\nStep 2: Consumer\\nStep 2: Technology\\nStep 3: Mid-Cap\\nStep 3: Small-Cap\\nStep 3: Large-Cap\\nAnother illustration is the assignment of members of a group to an equal number of \\npositions. For example, suppose you want to assign three security analysts to cover \\nthree different industries. In how many ways can the assignments be made? The first \\nanalyst can be assigned in three different ways. Then two industries remain. The sec-\\nond analyst can be assigned in two different ways. Then one industry remains. The \\nthird and last analyst can be assigned in only one way. The total number of different \\nassignments equals (3)(2)(1) = 6. The compact notation for the multiplication we have \\njust performed is 3! (read: 3 factorial). If we had n analysts, the number of ways we \\ncould assign them to n tasks would be\\n n! = n(n – 1)(n – 2)(n – 3)…1\\nor n factorial. (By convention, 0! = 1.) To review, in this application, we repeatedly \\ncarry out an operation (here, job assignment) until we use up all members of a group \\n(here, three analysts). With n members in the group, the multiplication formula \\nreduces to n factorial.\\nThe next type of counting problem can be called labeling problems.1 We want to \\ngive each object in a group a label, to place it in a category. The following example \\nillustrates this type of problem.\\nA mutual fund guide ranked 18 bond mutual funds by total returns for the last \\nyear. The guide also assigned each fund one of five risk labels: high risk (four funds), \\nabove-average risk (four funds), average risk (three funds), below-average risk (four \\nfunds), and low risk (three funds); as 4 + 4 + 3 + 4 + 3 = 18, all the funds are accounted \\nfor. How many different ways can we take 18 mutual funds and label 4 of them high \\nrisk, 4 above-average risk, 3 average risk, 4 below-average risk, and 3 low risk, so that \\neach fund is labeled?\\n1 This discussion follows Kemeny, Schleifer, Snell, and Thompson (1972) in terminology and approach.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n214\\nThe answer is close to 13 billion. We can label any of 18 funds high risk (the first \\nslot), then any of 17 remaining funds, then any of 16 remaining funds, then any of \\n15 remaining funds (now we have 4 funds in the high risk group); then we can label \\nany of 14 remaining funds above-average risk, then any of 13 remaining funds, and \\nso forth. There are 18! possible sequences. However, order of assignment within a \\ncategory does not matter. For example, whether a fund occupies the first or third slot \\nof the four funds labeled high risk, the fund has the same label (high risk). Thus, there \\nare 4! ways to assign a given group of four funds to the four high risk slots. Making the \\nsame argument for the other categories, in total there are (4!)(4!)(3!)(4!)(3!) equivalent \\nsequences. To eliminate such redundancies from the 18! total, we divide 18! by (4!)(4!)\\n(3!)(4!)(3!). We have 18!/[(4!)(4!)(3!)(4!)(3!)] = 18!/[(24)(24)(6)(24)(6)] = 12,864,852,000. \\nThis procedure generalizes as follows.\\n■ \\nMultinomial Formula (General Formula for Labeling Problems). The \\nnumber of ways that n objects can be labeled with k different labels, with n1 \\nof the first type, n2 of the second type, and so on, with n1 + n2 + … + nk = n, \\nis given by\\n \\nn\\u200a! \\n_ \\n n 1 \\u200a!   n 2 \\u200a!…\\u200a n k \\u200a!  \\n(21)\\nThe multinomial formula with two different labels (k = 2) is especially important. \\nThis special case is called the combination formula. A combination is a listing in which \\nthe order of the listed items does not matter. We state the combination formula in a \\ntraditional way, but no new concepts are involved. Using the notation in the formula \\nbelow, the number of objects with the first label is r = n1 and the number with the \\nsecond label is n − r = n2 (there are just two categories, so n1 + n2 = n). Here is the \\nformula:\\n■ \\nCombination Formula (Binomial Formula). The number of ways that we \\ncan choose r objects from a total of n objects, when the order in which the r \\nobjects are listed does not matter, is\\n n  C r =   ( n r  )  =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a! r\\u200a!  \\n(22)\\nHere nCr and    ( n r  )    are shorthand notations for n!/(n − r)!r! (read: n choose r, or n \\ncombination r).\\nIf we label the r objects as belongs to the group and the remaining objects as does \\nnot belong to the group, whatever the group of interest, the combination formula tells \\nus how many ways we can select a group of size r. We can illustrate this formula with \\nthe binomial option pricing model. (The binomial pricing model is covered later in \\nthe CFA curriculum. The only intuition we are concerned with here is that a number \\nof different pricing paths can end up with the same final stock price.) This model \\ndescribes the movement of the underlying asset as a series of moves, price up (U) \\nor price down (D). For example, two sequences of five moves containing three up \\nmoves, such as UUUDD and UDUUD, result in the same final stock price. At least \\nfor an option with a payoff dependent on final stock price, the number but not the \\norder of up moves in a sequence matters. How many sequences of five moves belong \\nto the group with three up moves? The answer is 10, calculated using the combination \\nformula (“5 choose 3”):\\n 5C3 = 5!/[(5 – 3)!3!]\\n = [(5)(4)(3)(2)(1)]/[(2)(1)(3)(2)(1)] = 120/12 = 10 ways\\nA useful fact can be illustrated as follows: 5C3 = 5!/(2!3!) equals 5C2 = 5!/(3!2!), as 3 + \\n2 = 5; 5C4 = 5!/(1!4!) equals 5C1 = 5!/(4!1!), as 4 + 1 = 5. This symmetrical relationship \\ncan save work when we need to calculate many possible combinations.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPrinciples of Counting\\n215\\nSuppose jurors want to select three companies out of a group of five to receive the \\nfirst-, second-, and third-place awards for the best annual report. In how many ways \\ncan the jurors make the three awards? Order does matter if we want to distinguish \\namong the three awards (the rank within the group of three); clearly the question \\nmakes order important. On the other hand, if the question were “In how many ways \\ncan the jurors choose three winners, without regard to place of finish?” we would use \\nthe combination formula.\\nTo address the first question above, we need to count ordered listings such as \\nfirst place, New Company; second place, Fir Company; third place, Well Company. An \\nordered listing is known as a permutation, and the formula that counts the number \\nof permutations is known as the permutation formula. A more formal definition states \\nthat a permutation is an ordered subset of n distinct objects.\\n■ \\nPermutation Formula. The number of ways that we can choose r objects \\nfrom a total of n objects, when the order in which the r objects are listed \\ndoes matter, is\\n n  P r =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!  \\n(23)\\nSo the jurors have 5P3 = 5!/(5 − 3)! = [(5)(4)(3)(2)(1)]/[(2)(1)] = 120/2 = 60 ways \\nin which they can make their awards. To see why this formula works, note that [(5)\\n(4)(3)(2)(1)]/[(2)(1)] reduces to (5)(4)(3), after cancellation of terms. This calculation \\ncounts the number of ways to fill three slots choosing from a group of five compa-\\nnies, according to the multiplication rule of counting. This number is naturally larger \\nthan it would be if order did not matter (compare 60 to the value of 10 for “5 choose \\n3” that we calculated above). For example, first place, Well Company; second place, \\nFir Company; third place, New Company contains the same three companies as first \\nplace, New Company; second place, Fir Company; third place, Well Company. If we \\nwere concerned only with award winners (without regard to place of finish), the two \\nlistings would count as one combination. But when we are concerned with the order \\nof finish, the listings count as two permutations.\\nEXAMPLE 17\\nPermutations and Combinations for Two Out of Four \\nOutcomes\\n1. There are four balls numbered 1, 2, 3, and 4 in a basket. You are running \\na contest in which two of the four balls are selected at random from the \\nbasket. To win, a player must have correctly chosen the numbers of the two \\nrandomly selected balls. Suppose the winning numbers are numbers 1 and \\n3. If the player must choose the balls in the same order in which they are \\ndrawn, she wins if she chose 1 first and 3 second. On the other hand, if order \\nis not important, the player wins if the balls drawn are 1 and then 3 or if the \\nballs drawn are 3 and then 1. The number of possible outcomes for permuta-\\ntions and combinations of choosing 2 out of 4 items is illustrated in Exhibit \\n22. If order is not important, for choosing 2 out of 4 items, the winner wins \\ntwice as often.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n216\\nExhibit 22: Permutations and Combinations for Choosing 2 out of 4 \\nItems\\n \\n \\nPermutations: Order matters\\nCombinations: Order does not matter\\nList of all possible outcomes: \\n(1 2) (2 1) (3 1) (4 1) \\n(1 3) (2 3) (3 2) (4 2) \\n(1 4) (2 4) (3 4) (4 3)\\nList of all possible outcomes: \\n(1 2) (2 3) (3 4) \\n(1 3) (2 4) \\n(1 4)\\nNumber of permutations: \\n nP  r =  \\nn ! \\n_ \\n(n − r ) !   \\n 4P  2 =  \\n4 ! \\n_ \\n(4 − 2 ) !  =  4 × 3 × 2 × 1 \\n_ \\n2 × 1 \\n = 12 \\nNumber of combinations: \\n nC  r =  \\nn ! \\n_ \\n(n − r ) !r !   \\n 4C  2 =  \\n4 ! \\n_ \\n(4 − 2 ) !2 !  =  4 × 3 × 2 × 1 \\n_ \\n2 × 1 × 2 × 1  = 6 \\n \\nIf order is important, the number of permutations (possible outcomes) is \\nmuch larger than the number of combinations when order is not important.\\nEXAMPLE 18\\nReorganizing the Analyst Team Assignments\\n1. Gehr-Flint Investors classifies the stocks in its investment universe into 11 \\nindustries and is assigning each research analyst one or two industries. Five \\nof the industries have been assigned, and you are asked to cover two indus-\\ntries from the remaining six.\\nHow many possible pairs of industries remain?\\nA. 12\\nB. 15\\nC. 36\\nSolution:\\nB is correct. The number of combinations of selecting two industries out of \\nsix is equal to\\n nC  r =   [ n _ r  ]  =  \\nn\\u200a! \\n_ \\n(n − r ) !r\\u200a!  =  6\\u200a! \\n_ \\n4\\u200a!2\\u200a!  = 15 \\nThe number of possible combinations for picking two industries out of six is \\n15.\\nEXAMPLE 19\\nAustralian Powerball Lottery\\nTo win the Australian Powerball jackpot, you must match the numbers of seven \\nballs pulled from a basket (the balls are numbered 1 through 35) plus the num-\\nber of the Powerball (numbered 1 through 20). The order in which the seven \\nballs are drawn is not important. The number of combinations of matching 7 \\nout of 35 balls is\\n nC  r =   [ n _ r  ]  =  \\nn\\u200a! \\n_ \\n(n − r ) !r\\u200a!  =  35\\u200a! \\n_ \\n28\\u200a!7\\u200a!  = 6, 724, 520 \\n© CFA Institute. For candidate use only. Not for distribution.\\nPrinciples of Counting\\n217\\nThe number of combinations for picking the Powerball, 1 out of 20, is\\n nC  r =   [ n _ r  ]  =  \\nn\\u200a! \\n_ \\n(n − r ) !r\\u200a!  =  20\\u200a! \\n_ \\n19\\u200a!1\\u200a!  = 20 \\nThe number of ways to pick the seven balls plus the Powerball is\\n 35C  7 × 20C  1 = 6, 724, 520 × 20 = 134, 490, 400 \\nYour probability of winning the Australian Powerball with one ticket is 1 in \\n134,490,400.\\nExhibit 23 is a flow chart that may help you apply the counting methods we have \\npresented in this section.\\nExhibit 23: ',\n",
       "     'children': []},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 228,\n",
       "     'content': 'SUMMARY\\nIn this reading, we have discussed the essential concepts and tools of probability. \\nWe have applied probability, expected value, and variance to a range of investment \\nproblems.\\n■ \\nA random variable is a quantity whose outcome is uncertain.\\n■ \\nProbability is a number between 0 and 1 that describes the chance that a \\nstated event will occur.\\n■ \\nAn event is a specified set of outcomes of a random variable.\\n■ \\nMutually exclusive events can occur only one at a time. Exhaustive events \\ncover or contain all possible outcomes.\\n■ \\nThe two defining properties of a probability are, first, that 0 ≤ P(E) ≤ 1 \\n(where P(E) denotes the probability of an event E) and, second, that the sum \\nof the probabilities of any set of mutually exclusive and exhaustive events \\nequals 1.\\n■ \\nA probability estimated from data as a relative frequency of occurrence is \\nan empirical probability. A probability drawing on personal or subjective \\njudgment is a subjective probability. A probability obtained based on logical \\nanalysis is an a priori probability.\\n■ \\nA probability of an event E, P(E), can be stated as odds for E = P(E)/[1 − \\nP(E)] or odds against E = [1 − P(E)]/P(E).\\n■ \\nProbabilities that are inconsistent create profit opportunities, according to \\nthe Dutch Book Theorem.\\n■ \\nA probability of an event not conditioned on another event is an uncondi-\\ntional probability. The unconditional probability of an event A is denoted \\nP(A). Unconditional probabilities are also called marginal probabilities.\\n■ \\nA probability of an event given (conditioned on) another event is a condi-\\ntional probability. The probability of an event A given an event B is denoted \\nP(A | B), and P(A | B) = P(AB)/P(B), P(B) ≠ 0.\\n■ \\nThe probability of both A and B occurring is the joint probability of A and B, \\ndenoted P(AB).\\n■ \\nThe multiplication rule for probabilities is P(AB) = P(A | B)P(B).\\n■ \\nThe probability that A or B occurs, or both occur, is denoted by P(A or B).\\n■ \\nThe addition rule for probabilities is P(A or B) = P(A) + P(B) − P(AB).\\n■ \\nWhen events are independent, the occurrence of one event does not affect \\nthe probability of occurrence of the other event. Otherwise, the events are \\ndependent.\\n■ \\nThe multiplication rule for independent events states that if A and B are \\nindependent events, P(AB) = P(A)P(B). The rule generalizes in similar fash-\\nion to more than two events.\\n■ \\nAccording to the total probability rule, if S1, S2, …, Sn are mutually exclusive \\nand exhaustive scenarios or events, then P(A) = P(A | S1)P(S1) + P(A | S2)\\nP(S2) + … + P(A | Sn)P(Sn).\\n■ \\nThe expected value of a random variable is a probability-weighted average of \\nthe possible outcomes of the random variable. For a random variable X, the \\nexpected value of X is denoted E(X).\\n© CFA Institute. For candidate use only. Not for distribution.\\nPrinciples of Counting\\n219\\n■ \\nThe total probability rule for expected value states that E(X) = E(X | S1)\\nP(S1) + E(X | S2)P(S2) + … + E(X | Sn)P(Sn), where S1, S2, …, Sn are mutually \\nexclusive and exhaustive scenarios or events.\\n■ \\nThe variance of a random variable is the expected value (the \\nprobability-weighted average) of squared deviations from the random vari-\\nable’s expected value E(X): σ2(X) = E{[X − E(X)]2}, where σ2(X) stands for \\nthe variance of X.\\n■ \\nVariance is a measure of dispersion about the mean. Increasing variance \\nindicates increasing dispersion. Variance is measured in squared units of the \\noriginal variable.\\n■ \\nStandard deviation is the positive square root of variance. Standard devia-\\ntion measures dispersion (as does variance), but it is measured in the same \\nunits as the variable.\\n■ \\nCovariance is a measure of the co-movement between random variables.\\n■ \\nThe covariance between two random variables Ri and Rj in a \\nforward-looking sense is the expected value of the cross-product of the \\ndeviations of the two random variables from their respective means: \\nCov(Ri,Rj) = E{[Ri − E(Ri)][Rj − E(Rj)]}. The covariance of a random variable \\nwith itself is its own variance.\\n■ \\nThe historical or sample covariance between two random variables Ri and Rj \\nbased on a sample of past data of size n is the average value of the product \\nof the deviations of observations on two random variables from their sample \\nmeans:\\n Cov  ( R i ,  R j )  =  ∑ \\nn=1\\n  \\nn\\n    ( R i,t −  \\n_\\n \\nR  i )   ( R j,t −  \\n_\\n \\nR  j )  \\u200a/\\u200a  ( n − 1 )    \\n■ \\nCorrelation is a number between −1 and +1 that measures the co-movement \\n(linear association) between two random variables: ρ(Ri,Rj) = Cov(Ri,Rj)/\\n[σ(Ri) σ(Rj)].\\n■ \\nIf two variables have a very strong (inverse) linear relation, then the absolute \\nvalue of their correlation will be close to 1 (-1). If two variables have a weak \\nlinear relation, then the absolute value of their correlation will be close to 0.\\n■ \\nIf the correlation coefficient is positive, the two variables are positively \\nrelated; if the correlation coefficient is negative, the two variables are \\ninversely related.\\n■ \\nTo calculate the variance of return on a portfolio of n assets, the inputs \\nneeded are the n expected returns on the individual assets, n variances of \\nreturn on the individual assets, and n(n − 1)/2 distinct covariances.\\n■ \\nPortfolio variance of return is   σ 2  ( R p )  =  ∑ \\ni=1\\n  \\nn\\n   ∑ \\nj=1\\n  \\nn\\n   w i  w j Cov   ( R i ,  R j )    .\\n■ \\nThe calculation of covariance in a forward-looking sense requires the spec-\\nification of a joint probability function, which gives the probability of joint \\noccurrences of values of the two random variables.\\n■ \\nWhen two random variables are independent, the joint probability func-\\ntion is the product of the individual probability functions of the random \\nvariables.\\n■ \\nBayes’ formula is a method for updating probabilities based on new \\ninformation.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n220\\n■ \\nBayes’ formula is expressed as follows: Updated probability of event given \\nthe new information = [(Probability of the new information given event)/\\n(Unconditional probability of the new information)] × Prior probability of \\nevent.\\n■ \\nThe multiplication rule of counting says, for example, that if the first step \\nin a process can be done in 10 ways, the second step, given the first, can \\nbe done in 5 ways, and the third step, given the first two, can be done in 7 \\nways, then the steps can be carried out in (10)(5)(7) = 350 ways.\\n■ \\nThe number of ways to assign every member of a group of size n to n slots is \\nn! = n (n − 1) (n − 2)(n − 3) … 1. (By convention, 0! = 1.)\\n■ \\nThe number of ways that n objects can be labeled with k different labels, \\nwith n1 of the first type, n2 of the second type, and so on, with n1 + n2 + … \\n+ nk = n, is given by n!/(n1!n2! … nk!). This expression is the multinomial \\nformula.\\n■ \\nA special case of the multinomial formula is the combination formula. The \\nnumber of ways to choose r objects from a total of n objects, when the order \\nin which the r objects are listed does not matter, is\\n n  C r =   ( n r  )  =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!r\\u200a!  \\n■ \\nThe number of ways to choose r objects from a total of n objects, when the \\norder in which the r objects are listed does matter, is\\n n  P r =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!  \\nThis expression is the permutation formula.\\nREFERENCES\\nKemeny, John G., Arthur Schleifer, J. Laurie Snell, Gerald L. Thompson. 1972. Finite Mathematics \\nwith Business Applications, 2nd edition. Englewood Cliffs: Prentice-Hall.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 231,\n",
       "     'content': 'Practice Problems\\n221\\nPRACTICE PROBLEMS\\n1. In probability theory, exhaustive events are best described as the set of events \\nthat:\\nA. have a probability of zero.\\nB. are mutually exclusive.\\nC. include all potential outcomes.\\n2. Which probability estimate most likely varies greatly between people?\\nA. An a priori probability\\nB. An empirical probability\\nC. A subjective probability\\n3. If the probability that Zolaf Company sales exceed last year’s sales is 0.167, the \\nodds for exceeding sales are closest to:\\nA. 1 to 5.\\nB. 1 to 6.\\nC. 5 to 1.\\n4. After six months, the growth portfolio that Rayan Khan manages has outper-\\nformed its benchmark. Khan states that his odds of beating the benchmark for \\nthe year are 3 to 1. If these odds are correct, what is the probability that Khan’s \\nportfolio will beat the benchmark for the year?\\nA. 0.33\\nB. 0.67\\nC. 0.75\\n5. Suppose that 5% of the stocks meeting your stock-selection criteria are in the \\ntelecommunications (telecom) industry. Also, dividend-paying telecom stocks \\nare 1% of the total number of stocks meeting your selection criteria. What is the \\nprobability that a stock is dividend paying, given that it is a telecom stock that has \\nmet your stock selection criteria?\\n6. You are using the following three criteria to screen potential acquisition targets \\nfrom a list of 500 companies:\\nCriterion\\nFraction of the 500 Companies \\nMeeting the Criterion\\nProduct lines compatible\\n0.20\\nCompany will increase combined sales growth rate\\n0.45\\nBalance sheet impact manageable\\n0.78\\nIf the criteria are independent, how many companies will pass the screen?\\n7.  Florence Hixon is screening a set of 100 stocks based on two criteria (Criterion \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n222\\n1 and Criterion 2). She set the passing level such that 50% of the stocks passed \\neach screen. For these stocks, the values for Criterion 1 and Criterion 2 are not \\nindependent but are positively related. How many stocks should pass Hixon’s two \\nscreens?\\nA. Less than 25\\nB. 25\\nC. More than 25\\n8. You apply both valuation criteria and financial strength criteria in choosing \\nstocks. The probability that a randomly selected stock (from your investment \\nuniverse) meets your valuation criteria is 0.25. Given that a stock meets your \\nvaluation criteria, the probability that the stock meets your financial strength \\ncriteria is 0.40. What is the probability that a stock meets both your valuation and \\nfinancial strength criteria?\\n9. The probability of an event given that another event has occurred is a:\\nA. joint probability.\\nB. marginal probability.\\nC. conditional probability.\\n10. After estimating the probability that an investment manager will exceed his \\nbenchmark return in each of the next two quarters, an analyst wants to forecast \\nthe probability that the investment manager will exceed his benchmark return \\nover the two-quarter period in total. Assuming that each quarter’s performance is \\nindependent of the other, which probability rule should the analyst select?\\nA. Addition rule\\nB. Multiplication rule\\nC. Total probability rule\\n11. Which of the following is a property of two dependent events?\\nA. The two events must occur simultaneously.\\nB. The probability of one event influences the probability of the other event.\\nC. The probability of the two events occurring is the product of each event’s \\nprobability.\\n12. Which of the following best describes how an analyst would estimate the expect-\\ned value of a firm using the scenarios of bankruptcy and non-bankruptcy? The \\nanalyst would use:\\nA. the addition rule.\\nB. conditional expected values.\\nC. the total probability rule for expected value.\\n13. Suppose the prospects for recovering principal for a defaulted bond issue depend \\non which of two economic scenarios prevails. Scenario 1 has probability 0.75 and \\nwill result in recovery of $0.90 per $1 principal value with probability 0.45, or \\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n223\\nin recovery of $0.80 per $1 principal value with probability 0.55. Scenario 2 has \\nprobability 0.25 and will result in recovery of $0.50 per $1 principal value with \\nprobability 0.85, or in recovery of $0.40 per $1 principal value with probability \\n0.15.\\nA. Compute the probability of each of the four possible recovery amounts: \\n$0.90, $0.80, $0.50, and $0.40.\\nB. Compute the expected recovery, given the first scenario.\\nC. Compute the expected recovery, given the second scenario.\\nD. Compute the expected recovery.\\nE. Graph the information in a probability tree diagram.\\n14. An analyst developed two scenarios with respect to the recovery of $100,000 \\nprincipal from defaulted loans:\\nScenario\\nProbability \\nof Scenario (%)\\nAmount \\nRecovered ($)\\nProbability \\nof Amount (%)\\n1\\n40\\n50,000\\n60\\n\\xa0\\n\\xa0\\n30,000\\n40\\n2\\n60\\n80,000\\n90\\n\\xa0\\n\\xa0\\n60,000\\n10\\nThe amount of the expected recovery is closest to:\\nA. $36,400.\\nB. $55,000.\\nC. $63,600.\\n15. The probability distribution for a company’s sales is:\\nProbability\\nSales ($ millions)\\n0.05\\n70\\n0.70\\n40\\n0.25\\n25\\nThe standard deviation of sales is closest to:\\nA. $9.81 million.\\nB. $12.20 million.\\nC. $32.40 million.\\n16. US and Spanish bonds have return standard deviations of 0.64 and 0.56, respec-\\ntively. If the correlation between the two bonds is 0.24, the covariance of returns \\nis closest to:\\nA. 0.086.\\nB. 0.335.\\nC. 0.390.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n224\\n17. The covariance of returns is positive when the returns on two assets tend to:\\nA. have the same expected values.\\nB. be above their expected value at different times.\\nC. be on the same side of their expected value at the same time.\\n18. Which of the following correlation coefficients indicates the weakest linear rela-\\ntionship between two variables?\\nA. –0.67\\nB. –0.24\\nC. 0.33\\n19. An analyst develops the following covariance matrix of returns:\\n\\xa0\\nHedge Fund\\nMarket Index\\nHedge fund\\n256\\n110\\nMarket index\\n110\\n81\\nThe correlation of returns between the hedge fund and the market index is closest \\nto:\\nA. 0.005.\\nB. 0.073.\\nC. 0.764.\\n20. All else being equal, as the correlation between two assets approaches +1.0, the \\ndiversification benefits:\\nA. decrease.\\nB. stay the same.\\nC. increase.\\n21. Given a portfolio of five stocks, how many unique covariance terms, excluding \\nvariances, are required to calculate the portfolio return variance?\\nA. 10\\nB. 20\\nC. 25\\n22. Which of the following statements is most accurate? If the covariance of returns \\nbetween two assets is 0.0023, then:\\nA. the assets’ risk is near zero.\\nB. the asset returns are unrelated.\\nC. the asset returns have a positive relationship.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n225\\n23. A two-stock portfolio includes stocks with the following characteristics:\\n\\xa0\\nStock 1\\nStock 2\\nExpected return\\n7%\\n10%\\nStandard deviation\\n12%\\n25%\\nPortfolio weights\\n0.30\\n0.70\\nCorrelation\\n0.20\\nWhat is the standard deviation of portfolio returns?\\nA. 14.91%\\nB. 18.56%\\nC. 21.10%\\n24. Lena Hunziger has designed the three-asset portfolio summarized below:\\n\\xa0\\nAsset 1\\nAsset 2\\nAsset 3\\nExpected return\\n5%\\n6%\\n7%\\nPortfolio weight\\n0.20\\n0.30\\n0.50\\nVariance-Covariance Matrix\\n\\xa0\\nAsset 1\\nAsset 2\\nAsset 3\\nAsset 1\\n196\\n105\\n140\\nAsset 2\\n105\\n225\\n150\\nAsset 3\\n140\\n150\\n400\\nHunziger estimated the portfolio return to be 6.3%. What is the portfolio stan-\\ndard deviation?\\nA. 13.07%\\nB. 13.88%\\nC. 14.62%\\n25. An analyst produces the following joint probability function for a foreign index \\n(FI) and a domestic index (DI).\\n\\xa0\\nRDI = 30%\\nRDI = 25%\\nRDI = 15%\\nRFI = 25%\\n0.25\\n\\xa0\\n\\xa0\\nRFI = 15%\\n\\xa0\\n0.50\\n\\xa0\\nRFI = 10%\\n\\xa0\\n\\xa0\\n0.25\\nThe covariance of returns on the foreign index and the returns on the domestic \\nindex is closest to:\\nA. 26.39.\\nB. 26.56.\\nC. 28.12.\\n26. You have developed a set of criteria for evaluating distressed credits. Companies \\nthat do not receive a passing score are classed as likely to go bankrupt within 12 \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n226\\nmonths. You gathered the following information when validating the criteria:\\n■ \\nForty percent of the companies to which the test is administered will go \\nbankrupt within 12 months: P(non-survivor) = 0.40.\\n■ \\nFifty-five percent of the companies to which the test is administered pass it: \\nP(pass test) = 0.55.\\n■ \\nThe probability that a company will pass the test given that it will subse-\\nquently survive 12 months, is 0.85: P(pass test | survivor) = 0.85.\\nA. What is P(pass test | non-survivor)?\\nB. Using Bayes’ formula, calculate the probability that a company is a survivor, \\ngiven that it passes the test; that is, calculate P(survivor | pass test).\\nC. What is the probability that a company is a non-survivor, given that it fails \\nthe test?\\nD. Is the test effective?\\n27. An analyst estimates that 20% of high-risk bonds will fail (go bankrupt). If she ap-\\nplies a bankruptcy prediction model, she finds that 70% of the bonds will receive \\na “good” rating, implying that they are less likely to fail. Of the bonds that failed, \\nonly 50% had a “good” rating. Use Bayes’ formula to predict the probability of fail-\\nure given a “good” rating. (Hint, let P(A) be the probability of failure, P(B) be the \\nprobability of a “good” rating, P(B | A) be the likelihood of a “good” rating given \\nfailure, and P(A | B) be the likelihood of failure given a “good” rating.)\\nA. 5.7%\\nB. 14.3%\\nC. 28.6%\\n28. In a typical year, 5% of all CEOs are fired for “performance” reasons. Assume \\nthat CEO performance is judged according to stock performance and that 50% of \\nstocks have above-average returns or “good” performance. Empirically, 30% of all \\nCEOs who were fired had “good” performance. Using Bayes’ formula, what is the \\nprobability that a CEO will be fired given “good” performance? (Hint, let P(A) be \\nthe probability of a CEO being fired, P(B) be the probability of a “good” perfor-\\nmance rating, P(B | A) be the likelihood of a “good” performance rating given that \\nthe CEO was fired, and P(A | B) be the likelihood of the CEO being fired given a \\n“good” performance rating.)\\nA. 1.5%\\nB. 2.5%\\nC. 3.0%\\n29. A manager will select 20 bonds out of his universe of 100 bonds to construct a \\nportfolio. Which formula provides the number of possible portfolios?\\nA. Permutation formula\\nB. Multinomial formula\\nC. Combination formula\\n30. A firm will select two of four vice presidents to be added to the investment com-\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n227\\nmittee. How many different groups of two are possible?\\nA. 6\\nB. 12\\nC. 24\\n31. From an approved list of 25 funds, a portfolio manager wants to rank 4 mutual \\nfunds from most recommended to least recommended. Which formula is most \\nappropriate to calculate the number of possible ways the funds could be ranked?\\nA. Permutation formula\\nB. Multinomial formula\\nC. Combination formula\\n32. Himari Fukumoto has joined a new firm and is selecting mutual funds in the \\nfirm’s pension plan. If 10 mutual funds are available, and she plans to select four, \\nhow many different sets of mutual funds can she choose?\\nA. 210\\nB. 720\\nC. 5,040\\nThe following information relates to questions \\n33-35\\nGerd Sturm wants to sponsor a contest with a $1 million prize. The winner must \\npick the stocks that will be the top five performers next year among the 30 stocks \\nin a well-known large-cap stock index. He asks you to estimate the chances that \\ncontestants can win the contest.\\n33. What are the chances of winning if the contestants must pick the five stocks in \\nthe correct order of their total return? If choosing five stocks randomly, a contes-\\ntant’s chance of winning is one out of:\\nA. 142,506.\\nB. 17,100,720.\\nC. 24,300,000.\\n34. What are the chances of winning if the contestants must pick the top five stocks \\nwithout regard to order? If choosing five stocks randomly, a contestant’s chance \\nof winning is one out of:\\nA. 142,506.\\nB. 17,100,720.\\nC. 24,300,000.\\n35. Sturm asks, “Can we trust these probabilities of winning?”\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n228\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 238,\n",
       "     'content': 'SOLUTIONS\\n1. C is correct. The term “exhaustive” means that the events cover all possible out-\\ncomes.\\n2. C is correct. A subjective probability draws on personal or subjective judgment \\nthat may be without reference to any particular data.\\n3. A is correct. Given odds for E of a to b, the implied probability of E = a/(a + b). \\nStated in terms of odds a to b with a = 1, b = 5, the probability of E = 1/(1 + 5) \\n= 1/6 = 0.167. This result confirms that a probability of 0.167 for beating sales is \\nodds of 1 to 5.\\n4. C is correct. The odds for beating the benchmark = P(beating benchmark) / [1 – \\nP(beating benchmark)]. Let P(A) = P(beating benchmark). Odds for beating the \\nbenchmark = P(A) / [1 – P(A)].\\n 3 = P(A) / [1 – P(A)]\\nSolving for P(A), the probability of beating the benchmark is 0.75.\\n5. Use this equation to find this conditional probability: P(stock is dividend paying \\n| telecom stock that meets criteria) = P(stock is dividend paying and telecom stock \\nthat meets criteria)/P(telecom stock that meets criteria) = 0.01/0.05 = 0.20.\\n6. According to the multiplication rule for independent events, the probability of \\na company meeting all three criteria is the product of the three probabilities. \\nLabeling the event that a company passes the first, second, and third criteria, A, \\nB, and C, respectively, P(ABC) = P(A)P(B)P(C) = (0.20)(0.45)(0.78) = 0.0702. As a \\nconsequence, (0.0702)(500) = 35.10, so 35 companies pass the screen.\\n7. C is correct. Let event A be a stock passing the first screen (Criterion 1) and \\nevent B be a stock passing the second screen (Criterion 2). The probability of \\npassing each screen is P(A) = 0.50 and P(B) = 0.50. If the two criteria are inde-\\npendent, the joint probability of passing both screens is P(AB) = P(A)P(B) = 0.50 \\n× 0.50 = 0.25, so 25 out of 100 stocks would pass both screens. However, the two \\ncriteria are positively related, and P(AB) ≠ 0.25. Using the multiplication rule for \\nprobabilities, the joint probability of A and B is P(AB) = P(A | B)P(B). If the two \\ncriteria are not independent, and if P(B) = 0.50, then the contingent probability \\nof P(A | B) is greater than 0.50. So the joint probability of P(AB) = P(A | B)P(B) is \\ngreater than 0.25. More than 25 stocks should pass the two screens.\\n8. Use the equation for the multiplication rule for probabilities P(AB) = P(A | B)\\nP(B), defining A as the event that a stock meets the financial strength criteria \\nand defining B as the event that a stock meets the valuation criteria. Then P(AB) \\n= P(A | B)P(B) = 0.40 × 0.25 = 0.10. The probability that a stock meets both the \\nfinancial and valuation criteria is 0.10.\\n9. C is correct. A conditional probability is the probability of an event given that \\nanother event has occurred.\\n10. B is correct. Because the events are independent, the multiplication rule is most \\nappropriate for forecasting their joint probability. The multiplication rule for \\nindependent events states that the joint probability of both A and B occurring is \\nP(AB) = P(A)P(B).\\n11. B is correct. The probability of the occurrence of one is related to the occurrence \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n229\\nof the other. If we are trying to forecast one event, information about a depen-\\ndent event may be useful.\\n12. C is correct. The total probability rule for expected value is used to estimate an \\nexpected value based on mutually exclusive and exhaustive scenarios.\\n13. \\nA. Outcomes associated with Scenario 1: With a 0.45 probability of a $0.90 \\nrecovery per $1 principal value, given Scenario 1, and with the probability \\nof Scenario 1 equal to 0.75, the probability of recovering $0.90 is 0.45 (0.75) \\n= 0.3375. By a similar calculation, the probability of recovering $0.80 is \\n0.55(0.75) = 0.4125.\\nOutcomes associated with Scenario 2: With a 0.85 probability of a $0.50 \\nrecovery per $1 principal value, given Scenario 2, and with the probability \\nof Scenario 2 equal to 0.25, the probability of recovering $0.50 is 0.85(0.25) \\n= 0.2125. By a similar calculation, the probability of recovering $0.40 is \\n0.15(0.25) = 0.0375.\\nB. E(recovery | Scenario 1) = 0.45($0.90) + 0.55($0.80) = $0.845\\nC. E(recovery | Scenario 2) = 0.85($0.50) + 0.15($0.40) = $0.485\\nD. E(recovery) = 0.75($0.845) + 0.25($0.485) = $0.755\\nE. \\xa0\\nRecovery = $0.755\\nScenario 1,\\nProbability = 0.75\\nScenario 2,\\nProbability = 0.25\\nRecovery = $0.90\\nProb = 0.3375\\nRecovery = $0.80\\nProb = 0.4125\\nRecovery = $0.50\\nProb = 0.2125\\nRecovery = $0.40\\nProb = 0.0375\\n0.45\\n0.55\\n0.85\\n0.15\\nExpected\\n14. C is correct. If Scenario 1 occurs, the expected recovery is 60% ($50,000) + 40% \\n($30,000) = $42,000, and if Scenario 2 occurs, the expected recovery is 90% \\n($80,000) + 10%($60,000) = $78,000. Weighting by the probability of each sce-\\nnario, the expected recovery is 40%($42,000) + 60%($78,000) = $63,600. Alter-\\nnatively, first calculating the probability of each amount occurring, the expected \\nrecovery is (40%)(60%)($50,000) + (40%)(40%)($30,000) + (60%)(90%)($80,000) + \\n(60%)(10%)($60,000) = $63,600.\\n15. A is correct. The analyst must first calculate expected sales as 0.05 × $70 + 0.70 \\n× $40 + 0.25 × $25 = $3.50 million + $28.00 million + $6.25 million = $37.75 \\nmillion.\\nAfter calculating expected sales, we can calculate the variance of sales:\\n σ2 (Sales) = P($70)[$70 – E(Sales)]2 + P($40)[$40 – E(Sales)]2 + P($25)\\n[$25 – E(Sales)]2\\n = 0.05($70 – 37.75)2 + 0.70($40 – 37.75)2 + 0.25($25 – 37.75)2\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n230\\n = $52.00 million + $3.54 million + $40.64 million = $96.18 million.\\nThe standard deviation of sales is thus σ = ($96.18)1/2 = $9.81 million.\\n16. A is correct. The covariance is the product of the standard deviations and cor-\\nrelation using the formula Cov(US bond returns, Spanish bond returns) = σ(US \\nbonds) × σ (Spanish bonds) × ρ(US bond returns, Spanish bond returns) = 0.64 × \\n0.56 × 0.24 = 0.086.\\n17. C is correct. The covariance of returns is positive when the returns on both assets \\ntend to be on the same side (above or below) their expected values at the same \\ntime, indicating an average positive relationship between returns.\\n18. B is correct. Correlations near +1 exhibit strong positive linearity, whereas cor-\\nrelations near –1 exhibit strong negative linearity. A correlation of 0 indicates an \\nabsence of any linear relationship between the variables. The closer the correla-\\ntion is to 0, the weaker the linear relationship.\\n19. C is correct. The correlation between two random variables Riand Rj is defined as \\nρ(Ri,Rj) = Cov(Ri,Rj)/[σ(Ri)σ(Rj)]. Using the subscript i to represent hedge funds \\nand the subscript j to represent the market index, the standard deviations are \\nσ(Ri) = 2561/2 = 16 and σ(Rj) = 811/2 = 9. Thus, ρ(Ri,Rj) = Cov(Ri,Rj)/[σ(Ri) σ(Rj)] = \\n110/(16 × 9) = 0.764.\\n20. A is correct. As the correlation between two assets approaches +1, diversification \\nbenefits decrease. In other words, an increasingly positive correlation indicates \\nan increasingly strong positive linear relationship and fewer diversification bene-\\nfits.\\n21. A is correct. A covariance matrix for five stocks has 5 × 5 = 25 entries. Subtract-\\ning the 5 diagonal variance terms results in 20 off-diagonal entries. Because a \\ncovariance matrix is symmetrical, only 10 entries are unique (20/2 = 10).\\n22. C is correct. The covariance of returns is positive when the returns on both assets \\ntend to be on the same side (above or below) their expected values at the same \\ntime.\\n23. B is correct. The covariance between the returns for the two stocks is Cov(R1,R2) \\n= ρ(R1,R2) σ(R1) σ(R2) = 0.20 (12) (25) = 60. The portfolio variance is \\n \\n σ 2 ( R p ) =  w 1 2  σ 2 ( R 1 ) +\\u200a w 2 2  σ 2 ( R 2 ) +\\u200a2  w 1  w 2 Cov( R 1 ,  R 2 )\\n  \\n \\n \\n \\n \\n=  (0.30) 2  (12) 2 +  (0.70) 2  (25) 2 + 2(0.30 ) (0.70 ) (60)  \\n \\n \\n \\n \\n= 12.96 + 306.25 + 25.2 = 344.41\\n \\n \\nThe portfolio standard deviation is\\n σ 2 ( R p ) =  344.41 1/2 = 18.56% \\n24. C is correct. For a three-asset portfolio, the portfolio variance is\\n \\n σ 2 ( R p ) =  w 1 2  σ 2 ( R 1 ) +  w 2 2  σ 2 ( R 2 ) +  w 3 2  σ 2 ( R 3 ) + 2  w 1  w 2 Cov ( R 1 ,  R 2 ) \\n  \\n \\n \\n \\n \\n \\n+\\u200a2  w 1  w 3 Cov ( R 1 ,  R 3 ) + 2  w 2  w 3 Cov ( R 2 ,  R 3 ) \\n  \\n \\n \\n \\n=  (0.20) 2 (196) +  (0.30) 2 (225) +  (0.50) 2 (400) + 2 (0.20) (0.30) (105)   \\n \\n \\n \\n \\n \\n+\\u200a (2) (0.20) (0.50) (140) +  (2) (0.30) (0.50) (150) \\n  \\n \\n \\n \\n= 7.84 + 20.25 + 100 + 12.6 + 28 + 45 = 213.69\\n \\n \\nThe portfolio standard deviation is\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n231\\n σ 2 ( R p ) =  213.69 1/2 = 14.62% \\n25. B is correct. The covariance is 26.56, calculated as follows. First, expected returns \\nare\\n E(RFI) = (0.25 × 25) + (0.50 × 15) + (0.25 × 10)\\n = 6.25 + 7.50 + 2.50 = 16.25 and\\n E(RDI) = (0.25 × 30) + (0.50 × 25) + (0.25 × 15)\\n = 7.50 + 12.50 + 3.75 = 23.75.\\nCovariance is\\n Cov(RFI,RDI) =  ∑ \\ni\\n   ∑ \\nj\\n  P  ( R FI,i ,  R DI,j )   ( R FI,i − E  R FI )   ( R DI,j − E  R DI )     \\n = 0.25[(25 – 16.25)(30 – 23.75)] + 0.50[(15 – 16.25)(25 – 23.75)] + \\n0.25[(10 – 16.25)(15 – 23.75)]\\n = 13.67 + (–0.78) + 13.67 = 26.56.\\n26. \\nA. We can set up the equation using the total probability rule:\\n P  ( pass test )  = P  ( pass test | survivor  )  P  ( survivor )    \\n \\n \\n \\n \\n+\\u200aP  ( pass test | non-survivor  )  P  ( non-survivor )    \\n \\nWe know that P(survivor) = 1 – P(non-survivor) = 1 – 0.40 = 0.60. Therefore, \\nP(pass test) = 0.55 = 0.85(0.60) + P(pass test | non-survivor)(0.40).\\nThus, P(pass test | non-survivor) = [0.55 – 0.85(0.60)]/0.40 = 0.10.\\n P  ( survivor | pass test  )  =   [ P  ( pass test | survivor  )  \\u200a/\\u200aP  ( pass test )  ]  P  ( survivor )    \\n \\n \\n \\n \\n \\n \\n=   ( 0.85\\u200a/\\u200a0.55 )  0.60 = 0.927273 \\n \\nB. The information that a company passes the test causes you to update your \\nprobability that it is a survivor from 0.60 to approximately 0.927.\\nC. According to Bayes’ formula, P(non-survivor | fail test) = [P(fail \\ntest | non-survivor)/ P(fail test)]P(non-survivor) = [P(fail test | \\nnon-survivor)/0.45]0.40.\\nWe can set up the following equation to obtain P(fail test | non-survivor):\\n \\nP  ( fail test )  = P  ( fail test | non-survivor  )  P  ( non-survivor )  \\n  \\n \\n \\n \\n \\n+\\u200aP  ( fail test | survivor  )  P  ( survivor )   \\n \\n \\n0.45 = P  ( fail test | non-survivor  )  0.40 + 0.15  ( 0.60 )   \\n \\n \\nwhere P(fail test | survivor) = 1 − P(pass test | survivor) = 1 − 0.85 = 0.15. So \\nP(fail test | non-survivor) = [0.45 − 0.15(0.60)]/0.40 = 0.90.\\nUsing this result with the formula above, we find P(non-survivor | fail test) \\n= [0.90/0.45]0.40 = 0.80. Seeing that a company fails the test causes us to \\nupdate the probability that it is a non-survivor from 0.40 to 0.80.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 3 \\nProbability Concepts\\n232\\nD. A company passing the test greatly increases our confidence that it is a \\nsurvivor. A company failing the test doubles the probability that it is a \\nnon-survivor. Therefore, the test appears to be useful.\\n27. B is correct. With Bayes’ formula, the probability of failure given a “good” rating \\nis\\n P(A | B ) =  P(B | A) \\n_ \\nP(B)  P(A)  \\nwhere\\nP(A) = 0.20 = probability of failure\\nP(B) = 0.70 = probability of a “good” rating\\nP(B | A) = 0.50 = probability of a “good” rating given failure\\nWith these estimates, the probability of failure given a “good” rating is\\n P(A | B ) =  P(B | A) \\n_ \\nP(B)  P(A ) =  0.50 \\n_ \\n0.70  × 0.20 = 0.143  \\nIf the analyst uses the bankruptcy prediction model as a guide, the probability of \\nfailure declines from 20% to 14.3%.\\n28. C is correct. With Bayes’ formula, the probability of the CEO being fired given a \\n“good” rating is\\n P(A | B ) =  P(B | A) \\n_ \\nP(B)  P(A)  \\nwhere\\nP(A) = 0.05 = probability of the CEO being fired\\nP(B) = 0.50 = probability of a “good” rating\\nP(B | A) = 0.30 = probability of a “good” rating given that the CEO is fired\\nWith these estimates, the probability of the CEO being fired given a “good” rating \\nis\\n P(A | B ) =  P(B | A) \\n_ \\nP(B)  P(A ) =  0.30 \\n_ \\n0.50  × 0.05 = 0.03  \\nAlthough 5% of all CEOs are fired, the probability of being fired given a “good” \\nperformance rating is 3%.\\n29. C is correct. The combination formula provides the number of ways that r objects \\ncan be chosen from a total of n objects, when the order in which the r objects \\nare listed does not matter. The order of the bonds within the portfolio does not \\nmatter.\\n30. A is correct. The answer is found using the combination formula\\n n  C r =   ( n r  )  =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!r\\u200a!  \\nHere, n = 4 and r = 2, so the answer is 4!/[(4 – 2)!2!] = 24/[(2) × (2)] = 6. This re-\\nsult can be verified by assuming there are four vice presidents, VP1–VP4. The six \\npossible additions to the investment committee are VP1 and VP2, VP1 and VP3, \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n233\\nVP1 and VP4, VP2 and VP3, VP2 and VP4, and VP3 and VP4.\\n31. A is correct. The permutation formula is used to choose r objects from a total of \\nn objects when order matters. Because the portfolio manager is trying to rank \\nthe four funds from most recommended to least recommended, the order of the \\nfunds matters; therefore, the permutation formula is most appropriate.\\n32. A is correct. The number of combinations is the number of ways to pick four \\nmutual funds out of 10 without regard to order, which is\\n n  C r =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!r\\u200a!  \\n 10  C 4 =  \\n10\\u200a! \\n_ \\n  ( 10 − 4 )  \\u200a!4\\u200a!  =  10 × 9 × 8 × 7 \\n_ \\n4 × 3 × 2 × 1  = 210 \\n33. B is correct. The number of permutations is the number of ways to pick five \\nstocks out of 30 in the correct order.\\n n  P r =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!r\\u200a!  \\n 30  P 5 =  \\n30\\u200a! \\n_ \\n  ( 30 − 5 )  \\u200a!  =  30\\u200a! \\n_ \\n25\\u200a!  = 30 × 29 × 28 × 27 × 26 = 17, 100, 720 \\nThe contestant’s chance of winning is one out of 17,100,720.\\n34. A is correct. The number of combinations is the number of ways to pick five \\nstocks out of 30 without regard to order.\\n n  C r =  \\nn\\u200a! \\n_ \\n  ( n − r )  \\u200a!r\\u200a!  \\n 30  C 5 =  \\n30\\u200a! \\n_ \\n  ( 30 − 5 )  \\u200a!5\\u200a!  =  30 × 29 × 28 × 27 × 26 \\n \\n_________________ \\n \\n5 × 4 × 3 × 2 × 1 \\n = 142, 506 \\nThe contestant’s chance of winning is one out of 142,506.\\n35. This contest does not resemble a usual lottery. Each of the 30 stocks does not \\nhave an equal chance of having the highest returns. Furthermore, contestants \\nmay have some favored investments, and the 30 stocks will not be chosen with \\nthe same frequencies. To guard against more than one person selecting the win-\\nners correctly, Sturm may wish to stipulate that if there is more than one winner, \\nthe winners will share the $1 million prize.\\n© CFA Institute. For candidate use only. Not for distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\nCommon Probability Distributions\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska–Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA. David E. Runkle, PhD, CFA, is at Jacobs Levy Equity Management (USA).\\nCFA Institute would like to thank Adam Kobor, PhD, CFA, at New York University \\nInvestment Office (USA), for this major revision of “Common Probability Distributions,” \\nincluding new visuals, graphics, Microsoft Excel functions, code snippets, and related text \\ncontent throughout the reading.\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndefine a probability distribution and compare and contrast discrete \\nand continuous random variables and their probability functions\\ncalculate and interpret probabilities for a random variable given its \\ncumulative distribution function\\ndescribe the properties of a discrete uniform random variable, and \\ncalculate and interpret probabilities given the discrete uniform \\ndistribution function\\ndescribe the properties of the continuous uniform distribution, and \\ncalculate and interpret probabilities given a continuous uniform \\ndistribution\\ndescribe the properties of a Bernoulli random variable and a \\nbinomial random variable, and calculate and interpret probabilities \\ngiven the binomial distribution function\\nexplain the key properties of the normal distribution\\ncontrast a multivariate distribution and a univariate distribution, and \\nexplain the role of correlation in the multivariate normal distribution\\ncalculate the probability that a normally distributed random variable \\nlies inside a given interval\\nexplain how to standardize a random variable\\ncalculate and interpret probabilities using the standard normal \\ndistribution\\ndefine shortfall risk, calculate the safety-first ratio, and identify an \\noptimal portfolio using Roy’s safety-first criterion\\nL E A R N I N G  M O D U L E\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]},\n",
       "  {'title': 'Learning Module 4\\tCommon Probability Distributions',\n",
       "   'page_number': 245,\n",
       "   'content': 'Common Probability Distributions\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska–Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA. David E. Runkle, PhD, CFA, is at Jacobs Levy Equity Management (USA).\\nCFA Institute would like to thank Adam Kobor, PhD, CFA, at New York University \\nInvestment Office (USA), for this major revision of “Common Probability Distributions,” \\nincluding new visuals, graphics, Microsoft Excel functions, code snippets, and related text \\ncontent throughout the reading.\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndefine a probability distribution and compare and contrast discrete \\nand continuous random variables and their probability functions\\ncalculate and interpret probabilities for a random variable given its \\ncumulative distribution function\\ndescribe the properties of a discrete uniform random variable, and \\ncalculate and interpret probabilities given the discrete uniform \\ndistribution function\\ndescribe the properties of the continuous uniform distribution, and \\ncalculate and interpret probabilities given a continuous uniform \\ndistribution\\ndescribe the properties of a Bernoulli random variable and a \\nbinomial random variable, and calculate and interpret probabilities \\ngiven the binomial distribution function\\nexplain the key properties of the normal distribution\\ncontrast a multivariate distribution and a univariate distribution, and \\nexplain the role of correlation in the multivariate normal distribution\\ncalculate the probability that a normally distributed random variable \\nlies inside a given interval\\nexplain how to standardize a random variable\\ncalculate and interpret probabilities using the standard normal \\ndistribution\\ndefine shortfall risk, calculate the safety-first ratio, and identify an \\noptimal portfolio using Roy’s safety-first criterion\\nL E A R N I N G  M O D U L E\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n236\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\nexplain the relationship between normal and lognormal distributions \\nand why the lognormal distribution is used to model asset prices\\ncalculate and interpret a continuously compounded rate of return, \\ngiven a specific holding period return\\ndescribe the properties of the Student’s t-distribution, and calculate \\nand interpret its degrees of freedom\\ndescribe the properties of the chi-square distribution and the \\nF-distribution, and calculate and interpret their degrees of freedom\\ndescribe Monte Carlo simulation\\nDISCRETE RANDOM VARIABLES\\ndefine a probability distribution and compare and contrast discrete \\nand continuous random variables and their probability functions\\ncalculate and interpret probabilities for a random variable given its \\ncumulative distribution function\\nProbabilities play a critical role in investment decisions. Although we cannot predict the \\nfuture, informed investment decisions are based on some kind of probabilistic thinking. \\nAn analyst may put probability estimates behind the success of her high-conviction \\nor low-conviction stock recommendations. Risk managers would typically think in \\nprobabilistic terms: What is the probability of not achieving the target return, or \\nwhat kind of losses are we facing with high likelihood over the relevant time horizon? \\nProbability distributions also underpin validating trade signal–generating models: For \\nexample, does earnings revision play a significant role in forecasting stock returns?\\nIn nearly all investment decisions, we work with random variables. The return on \\na stock and its earnings per share are familiar examples of random variables. To make \\nprobability statements about a random variable, we need to understand its probability \\ndistribution. A probability distribution specifies the probabilities associated with \\nthe possible outcomes of a random variable.\\nIn this reading, we present important facts about seven probability distributions \\nand their investment uses. These seven distributions—the uniform, binomial, nor-\\nmal, lognormal, Student’s t-, chi-square, and F-distributions—are used extensively \\nin investment analysis. Normal and binomial distributions are used in such basic \\nvaluation models as the Black–Scholes–Merton option pricing model, the binomial \\noption pricing model, and the capital asset pricing model. Student’s t-, chi-square, \\nand F-distributions are applied in validating statistical significance and in hypothesis \\ntesting. With the working knowledge of probability distributions provided in this \\nreading, you will be better prepared to study and use other quantitative methods, such \\nas regression analysis, time-series analysis, and hypothesis testing. After discussing \\nprobability distributions, we end with an introduction to Monte Carlo simulation, a \\ncomputer-based tool for obtaining information on complex investment problems.\\n1\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "   'children': [{'title': 'Discrete Random Variables',\n",
       "     'page_number': 246,\n",
       "     'content': '',\n",
       "     'children': [{'title': 'Discrete Random Variables',\n",
       "       'page_number': 247,\n",
       "       'content': 'Discrete Random Variables\\n237\\nWe start by defining basic concepts and terms, then illustrate the operation of \\nthese concepts through the simplest distribution, the uniform distribution, and then \\naddress probability distributions that have more applications in investment work but \\nalso greater complexity.\\nDiscrete Random Variables\\nA random variable is a quantity whose future outcomes are uncertain. The two basic \\ntypes of random variables are discrete random variables and continuous random vari-\\nables. A discrete random variable can take on at most a countable (possibly infinite) \\nnumber of possible values. For example, a discrete random variable X can take on a \\nlimited number of outcomes x1, x2, . . ., xn (n possible outcomes), or a discrete random \\nvariable Y can take on an unlimited number of outcomes y1, y2, . . . (without end). \\nThe number of “yes” votes at a corporate board meeting, for example, is a discrete \\nvariable that is countable and finite (from 0 to the voting number of board members). \\nThe number of trades at a stock exchange is also countable but is infinite, since there \\nis no limit to the number of trades by the market participants. Note that X refers to \\nthe random variable, and x refers to an outcome of X. We subscript outcomes, as in x1 \\nand x2, when we need to distinguish among different outcomes in a list of outcomes \\nof a random variable. Since we can count all the possible outcomes of X and Y (even \\nif we go on forever in the case of Y), both X and Y satisfy the definition of a discrete \\nrandom variable.\\nIn contrast, we cannot count the outcomes of a continuous random variable. \\nWe cannot describe the possible outcomes of a continuous random variable Z with a \\nlist z1, z2, . . ., because the outcome (z1 + z2)/2, not in the list, would always be pos-\\nsible. The volume of water in a glass is an example of a continuous random variable \\nsince we cannot “count” water on a discrete scale but can only measure its volume. In \\nfinance, unless a variable exhibits truly discrete behavior—for example, a positive or \\nnegative earnings surprise or the number of central bank board members voting for \\na rate hike—it is practical to work with a continuous distribution in many cases. The \\nrate of return on an investment is an example of such a continuous random variable.\\nIn working with a random variable, we need to understand its possible outcomes. \\nFor example, a majority of the stocks traded on the New Zealand Stock Exchange are \\nquoted in increments of NZ$0.01. Quoted stock price is thus a discrete random variable \\nwith possible values NZ$0, NZ$0.01, NZ$0.02, . . ., but we can also model stock price \\nas a continuous random variable (as a lognormal random variable, to look ahead). In \\nmany applications, we have a choice between using a discrete or a continuous distri-\\nbution. We are usually guided by which distribution is most efficient for the task we \\nface. This opportunity for choice is not surprising, because many discrete distributions \\ncan be approximated with a continuous distribution, and vice versa. In most practical \\ncases, a probability distribution is only a mathematical idealization, or approximate \\nmodel, of the relative frequencies of a random variable’s possible outcomes.\\nEvery random variable is associated with a probability distribution that describes \\nthe variable completely. We can view a probability distribution in two ways. The basic \\nview is the probability function, which specifies the probability that the random \\nvariable takes on a specific value: P(X = x) is the probability that a random variable X \\ntakes on the value x. For a discrete random variable, the shorthand notation for the \\nprobability function (sometimes referred to as the “probability mass function”) is p(x) \\n= P(X = x). For continuous random variables, the probability function is denoted f(x) \\nand called the probability density function (pdf), or just the density.\\nA probability function has two key properties (which we state, without loss of \\ngenerality, using the notation for a discrete random variable):\\n■ \\n0 ≤ p(x) ≤ 1, because probability is a number between 0 and 1.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n238\\n■ \\nThe sum of the probabilities p(x) over all values of X equals 1. If we add up \\nthe probabilities of all the distinct possible outcomes of a random variable, \\nthat sum must equal 1.\\nWe are often interested in finding the probability of a range of outcomes rather than \\na specific outcome. In these cases, we take the second view of a probability distribution, \\nthe cumulative distribution function (cdf). The cumulative distribution function, \\nor distribution function for short, gives the probability that a random variable X is \\nless than or equal to a particular value x, P(X ≤ x). For both discrete and continuous \\nrandom variables, the shorthand notation is F(x) = P(X ≤ x). How does the cumulative \\ndistribution function relate to the probability function? The word “cumulative” tells \\nthe story. To find F(x), we sum up, or accumulate, values of the probability function \\nfor all outcomes less than or equal to x. The function of the cdf is parallel to that of \\ncumulative relative frequency.\\nWe illustrate the concepts of probability density functions and cumulative dis-\\ntribution functions with an empirical example using daily returns (i.e., percentage \\nchanges) of the fictitious Euro-Asia-Africa (EAA) Equity Index. This dataset spans five \\nyears and consists of 1,258 observations, with a minimum value of −4.1%, a maximum \\nvalue of 5.0%, a range of 9.1%, and a mean daily return of 0.04%.\\nExhibit 1 depicts the histograms, representing pdfs, and empirical cdfs (i.e., accu-\\nmulated values of the bars in the histograms) based on daily returns of the EAA Equity \\nIndex. Panels A and B represent the same dataset; the only difference is the histogram \\nbins used in Panel A are wider than those used in Panel B, so naturally Panel B has \\nmore bins. Note that in Panel A, we divided the range of observed daily returns (−5% \\nto 5%) into 10 bins, so we chose the bin width to be 1.0%. In Panel B, we wanted a \\nmore granular histogram with a narrower range, so we divided the range into 20 bins, \\nresulting in a bin width of 0.5%. Panel A gives a sense of the observed range of daily \\nindex returns, whereas Panel B is much more granular, so it more closely resembles \\ncontinuous pdf and cdf graphs.\\n© CFA Institute. For candidate use only. Not for distribution.\\nDiscrete Random Variables\\n239\\nExhibit 1: PDFs and CDFs of Daily Returns for EAA Equity Index\\nHistogram Frequency (%)\\nA. Wide Bin Widths\\n50\\n50\\n45\\n45\\n40\\n40\\n40\\n40\\n35\\n35\\n30\\n30\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n0\\nCumulative Distribution (%)\\n100\\n100\\n90\\n90\\n80\\n80\\n70\\n70\\n60\\n60\\n50\\n50\\n40\\n40\\n30\\n30\\n20\\n20\\n10\\n10\\n0\\n–2 to –1\\n–2 to –1\\n–5 to –4\\n–5 to –4\\n2 to 3\\n2 to 3\\n1 to 2\\n1 to 2\\n0 to 1\\n0 to 1\\n–1 to –0\\n–1 to –0\\n–3 to –2\\n–3 to –2\\n–4 to –3\\n–4 to –3\\n3 to 4\\n3 to 4\\n4 to 5\\n4 to 5\\nBin (%)\\nCDF\\nHistogram\\nHistogram Frequency (%)\\nB. Narrow Bin Widths\\n35\\n35\\n30\\n30\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\nCumulative Distribution (%)\\n100\\n100\\n90\\n90\\n80\\n80\\n70\\n70\\n60\\n60\\n50\\n50\\n40\\n40\\n30\\n30\\n20\\n20\\n10\\n10\\n0\\nBin (%)\\n–3.5 to –3.0\\n–3.5 to –3.0\\n–5.0 to –4.5\\n–5.0 to –4.5\\n–1.5 to –1.0\\n–1.5 to –1.0\\n–2.0 to –1.5\\n–2.0 to –1.5\\n–2.5 to –2.0\\n–2.5 to –2.0\\n–3.0 to –2.5\\n–3.0 to –2.5\\n–4.0 to –3.5\\n–4.0 to –3.5\\n–4.5 to –4.0\\n–4.5 to –4.0\\n–1.0 to –0.5\\n–1.0 to –0.5\\n2.0 to 2.5\\n2.0 to 2.5\\n0.5 to 1.0\\n0.5 to 1.0\\n0 to 0.5\\n0 to 0.5\\n–0.5 to –0\\n–0.5 to –0\\n4.0 to 4.5\\n4.0 to 4.5\\n3.5 to 4.0\\n3.5 to 4.0\\n3.0 to 3.5\\n3.0 to 3.5\\n2.5 to 3.0\\n2.5 to 3.0\\n1.5 to 2.0\\n1.5 to 2.0\\n1.0 to 1.5\\n1.0 to 1.5\\n4.5 to 5.0\\n4.5 to 5.0\\nEXAMPLE 1\\nUsing PDFs and CDFs of Discrete and Continuous Random \\nVariables to Calculate Probabilities\\nDiscrete Random Variables: Rolling a Die\\nThe example of rolling a six-sided die is an easy and intuitive way to illustrate \\na discrete random variable’s pdf and cdf.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n240\\n1. What is the probability that you would roll the number 3?\\nSolution to 1:\\nAssuming the die is fair, rolling any number from 1 to 6 has a probability of \\n1/6 each, so the chance of rolling the number 3 would also equal 1/6. This \\noutcome represents the pdf of this game; the pdf at number 3 takes a value \\nof 1/6. In fact, it takes a value of 1/6 at every number from 1 to 6.\\n2. What is the probability that you would roll a number less than or equal to 3?\\nSolution to 2:\\nAnswering this question involves the cdf of the die game. Three possible \\nevents would satisfy our criterion—namely, rolling 1, 2, or 3. The probability \\nof rolling any of these numbers would be 1/6, so by accumulating them from \\n1 to 3, we get a probability of 3/6, or ½. The cdf of rolling a die takes a value \\nof 1/6 at number 1, 3/6 (or 50%) at number 3, and 6/6 (or 100%) at number \\n6.\\nContinuous Random Variables: EAA Index Return\\n3. We use the EAA Index to illustrate the pdf and cdf for a continuously \\ndistributed random variable. Since daily returns can take on any numbers \\nwithin a reasonable range rather than having discrete outcomes, we repre-\\nsent the pdf in the context of bins (as shown in the following table).\\n \\nBin\\nPDF\\nBin\\nPDF\\n−5% to −4%\\n0.1%\\n0% to 1%\\n44.1%\\n−4% to −3%\\n0.6%\\n1% to 2%\\n8.8%\\n−3% to −2%\\n1.8%\\n2% to 3%\\n1.0%\\n−2% to −1%\\n6.1%\\n3% to 4%\\n0.1%\\n−1% to 0%\\n37.4%\\n4% to 5%\\n0.1%\\n \\nIn our sample, we did not find any daily returns below −5%, and we found \\nonly 0.1% of the total observations between −5% and −4%. In the next bin, \\n−4% to −3%, we found 0.6% of the total observations, and so on.\\nIf this empirical pdf is a guide for the future, what is the probability that we \\nwill see a daily return less than −2%?\\nSolution to 3:\\nWe must calculate the cdf up to −2%. The answer is the sum of the pdfs of \\nthe first three bins (see the shaded rectangle in the table provided); 0.1% + \\n0.6% + 1.8% = 2.5%. So, the probability that we will see a daily return less \\nthan −2% is 2.5%.\\nNext, we illustrate these concepts with examples and show how we use discrete \\nand continuous distributions. We start with the simplest distribution, the discrete \\nuniform distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Discrete and Continuous Uniform Distribution',\n",
       "     'page_number': 251,\n",
       "     'content': 'Discrete and ',\n",
       "     'children': [{'title': 'Continuous Uniform Distribution',\n",
       "       'page_number': 253,\n",
       "       'content': 'Continuous Uniform Distribution\\n243\\np(x) equals 0 for numbers x that are not in that column, such as −14 or 12.215.] The \\nfirst property is satisfied. The second property is that the probabilities sum to 1. The \\nentries in the second column of Panel A do sum to 1.\\nThe cdf has two other characteristic properties:\\n■ \\nThe cdf lies between 0 and 1 for any x: 0 ≤ F(x) ≤ 1.\\n■ \\nAs x increases, the cdf either increases or remains constant.\\nCheck these statements by looking at the third column in the table in Panel A and \\nat the graph in Panel B.\\nWe now have some experience working with probability functions and cdfs for dis-\\ncrete random variables. Later, we will discuss Monte Carlo simulation, a methodology \\ndriven by random numbers. As we will see, the uniform distribution has an important \\ntechnical use: It is the basis for generating random numbers, which, in turn, produce \\nrandom observations for all other probability distributions.\\nContinuous Uniform Distribution\\nThe continuous uniform distribution is the simplest continuous probability distri-\\nbution. The uniform distribution has two main uses. As the basis of techniques for \\ngenerating random numbers, the uniform distribution plays a role in Monte Carlo \\nsimulation. As the probability distribution that describes equally likely outcomes, the \\nuniform distribution is an appropriate probability model to represent a particular kind \\nof uncertainty in beliefs in which all outcomes appear equally likely.\\nThe pdf for a uniform random variable is\\n f  ( x )  =   {  1 \\n_ \\nb − a    \\n0\\n \\n  for a ≤ x ≤ b  \\n \\notherwise \\n  \\nFor example, with a = 0 and b = 8, f(x) = 1/8, or 0.125. We graph this density in \\nExhibit 3.\\nExhibit 3: Probability Density Function for a Continuous Uniform \\nDistribution\\nf(x)\\nF(3) = P(X ≤ 3)\\n0.14\\n0.14\\n0.12\\n0.12\\n0.10\\n0.10\\n0.08\\n0.08\\n0.06\\n0.06\\n0.04\\n0.04\\n0.02\\n0.02\\n0\\n0\\n99\\n3\\n1\\n2\\n4\\n5\\n8\\n7\\n6\\nx\\nThe graph of the density function plots as a horizontal line with a value of 0.125.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n244\\nWhat is the probability that a uniform random variable with limits a = 0 and b = 8 \\nis less than or equal to 3, or F(3) = P(X ≤ 3)? When we were working with the discrete \\nuniform random variable with possible outcomes 1, 2, . . ., 8, we summed individual \\nprobabilities: p(1) + p(2) + p(3) = 0.375. In contrast, the probability that a continuous \\nuniform random variable or any continuous random variable assumes any given fixed \\nvalue is 0. To illustrate this point, consider the narrow interval 2.510–2.511. Because \\nthat interval holds an infinity of possible values, the sum of the probabilities of values \\nin that interval alone would be infinite if each individual value in it had a positive \\nprobability. To find the probability F(3), we find the area under the curve graphing \\nthe pdf, between 0 and 3 on the x-axis (shaded area in Exhibit 3). In calculus, this \\noperation is called integrating the probability function f(x) from 0 to 3. This area under \\nthe curve is a rectangle with base 3 − 0 = 3 and height 1/8. The area of this rectangle \\nequals base times height: 3(1/8) = 3/8, or 0.375. So F(3) = 3/8, or 0.375.\\nThe interval from 0 to 3 is three-eighths of the total length between the limits of \\n0 and 8, and F(3) is three-eighths of the total probability of 1. The middle line of the \\nexpression for the cdf captures this relationship:\\n F(x ) =   {\\n \\n 0 for x < a  \\nx − a \\n_   \\n b − a  \\n1 for x < b \\n   for a ≤ x ≤ b. \\nFor our problem, F(x) = 0 for x ≤ 0, F(x) = x/8 for 0 < x < 8, and F(x) = 1 for x ≥ 8. \\nExhibit 4 shows a graph of this cdf.\\nExhibit 4: Continuous Uniform Cumulative Distribution\\nCDF\\n1.0\\n1.0\\n0.8\\n0.8\\n0.6\\n0.6\\n0.4\\n0.4\\n0.2\\n0.2\\n0\\n00\\n99\\n3\\n1\\n2\\n4\\n5\\n8\\n7\\n6\\nx\\nThe mathematical operation that corresponds to finding the area under the curve of \\na pdf f(x) from a to b is the definite integral of f(x) from a to b:\\n P  ( a ≤ X ≤ b )  =  ∫ a b f  ( x )  dx , \\n(1)\\nwhere ∫ dx is the symbol for summing ∫ over small changes dx and the limits of integra-\\ntion (a and b) can be any real numbers or −∞ and +∞. All probabilities of continuous \\nrandom variables can be computed using Equation 1. For the uniform distribution \\nexample considered previously, F(7) is Equation 1 with lower limit a = 0 and upper \\nlimit b = 7. The integral corresponding to the cdf of a uniform distribution reduces to \\nthe three-line expression given previously. To evaluate Equation 1 for nearly all other \\n© CFA Institute. For candidate use only. Not for distribution.\\nDiscrete and Continuous Uniform Distribution\\n245\\ncontinuous distributions, including the normal and lognormal, we rely on spreadsheet \\nfunctions, computer programs, or tables of values to calculate probabilities. Those \\ntools use various numerical methods to evaluate the integral in Equation 1.\\nRecall that the probability of a continuous random variable equaling any fixed \\npoint is 0. This fact has an important consequence for working with the cumulative \\ndistribution function of a continuous random variable: For any continuous random \\nvariable X, P(a ≤ X ≤ b) = P(a < X ≤ b) = P(a ≤ X < b) = P(a < X < b), because the \\nprobabilities at the endpoints a and b are 0. For discrete random variables, these \\nrelations of equality are not true, because for them probability accumulates at points.\\nEXAMPLE 2\\nProbability That a Lending Facility Covenant Is Breached\\nYou are evaluating the bonds of a below-investment-grade borrower at a low \\npoint in its business cycle. You have many factors to consider, including the \\nterms of the company’s bank lending facilities. The contract creating a bank \\nlending facility such as an unsecured line of credit typically has clauses known \\nas covenants. These covenants place restrictions on what the borrower can do. \\nThe company will be in breach of a covenant in the lending facility if the interest \\ncoverage ratio, EBITDA/interest, calculated on EBITDA over the four trailing \\nquarters, falls below 2.0. EBITDA is earnings before interest, taxes, depreciation, \\nand amortization. Compliance with the covenants will be checked at the end of \\nthe current quarter. If the covenant is breached, the bank can demand immediate \\nrepayment of all borrowings on the facility. That action would probably trigger a \\nliquidity crisis for the company. With a high degree of confidence, you forecast \\ninterest charges of $25 million. Your estimate of EBITDA runs from $40 million \\non the low end to $60 million on the high end.\\nAddress two questions (treating projected interest charges as a constant):\\n1. If the outcomes for EBITDA are equally likely, what is the probability that \\nEBITDA/interest will fall below 2.0, breaching the covenant?\\nSolution to 1:\\nEBITDA/interest is a continuous uniform random variable because all \\noutcomes are equally likely. The ratio can take on values between 1.6 = ($40 \\nmillion)/($25 million) on the low end and 2.4 = ($60 million/$25 million) \\non the high end. The range of possible values is 2.4 − 1.6 = 0.8. The fraction \\nof possible values falling below 2.0, the level that triggers default, is the \\ndistance between 2.0 and 1.6, or 0.40; the value 0.40 is one-half the total \\nlength of 0.8, or 0.4/0.8 = 0.50. So, the probability that the covenant will be \\nbreached is 50%.\\n2. Estimate the mean and standard deviation of EBITDA/interest. For a contin-\\nuous uniform random variable, the mean is given by μ = (a + b)/2 and the \\nvariance is given by σ2 = (b − a)2/12.\\nSolution to 2:\\nIn Solution 1, we found that the lower limit of EBITDA/interest is 1.6. This \\nlower limit is a. We found that the upper limit is 2.4. This upper limit is b. \\nUsing the formula given previously,\\n μ = (a + b)/2 = (1.6 + 2.4)/2 = 2.0.\\nThe variance of the interest coverage ratio is\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n246\\n σ2 = (b − a)2/12 = (2.4 − 1.6)2/12 = 0.053333.\\nThe standard deviation is the positive square root of the variance, 0.230940 \\n= (0.053333)1/2. However, the standard deviation is not particularly useful \\nas a risk measure for a uniform distribution. The probability that lies within \\nvarious standard deviation bands around the mean is sensitive to different \\nspecifications of the upper and lower. Here, a one standard deviation inter-\\nval around the mean of 2.0 runs from 1.769 to 2.231 and captures 0.462/0.80 \\n= 0.5775, or 57.8%, of the probability. A two standard deviation interval runs \\nfrom 1.538 to 2.462, which extends past both the lower and upper limits of \\nthe random variable.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Binomial Distribution',\n",
       "     'page_number': 256,\n",
       "     'content': 'BINOMIAL DISTRIBUTION\\ndescribe the properties of a Bernoulli random variable and a \\nbinomial random variable, and calculate and interpret probabilities \\ngiven the binomial distribution function\\nIn many investment contexts, we view a result as either a success or a failure or as \\nbinary (twofold) in some other way. When we make probability statements about a \\nrecord of successes and failures or about anything with binary outcomes, we often \\nuse the binomial distribution. What is a good model for how a stock price moves over \\ntime? Different models are appropriate for different uses. Cox, Ross, and Rubinstein \\n(1979) developed an option pricing model based on binary moves—price up or price \\ndown—for the asset underlying the option. Their binomial option pricing model was \\nthe first of a class of related option pricing models that have played an important role \\nin the development of the derivatives industry. That fact alone would be sufficient \\nreason for studying the binomial distribution, but the binomial distribution has uses \\nin decision making as well.\\nThe building block of the binomial distribution is the Bernoulli random variable, \\nnamed after the Swiss probabilist Jakob Bernoulli (1654–1704). Suppose we have a \\ntrial (an event that may repeat) that produces one of two outcomes. Such a trial is a \\nBernoulli trial. If we let Y equal 1 when the outcome is success and Y equal 0 when the \\noutcome is failure, then the probability function of the Bernoulli random variable Y is\\n p(1) = P(Y = 1) = p\\nand\\n p(0) = P(Y = 0) = 1 − p,\\nwhere p is the probability that the trial is a success.\\nIn n Bernoulli trials, we can have 0 to n successes. If the outcome of an individual \\ntrial is random, the total number of successes in n trials is also random. A binomial \\nrandom variableX is defined as the number of successes in n Bernoulli trials. A bino-\\nmial random variable is the sum of Bernoulli random variables Yi, where i = 1, 2, . . ., n:\\n X = Y1 + Y2 + . . . + Yn,\\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nBinomial Distribution\\n247\\nwhere Yi is the outcome on the ith trial (1 if a success, 0 if a failure). We know \\nthat a Bernoulli random variable is defined by the parameter p. The number of trials, \\nn, is the second parameter of a binomial random variable. The binomial distribution \\nmakes these assumptions:\\n■ \\nThe probability, p, of success is constant for all trials.\\n■ \\nThe trials are independent.\\nThe second assumption has great simplifying force. If individual trials were cor-\\nrelated, calculating the probability of a given number of successes in n trials would \\nbe much more complicated.\\nUnder these two assumptions, a binomial random variable is completely described \\nby two parameters, n and p. We write\\n X ~ B(n, p),\\nwhich we read as “X has a binomial distribution with parameters n and p.” You \\ncan see that a Bernoulli random variable is a binomial random variable with n = 1: \\nY ~ B(1, p).\\nNow we can find the general expression for the probability that a binomial random \\nvariable shows x successes in n trials (also known as the probability mass function). \\nWe can think in terms of a model of stock price dynamics that can be generalized to \\nallow any possible stock price movements if the periods are made extremely small. \\nEach period is a Bernoulli trial: With probability p, the stock price moves up; with \\nprobability 1 − p, the price moves down. A success is an up move, and x is the number \\nof up moves or successes in n periods (trials). With each period’s moves independent \\nand p constant, the number of up moves in n periods is a binomial random variable. \\nWe now develop an expression for P(X = x), the probability function for a binomial \\nrandom variable.\\nAny sequence of n periods that shows exactly x up moves must show n − x down \\nmoves. We have many different ways to order the up moves and down moves to get \\na total of x up moves, but given independent trials, any sequence with x up moves \\nmust occur with probability px(1 − p)n−x. Now we need to multiply this probability by \\nthe number of different ways we can get a sequence with x up moves. Using a basic \\nresult in counting, there are\\n \\nn\\u200a! \\n_ \\n  ( n − x )  \\u200a!x\\u200a!  \\ndifferent sequences in n trials that result in x up moves (or successes) and n − x \\ndown moves (or failures). Recall that for positive integers n, n factorial (n!) is defined \\nas n(n − 1)(n − 2) . . . 1 (and 0! = 1 by convention). For example, 5! = (5)(4)(3)(2)(1) = \\n120. The combination formula n!/[(n − x)!x!] is denoted by\\n   ( n x  )   \\n(read “n combination x” or “n choose x”). For example, over three periods, exactly \\nthree different sequences have two up moves: uud, udu, and duu. We confirm this by\\n  ( 3 2 )  =  \\n3\\u200a! \\n_ \\n  ( 3 − 2 )  \\u200a!2\\u200a!  =    ( 3 )   ( 2 )   ( 1 )   \\n_ \\n  ( 1 )   ( 2 )   ( 1 )    = 3. \\nIf, hypothetically, each sequence with two up moves had a probability of 0.15, \\nthen the total probability of two up moves in three periods would be 3 × 0.15 = 0.45. \\nThis example should persuade you that for X distributed B(n, p), the probability of x \\nsuccesses in n trials is given by\\n p  ( x )  = P  ( X = x )  =   ( n x  )  p x  ( 1 − p ) \\nn−x =  \\nn\\u200a! \\n_ \\n  ( n − x )  \\u200a!x\\u200a!   p x  ( 1 − p ) \\nn−x . \\n(2)\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n248\\nSome distributions are always symmetric, such as the normal, and others are always \\nasymmetric or skewed, such as the lognormal. The binomial distribution is symmet-\\nric when the probability of success on a trial is 0.50, but it is asymmetric or skewed \\notherwise.\\nWe illustrate Equation 2 (the probability function) and the cdf through the sym-\\nmetrical case by modeling the behavior of stock price movements on four consecutive \\ntrading days in a binomial tree framework. Each day is an independent trial. The stock \\nmoves up with constant probability p (the up transition probability); if it moves up, \\nu is 1 plus the rate of return for an up move. The stock moves down with constant \\nprobability 1 − p (the down transition probability); if it moves down, d is 1 plus the \\nrate of return for a down move. The binomial tree is shown in Exhibit 5, where we \\nnow associate each of the n = 4 stock price moves with time indexed by t; the shape \\nof the graph suggests why it is a called a binomial tree. Each boxed value from which \\nsuccessive moves or outcomes branch out in the tree is called a node. The initial \\nnode, at t = 0, shows the beginning stock price, S. Each subsequent node represents \\na potential value for the stock price at the specified future time.\\nExhibit 5: A Binomial Model of Stock Price Movement\\nt = 0\\nt = 1\\nt = 2\\nt = 3\\nt = 4\\nPossible Paths\\nPossible Paths\\nuuuuS\\nuuuu: 1\\nuuuS\\nuuS\\nuuudS\\nuuud; uudu; uduu; duuu: 4\\nuS\\nuudS\\nS\\nudS\\nuuddS\\nuudd; uddu; udud; dudu; duud; dduu: 6 \\ndS\\ndduS\\nddS\\nddduS\\ndddu; ddud; dudd; uddd: 4\\ndddS\\nddddS\\ndddd: 1\\nWe see from the tree that the stock price at t = 4 has five possible values: uuuuS, uuudS, \\nuuddS, ddduS, and ddddS. The probability that the stock price equals any one of these \\nfive values is given by the binomial distribution. For example, four sequences of moves \\nresult in a final stock price of uuudS: These are uuud, uudu, uduu, and duuu. These \\nsequences have three up moves out of four moves in total; the combination formula \\nconfirms that the number of ways to get three up moves (successes) in four periods \\n(trials) is 4!/(4 − 3)!3! = 4. Next, note that each of these sequences—uuud, uudu, \\nuduu, and duuu—has probability p3(1 − p)1, which equals 0.0625 (= 0.503 × 0.501). \\nSo, P(S4 = uuudS) = 4[p3(1 − p)], or 0.25, where S4 indicates the stock’s price after four \\nmoves. This is shown numerically in Panel A of Exhibit 6, in the line indicating three \\nup moves in x, as well as graphically in Panel B, as the height of the bar above x = 3. \\nNote that in Exhibit 6, columns 5 and 6 in Panel A show the pdf and cdf, respectively, \\nfor this binomial distribution, and in Panel B, the pdf and cdf are represented by the \\nbars and the line graph, respectively.\\n© CFA Institute. For candidate use only. Not for distribution.\\nBinomial Distribution\\n249\\nExhibit 6: PDF and CDF of Binomial Probabilities for Stock Price Movements \\nA. Binomial Probabilities, n = 4 and p = 0.50\\nCol. 1 \\nNumber of Up \\nMoves, \\nx\\nCol. 2 \\nImplied Number \\nof Down Moves, \\nn − x\\nCol. 3A \\nNumber of Possible \\nWays to Reach x Up \\nMoves\\nCol. 4B \\nProbability for \\nEach Way, \\np(x)\\nCol. 5C \\nProbability for \\nx p(x)\\nCol. 6 \\nF(x) = P (X ≤ x)\\n0\\n4\\n1\\n0.0625\\n0.0625\\n0.0625\\n1\\n3\\n4\\n0.0625\\n0.2500\\n0.3125\\n2\\n2\\n6\\n0.0625\\n0.3750\\n0.6875\\n3\\n1\\n4\\n0.0625\\n0.2500\\n0.9375\\n4\\n0\\n1\\n0.0625\\n0.0625\\n1.0000\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n1.0000\\n\\xa0\\nA: Column 3 = n! / [(n − x)! x!]\\nB: Column 4 = px(1 − p)n−x\\nC: Column 5 = Column 3 × Column 4\\nBinomial PDF (%)\\n40\\n40\\n35\\n35\\n30\\n30\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\nBinomial CDF (%)\\n100\\n100\\n90\\n90\\n80\\n80\\n70\\n70\\n60\\n60\\n50\\n50\\n40\\n40\\n30\\n30\\n20\\n20\\n10\\n10\\n0\\n1\\n0\\n4\\n3\\n2\\nX\\nCDF\\nPDF\\nB. Graphs of Binomial PDF and CDF\\nTo be clear, the binomial random variable in this application is the number of up \\nmoves. Final stock price distribution is a function of the initial stock price, the num-\\nber of up moves, and the size of the up moves and down moves. We cannot say that \\nstock price itself is a binomial random variable; rather, it is a function of a binomial \\nrandom variable, as well as of u and d, and initial price, S. This richness is actually one \\nkey to why this way of modeling stock price is useful: It allows us to choose values of \\nthese parameters to approximate various distributions for stock price (using a large \\nnumber of time periods). One distribution that can be approximated is the lognormal, \\nan important continuous distribution model for stock price that we will discuss later. \\nThe flexibility extends further. In the binomial tree shown in Exhibit 5, the transition \\nprobabilities are the same at each node: p for an up move and 1 − p for a down move. \\nThat standard formula describes a process in which stock return volatility is constant \\nover time. Derivatives experts, however, sometimes model changing volatility over \\ntime using a binomial tree in which the probabilities for up and down moves differ \\nat different nodes.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n250\\nEXAMPLE 3\\nA Trading Desk Evaluates Block Brokers\\nBlocks are orders to sell or buy that are too large for the liquidity ordinarily \\navailable in dealer networks or stock exchanges. Your firm has known interests \\nin certain kinds of stock. Block brokers call your trading desk when they want \\nto sell blocks of stocks that they think your firm may be interested in buying. \\nYou know that these transactions have definite risks. For example, if the broker’s \\nclient (the seller) has unfavorable information on the stock or if the total amount \\nhe or she is selling through all channels is not truthfully communicated, you may \\nsee an immediate loss on the trade. Your firm regularly audits the performance \\nof block brokers by calculating the post-trade, market-risk-adjusted returns on \\nstocks purchased from block brokers. On that basis, you classify each trade as \\nunprofitable or profitable. You have summarized the performance of the brokers \\nin a spreadsheet, excerpted in the following table for November of last year. The \\nbroker names are coded BB001 and BB002.\\n \\nBlock Trading Gains and Losses\\n \\n \\n\\xa0\\nProfitable Trades\\nLosing Trades\\nBB001\\n3\\n9\\nBB002\\n5\\n3\\n \\nYou now want to evaluate the performance of the block brokers, and you begin \\nwith two questions:\\n1. If you are paying a fair price on average in your trades with a broker, what \\nshould be the probability of a profitable trade?\\nSolution to 1:\\nIf the price you trade at is fair, then 50% of the trades you do with a broker \\nshould be profitable.\\n2. Did each broker meet or miss that expectation on probability?\\nSolution to 2:\\nYour firm has logged 3 + 9 = 12 trades with block broker BB001. Since 3 of \\nthe 12 trades were profitable, the portion of profitable trades was 3/12, or \\n25%. With broker BB002, the portion of profitable trades was 5/8, or 62.5%. \\nThe rate of profitable trades with broker BB001 of 25% clearly missed your \\nperformance expectation of 50%. Broker BB002, at 62.5% profitable trades, \\nexceeded your expectation.\\n3. You also realize that the brokers’ performance has to be evaluated in light \\nof the sample sizes, and for that you need to use the binomial probability \\nfunction (Equation 2).\\nUnder the assumption that the prices of trades were fair,\\nA. calculate the probability of three or fewer profitable trades with broker \\nBB001.\\n© CFA Institute. For candidate use only. Not for distribution.\\nBinomial Distribution\\n251\\nB. calculate the probability of five or more profitable trades with broker \\nBB002.\\nSolution to 3:\\nA. For broker BB001, the number of trades (the trials) was n = 12, and 3 \\nwere profitable. You are asked to calculate the probability of three or \\nfewer profitable trades, F(3) = p(3) + p(2) + p(1) + p(0).\\nSuppose the underlying probability of a profitable trade with BB001 is \\np = 0.50. With n = 12 and p = 0.50, according to Equation 2 the proba-\\nbility of three profitable trades is\\n \\np  ( 3 )  =   ( n x  )  p x  ( 1 − p ) \\nn−x =   ( 3 \\n12 )   ( 0.50 3 )   ( 0.50 9 )  \\n  \\n \\n \\n \\n \\n=  \\n12\\u200a! \\n_ \\n  ( 12 − 3 )  \\u200a!3\\u200a!   0.50 12 = 220  ( 0.000244 )  = 0.053711.\\n  \\nThe probability of exactly 3 profitable trades out of 12 is 5.4% if broker \\nBB001 were giving you fair prices. Now you need to calculate the other \\nprobabilities:\\n p(2) = [12!/(12 − 2)!2!](0.502)(0.5010) = 66(0.000244) = 0.016113.\\n p(1) = [12!/(12 − 1)!1!](0.501)(0.5011) = 12(0.000244) = 0.00293.\\n p(0) = [12!/(12 − 0)!0!](0.500)(0.5012) = 1(0.000244) = 0.000244.\\nAdding all the probabilities, F(3) = 0.053711 + 0.016113 + 0.00293 + \\n0.000244 = 0.072998, or 7.3%. The probability of making 3 or fewer \\nprofitable trades out of 12 would be 7.3% if your trading desk were \\ngetting fair prices from broker BB001.\\nB. For broker BB002, you are assessing the probability that the underlying \\nprobability of a profitable trade with this broker was 50%, despite the \\ngood results. The question was framed as the probability of making \\nfive or more profitable trades if the underlying probability is 50%: 1 − \\nF(4) = p(5) + p(6) + p(7) + p(8). You could calculate F(4) and subtract \\nit from 1, but you can also calculate p(5) + p(6) + p(7) + p(8) directly.\\nYou begin by calculating the probability that exactly five out of eight \\ntrades would be profitable if BB002 were giving you fair prices:\\n p  ( 5 )  =   ( 8 5 )   ( 0.50 5 )   ( 0.50 3 )    \\n \\n \\n= 56  ( 0.003906 )  = 0.21875.\\n  \\nThe probability is about 21.9%. The other probabilities are as follows:\\n p(6) = 28(0.003906) = 0.109375.\\n p(7) = 8(0.003906) = 0.03125.\\n p(8) = 1(0.003906) = 0.003906.\\nSo, p(5) + p(6) + p(7) + p(8) = 0.21875 + 0.109375 + 0.03125 + \\n0.003906 = 0.363281, or 36.3%. A 36.3% probability is substantial; the \\nunderlying probability of executing a fair trade with BB002 might \\nwell have been 0.50 despite your success with BB002 in November of \\nlast year. If one of the trades with BB002 had been reclassified from \\nprofitable to unprofitable, exactly half the trades would have been \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n252\\nprofitable. In summary, your trading desk is getting at least fair prices \\nfrom BB002; you will probably want to accumulate additional evidence \\nbefore concluding that you are trading at better-than-fair prices.\\nThe magnitude of the profits and losses in these trades is another \\nimportant consideration. If all profitable trades had small profits but \\nall unprofitable trades had large losses, for example, you might lose \\nmoney on your trades even if the majority of them were profitable.\\nTwo descriptors of a distribution that are often used in investments are the \\nmean and the variance (or the standard deviation, the positive square root of vari-\\nance). Exhibit 7 gives the expressions for the mean and variance of binomial random \\nvariables.\\nExhibit 7: Mean and Variance of Binomial Random \\nVariables\\n\\xa0\\nMean\\nVariance\\nBernoulli, B(1, p)\\nP\\np(1 − p)\\nBinomial, B(n, p)\\nNp\\nnp(1 − p)\\nBecause a single Bernoulli random variable, Y ~ B(1, p), takes on the value 1 with \\nprobability p and the value 0 with probability 1 − p, its mean or weighted-average \\noutcome is p. Its variance is p(1 − p). A general binomial random variable, B(n, p), \\nis the sum of n Bernoulli random variables, and so the mean of a B(n, p) random \\nvariable is np. Given that a B(1, p) variable has variance p(1 − p), the variance of a \\nB(n, p) random variable is n times that value, or np(1 − p), assuming that all the trials \\n(Bernoulli random variables) are independent. We can illustrate the calculation for \\ntwo binomial random variables with differing probabilities as follows:\\nRandom Variable\\nMean\\nVariance\\nB(n = 5, p = 0.50)\\n2.50 = 5(0.50)\\n1.25 = 5(0.50)(0.50)\\nB(n = 5, p = 0.10)\\n0.50 = 5(0.10)\\n0.45 = 5(0.10)(0.90)\\nFor a B(n = 5, p = 0.50) random variable, the expected number of successes is 2.5, with \\na standard deviation of 1.118 = (1.25)1/2; for a B(n = 5, p = 0.10) random variable, the \\nexpected number of successes is 0.50, with a standard deviation of 0.67 = (0.45)1/2.\\n© CFA Institute. For candidate use only. Not for distribution.\\nBinomial Distribution\\n253\\nEXAMPLE 4\\nThe Expected Number of Defaults in a Bond Portfolio\\nSuppose as a bond analyst you are asked to estimate the number of bond issues \\nexpected to default over the next year in an unmanaged high-yield bond portfolio \\nwith 25 US issues from distinct issuers. The credit ratings of the bonds in the \\nportfolio are tightly clustered around Moody’s B2/Standard & Poor’s B, meaning \\nthat the bonds are speculative with respect to the capacity to pay interest and \\nrepay principal. The estimated annual default rate for B2/B rated bonds is 10.7%.\\n1. Over the next year, what is the expected number of defaults in the portfolio, \\nassuming a binomial model for defaults?\\nSolution to 1:\\nFor each bond, we can define a Bernoulli random variable equal to 1 if \\nthe bond defaults during the year and zero otherwise. With 25 bonds, the \\nexpected number of defaults over the year is np = 25(0.107) = 2.675, or \\napproximately 3.\\n2. Estimate the standard deviation of the number of defaults over the coming \\nyear.\\nSolution to 2:\\nThe variance is np(1 − p) = 25(0.107)(0.893) = 2.388775. The standard \\ndeviation is (2.388775)1/2 = 1.55. Thus, a two standard deviation confidence \\ninterval (±3.10) about the expected number of defaults (≈ 3), for example, \\nwould run from approximately 0 to approximately 6.\\n3. Critique the use of the binomial probability model in this context.\\nSolution to 3:\\nAn assumption of the binomial model is that the trials are independent. In \\nthis context, a trial relates to whether an individual bond issue will default \\nover the next year. Because the issuing companies probably share exposure \\nto common economic factors, the trials may not be independent. Neverthe-\\nless, for a quick estimate of the expected number of defaults, the binomial \\nmodel may be adequate.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n254\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Normal Distribution',\n",
       "     'page_number': 264,\n",
       "     'content': 'Learning Module 4 \\nCommon Probability Distributions\\n254\\nNORMAL DISTRIBUTION\\nexplain the key properties of ',\n",
       "     'children': [{'title': 'The Normal Distribution',\n",
       "       'page_number': 264,\n",
       "       'content': 'the normal distribution\\ncontrast a multivariate distribution and a univariate distribution, and \\nexplain the role of correlation in the multivariate normal distribution\\ncalculate the probability that a normally distributed random variable \\nlies inside a given interval\\nexplain how to standardize a random variable\\ncalculate and interpret probabilities using the standard normal \\ndistribution\\nIn this section, we focus on the two most important continuous distributions in \\ninvestment work, the normal and lognormal.\\nThe Normal Distribution\\nThe normal distribution may be the most extensively used probability distribution in \\nquantitative work. It plays key roles in modern portfolio theory and in several risk \\nmanagement technologies. Because it has so many uses, the normal distribution must \\nbe thoroughly understood by investment professionals.\\nThe role of the normal distribution in statistical inference and regression analysis \\nis vastly extended by a crucial result known as the central limit theorem. The central \\nlimit theorem states that the sum (and mean) of a large number of independent random \\nvariables (with finite variance) is approximately normally distributed.\\nThe French mathematician Abraham de Moivre (1667–1754) introduced the \\nnormal distribution in 1733 in developing a version of the central limit theorem. As \\nExhibit 8 shows, the normal distribution is symmetrical and bell-shaped. The range of \\npossible outcomes of the normal distribution is the entire real line: all real numbers \\nlying between −∞ and +∞. The tails of the bell curve extend without limit to the left \\nand to the right.\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nNormal Distribution\\n255\\nExhibit 8: PDFs of Two Different Normal Distributions\\nPDF with\\nµ = 0, σ = 1\\nPDF with\\nµ = 0, σ = 2\\n–8\\n–8\\n8\\n–4\\n–4\\n–6\\n–6\\n–2\\n–2\\n2\\n4\\n0\\n6\\nx\\nThe defining characteristics of a normal distribution are as follows:\\n■ \\nThe normal distribution is completely described by two parameters—its \\nmean, μ, and variance, σ2. We indicate this as X ~ N(μ, σ2) (read “X follows \\na normal distribution with mean μ and variance σ2”). We can also define a \\nnormal distribution in terms of the mean and the standard deviation, σ (this \\nis often convenient because σ is measured in the same units as X and μ). \\nAs a consequence, we can answer any probability question about a normal \\nrandom variable if we know its mean and variance (or standard deviation).\\n■ \\nThe normal distribution has a skewness of 0 (it is symmetric). The normal \\ndistribution has a kurtosis of 3; its excess kurtosis (kurtosis − 3.0) equals 0. \\nAs a consequence of symmetry, the mean, the median, and the mode are all \\nequal for a normal random variable.\\n■ \\nA linear combination of two or more normal random variables is also nor-\\nmally distributed.\\nThe foregoing bullet points and descriptions concern a single variable or univariate \\nnormal distribution: the distribution of one normal random variable. A univariate \\ndistribution describes a single random variable. A multivariate distribution speci-\\nfies the probabilities for a group of related random variables. You will encounter the \\nmultivariate normal distribution in investment work and readings and should know \\nthe following about it.\\nWhen we have a group of assets, we can model the distribution of returns on \\neach asset individually or on the assets as a group. “As a group” implies that we take \\naccount of all the statistical interrelationships among the return series. One model \\nthat has often been used for security returns is the multivariate normal distribution. \\nA multivariate normal distribution for the returns on n stocks is completely defined \\nby three lists of parameters:\\n■ \\nthe list of the mean returns on the individual securities (n means in total);\\n■ \\nthe list of the securities’ variances of return (n variances in total); and\\n■ \\nthe list of all the distinct pairwise return correlations: n(n − 1)/2 distinct \\ncorrelations in total.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n256\\nThe need to specify correlations is a distinguishing feature of the multivariate \\nnormal distribution in contrast to the univariate normal distribution.\\nThe statement “assume returns are normally distributed” is sometimes used to \\nmean a joint normal distribution. For a portfolio of 30 securities, for example, portfolio \\nreturn is a weighted average of the returns on the 30 securities. A weighted average is \\na linear combination. Thus, portfolio return is normally distributed if the individual \\nsecurity returns are (joint) normally distributed. To review, in order to specify the \\nnormal distribution for portfolio return, we need the means, the variances, and the \\ndistinct pairwise correlations of the component securities.\\nWith these concepts in mind, we can return to the normal distribution for one \\nrandom variable. The curves graphed in Exhibit 8 are the normal density function:\\n f  ( x )  =  1 \\n_ \\nσ  √ _ \\n2π    exp  ( \\n−\\u200a ( x − μ ) 2  \\n_ \\n2  σ 2   )  for − ∞  < x < +\\u200a  ∞ . \\n(3)\\nThe two densities graphed in Exhibit 8 correspond to a mean of μ = 0 and standard \\ndeviations of σ = 1 and σ = 2. The normal density with μ = 0 and σ = 1 is called the \\nstandard normal distribution (or unit normal distribution). Plotting two normal \\ndistributions with the same mean and different standard deviations helps us appreciate \\nwhy standard deviation is a good measure of dispersion for the normal distribution: \\nObservations are much more concentrated around the mean for the normal distri-\\nbution with σ = 1 than for the normal distribution with σ = 2.\\nExhibit 9 illustrates the relationship between the pdf (density function) and cdf \\n(distribution function) of the standard normal distribution (mean = 0, standard devi-\\nation = 1). Most of the time, we associate a normal distribution with the “bell curve,” \\nwhich, in fact, is the probability density function of the normal distribution, depicted \\nin Panel A. The cumulative distribution function, depicted in Panel B, in fact plots the \\nsize of the shaded areas of the pdfs. Let’s take a look at the third row: In Panel A, we \\nhave shaded the bell curve up to x = 0, the mean of the standard normal distribution. \\nThis shaded area corresponds to 50% in the cdf graph, as seen in Panel B, meaning \\nthat 50% of the observations of a normally distributed random variable would be \\nequal or less than the mean.\\n© CFA Institute. For candidate use only. Not for distribution.\\nNormal Distribution\\n257\\nExhibit 9: Density and Distribution Functions of the Standard Normal \\nDistribution\\nA. PDFs\\n0.03\\n0.03\\n0.04\\n0.04\\n0.01\\n0.01\\n0\\n0.02\\n0.02\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\nB. CDFs\\n0.6\\n0.6\\n0.2\\n0.2\\n0\\n0.4\\n0.4\\n0.8\\n0.8\\n1.0\\n1.0\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\n0.03\\n0.03\\n0.04\\n0.04\\n0.01\\n0.01\\n0\\n0.02\\n0.02\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\n0.6\\n0.6\\n0.2\\n0.2\\n0\\n0.4\\n0.4\\n0.8\\n0.8\\n1.0\\n1.0\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\n0.03\\n0.03\\n0.04\\n0.04\\n0.01\\n0.01\\n0\\n0.02\\n0.02\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\n0.6\\n0.6\\n0.2\\n0.2\\n0\\n0.4\\n0.4\\n0.8\\n0.8\\n1.0\\n1.0\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\n0.03\\n0.03\\n0.04\\n0.04\\n0.01\\n0.01\\n0\\n0.02\\n0.02\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\n0.6\\n0.6\\n0.2\\n0.2\\n0\\n0.4\\n0.4\\n0.8\\n0.8\\n1.0\\n1.0\\n–3.0\\n–3.0\\n3.0\\n3.0\\n1.6\\n1.6\\n0\\n–1.6\\n–1.6\\nAlthough not literally accurate, the normal distribution can be considered an approx-\\nimate model for asset returns. Nearly all the probability of a normal random variable \\nis contained within three standard deviations of the mean. For realistic values of \\nmean return and return standard deviation for many assets, the normal probability \\nof outcomes below −100% is very small.\\nWhether the approximation is useful in a given application is an empirical ques-\\ntion. For example, Fama (1976) and Campbell, Lo, and MacKinlay (1997) showed that \\nthe normal distribution is a closer fit for quarterly and yearly holding period returns \\non a diversified equity portfolio than it is for daily or weekly returns. A persistent \\ndeparture from normality in most equity return series is kurtosis greater than 3, the \\nfat-tails problem. So when we approximate equity return distributions with the normal \\ndistribution, we should be aware that the normal distribution tends to underestimate \\nthe probability of extreme returns.\\nFat tails can be modeled, among other things, by a mixture of normal random \\nvariables or by a Student’s t-distribution (which we shall cover shortly). In addition, \\nsince option returns are skewed, we should be cautious in using the symmetrical nor-\\nmal distribution to model the returns on portfolios containing significant positions \\nin options.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n258\\nThe normal distribution is also less suitable as a model for asset prices than as a \\nmodel for returns. An asset price can drop only to 0, at which point the asset becomes \\nworthless. As a result, practitioners generally do not use the normal distribution to \\nmodel the distribution of asset prices but work with the lognormal distribution, which \\nwe will discuss later.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Probabilities Using the Normal Distribution',\n",
       "       'page_number': 268,\n",
       "       'content': 'Probabilities Using the Normal Distribution\\nHaving established that the normal distribution is the appropriate model for a variable \\nof interest, we can use it to make the following probability statements:\\n■ \\nApproximately 50% of all observations fall in the interval μ ± (2/3)σ.\\n■ \\nApproximately 68% of all observations fall in the interval μ ± σ.\\n■ \\nApproximately 95% of all observations fall in the interval μ ± 2σ.\\n■ \\nApproximately 99% of all observations fall in the interval μ ± 3σ.\\nOne, two, and three standard deviation intervals are illustrated in Exhibit 10. \\nThe intervals indicated are easy to remember but are only approximate for the stated \\nprobabilities. More precise intervals are μ ± 1.96σ for 95% of the observations and μ \\n± 2.58σ for 99% of the observations.\\nExhibit 10: Units of Standard Deviation\\n–1s\\n–1s\\n–3s\\n–3s\\n–2s\\n–2s\\nx–\\n1s\\n1s\\n2s\\n2s\\n3s\\n3s\\n2.14%\\n13.59%\\n34.13%\\n34.13%\\n13.59%\\n2.14%\\nIn general, we do not observe the mean or the standard deviation of the distribution \\nof the whole population, so we need to estimate them from an observable sample. \\nWe estimate the population mean, μ, using the sample mean,  _\\n \\nX  (sometimes denoted \\nas   ˆ \\nμ  ), and estimate the population standard deviation, σ, using the sample standard \\ndeviation, s (sometimes denoted as   ˆ \\nσ  ).\\nEXAMPLE 5\\nCalculating Probabilities from the Normal Distribution\\nThe chief investment officer of Fund XYZ would like to present some investment \\nreturn scenarios to the Investment Committee, so she asks your assistance with \\nsome indicative numbers. Assuming daily asset returns are normally distributed, \\nshe would like to know the following:\\nNote on Answering Questions 1–4:\\nNormal distribution–related functions are part of spreadsheets, R, Python, and \\nall statistical packages. Here, we use Microsoft Excel functions to answer these \\n© CFA Institute. For candidate use only. Not for distribution.\\nNormal Distribution\\n259\\nquestions. When we speak in terms of “number of standard deviations above or \\nbelow the mean,” we are referring to the standard normal distribution (i.e., mean \\nof 0 and standard deviation of 1), so it is best to use Excel’s “=NORM.S.DIST(Z, \\n0 or 1)” function. “Z” represents the distance in number of standard deviations \\naway from the mean, and the second parameter of the function is either 0 (Excel \\nreturns pdf value) or 1 (Excel returns cdf value).\\n1. What is the probability that returns would be less than or equal to 1 stan-\\ndard deviation below the mean?\\nSolution to 1:\\nTo answer Question 1, we need the normal cdf value (so, set the second pa-\\nrameter equal to 1) that is associated with a Z value of −1 (i.e., one standard \\ndeviation below the mean). Thus, “=NORM.S.DIST(-1,1)” returns 0.1587, or \\n15.9%.\\n2. What is the probability that returns would be between +1 and −1 standard \\ndeviation around the mean?\\nSolution to 2:\\nHere, we need to calculate the area under the normal pdf within the range \\nof the mean ±1 standard deviation. The area under the pdf is the cdf, so we \\nmust calculate the difference between the cdf one standard deviation above \\nthe mean and the cdf one standard deviation below the mean. Note that \\n“=NORM.S.DIST(1,1)” returns 0.8413, or 84.1%, which means that 84.1% of \\nall observations of a normally distributed random variable would fall below \\nthe mean plus one standard deviation. We already calculated 15.9% for the \\nprobability that observations for such a variable would fall less than one \\nstandard deviation below the mean in the Solution to 1, so the answer here \\nis 84.1% − 15.9% = 68.3%.\\n3. What is the probability that returns would be less than or equal to −2 stan-\\ndard deviations below the mean?\\nSolution to 3:\\nSimilar to Solution 1, use the Excel function “=NORM.S.DIST(-2,1)”—\\nwhich returns a probability of 0.0228, or 2.3%.\\n4. How far (in terms of standard deviation) must returns fall below the mean \\nfor the probability to equal 95%?\\nSolution to 4:\\nThis question is a typical way of phrasing “value at risk.” In statistical \\nterms, we want to know the lowest return value below which only 5% of \\nthe observations would fall. Thus, we need to find the Z value for which the \\nnormal cdf would be 5% probability. To do this, we use the inverse of the \\ncdf function—that is, “=NORM.S.INV(0.05)”—which results in −1.6449, \\nor −1.64. In other words, only 5% of the observations should fall below the \\nmean minus 1.64 standard deviations, or equivalently, 95% of the observa-\\ntions should exceed this threshold.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n260\\nThere are as many different normal distributions as there are choices for mean (μ) \\nand variance (σ2). We can answer all the previous questions in terms of any normal \\ndistribution. Spreadsheets, for example, have functions for the normal cdf for any \\nspecification of mean and variance. For the sake of efficiency, however, we would like \\nto refer all probability statements to a single normal distribution. The standard normal \\ndistribution (the normal distribution with μ = 0 and σ = 1) fills that role.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Standardizing a Random Variable',\n",
       "       'page_number': 270,\n",
       "       'content': 'Learning Module 4 \\nCommon Probability Distributions\\n260\\nThere are as many different normal distributions as there are choices for mean (μ) \\nand variance (σ2). We can answer all the previous questions in terms of any normal \\ndistribution. Spreadsheets, for example, have functions for the normal cdf for any \\nspecification of mean and variance. For the sake of efficiency, however, we would like \\nto refer all probability statements to a single normal distribution. The standard normal \\ndistribution (the normal distribution with μ = 0 and σ = 1) fills that role.\\nStandardizing a Random Variable\\nThere are two steps in standardizing a normal random variable X: Subtract the \\nmean of X from X and then divide that result by the standard deviation of X (this is \\nalso known as computing the Z-score). If we have a list of observations on a normal \\nrandom variable, X, we subtract the mean from each observation to get a list of \\ndeviations from the mean and then divide each deviation by the standard deviation. \\nThe result is the standard normal random variable, Z (Z is the conventional symbol \\nfor a standard normal random variable). If we have X ~ N(μ, σ2) (read “X follows the \\nnormal distribution with parameters μ and σ2”), we standardize it using the formula\\n Z = (X − μ)/σ.   \\n(4)\\nSuppose we have a normal random variable, X, with μ = 5 and σ = 1.5. We standardize \\nX with Z = (X − 5)/1.5. For example, a value X = 9.5 corresponds to a standardized \\nvalue of 3, calculated as Z = (9.5 − 5)/1.5 = 3. The probability that we will observe \\na value as small as or smaller than 9.5 for X ~ N(5, 1.5) is exactly the same as the \\nprobability that we will observe a value as small as or smaller than 3 for Z ~ N(0, 1).\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Probabilities Using the Standard Normal Distribution',\n",
       "       'page_number': 270,\n",
       "       'content': 'Probabilities Using the Standard Normal Distribution\\nWe can answer all probability questions about X using standardized values. We \\ngenerally do not know the population mean and standard deviation, so we often use \\nthe sample mean   _\\n \\nX  for μ and the sample standard deviation s for σ. Standard normal \\nprobabilities are computed with spreadsheets, statistical and econometric software, \\nand programming languages. Tables of the cumulative distribution function for the \\nstandard normal random variable are also readily available.\\nTo find the probability that a standard normal variable is less than or equal to 0.24, \\nfor example, calculate NORM.S.DIST(0.24,1)=0.5948; thus, P(Z ≤ 0.24) = 0.5948, or \\n59.48%. If we want to find the probability of observing a value 1.65 standard deviations \\nbelow the mean, calculate NORM.S.DIST(-1.65,1)=0.04947, or roughly 5%.\\nThe following are some of the most frequently referenced values when using the \\nnormal distribution, and for these values, =NORM.S.INV(Probability) is a convenient \\nExcel function:\\n■ \\nThe 90th percentile point is 1.282, or NORM.S.INV(0.90)=1.28155. Thus, \\nonly 10% of values remain in the right tail beyond the mean plus 1.28 stan-\\ndard deviations;\\n■ \\nThe 95th percentile point is 1.65, or NORM.S.INV(0.95)=1.64485, \\nwhich means that P(Z ≤ 1.65) = N(1.65) = 0.95, or 95%, and 5% of val-\\nues remain in the right tail. The 5th percentile point, in contrast, is \\nNORM.S.INV(0.05)=-1.64485—that is, the same number as for 95%, but \\nwith a negative sign.\\n■ \\nNote the difference between the use of a percentile point when dealing with \\none tail rather than two tails. We used 1.65 because we are concerned with \\nthe 5% of values that lie only on one side, the right tail. If we want to cut \\noff both the left and right 5% tails, then 90% of values would stay within the \\nmean ±1.65 standard deviations range.\\n© CFA Institute. For candidate use only. Not for distribution.\\nNormal Distribution\\n261\\n■ \\nThe 99th percentile point is 2.327: P(Z ≤ 2.327) = N(2.327) = 0.99, or 99%, \\nand 1% of values remain in the right tail.\\nEXAMPLE 6\\nProbabilities for a Common Stock Portfolio\\nAssume the portfolio mean return is 12% and the standard deviation of return \\nestimate is 22% per year. Note also that if X is portfolio return, the standard-\\nized portfolio return is Z = (X −  _\\n \\nX  )/s = (X − 12%)/22%. We use this expression \\nthroughout the solutions.\\nYou want to calculate the following probabilities, assuming that a normal \\ndistribution describes returns.\\n1. What is the probability that portfolio return will exceed 20%?\\nSolution to 1:\\nFor X = 20%, Z = (20% − 12%)/22% = 0.363636. You want to find P(Z > \\n0.363636). First, note that P(Z > x) = P(Z ≥ x) because the normal distribu-\\ntion is a continuous distribution. Also, recall that P(Z ≥ x) = 1.0 − P(Z ≤ x) \\nor 1 − N(x). Next, NORM.S.DIST(0.363636,1)=0.64194, so, 1 − 0.6419 = \\n0.3581. Therefore, the probability that portfolio return will exceed 20% is \\nabout 36% if your normality assumption is accurate.\\n2. What is the probability that portfolio return will be between 12% and 20%? \\nIn other words, what is P(12% ≤ portfolio return ≤ 20%)?\\nSolution to 2:\\nP(12% ≤ Portfolio return ≤ 20%) = N(Z corresponding to 20%) − N(Z corre-\\nsponding to 12%). For the first term, Z = (20% − 12%)/22% = 0.363636, and \\nN(0.363636) = 0.6419 (as in Solution 1). To get the second term immedi-\\nately, note that 12% is the mean, and for the normal distribution, 50% of the \\nprobability lies on either side of the mean. Therefore, N(Z corresponding to \\n12%) must equal 50%. So P(12% ≤ Portfolio return ≤ 20%) = 0.6419 − 0.50 = \\n0.1419, or approximately 14%.\\n3. You can buy a one-year T-bill that yields 5.5%. This yield is effectively a \\none-year risk-free interest rate. What is the probability that your portfolio’s \\nreturn will be equal to or less than the risk-free rate?\\nSolution to 3:\\nIf X is portfolio return, then we want to find P(Portfolio return \\n≤ 5.5%). For X = 5.5%, Z = (5.5% − 12%)/22% = −0.2955. Using \\nNORM.S.DIST(-0.2955,1)=0.3838, we see an approximately 38% chance the \\nportfolio’s return will be equal to or less than the risk-free rate.\\nNext, we will briefly discuss and illustrate the concept of the central limit theorem, \\naccording to which the sum (as well as the mean) of a set of independent, identically \\ndistributed random variables with finite variances is normally distributed, whatever \\ndistribution the random variables follow.\\nTo illustrate this concept, consider a sample of 30 observations of a random vari-\\nable that can take a value of just −100, 0, or 100, with equal probability. Clearly, this \\nsample is drawn from a simple discrete uniform distribution, where the possible values \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n262\\nof −100, 0, and 100 each have 1/3 probability. We randomly pick 10 elements of this \\nsample and calculate the sum of these elements, and then we repeat this process a \\ntotal of 100 times. The histogram in Exhibit 11 shows the distribution of these sums: \\nThe underlying distribution is a very simple discrete uniform distribution, but the \\nsums converge toward a normal distribution.\\nExhibit 11: Central Limit Theorem: Sums of Elements from Discrete Uniform \\nDistribution Converge to Normal Distribution\\n20\\n20\\n18\\n18\\n16\\n16\\n14\\n14\\n12\\n12\\n10\\n10\\n2468\\n0\\n–300\\n–300\\n–200\\n–200\\n–100\\n–100\\n–600\\n–600\\n–500\\n–500\\n–400\\n–400\\n–1,000\\n–1,000\\n–900\\n–900\\n–800\\n–800\\n–700\\n–700\\n800\\n800\\n900\\n900\\n1,000\\n1,000\\n500\\n500\\n600\\n600\\n700\\n700\\n100\\n100\\n0\\n200\\n200\\n300\\n300\\n400\\n400\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Applications of the Normal Distribution',\n",
       "     'page_number': 272,\n",
       "     'content': 'APPLICATIONS OF THE NORMAL DISTRIBUTION\\ndefine shortfall risk, calculate the safety-first ratio, and identify an \\noptimal portfolio using Roy’s safety-first criterion\\nModern portfolio theory (MPT) makes wide use of the idea that the value of investment \\nopportunities can be meaningfully measured in terms of mean return and variance of \\nreturn. In economic theory, mean–variance analysis holds exactly when investors \\nare risk averse; when they choose investments so as to maximize expected utility, or \\nsatisfaction; and when either (1) returns are normally distributed or (2) investors have \\nquadratic utility functions, a concept used in economics for a mathematical repre-\\nsentation of attitudes toward risk and return. Mean–variance analysis, however, can \\nstill be useful—that is, it can hold approximately—when either Assumption 1 or 2 is \\nviolated. Because practitioners prefer to work with observables, such as returns, the \\nproposition that returns are at least approximately normally distributed has played a \\nkey role in much of MPT.\\nTo illustrate this concept, assume an investor is saving for retirement, and although \\nher goal is to earn the highest real return possible, she believes that the portfolio \\nshould at least achieve real capital preservation over the long term. Assuming a \\nlong-term expected inflation rate of 2%, the minimum acceptable return would be 2%. \\nExhibit 12 compares three investment alternatives in terms of their expected returns \\nand standard deviation of returns. The probability of falling below 2% is calculated \\non basis of the assumption of normally distributed returns. In the table, we see that \\nPortfolio II, which combines the highest expected return and the lowest volatility, has \\nthe lowest probability of earning less than 2% (or equivalently, the highest probability \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nApplications of the Normal Distribution\\n263\\nof earning at least 2%). This can also be seen in Panel B, where Portfolio II has the \\nsmallest shaded area to the left of 2% (the probability of earning less than the mini-\\nmum acceptable return).\\nExhibit 12: Probability of Earning a Minimum Acceptable Return \\nPortfolio\\nI\\nII\\nII\\nExpected return\\n5%\\n8%\\n5%\\nStandard deviation of return\\n8%\\n8%\\n12%\\nProbability of earning < 2% [P(x < 2)]\\n37.7%\\n24.6%\\n41.7%\\nProbability of earning ≥ 2% [P(x ≥ 2)]\\n62.3%\\n75.4%\\n58.3%\\nA. Portfolio I\\n0.05\\n0.05\\n0.03\\n0.03\\n0.02\\n0.02\\n0.01\\n0.01\\n0.04\\n0.04\\n0\\n–40\\n–40\\n40\\n40\\n–20\\n–30–20\\n–30\\n–10\\n–10\\n0\\n20\\n20\\n10\\n10\\n30\\n30\\nReturn\\nMinimum Acceptable\\nReturn = 2%\\nMean = 5%\\nB. Portfolio II\\n0.05\\n0.05\\n0.03\\n0.03\\n0.02\\n0.02\\n0.01\\n0.01\\n0.04\\n0.04\\n0\\n–40\\n–40\\n40\\n40\\n–20\\n–30–20\\n–30\\n–10\\n–10\\n0\\n20\\n20\\n10\\n10\\n30\\n30\\nReturn\\nMinimum Acceptable\\nReturn = 2%\\nMean = 8 %\\nC. Portfolio III\\n0.05\\n0.05\\n0.03\\n0.03\\n0.02\\n0.02\\n0.01\\n0.01\\n0.04\\n0.04\\n0\\n–40\\n–40\\n40\\n40\\n–20\\n–30–20\\n–30\\n–10\\n–10\\n0\\n20\\n20\\n10\\n10\\n30\\n30\\nReturn\\nMinimum Acceptable\\nReturn = 2%\\nMean = 5%\\nMean–variance analysis generally considers risk symmetrically in the sense that \\nstandard deviation captures variability both above and below the mean. An alternative \\napproach evaluates only downside risk. We discuss one such approach, safety-first \\nrules, because they provide an excellent illustration of the application of normal distri-\\nbution theory to practical investment problems. Safety-first rules focus on shortfall \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n264\\nrisk, the risk that portfolio value (or portfolio return) will fall below some minimum \\nacceptable level over some time horizon. The risk that the assets in a defined benefit \\nplan will fall below plan liabilities is an example of a shortfall risk.\\nSuppose an investor views any return below a level of RL as unacceptable. Roy’s \\nsafety-first criterion (Roy 1952) states that the optimal portfolio minimizes the prob-\\nability that portfolio return, RP, will fall below the threshold level, RL. In symbols, the \\ninvestor’s objective is to choose a portfolio that minimizes P(RP < RL). When portfolio \\nreturns are normally distributed, we can calculate P(RP < RL) using the number of \\nstandard deviations that RL lies below the expected portfolio return, E(RP). The port-\\nfolio for which E(RP) − RL is largest relative to standard deviation minimizes P(RP < \\nRL). Therefore, if returns are normally distributed, the safety-first optimal portfolio \\nmaximizes the safety-first ratio (SFRatio):\\n SFRatio = [E(RP) − RL]/σP . \\n(5)\\nThe quantity E(RP) − RL is the distance from the mean return to the shortfall level. \\nDividing this distance by σP gives the distance in units of standard deviation. There are \\ntwo steps in choosing among portfolios using Roy’s criterion (assuming normality):\\n1. Calculate each portfolio’s SFRatio.\\n2. Choose the portfolio with the highest SFRatio.\\nFor a portfolio with a given safety-first ratio, the probability that its return will be \\nless than RL is N(–SFRatio), and the safety-first optimal portfolio has the lowest such \\nprobability. For example, suppose an investor’s threshold return, RL, is 2%. He is pre-\\nsented with two portfolios. Portfolio 1 has an expected return of 12%, with a standard \\ndeviation of 15%. Portfolio 2 has an expected return of 14%, with a standard deviation \\nof 16%. The SFRatios, using Equation 5, are 0.667 = (12 − 2)/15 and 0.75 = (14 − 2)/16 \\nfor Portfolios 1 and 2, respectively. For the superior Portfolio 2, the probability that \\nportfolio return will be less than 2% is N(−0.75) = 1 − N(0.75) = 1 − 0.7734 = 0.227, \\nor about 23%, assuming that portfolio returns are normally distributed.\\nYou may have noticed the similarity of the SFRatio to the Sharpe ratio. If we sub-\\nstitute the risk-free rate, RF, for the critical level RL, the SFRatio becomes the Sharpe \\nratio. The safety-first approach provides a new perspective on the Sharpe ratio: When \\nwe evaluate portfolios using the Sharpe ratio, the portfolio with the highest Sharpe \\nratio is the one that minimizes the probability that portfolio return will be less than \\nthe risk-free rate (given a normality assumption).\\nEXAMPLE 7\\nThe Safety-First Optimal Portfolio for a Client\\nYou are researching asset allocations for a client in Canada with a C$800,000 \\nportfolio. Although her investment objective is long-term growth, at the end of \\na year she may want to liquidate C$30,000 of the portfolio to fund educational \\nexpenses. If that need arises, she would like to be able to take out the C$30,000 \\nwithout invading the initial capital of C$800,000. The table below shows three \\nalternative allocations.\\n \\nMean and Standard Deviation for Three Allocations (in \\nPercent) \\n \\n \\nAllocation\\nA\\nB\\nC\\nExpected annual return\\n25\\n11\\n14\\nStandard deviation of return\\n27\\n8\\n20\\n© CFA Institute. For candidate use only. Not for distribution.\\nApplications of the Normal Distribution\\n265\\n \\nAddress these questions (assume normality for Questions 2 and 3):\\n1. Given the client’s desire not to invade the C$800,000 principal, what is the \\nshortfall level, RL? Use this shortfall level to answer Question 2.\\nSolution to 1:\\nBecause C$30,000/C$800,000 is 3.75%, for any return less than 3.75% the \\nclient will need to invade principal if she takes out C$30,000. So, RL = 3.75%.\\n2. According to the safety-first criterion, which of the three allocations is the \\nbest?\\nSolution to 2:\\nTo decide which of the three allocations is safety-first optimal, select the \\nalternative with the highest ratio [E(RP) − RL]/σP:\\n Allocation A: 0.787037 = (25 − 3.75)/27.\\n Allocation B: 0.90625 = (11 − 3.75)/8.\\n Allocation C: 0.5125 = (14 − 3.75)/20.\\nAllocation B, with the largest ratio (0.90625), is the best alternative accord-\\ning to the safety-first criterion.\\n3. What is the probability that the return on the safety-first optimal portfolio \\nwill be less than the shortfall level?\\nSolution to 3:\\nTo answer this question, note that P(RB < 3.75) = N(−0.90625). We can \\nround 0.90625 to 0.91 for use with tables of the standard normal cdf. First, \\nwe calculate N(−0.91) = 1 − N(0.91) = 1 − 0.8186 = 0.1814, or about 18.1%. \\nUsing a spreadsheet function for the standard normal cdf on −0.90625 \\nwithout rounding, we get 0.182402, or about 18.2%. The safety-first optimal \\nportfolio has a roughly 18% chance of not meeting a 3.75% return threshold. \\nThis can be seen in the following graphic, where Allocation B has the small-\\nest area under the distribution curve to the left of 3.75%.\\n0.10\\n0.10\\n0.08\\n0.08\\n0.06\\n0.06\\n0.04\\n0.04\\n0.02\\n0.02\\n0\\n–80.3\\n–80.3\\n–56.3\\n–56.3\\n63.8\\n63.8\\n87.8\\n87.8\\n–32.3\\n–32.3\\n–8.3\\n–8.3\\n15.8\\n15.8\\n39.8\\n39.8\\nPercent\\nA\\nB\\nC\\nTarget Return = 3.75%\\nSeveral points are worth noting. First, if the inputs were slightly different, we \\ncould get a different ranking. For example, if the mean return on B were 10% \\nrather than 11%, Allocation A would be superior to B. Second, if meeting \\nthe 3.75% return threshold were a necessity rather than a wish, C$830,000 \\nin one year could be modeled as a liability. Fixed-income strategies, such \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n266\\nas cash flow matching, could be used to offset or immunize the C$830,000 \\nquasi-liability.\\nIn many investment contexts besides Roy’s safety-first criterion, we use the normal \\ndistribution to estimate a probability. Another arena in which the normal distribution \\nplays an important role is financial risk management. Financial institutions, such as \\ninvestment banks, security dealers, and commercial banks, have formal systems to \\nmeasure and control financial risk at various levels, from trading positions to the \\noverall risk for the firm. Two mainstays in managing financial risk are value at risk \\n(VaR) and stress testing/scenario analysis. Stress testing and scenario analysis refer \\nto a set of techniques for estimating losses in extremely unfavorable combinations of \\nevents or scenarios. Value at risk (VaR) is a money measure of the minimum value of \\nlosses expected over a specified time period (for example, a day, a quarter, or a year) \\nat a given level of probability (often 0.05 or 0.01). Suppose we specify a one-day time \\nhorizon and a level of probability of 0.05, which would be called a 95% one-day VaR. \\nIf this VaR equaled €5 million for a portfolio, there would be a 0.05 probability that \\nthe portfolio would lose €5 million or more in a single day (assuming our assumptions \\nwere correct). One of the basic approaches to estimating VaR, the variance–covariance \\nor analytical method, assumes that returns follow a normal distribution.\\nLOGNORMAL DISTRIBUTION AND CONTINUOUS \\nCOMPOUNDING\\nexplain the relationship between normal and lognormal distributions \\nand why the lognormal distribution is used to model asset prices\\ncalculate and interpret a continuously compounded rate of return, \\ngiven a specific holding period return\\nThe Lognormal Distribution \\nClosely related to the normal distribution, the lognormal distribution is widely used for \\nmodeling the probability distribution of share and other asset prices. For example, the \\nlognormal distribution appears in the Black–Scholes–Merton option pricing model. \\nThe Black–Scholes–Merton model assumes that the price of the asset underlying the \\noption is lognormally distributed.\\nA random variable Y follows a lognormal distribution if its natural logarithm, ln \\nY, is normally distributed. The reverse is also true: If the natural logarithm of random \\nvariable Y, ln Y, is normally distributed, then Y follows a lognormal distribution. \\nIf you think of the term lognormal as “the log is normal,” you will have no trouble \\nremembering this relationship.\\nThe two most noteworthy observations about the lognormal distribution are \\nthat it is bounded below by 0 and it is skewed to the right (it has a long right tail). \\nNote these two properties in the graphs of the pdfs of two lognormal distributions \\nin Exhibit 13. Asset prices are bounded from below by 0. In practice, the lognormal \\ndistribution has been found to be a usefully accurate description of the distribution \\nof prices for many financial assets. However, the normal distribution is often a good \\napproximation for returns. For this reason, both distributions are very important for \\nfinance professionals.\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Lognormal Distribution and Continuous Compounding',\n",
       "     'page_number': 276,\n",
       "     'content': 'Learning Module 4 \\nCommon Probability Distributions\\n266\\nas cash flow matching, could be used to offset or immunize the C$830,000 \\nquasi-liability.\\nIn many investment contexts besides Roy’s safety-first criterion, we use the normal \\ndistribution to estimate a probability. Another arena in which the normal distribution \\nplays an important role is financial risk management. Financial institutions, such as \\ninvestment banks, security dealers, and commercial banks, have formal systems to \\nmeasure and control financial risk at various levels, from trading positions to the \\noverall risk for the firm. Two mainstays in managing financial risk are value at risk \\n(VaR) and stress testing/scenario analysis. Stress testing and scenario analysis refer \\nto a set of techniques for estimating losses in extremely unfavorable combinations of \\nevents or scenarios. Value at risk (VaR) is a money measure of the minimum value of \\nlosses expected over a specified time period (for example, a day, a quarter, or a year) \\nat a given level of probability (often 0.05 or 0.01). Suppose we specify a one-day time \\nhorizon and a level of probability of 0.05, which would be called a 95% one-day VaR. \\nIf this VaR equaled €5 million for a portfolio, there would be a 0.05 probability that \\nthe portfolio would lose €5 million or more in a single day (assuming our assumptions \\nwere correct). One of the basic approaches to estimating VaR, the variance–covariance \\nor analytical method, assumes that returns follow a normal distribution.\\nLOGNORMAL DISTRIBUTION AND CONTINUOUS \\nCOMPOUNDING\\nexplain the relationship between normal and lognormal distributions \\nand why ',\n",
       "     'children': [{'title': 'The Lognormal Distribution ',\n",
       "       'page_number': 276,\n",
       "       'content': 'the lognormal distribution is used to model asset prices\\ncalculate and interpret a continuously compounded rate of return, \\ngiven a specific holding period return\\nThe Lognormal Distribution \\nClosely related to the normal distribution, the lognormal distribution is widely used for \\nmodeling the probability distribution of share and other asset prices. For example, the \\nlognormal distribution appears in the Black–Scholes–Merton option pricing model. \\nThe Black–Scholes–Merton model assumes that the price of the asset underlying the \\noption is lognormally distributed.\\nA random variable Y follows a lognormal distribution if its natural logarithm, ln \\nY, is normally distributed. The reverse is also true: If the natural logarithm of random \\nvariable Y, ln Y, is normally distributed, then Y follows a lognormal distribution. \\nIf you think of the term lognormal as “the log is normal,” you will have no trouble \\nremembering this relationship.\\nThe two most noteworthy observations about the lognormal distribution are \\nthat it is bounded below by 0 and it is skewed to the right (it has a long right tail). \\nNote these two properties in the graphs of the pdfs of two lognormal distributions \\nin Exhibit 13. Asset prices are bounded from below by 0. In practice, the lognormal \\ndistribution has been found to be a usefully accurate description of the distribution \\nof prices for many financial assets. However, the normal distribution is often a good \\napproximation for returns. For this reason, both distributions are very important for \\nfinance professionals.\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nLognormal Distribution and Continuous Compounding\\n267\\nExhibit 13: Two Lognormal Distributions\\n0\\n4.5\\n4.5\\n0.5\\n0.5\\n1.0\\n1.0\\n1.5\\n1.5\\n2.0\\n2.0\\n2.5\\n2.5\\n3.0\\n3.0\\n3.5\\n3.5\\n4.0\\n4.0\\nLike the normal distribution, the lognormal distribution is completely described \\nby two parameters. Unlike the other distributions we have considered, a lognormal \\ndistribution is defined in terms of the parameters of a different distribution. The \\ntwo parameters of a lognormal distribution are the mean and standard deviation (or \\nvariance) of its associated normal distribution: the mean and variance of ln Y, given \\nthat Y is lognormal. Remember, we must keep track of two sets of means and stan-\\ndard deviations (or variances): the mean and standard deviation (or variance) of the \\nassociated normal distribution (these are the parameters) and the mean and standard \\ndeviation (or variance) of the lognormal variable itself.\\nTo illustrate this relationship, we simulated 1,000 scenarios of yearly asset returns, \\nassuming that returns are normally distributed with 7% mean and 12% standard \\ndeviation. For each scenario i, we converted the simulated continuously compounded \\nreturns (ri) to future asset prices with the formula Price(1 year later)i = $1 x exp(ri), \\nwhere exp is the exponential function and assuming that the asset’s price is $1 today. \\nIn Exhibit 14, Panel A shows the distribution of the simulated returns together with \\nthe fitted normal pdf, whereas Panel B shows the distribution of the corresponding \\nfuture asset prices together with the fitted lognormal pdf. Again, note that the lognor-\\nmal distribution of future asset prices is bounded below by 0 and has a long right tail.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n268\\nExhibit 14: Simulated Returns (Normal PDF) and Asset Prices (Lognormal \\nPDF)\\nA. Normal PDF\\n120\\n120\\n100\\n100\\n80\\n80\\n60\\n60\\n40\\n40\\n20\\n20\\n0\\n–28.76\\n–28.76\\n–18.58\\n–18.58\\n22.1\\n22.1\\n11.93\\n11.93\\n1.76\\n1.76\\n–8.41\\n–8.41\\n32.27\\n32.27\\n42.45\\n42.45\\nYearly Returns (%)\\nNormal Fit\\nHistogram\\nB. Lognormal PDF\\n120\\n120\\n100\\n100\\n80\\n80\\n60\\n60\\n40\\n40\\n20\\n20\\n0\\n0.75\\n0.75\\n0.86\\n0.86\\n1.32\\n1.32\\n1.21\\n1.21\\n1.09\\n1.09\\n0.98\\n0.98\\n1.44\\n1.44\\n1.55\\n1.55\\nAsset Values 1 Year Later (USD 1 Initial Investment)\\nLognormal Fit\\nHistogram\\nThe expressions for the mean and variance of the lognormal variable itself are chal-\\nlenging. Suppose a normal random variable X has expected value μ and variance σ2. \\nDefine Y = exp(X). Remember that the operation indicated by exp(X) or eX (where e \\n≈ 2.7183) is the opposite operation from taking logs. Because ln Y = ln [exp(X)] = X \\nis normal (we assume X is normal), Y is lognormal. What is the expected value of Y \\n= exp(X)? A guess might be that the expected value of Y is exp(μ). The expected value \\nis actually exp(μ + 0.50σ2), which is larger than exp(μ) by a factor of exp(0.50 σ2) > 1. \\nTo get some insight into this concept, think of what happens if we increase σ2. The \\ndistribution spreads out; it can spread upward, but it cannot spread downward past \\n0. As a result, the center of its distribution is pushed to the right: The distribution’s \\nmean increases.\\nThe expressions for the mean and variance of a lognormal variable are summarized \\nbelow, where μ and σ2 are the mean and variance of the associated normal distribution \\n(refer to these expressions as needed, rather than memorizing them):\\n■ \\nMean (μL) of a lognormal random variable = exp(μ + 0.50σ2).\\n© CFA Institute. For candidate use only. Not for distribution.\\nLognormal Distribution and Continuous Compounding\\n269\\n■ \\nVariance (σL2) of a lognormal random variable = exp(2μ + σ2) × [exp(σ2) \\n− 1].\\nContinuously Compounded Rates of Return \\nWe now explore the relationship between the distribution of stock return and stock \\nprice. In this section, we show that if a stock’s continuously compounded return is \\nnormally distributed, then future stock price is necessarily lognormally distributed. \\nFurthermore, we show that stock price may be well described by the lognormal \\ndistribution even when continuously compounded returns do not follow a normal \\ndistribution. These results provide the theoretical foundation for using the lognormal \\ndistribution to model asset prices.\\nTo outline the presentation that follows, we first show that the stock price at some \\nfuture time T, ST, equals the current stock price, S0, multiplied by e raised to power \\nr0,T, the continuously compounded return from 0 to T; this relationship is expressed \\nas ST = S0exp(r0,T). We then show that we can write r0,T as the sum of shorter-term \\ncontinuously compounded returns and that if these shorter-period returns are normally \\ndistributed, then r0,T is normally distributed (given certain assumptions) or approx-\\nimately normally distributed (not making those assumptions). As ST is proportional \\nto the log of a normal random variable, ST is lognormal.\\nTo supply a framework for our discussion, suppose we have a series of equally \\nspaced observations on stock price: S0, S1, S2, . . ., ST. Current stock price, S0, is a \\nknown quantity and thus is nonrandom. The future prices (such as S1), however, are \\nrandom variables. The price relative, S1/S0, is an ending price, S1, over a beginning \\nprice, S0; it is equal to 1 plus the holding period return on the stock from t = 0 to t = 1:\\n S1/S0 = 1 + R0,1.\\nFor example, if S0 = $30 and S1 = $34.50, then S1/S0 = $34.50/$30 = 1.15. Therefore, \\nR0,1 = 0.15, or 15%. In general, price relatives have the form\\n St+1/St = 1 + Rt,t+1,\\nwhere Rt,t+1 is the rate of return from t to t + 1.\\nAn important concept is the continuously compounded return associated with a \\nholding period return, such as R0,1. The continuously compounded return associated \\nwith a holding period return is the natural logarithm of 1 plus that holding period \\nreturn, or equivalently, the natural logarithm of the ending price over the beginning \\nprice (the price relative). Note that here we are using lowercase r to refer specifically \\nto continuously compounded returns. For example, if we observe a one-week holding \\nperiod return of 0.04, the equivalent continuously compounded return, called the \\none-week continuously compounded return, is ln(1.04) = 0.039221; €1.00 invested \\nfor one week at 0.039221 continuously compounded gives €1.04, equivalent to a 4% \\none-week holding period return. The continuously compounded return from t to t + 1 is\\n rt,t+1 = ln(St+1/St) = ln(1 + Rt,t+1).   \\n(6)\\nFor our example, r0,1 = ln(S1/S0) = ln(1 + R0,1) = ln($34.50/$30) = ln(1.15) = \\n0.139762. Thus, 13.98% is the continuously compounded return from t = 0 to t = 1. \\nThe continuously compounded return is smaller than the associated holding period \\nreturn. If our investment horizon extends from t = 0 to t = T, then the continuously \\ncompounded return to T is\\n r0,T = ln(ST/S0).\\nApplying the function exp to both sides of the equation, we have exp(r0,T) = \\nexp[ln(ST/S0)] = ST/S0, so\\n ST = S0exp(r0,T).\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Continuously Compounded Rates of Return ',\n",
       "       'page_number': 279,\n",
       "       'content': 'Lognormal Distribution and Continuous Compounding\\n269\\n■ \\nVariance (σL2) of a lognormal random variable = exp(2μ + σ2) × [exp(σ2) \\n− 1].\\nContinuously Compounded Rates of Return \\nWe now explore the relationship between the distribution of stock return and stock \\nprice. In this section, we show that if a stock’s continuously compounded return is \\nnormally distributed, then future stock price is necessarily lognormally distributed. \\nFurthermore, we show that stock price may be well described by the lognormal \\ndistribution even when continuously compounded returns do not follow a normal \\ndistribution. These results provide the theoretical foundation for using the lognormal \\ndistribution to model asset prices.\\nTo outline the presentation that follows, we first show that the stock price at some \\nfuture time T, ST, equals the current stock price, S0, multiplied by e raised to power \\nr0,T, the continuously compounded return from 0 to T; this relationship is expressed \\nas ST = S0exp(r0,T). We then show that we can write r0,T as the sum of shorter-term \\ncontinuously compounded returns and that if these shorter-period returns are normally \\ndistributed, then r0,T is normally distributed (given certain assumptions) or approx-\\nimately normally distributed (not making those assumptions). As ST is proportional \\nto the log of a normal random variable, ST is lognormal.\\nTo supply a framework for our discussion, suppose we have a series of equally \\nspaced observations on stock price: S0, S1, S2, . . ., ST. Current stock price, S0, is a \\nknown quantity and thus is nonrandom. The future prices (such as S1), however, are \\nrandom variables. The price relative, S1/S0, is an ending price, S1, over a beginning \\nprice, S0; it is equal to 1 plus the holding period return on the stock from t = 0 to t = 1:\\n S1/S0 = 1 + R0,1.\\nFor example, if S0 = $30 and S1 = $34.50, then S1/S0 = $34.50/$30 = 1.15. Therefore, \\nR0,1 = 0.15, or 15%. In general, price relatives have the form\\n St+1/St = 1 + Rt,t+1,\\nwhere Rt,t+1 is the rate of return from t to t + 1.\\nAn important concept is the continuously compounded return associated with a \\nholding period return, such as R0,1. The continuously compounded return associated \\nwith a holding period return is the natural logarithm of 1 plus that holding period \\nreturn, or equivalently, the natural logarithm of the ending price over the beginning \\nprice (the price relative). Note that here we are using lowercase r to refer specifically \\nto continuously compounded returns. For example, if we observe a one-week holding \\nperiod return of 0.04, the equivalent continuously compounded return, called the \\none-week continuously compounded return, is ln(1.04) = 0.039221; €1.00 invested \\nfor one week at 0.039221 continuously compounded gives €1.04, equivalent to a 4% \\none-week holding period return. The continuously compounded return from t to t + 1 is\\n rt,t+1 = ln(St+1/St) = ln(1 + Rt,t+1).   \\n(6)\\nFor our example, r0,1 = ln(S1/S0) = ln(1 + R0,1) = ln($34.50/$30) = ln(1.15) = \\n0.139762. Thus, 13.98% is the continuously compounded return from t = 0 to t = 1. \\nThe continuously compounded return is smaller than the associated holding period \\nreturn. If our investment horizon extends from t = 0 to t = T, then the continuously \\ncompounded return to T is\\n r0,T = ln(ST/S0).\\nApplying the function exp to both sides of the equation, we have exp(r0,T) = \\nexp[ln(ST/S0)] = ST/S0, so\\n ST = S0exp(r0,T).\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n270\\nWe can also express ST/S0 as the product of price relatives:\\n ST/S0 = (ST/ST−1)(ST−1/ST−2) . . . (S1/S0).\\nTaking logs of both sides of this equation, we find that the continuously com-\\npounded return to time T is the sum of the one-period continuously compounded \\nreturns:\\n r0,T = rT−1,T + rT−2,T−1 + . . . + r0,1.   \\n(7)\\nUsing holding period returns to find the ending value of a $1 investment involves the \\nmultiplication of quantities (1 + holding period return). Using continuously com-\\npounded returns involves addition (as shown in Equation 7).\\nA key assumption in many investment applications is that returns are inde-\\npendently and identically distributed (i.i.d.). Independence captures the proposition \\nthat investors cannot predict future returns using past returns. Identical distribution \\ncaptures the assumption of stationarity, a property implying that the mean and vari-\\nance of return do not change from period to period.\\nAssume that the one-period continuously compounded returns (such as r0,1) are \\ni.i.d. random variables with mean μ and variance σ2 (but making no normality or other \\ndistributional assumption). Then,\\n E(r0,T) = E(rT−1,T) + E(rT−2,T−1) + . . . + E(r0,1) = μT    \\n(8)\\n(we add up μ for a total of T times), and\\n σ2(r0,T) = σ2T    \\n(9)\\n(as a consequence of the independence assumption). The variance of the T hold-\\ning period continuously compounded return is T multiplied by the variance of the \\none-period continuously compounded return; also, σ(r0,T) =  σ  √ \\n_\\n \\nT  . If the one-period \\ncontinuously compounded returns on the right-hand side of Equation 7 are nor-\\nmally distributed, then the T holding period continuously compounded return, r0,T, \\nis also normally distributed with mean μT and variance σ2T. This relationship is so \\nbecause a linear combination of normal random variables is also normal. But even \\nif the one-period continuously compounded returns are not normal, their sum, r0,T, \\nis approximately normal according to the central limit theorem. Now compare ST = \\nS0exp(r0,T) to Y = exp(X), where X is normal and Y is lognormal (as we discussed pre-\\nviously). Clearly, we can model future stock price ST as a lognormal random variable \\nbecause r0,T should be at least approximately normal. This assumption of normally \\ndistributed returns is the basis in theory for the lognormal distribution as a model \\nfor the distribution of prices of shares and other assets.\\nContinuously compounded returns play a role in many asset pricing models, as well \\nas in risk management. Volatility measures the standard deviation of the continuously \\ncompounded returns on the underlying asset; by convention, it is stated as an annu-\\nalized measure. In practice, we very often estimate volatility using a historical series \\nof continuously compounded daily returns. We gather a set of daily holding period \\nreturns and then use Equation 6 to convert them into continuously compounded daily \\nreturns. We then compute the standard deviation of the continuously compounded \\ndaily returns and annualize that number using Equation 9.\\nTo compute the standard deviation of a set (or sample) of n returns, we sum the \\nsquared deviation of each return from the mean return and then divide that sum by \\nn − 1. The result is the sample variance. Taking the square root of the sample vari-\\nance gives the sample standard deviation. Annualizing is typically done on the basis \\nof 250 days in a year, the approximate number of days markets are open for trading. \\nThus if daily volatility were 0.01, we would state volatility (on an annual basis) as  \\n0.01  √ _ \\n250  = 0.1581 . Example 8 illustrates the estimation of volatility for the shares \\nof Astra International.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLognormal Distribution and Continuous Compounding\\n271\\nEXAMPLE 8\\nVolatility of Share Price\\nSuppose you are researching Astra International (Indonesia Stock Exchange: \\nASII) and are interested in Astra’s price action in a week in which international \\neconomic news had significantly affected the Indonesian stock market. You \\ndecide to use volatility as a measure of the variability of Astra shares during \\nthat week. The following shows closing prices during that week.\\n \\nAstra International Daily Closing Prices\\n \\n \\nDay\\nClosing Price (IDR)\\nMonday\\n6,950\\nTuesday\\n7,000\\nWednesday\\n6,850\\nThursday\\n6,600\\nFriday\\n6,350\\n \\nUse the data provided to do the following:\\n1. Estimate the volatility of Astra shares. (Annualize volatility on the basis of \\n250 days in a year.)\\nSolution to 1:\\nFirst, use Equation 6 to calculate the continuously compounded daily \\nreturns; then, find their standard deviation in the usual way. In calculating \\nsample variance, to get sample standard deviation, the divisor is sample size \\nminus 1.\\n ln(7,000/6,950) = 0.007168.\\n ln(6,850/7,000) = −0.021661.\\n ln(6,600/6,850) = −0.037179.\\n ln(6,350/6,600) = −0.038615.\\n Sum = −0.090287.\\n Mean = −0.022572.\\n Variance = 0.000452.\\n Standard deviation = 0.021261.\\nThe standard deviation of continuously compounded daily returns is \\n0.021261. Equation 9 states that  ˆ σ   ( r 0,T )  =  ˆ σ   √ \\n_\\n \\nT  . In this example,  ˆ σ  is the \\nsample standard deviation of one-period continuously compounded returns. \\nThus,  ˆ σ  refers to 0.021261. We want to annualize, so the horizon T corre-\\nsponds to one year. Because  ˆ σ  is in days, we set T equal to the number of \\ntrading days in a year (250).\\nWe find that annualized volatility for Astra stock that week was 33.6%, cal-\\nculated as  0.021261  √ _ \\n250  = 0.336165 .\\nNote that the sample mean, −0.022572, is a possible estimate of the mean, \\nμ, of the continuously compounded one-period or daily returns. The sample \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n272\\nmean can be translated into an estimate of the expected continuously com-\\npounded annual return using Equation 8:  ˆ \\nμ  T = − 0.022572  ( 250 )  (using 250 \\nto be consistent with the calculation of volatility). But four observations are \\nfar too few to estimate expected returns. The variability in the daily returns \\noverwhelms any information about expected return in a series this short.\\n2. Identify the probability distribution for Astra share prices if continuously \\ncompounded daily returns follow the normal distribution.\\nSolution to 2:\\nAstra share prices should follow the lognormal distribution if the continuously \\ncompounded daily returns on Astra shares follow the normal distribution.\\nWe have shown that the distribution of stock price is lognormal, given certain \\nassumptions. What are the mean and variance of ST if ST follows the lognormal \\ndistribution? Earlier we gave bullet-point expressions for the mean and variance of a \\nlognormal random variable. In the bullet-point expressions, the    ˆ \\nμ  \\xa0and\\xa0    ˆ \\nσ     2   would refer, \\nin the context of this discussion, to the mean and variance of the T horizon (not the \\none-period) continuously compounded returns (assumed to follow a normal distri-\\nbution), compatible with the horizon of ST. Related to the use of mean and variance \\n(or standard deviation), previously we used those quantities to construct intervals in \\nwhich we expect to find a certain percentage of the observations of a normally dis-\\ntributed random variable. Those intervals were symmetric about the mean. Can we \\nstate similar symmetric intervals for a lognormal random variable? Unfortunately, we \\ncannot; because the lognormal distribution is not symmetric, such intervals are more \\ncomplicated than for the normal distribution, and we will not discuss this specialist \\ntopic here.\\nSTUDENT’S T-, CHI-SQUARE, AND F-DISTRIBUTIONS\\ndescribe the properties of the Student’s t-distribution, and calculate \\nand interpret its degrees of freedom\\ndescribe the properties of the chi-square distribution and the \\nF-distribution, and calculate and interpret their degrees of freedom\\nStudent’s t-Distribution \\nTo complete the review of probability distributions commonly used in finance, we \\ndiscuss Student’s t-, chi-square, and F-distributions. Most of the time, these distribu-\\ntions are used to support statistical analyses, such as sampling, testing the statistical \\nsignificance of estimated model parameters, or hypothesis testing. In addition, Student’s \\nt-distribution is also sometimes used to model asset returns in a manner similar to \\nthat of the normal distribution. However, since the t-distribution has “longer tails,” it \\nmay provide a more reliable, more conservative downside risk estimate.\\nThe standard t-distribution is a symmetrical probability distribution defined by \\na single parameter known as degrees of freedom (df), the number of independent \\nvariables used in defining sample statistics, such as variance, and the probability \\ndistributions they measure.\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Student’s t-, Chi-Square, and F-Distributions',\n",
       "     'page_number': 282,\n",
       "     'content': 'Learning Module 4 \\nCommon Probability Distributions\\n272\\nmean can be translated into an estimate of the expected continuously com-\\npounded annual return using Equation 8:  ˆ \\nμ  T = − 0.022572  ( 250 )  (using 250 \\nto be consistent with the calculation of volatility). But four observations are \\nfar too few to estimate expected returns. The variability in the daily returns \\noverwhelms any information about expected return in a series this short.\\n2. Identify the probability distribution for Astra share prices if continuously \\ncompounded daily returns follow the normal distribution.\\nSolution to 2:\\nAstra share prices should follow the lognormal distribution if the continuously \\ncompounded daily returns on Astra shares follow the normal distribution.\\nWe have shown that the distribution of stock price is lognormal, given certain \\nassumptions. What are the mean and variance of ST if ST follows the lognormal \\ndistribution? Earlier we gave bullet-point expressions for the mean and variance of a \\nlognormal random variable. In the bullet-point expressions, the    ˆ \\nμ  \\xa0and\\xa0    ˆ \\nσ     2   would refer, \\nin the context of this discussion, to the mean and variance of the T horizon (not the \\none-period) continuously compounded returns (assumed to follow a normal distri-\\nbution), compatible with the horizon of ST. Related to the use of mean and variance \\n(or standard deviation), previously we used those quantities to construct intervals in \\nwhich we expect to find a certain percentage of the observations of a normally dis-\\ntributed random variable. Those intervals were symmetric about the mean. Can we \\nstate similar symmetric intervals for a lognormal random variable? Unfortunately, we \\ncannot; because the lognormal distribution is not symmetric, such intervals are more \\ncomplicated than for the normal distribution, and we will not discuss this specialist \\ntopic here.\\nSTUDENT’S T-, CHI-SQUARE, AND F-DISTRIBUTIONS\\ndescribe the properties of the Student’s t-distribution, and calculate \\nand interpret its degrees of freedom\\ndescribe the properties of the chi-square distribution and the \\nF-distribution, and calculate and interpret their degrees of freedom\\nStudent’s t-Distribution \\nTo complete the review of probability distributions commonly used in finance, we \\ndiscuss Student’s t-, chi-square, and F-distributions. Most of the time, these distribu-\\ntions are used to support statistical analyses, such as sampling, testing the statistical \\nsignificance of estimated model parameters, or hypothesis testing. In addition, Student’s \\nt-distribution is also sometimes used to model asset returns in a manner similar to \\nthat of the normal distribution. However, since the t-distribution has “longer tails,” it \\nmay provide a more reliable, more conservative downside risk estimate.\\nThe standard t-distribution is a symmetrical probability distribution defined by \\na single parameter known as degrees of freedom (df), the number of independent \\nvariables used in defining sample statistics, such as variance, and the probability \\ndistributions they measure.\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': [{'title': 'Student’s t-Distribution ',\n",
       "       'page_number': 282,\n",
       "       'content': 'Learning Module 4 \\nCommon Probability Distributions\\n272\\nmean can be translated into an estimate of the expected continuously com-\\npounded annual return using Equation 8:  ˆ \\nμ  T = − 0.022572  ( 250 )  (using 250 \\nto be consistent with the calculation of volatility). But four observations are \\nfar too few to estimate expected returns. The variability in the daily returns \\noverwhelms any information about expected return in a series this short.\\n2. Identify the probability distribution for Astra share prices if continuously \\ncompounded daily returns follow the normal distribution.\\nSolution to 2:\\nAstra share prices should follow the lognormal distribution if the continuously \\ncompounded daily returns on Astra shares follow the normal distribution.\\nWe have shown that the distribution of stock price is lognormal, given certain \\nassumptions. What are the mean and variance of ST if ST follows the lognormal \\ndistribution? Earlier we gave bullet-point expressions for the mean and variance of a \\nlognormal random variable. In the bullet-point expressions, the    ˆ \\nμ  \\xa0and\\xa0    ˆ \\nσ     2   would refer, \\nin the context of this discussion, to the mean and variance of the T horizon (not the \\none-period) continuously compounded returns (assumed to follow a normal distri-\\nbution), compatible with the horizon of ST. Related to the use of mean and variance \\n(or standard deviation), previously we used those quantities to construct intervals in \\nwhich we expect to find a certain percentage of the observations of a normally dis-\\ntributed random variable. Those intervals were symmetric about the mean. Can we \\nstate similar symmetric intervals for a lognormal random variable? Unfortunately, we \\ncannot; because the lognormal distribution is not symmetric, such intervals are more \\ncomplicated than for the normal distribution, and we will not discuss this specialist \\ntopic here.\\nSTUDENT’S T-, CHI-SQUARE, AND F-DISTRIBUTIONS\\ndescribe the properties of the Student’s t-distribution, and calculate \\nand interpret its degrees of freedom\\ndescribe the properties of the chi-square distribution and the \\nF-distribution, and calculate and interpret their degrees of freedom\\nStudent’s t-Distribution \\nTo complete the review of probability distributions commonly used in finance, we \\ndiscuss Student’s t-, chi-square, and F-distributions. Most of the time, these distribu-\\ntions are used to support statistical analyses, such as sampling, testing the statistical \\nsignificance of estimated model parameters, or hypothesis testing. In addition, Student’s \\nt-distribution is also sometimes used to model asset returns in a manner similar to \\nthat of the normal distribution. However, since the t-distribution has “longer tails,” it \\nmay provide a more reliable, more conservative downside risk estimate.\\nThe standard t-distribution is a symmetrical probability distribution defined by \\na single parameter known as degrees of freedom (df), the number of independent \\nvariables used in defining sample statistics, such as variance, and the probability \\ndistributions they measure.\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\nStudent’s t-, Chi-Square, and F-Distributions\\n273\\nEach value for the number of degrees of freedom defines one distribution in this \\nfamily of distributions. We will shortly compare t-distributions with the standard nor-\\nmal distribution, but first we need to understand the concept of degrees of freedom. \\nWe can do so by examining the calculation of the sample variance,\\n s 2 =  \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  ) \\n_ 2   \\nn − 1 \\n . \\n(10)\\nEquation 10 gives the unbiased estimator of the sample variance that we use. The term \\nin the denominator, n − 1, which is the sample size minus 1, is the number of degrees \\nof freedom in estimating the population variance when using Equation 10. We also use \\nn − 1 as the number of degrees of freedom for determining reliability factors based \\non the t-distribution. The term “degrees of freedom” is used because in a random \\nsample, we assume that observations are selected independently of each other. The \\nnumerator of the sample variance, however, uses the sample mean. How does the use \\nof the sample mean affect the number of observations collected independently for the \\nsample variance formula? With a sample size of 10 and a mean of 10%, for example, \\nwe can freely select only 9 observations. Regardless of the 9 observations selected, \\nwe can always find the value for the 10th observation that gives a mean equal to 10%. \\nFrom the standpoint of the sample variance formula, then, there are nine degrees \\nof freedom. Given that we must first compute the sample mean from the total of n \\nindependent observations, only n − 1 observations can be chosen independently for \\nthe calculation of the sample variance. The concept of degrees of freedom comes up \\nfrequently in statistics, and you will see it often later in the CFA Program curriculum.\\nSuppose we sample from a normal distribution. The ratio  z =   ( _\\n \\nX  − μ )   /    ( σ /  √ _ \\nn \\n)  is distributed normally with a mean of 0 and standard deviation of 1; however, \\nthe ratio  t =   ( _\\n \\nX  − μ )   /    ( s /  √ _ \\nn  )  follows the t-distribution with a mean of 0 and n − 1 \\ndegrees of freedom. The ratio represented by t is not normal because t is the ratio \\nof two random variables, the sample mean and the sample standard deviation. The \\ndefinition of the standard normal random variable involves only one random vari-\\nable, the sample mean. As degrees of freedom increase (i.e., as sample size increases), \\nhowever, the t-distribution approaches the standard normal distribution. Exhibit 15 \\nshows the probability density functions for the standard normal distribution and two \\nt-distributions, one with df = 2 and one with df = 8.\\nExhibit 15: Student’s t-Distributions vs. Standard Normal Distribution\\nNormal Distribution\\nt (df = 2)\\nt (df = 8)\\n–6\\n–6\\n6\\n–2\\n–2\\n–4\\n–4\\n4\\n2\\n0\\nOf the three distributions shown in Exhibit 15, the standard normal distribution has \\ntails that approach zero faster than the tails of the two t-distributions. The t-distribution \\nis also symmetrically distributed around its mean value of zero, just like the normal \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n274\\ndistribution. As the degrees of freedom increase, the t-distribution approaches the \\nstandard normal distribution. The t-distribution with df = 8 is closer to the standard \\nnormal distribution than the t-distribution with df = 2.\\nBeyond plus and minus four standard deviations from the mean, the area under \\nthe standard normal distribution appears to approach 0; both t-distributions, however, \\ncontinue to show some area under each curve beyond four standard deviations. The \\nt-distributions have fatter tails, but the tails of the t-distribution with df = 8 more \\nclosely resemble the normal distribution’s tails. As the degrees of freedom increase, \\nthe tails of the t-distribution become less fat.\\nProbabilities for the t-distribution can be readily computed with spreadsheets, \\nstatistical software, and programming languages. As an example of the latter, see the \\nfinal sidebar at the end of this section for sample code in the R programming language.\\nChi-Square and F-Distribution\\nThe chi-square distribution, unlike the normal and t-distributions, is asymmetrical. \\nLike the t-distribution, the chi-square distribution is a family of distributions. The \\nchi-square distribution with k degrees of freedom is the distribution of the sum of \\nthe squares of k independent standard normally distributed random variables; hence, \\nthis distribution does not take on negative values. A different distribution exists for \\neach possible value of degrees of freedom, n − 1 (n is sample size).\\nLike the chi-square distribution, the F-distribution is a family of asymmetrical \\ndistributions bounded from below by 0. Each F-distribution is defined by two values \\nof degrees of freedom, called the numerator and denominator degrees of freedom.\\nThe relationship between the chi-square and F-distributions is as follows: If   χ 1 2 \\nis one chi-square random variable with m degrees of freedom and   χ  2  2   is another \\nchi-square random variable with n degrees of freedom, then  F =   ( χ 1 2 / m )  /   ( χ 2 2 / n ) \\nfollows an F-distribution with m numerator and n denominator degrees of freedom.\\nChi-square and F-distributions are asymmetric, and as shown in Exhibit 16, the \\ndomain of their pdfs are positive numbers. Like Student’s t-distribution, as the degrees \\nof freedom of the chi-square distribution increase, the shape of its pdf becomes more \\nsimilar to a bell curve (see Panel A). For the F-distribution, as both the numerator \\n(df1) and the denominator (df2) degrees of freedom increase, the density function will \\nalso become more bell curve–like (see Panel B).\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Chi-Square and F-Distribution',\n",
       "       'page_number': 284,\n",
       "       'content': 'Chi-Square and F-Distribution\\nThe chi-square distribution, unlike the normal and t-distributions, is asymmetrical. \\nLike the t-distribution, the chi-square distribution is a family of distributions. The \\nchi-square distribution with k degrees of freedom is the distribution of the sum of \\nthe squares of k independent standard normally distributed random variables; hence, \\nthis distribution does not take on negative values. A different distribution exists for \\neach possible value of degrees of freedom, n − 1 (n is sample size).\\nLike the chi-square distribution, the F-distribution is a family of asymmetrical \\ndistributions bounded from below by 0. Each F-distribution is defined by two values \\nof degrees of freedom, called the numerator and denominator degrees of freedom.\\nThe relationship between the chi-square and F-distributions is as follows: If   χ 1 2 \\nis one chi-square random variable with m degrees of freedom and   χ  2  2   is another \\nchi-square random variable with n degrees of freedom, then  F =   ( χ 1 2 / m )  /   ( χ 2 2 / n ) \\nfollows an F-distribution with m numerator and n denominator degrees of freedom.\\nChi-square and F-distributions are asymmetric, and as shown in Exhibit 16, the \\ndomain of their pdfs are positive numbers. Like Student’s t-distribution, as the degrees \\nof freedom of the chi-square distribution increase, the shape of its pdf becomes more \\nsimilar to a bell curve (see Panel A). For the F-distribution, as both the numerator \\n(df1) and the denominator (df2) degrees of freedom increase, the density function will \\nalso become more bell curve–like (see Panel B).\\n© CFA Institute. For candidate use only. Not for distribution.\\nStudent’s t-, Chi-Square, and F-Distributions\\n275\\nExhibit 16: PDFs of Chi-Square and F-Distributions\\nA. Chi-Square Distributions\\n0.014\\n0.014\\n0.012\\n0.012\\n0.010\\n0.010\\n0.008\\n0.008\\n0.006\\n0.006\\n0.004\\n0.004\\n0.002\\n0.002\\n0\\n1.40\\n1.40\\n0.35\\n0.35\\n2.45\\n2.45\\n3.50\\n3.50\\n4.55\\n4.55\\n5.60\\n5.60\\n6.65\\n6.65\\n7.70\\n7.70\\n8.75\\n8.75\\n9.80\\n9.80\\nChi-Square Dist (df = 2)\\nChi-Square Dist (df = 3)\\nChi-Square Dist (df = 5)\\nChi-Square Dist (df = 7)\\nB. F-Distributions\\n0.06\\n0.06\\n0.05\\n0.05\\n0.04\\n0.04\\n0.03\\n0.03\\n0.02\\n0.02\\n0.01\\n0.01\\n0\\n0.63\\n0.63\\n0.13\\n0.13\\n1.13\\n1.13\\n1.63\\n1.63\\n2.13\\n2.13\\n2.62\\n2.62\\nF-Dist (df1 = 3, df2 = 10)\\nF-Dist (df1 = 5, df2 = 20)\\nF-Dist (df1 = 10, df2 = 50)\\nF-Dist (df1 = 50, df2 = 1,000)\\nAs for typical investment applications, Student’s t, chi-square, and F-distributions are \\nthe basis for test statistics used in performing various types of hypothesis tests on \\nportfolio returns, such as those summarized in Exhibit 17.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n276\\nExhibit 17: Student’s t, Chi-Square, and F-Distributions: Basis for \\nHypothesis Tests of Investment Returns\\nDistribution\\nTest Statistic\\nHypothesis Tests of Returns\\nStudent’s t\\nt-Statistic\\nTests of a single population mean, of differences \\nbetween two population means, of mean difference \\nbetween paired (dependent) populations, and of pop-\\nulation correlation coefficient\\nChi-square\\nChi-square \\nstatistic\\nTest of variance of a normally distributed population\\nF\\nF-statistic\\nTest of equality of variances of two normally dis-\\ntributed populations from two independent random \\nsamples\\nEXAMPLE 9\\nProbabilities Using Student’s-t, Chi-Square, and \\nF-Distributions\\n1. Of the distributions we have covered in this reading, which can take values \\nthat are only positive numbers (i.e., no negative values)?\\nSolution to 1:\\nOf the probability distributions covered in this reading, the domains of the \\npdfs of the lognormal, the chi-square, and the F-distribution are only posi-\\ntive numbers.\\n2. Interpret the degrees of freedom for a chi-square distribution, and describe \\nhow a larger value of df affects the shape of the chi-square pdf.\\nSolution to 2:\\nA chi-square distribution with k degrees of freedom is the distribution of \\nthe sum of the squares of k independent standard normally distributed ran-\\ndom variables. The greater the degrees of freedom, the more symmetrical \\nand bell curve–like the pdf becomes.\\n3. Generate cdf tables in Excel for values 1, 2, and 3 for the following dis-\\ntributions: standard normal, Student’s t- (df = 5), chi-square (df = 5), and \\nF-distribution (df1 = 5, df2 = 1). Then, calculate the distance from the mean \\nfor probability (p) = 90%, 95%, and 99% for each distribution.\\nSolution to 3:\\nIn Excel, we can calculate cdfs using the NORM.S.DIST(value,1), \\nT.DIST(value,DF,1), CHISQ.DIST(value,DF,1), and F.DIST(value,D-\\nF1,DF2,1) functions for the standard normal, Student’s t-, chi-square, and \\nF-distributions, respectively. At the end of this question set, we also show \\ncode snippets in the R language for generating cdfs for the requested values. \\nFor values 1, 2, and 3, the following are the results using the Excel functions:\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nStudent’s t-, Chi-Square, and F-Distributions\\n277\\nCDF Values Using Different Probability Distributions\\n \\n \\nValue\\nNormal\\nStudent’s t \\n(df = 5)\\nChi-Square \\n(df = 5)\\nF \\n(df1 = 5, \\ndf2 = 1)\\n1\\n84.1%\\n81.8%\\n3.7%\\n36.3%\\n2\\n97.7%\\n94.9%\\n15.1%\\n51.1%\\n3\\n99.9%\\n98.5%\\n30.0%\\n58.9%\\n \\nTo calculate distances from the mean given probability p, we must use \\nthe inverse of the distribution functions: NORM.S.INV(p), T.INV(p,DF), \\nCHISQ.INV(p,DF), and F.INV(p,DF1,DF2), respectively. At the end of this \\nquestion set, we also show code snippets in the R language for calculating \\ndistances from the mean for the requested probabilities. The results using \\nthe inverse functions and the requested probabilities are as follows:\\n \\nDistance from the Mean for a Given Probability (p)\\n \\n \\nProbability\\nNormal\\nStudent’s t \\n(df = 5)\\nChi-Square \\n(df = 5)\\nF \\n(df1 = 5, \\ndf2 = 1)\\n90%\\n1.28\\n1.48\\n9.24\\n57.24\\n95%\\n1.64\\n2.02\\n11.07\\n230.16\\n99%\\n2.33\\n3.36\\n15.09\\n5,763.65\\n \\n4. You fit a Student’s t-distribution to historically observed returns of stock \\nmarket index ABC. Your best fit comes with five degrees of freedom. Com-\\npare this Student’s t-distribution (df = 5) to a standard normal distribution \\non the basis of your answer to Question 3.\\nSolution to 4:\\nStudent’s t-distribution with df of 5 has longer tails than the standard nor-\\nmal distribution. For probabilities 90%, 95%, and 99%, such t-distributed \\nrandom variables would fall farther away from their mean (1.48, 2.02, and \\n3.36 standard deviations, respectively) than a normally distributed random \\nvariable (1.28, 1.64, and 2.33 standard deviations, respectively).\\nR CODE FOR PROBABILITIES INVOLVING STUDENT’S T-, CHI-SQUARE, AND \\nF-DISTRIBUTIONS\\nFor those of you with a knowledge of (or interest in learning) readily accessible computer \\ncode to find probabilities involving Student’s t-, chi-square, and F-distributions, you can try \\nout the following program. Specifically, this program uses code in the R language to solve \\nfor the answers to Example 9, which you have just completed. Good luck and have fun!\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n278\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Monte Carlo Simulation',\n",
       "     'page_number': 289,\n",
       "     'content': 'Monte Carlo Simulation\\n279\\nMONTE CARLO SIMULATION\\ndescribe Monte Carlo simulation\\nAfter gaining an understanding of probability distributions, we now learn about a \\ntechnique in which probability distributions play an integral role. The technique is \\ncalled Monte Carlo simulation, and in finance it involves the use of computer soft-\\nware to represent the operation of a complex financial system. A characteristic feature \\nof Monte Carlo simulation is the generation of a large number of random samples \\nfrom a specified probability distribution or distributions to represent the role of risk \\nin the system.\\nMonte Carlo simulation is widely used to estimate risk and return in investment \\napplications. In this setting, we simulate the portfolio’s profit and loss performance \\nfor a specified time horizon. Repeated trials within the simulation (each trial involving \\na draw of random observations from a probability distribution) produce a simulated \\nfrequency distribution of portfolio returns from which performance and risk mea-\\nsures are derived.\\n8\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n280\\nAnother important use of Monte Carlo simulation in investments is as a tool for \\nvaluing complex securities for which no analytic pricing formula is available. For \\nother securities, such as mortgage-backed securities with complex embedded options, \\nMonte Carlo simulation is also an important modeling resource. Since we control the \\nassumptions when we carry out a simulation, we can run a model for valuing such \\nsecurities through a Monte Carlo simulation to examine the model’s sensitivity to a \\nchange in key assumptions.\\nTo understand the technique of Monte Carlo simulation, we present the process as \\na series of steps; these can be viewed as providing an overview rather than a detailed \\nrecipe for implementing a Monte Carlo simulation in its many varied applications. \\nTo illustrate the steps, we use Monte Carlo simulation to value a contingent claim \\nsecurity (a security whose value is based on some other underlying security) for which \\nno analytic pricing formula is available. For our purposes, such a contingent claim \\nsecurity has a value at its maturity equal to the difference between the underlying stock \\nprice at that maturity and the average stock price during the life of the contingent \\nclaim or $0, whichever is greater. For instance, if the final underlying stock price is \\n$34 and the average value over the life of the claim is $31, the value of the contingent \\nclaim at its maturity is $3 (the greater of $34 − $31 = $3 and $0).\\nAssume that the maturity of the claim is one year from today; we will simulate \\nstock prices in monthly steps over the next 12 months and will generate 1,000 sce-\\nnarios to evaluate this claim. The payoff diagram of this contingent claim security is \\ndepicted in Panel A of Exhibit 18, a histogram of simulated average and final stock \\nprices is shown in Panel B, and a histogram of simulated payoffs of the contingent \\nclaim is presented in Panel C.\\nThe payoff diagram (Panel A) is a snapshot of the contingent claim at maturity. If \\nthe stock’s final price is less than or equal to its average over the life of the contingent \\nclaim, then the payoff would be zero. However, if the final price exceeds the average \\nprice, the payoff is equal to this difference. Panel B shows histograms of the simulated \\nfinal and average stock prices. Note that the simulated final price distribution is wider \\nthan the simulated average price distribution. Also, note that the contingent claim’s \\nvalue depends on the difference between the final and average stock prices, which \\ncannot be directly inferred from these histograms.\\n© CFA Institute. For candidate use only. Not for distribution.\\nMonte Carlo Simulation\\n281\\nExhibit 18: Payoff Diagram, Histogram of Simulated Average and Final \\nStock Prices, and Histogram of Simulated Payoffs for Contingent Claim\\nPayoff (USD)\\nA. Contingent Claim Payoff Diagram\\n35\\n35\\n30\\n30\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\n–30\\n–30\\n30\\n30\\n–20\\n–20\\n–10\\n–10\\n10\\n10\\n20\\n20\\n0\\nFinal Price Minus Average Price (USD)\\nAverage Stock Price\\nFinal Stock Price\\nNumber of Trials\\nB. Histogram of Simulated Average and Final Stock Prices\\n250\\n250\\n200\\n200\\n150\\n150\\n100\\n100\\n50\\n50\\n0\\n20.7\\n20.7\\n16.0\\n16.0\\n25.5\\n25.5\\n34.9\\n34.9\\n39.7\\n39.7\\n44.4\\n44.4\\n30.2\\n30.2\\nStock Price (USD)\\nNumber of Trials\\nC. Histogram of Simulated Contingent Claim Payoffs\\n700\\n700\\n500\\n500\\n600\\n600\\n400\\n400\\n300\\n300\\n100\\n100\\n200\\n200\\n0\\n1.7\\n1.7\\n0\\n3.5\\n3.5\\n6.9\\n6.9\\n8.7\\n8.7\\n10.4\\n10.4\\n5.2\\n5.2\\nContingent Claim Payoff (USD)\\nFinally, Panel C shows the histogram of the contingent claim’s simulated payoffs. In \\n654 of 1,000 total trials, the final stock price was less than or equal to the average \\nprice, so in 65.4% of the trials the contingent claim paid off zero. In the remaining \\n34.6% of the trials, however, the claim paid the positive difference between the final \\nand average prices, with the maximum payoff being $11.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n282\\nThe process flowchart in Exhibit 19 shows the steps for implementing the Monte \\nCarlo simulation for valuing this contingent claim. Steps 1 through 3 of the process \\ndescribe specifying the simulation; Steps 4 through 7 describe running the simulation.\\nExhibit 19: Steps in Implementing the Monte Carlo Simulation\\nStep 1: Specify the quantity of interest; (e.g., value \\nof the contingent claim).\\nSpecify the\\nsimulation\\nRun the\\nsimulation\\nover the\\nspecified\\nnumber of\\ntrials\\nStep 2: Specify a time grid: K sub-periods with Δt \\nincrement for the full time horizon.\\nStep 3: Specify distributional assumptions \\nfor the key risk factors.\\nStep 4: Draw standard normal random numbers for \\neach key risk factor over each of the K sub-periods.\\nStep 5: Convert the standard normal random numbers \\nto stock prices, average stock price, and other \\nrelevant risk factors.\\nStep 6: Calculate the value and the present \\nvalue of the contingent claim payoff.\\nStep 7: Repeat Steps 4-6 over the specified number of \\ntrials. Then, calculate ',\n",
       "     'children': []},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 295,\n",
       "     'content': 'SUMMARY\\nIn this reading, we have presented the most frequently used probability distributions \\nin investment analysis and Monte Carlo simulation.\\n■ \\nA probability distribution specifies the probabilities of the possible out-\\ncomes of a random variable.\\n■ \\nThe two basic types of random variables are discrete random variables and \\ncontinuous random variables. Discrete random variables take on at most \\na countable number of possible outcomes that we can list as x1, x2, . . . . In \\ncontrast, we cannot describe the possible outcomes of a continuous random \\nvariable Z with a list z1, z2, . . ., because the outcome (z1 + z2)/2, not in the \\nlist, would always be possible.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n286\\n■ \\nThe probability function specifies the probability that the random variable \\nwill take on a specific value. The probability function is denoted p(x) for a \\ndiscrete random variable and f(x) for a continuous random variable. For any \\nprobability function p(x), 0 ≤ p(x) ≤ 1, and the sum of p(x) over all values of \\nX equals 1.\\n■ \\nThe cumulative distribution function, denoted F(x) for both continuous and \\ndiscrete random variables, gives the probability that the random variable is \\nless than or equal to x.\\n■ \\nThe discrete uniform and the continuous uniform distributions are the dis-\\ntributions of equally likely outcomes.\\n■ \\nThe binomial random variable is defined as the number of successes in n \\nBernoulli trials, where the probability of success, p, is constant for all trials \\nand the trials are independent. A Bernoulli trial is an experiment with two \\noutcomes, which can represent success or failure, an up move or a down \\nmove, or another binary (twofold) outcome.\\n■ \\nA binomial random variable has an expected value or mean equal to np and \\nvariance equal to np(1 − p).\\n■ \\nA binomial tree is the graphical representation of a model of asset price \\ndynamics in which, at each period, the asset moves up with probability p \\nor down with probability (1 − p). The binomial tree is a flexible method for \\nmodeling asset price movement and is widely used in pricing options.\\n■ \\nThe normal distribution is a continuous symmetric probability distribution \\nthat is completely described by two parameters: its mean, μ, and its vari-\\nance, σ2.\\n■ \\nA univariate distribution specifies the probabilities for a single random \\nvariable. A multivariate distribution specifies the probabilities for a group of \\nrelated random variables.\\n■ \\nTo specify the normal distribution for a portfolio when its component secu-\\nrities are normally distributed, we need the means, the standard deviations, \\nand all the distinct pairwise correlations of the securities. When we have \\nthose statistics, we have also specified a multivariate normal distribution for \\nthe securities.\\n■ \\nFor a normal random variable, approximately 68% of all possible outcomes \\nare within a one standard deviation interval about the mean, approximately \\n95% are within a two standard deviation interval about the mean, and \\napproximately 99% are within a three standard deviation interval about the \\nmean.\\n■ \\nA normal random variable, X, is standardized using the expression Z = (X \\n− μ)/σ, where μ and σ are the mean and standard deviation of X. Generally, \\nwe use the sample mean,   _\\n \\nX  , as an estimate of μ and the sample standard \\ndeviation, s, as an estimate of σ in this expression.\\n■ \\nThe standard normal random variable, denoted Z, has a mean equal to 0 and \\nvariance equal to 1. All questions about any normal random variable can be \\nanswered by referring to the cumulative distribution function of a standard \\nnormal random variable, denoted N(x) or N(z).\\n■ \\nShortfall risk is the risk that portfolio value or portfolio return will fall \\nbelow some minimum acceptable level over some time horizon.\\n■ \\nRoy’s safety-first criterion, addressing shortfall risk, asserts that the optimal \\nportfolio is the one that minimizes the probability that portfolio return falls \\nbelow a threshold level. According to Roy’s safety-first criterion, if returns \\n© CFA Institute. For candidate use only. Not for distribution.\\nMonte Carlo Simulation\\n287\\nare normally distributed, the safety-first optimal portfolio P is the one that \\nmaximizes the quantity [E(RP) − RL]/σP, where RL is the minimum accept-\\nable level of return.\\n■ \\nA random variable follows a lognormal distribution if the natural logarithm \\nof the random variable is normally distributed. The lognormal distribution is \\ndefined in terms of the mean and variance of its associated normal distribu-\\ntion. The lognormal distribution is bounded below by 0 and skewed to the \\nright (it has a long right tail).\\n■ \\nThe lognormal distribution is frequently used to model the probability dis-\\ntribution of asset prices because it is bounded below by zero.\\n■ \\nContinuous compounding views time as essentially continuous or unbroken; \\ndiscrete compounding views time as advancing in discrete finite intervals.\\n■ \\nThe continuously compounded return associated with a holding period is \\nthe natural log of 1 plus the holding period return, or equivalently, the natu-\\nral log of ending price over beginning price.\\n■ \\nIf continuously compounded returns are normally distributed, asset prices \\nare lognormally distributed. This relationship is used to move back and \\nforth between the distributions for return and price. Because of the central \\nlimit theorem, continuously compounded returns need not be normally \\ndistributed for asset prices to be reasonably well described by a lognormal \\ndistribution.\\n■ \\nStudent’s t-, chi-square, and F-distributions are used to support statistical \\nanalyses, such as sampling, testing the statistical significance of estimated \\nmodel parameters, or hypothesis testing.\\n■ \\nThe standard t-distribution is a symmetrical probability distribution defined \\nby degrees of freedom (df) and characterized by fat tails. As df increase, the \\nt-distribution approaches the standard normal distribution.\\n■ \\nThe chi-square distribution is asymmetrical, defined by degrees of freedom, \\nand with k df is the distribution of the sum of the squares of k independent \\nstandard normally distributed random variables, so it does not take on nega-\\ntive values. A different distribution exists for each value of df, n − 1.\\n■ \\nThe F-distribution is a family of asymmetrical distributions bounded from \\nbelow by 0. Each F-distribution is defined by two values of degrees of free-\\ndom, the numerator df and the denominator df. If   χ 1 2 is one chi-square ran-\\ndom variable with m df and   χ 2 2 is another chi-square random variable with n \\ndf, then  F =   ( χ 1 2 / m )  /   ( χ 2 2 / n )  follows an F-distribution with m numerator \\ndf and n denominator df.\\n■ \\nMonte Carlo simulation involves the use of a computer to represent the \\noperation of a complex financial system. A characteristic feature of Monte \\nCarlo simulation is the generation of a large number of random samples \\nfrom specified probability distributions to represent the operation of risk \\nin the system. Monte Carlo simulation is used in planning, in financial risk \\nmanagement, and in valuing complex securities. Monte Carlo simulation is \\na complement to analytical methods but provides only statistical estimates, \\nnot exact results.\\n■ \\nRandom observations from any distribution can be produced using the \\nuniform random variable with endpoints 0 and 1 via the inverse transforma-\\ntion method. The randomly generated uniform random number is mapped \\nonto the inverted cdf of any distribution from which random observations \\nare desired. The point on the given distribution’s cdf is then mapped onto its \\npdf, and the random observation is identified.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n288\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 298,\n",
       "     'content': 'PRACTICE PROBLEMS\\n1. A European put option on stock conveys the right to sell the stock at a prespeci-\\nfied price, called the exercise price, at the maturity date of the option. The value \\nof this put at maturity is (exercise price – stock price) or $0, whichever is greater. \\nSuppose the exercise price is $100 and the underlying stock trades in increments \\nof $0.01. At any time before maturity, the terminal value of the put is a random \\nvariable.\\nA. Describe the distinct possible outcomes for terminal put value. (Think of the \\nput’s maximum and minimum values and its minimum price increments.)\\nB. Is terminal put value, at a time before maturity, a discrete or continuous \\nrandom variable?\\nC. Letting Y stand for terminal put value, express in standard notation the \\nprobability that terminal put value is less than or equal to $24. No calcula-\\ntions or formulas are necessary.\\n2. Which of the following is a continuous random variable?\\nA. The value of a futures contract quoted in increments of $0.05\\nB. The total number of heads recorded in 1 million tosses of a coin\\nC. The rate of return on a diversified portfolio of stocks over a three-month \\nperiod\\n3. X is a discrete random variable with possible outcomes X = {1, 2, 3, 4}. Three \\nfunctions—f(x), g(x), and h(x)—are proposed to describe the probabilities of the \\noutcomes in X.\\n\\xa0\\nProbability Function\\nX = x\\nf(x) = P(X = x)\\ng(x) = P(X = x)\\nh(x) = P(X = x)\\n1\\n−0.25\\n0.20\\n0.20\\n2\\n0.25\\n0.25\\n0.25\\n3\\n0.50\\n0.50\\n0.30\\n4\\n0.25\\n0.05\\n0.35\\nThe conditions for a probability function are satisfied by:\\nA. f(x).\\nB. g(x).\\nC. h(x).\\n4. The value of the cumulative distribution function F(x), where x is a particular \\noutcome, for a discrete uniform distribution:\\nA. sums to 1.\\nB. lies between 0 and 1.\\nC. decreases as x increases.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n289\\n5. In a discrete uniform distribution with 20 potential outcomes of integers 1–20, \\nthe probability that X is greater than or equal to 3 but less than 6, P(3 ≤ X < 6), is:\\nA. 0.10.\\nB. 0.15.\\nC. 0.20.\\n6. You are forecasting sales for a company in the fourth quarter of its fiscal year. \\nYour low-end estimate of sales is €14 million, and your high-end estimate is €15 \\nmillion. You decide to treat all outcomes for sales between these two values as \\nequally likely, using a continuous uniform distribution.\\nA. What is the expected value of sales for the fourth quarter?\\nB. What is the probability that fourth-quarter sales will be less than or equal to \\n€14,125,000?\\n7. The cumulative distribution function for a discrete random variable is shown in \\nthe following table.\\nX = x\\nCumulative Distribution Function \\nF(x) = P(X ≤ x)\\n1\\n0.15\\n2\\n0.25\\n3\\n0.50\\n4\\n0.60\\n5\\n0.95\\n6\\n1.00\\nThe probability that X will take on a value of either 2 or 4 is closest to:\\nA. 0.20.\\nB. 0.35.\\nC. 0.85.\\n8. A random number between zero and one is generated according to a continuous \\nuniform distribution. What is the probability that the first number generated will \\nhave a value of exactly 0.30?\\nA. 0%\\nB. 30%\\nC. 70%\\n9. Define the term “binomial random variable.” Describe the types of problems for \\nwhich the binomial distribution is used.\\n10. For a binomial random variable with five trials and a probability of success on \\neach trial of 0.50, the distribution will be:\\nA. skewed.\\nB. uniform.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n290\\nC. symmetric.\\n11. Over the last 10 years, a company’s annual earnings increased year over year \\nseven times and decreased year over year three times. You decide to model the \\nnumber of earnings increases for the next decade as a binomial random variable.\\nFor Parts B, C, and D of this problem, assume the estimated probability is the \\nactual probability for the next decade.\\nA. What is your estimate of the probability of success, defined as an increase in \\nannual earnings?\\nB. What is the probability that earnings will increase in exactly 5 of the next 10 \\nyears?\\nC. Calculate the expected number of yearly earnings increases during the next \\n10 years.\\nD. Calculate the variance and standard deviation of the number of yearly earn-\\nings increases during the next 10 years.\\nE. The expression for the probability function of a binomial random variable \\ndepends on two major assumptions. In the context of this problem, what \\nmust you assume about annual earnings increases to apply the binomial \\ndistribution in Part B? What reservations might you have about the validity \\nof these assumptions?\\n12. A portfolio manager annually outperforms her benchmark 60% of the time. \\nAssuming independent annual trials, what is the probability that she will outper-\\nform her benchmark four or more times over the next five years?\\nA. 0.26\\nB. 0.34\\nC. 0.48\\n13. You are examining the record of an investment newsletter writer who claims a \\n70% success rate in making investment recommendations that are profitable over \\na one-year time horizon. You have the one-year record of the newsletter’s seven \\nmost recent recommendations. Four of those recommendations were profitable. \\nIf all the recommendations are independent and the newsletter writer’s skill is as \\nclaimed, what is the probability of observing four or fewer profitable recommen-\\ndations out of seven in total?\\n14. If the probability that a portfolio outperforms its benchmark in any quarter is \\n0.75, the probability that the portfolio outperforms its benchmark in three or \\nfewer quarters over the course of a year is closest to:\\nA. 0.26\\nB. 0.42\\nC. 0.68\\n15. Which of the following events can be represented as a Bernoulli trial?\\nA. The flip of a coin\\nB. The closing price of a stock\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n291\\nC. The picking of a random integer between 1 and 10\\n16. A stock is priced at $100.00 and follows a one-period binomial process with an \\nup move that equals 1.05 and a down move that equals 0.97. If 1 million Bernoulli \\ntrials are conducted and the average terminal stock price is $102.00, the probabil-\\nity of an up move (p) is closest to:\\nA. 0.375.\\nB. 0.500.\\nC. 0.625.\\n17. A call option on a stock index is valued using a three-step binomial tree with an \\nup move that equals 1.05 and a down move that equals 0.95. The current level of \\nthe index is $190, and the option exercise price is $200. If the option value is posi-\\ntive when the stock price exceeds the exercise price at expiration and $0 other-\\nwise, the number of terminal nodes with a positive payoff is:\\nA. one.\\nB. two.\\nC. three.\\n18. State the approximate probability that a normal random variable will fall within \\nthe following intervals:\\nA. Mean plus or minus one standard deviation.\\nB. Mean plus or minus two standard deviations.\\nC. Mean plus or minus three standard deviations.\\n19. In futures markets, profits or losses on contracts are settled at the end of each \\ntrading day. This procedure is called marking to market or daily resettlement. \\nBy preventing a trader’s losses from accumulating over many days, marking to \\nmarket reduces the risk that traders will default on their obligations. A futures \\nmarkets trader needs a liquidity pool to meet the daily mark to market. If liquidi-\\nty is exhausted, the trader may be forced to unwind his position at an unfavorable \\ntime.\\nSuppose you are using financial futures contracts to hedge a risk in your portfo-\\nlio. You have a liquidity pool (cash and cash equivalents) of λ dollars per contract \\nand a time horizon of T trading days. For a given size liquidity pool, λ, Kolb, \\nGay, and Hunter developed an expression for the probability stating that you will \\nexhaust your liquidity pool within a T-day horizon as a result of the daily mark-\\ning to market. Kolb et al. assumed that the expected change in futures price is 0 \\nand that futures price changes are normally distributed. With σ representing the \\nstandard deviation of daily futures price changes, the standard deviation of price \\nchanges over a time horizon to day T is  σ  √ \\n_\\n \\nT  , given continuous compounding. \\nWith that background, the Kolb et al. expression is\\n Probability of exhausting liquidity pool = 2[1 – N(x)],\\nwhere  x = λ /   ( σ  √ \\n_\\n \\nT  )  . Here, x is a standardized value of λ. N(x) is the standard \\nnormal cumulative distribution function. For some intuition about 1 – N(x) in \\nthe expression, note that the liquidity pool is exhausted if losses exceed the size \\nof the liquidity pool at any time up to and including T; the probability of that \\nevent happening can be shown to be proportional to an area in the right tail of a \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n292\\nstandard normal distribution, 1 – N(x).\\nUsing the Kolb et al. expression, answer the following questions:\\nA. Your hedging horizon is five days, and your liquidity pool is $2,000 per con-\\ntract. You estimate that the standard deviation of daily price changes for the \\ncontract is $450. What is the probability that you will exhaust your liquidity \\npool in the five-day period?\\nB. Suppose your hedging horizon is 20 days but all the other facts given in \\nPart A remain the same. What is the probability that you will exhaust your \\nliquidity pool in the 20-day period?\\n20. Which of the following is characteristic of the normal distribution?\\nA. Asymmetry\\nB. Kurtosis of 3\\nC. Definitive limits or boundaries\\n21. Which of the following assets most likely requires the use of a multivariate distri-\\nbution for modeling returns?\\nA. A call option on a bond\\nB. A portfolio of technology stocks\\nC. A stock in a market index\\n22. The total number of parameters that fully characterizes a multivariate normal \\ndistribution for the returns on two stocks is:\\nA. 3.\\nB. 4.\\nC. 5.\\n23. A portfolio has an expected mean return of 8% and standard deviation of 14%. \\nThe probability that its return falls between 8% and 11% is closest to:\\nA. 8.5%.\\nB. 14.8%.\\nC. 58.3%.\\n24. A portfolio has an expected return of 7%, with a standard deviation of 13%. For \\nan investor with a minimum annual return target of 4%, the probability that the \\nportfolio return will fail to meet the target is closest to:\\nA. 33%.\\nB. 41%.\\nC. 59%.\\n25. Which parameter equals zero in a normal distribution?\\nA. Kurtosis\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n293\\nB. Skewness\\nC. Standard deviation\\n26. An analyst develops the following capital market projections.\\n\\xa0\\nStocks\\nBonds\\nMean Return\\n10%\\n2%\\nStandard Deviation\\n15%\\n5%\\nAssuming the returns of the asset classes are described by normal distributions, \\nwhich of the following statements is correct?\\nA. Bonds have a higher probability of a negative return than stocks.\\nB. On average, 99% of stock returns will fall within two standard deviations of \\nthe mean.\\nC. The probability of a bond return less than or equal to 3% is determined \\nusing a Z-score of 0.25.\\n27. A client has a portfolio of common stocks and fixed-income instruments with a \\ncurrent value of £1,350,000. She intends to liquidate £50,000 from the portfolio \\nat the end of the year to purchase a partnership share in a business. Further-\\nmore, the client would like to be able to withdraw the £50,000 without reducing \\nthe initial capital of £1,350,000. The following table shows four alternative asset \\nallocations.\\nMean and Standard Deviation for Four Allocations (in \\nPercent)\\n\\xa0\\nA\\nB\\nC\\nD\\nExpected annual return\\n16\\n12\\n10\\n9\\nStandard deviation of return\\n24\\n17\\n12\\n11\\nAddress the following questions (assume normality for Parts B and C):\\nA. Given the client’s desire not to invade the £1,350,000 principal, what is the \\nshortfall level, RL? Use this shortfall level to answer Part B.\\nB. According to the safety-first criterion, which of the allocations is the best?\\nC. What is the probability that the return on the safety-first optimal portfolio \\nwill be less than the shortfall level, RL?\\n28. A client holding a £2,000,000 portfolio wants to withdraw £90,000 in one year \\nwithout invading the principal. According to Roy’s safety-first criterion, which of \\nthe following portfolio allocations is optimal?\\n\\xa0\\nAllocation A\\nAllocation B\\nAllocation C\\nExpected annual return\\n6.5%\\n7.5%\\n8.5%\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n294\\n\\xa0\\nAllocation A\\nAllocation B\\nAllocation C\\nStandard deviation of returns\\n8.35%\\n10.21%\\n14.34%\\nA. Allocation A\\nB. Allocation B\\nC. Allocation C\\n29. The weekly closing prices of Mordice Corporation shares are as follows:\\nDate\\nClosing Price (€)\\n1 August\\n112\\n8 August\\n160\\n15 August\\n120\\nThe continuously compounded return of Mordice Corporation shares for the \\nperiod August 1 to August 15 is closest to:\\nA. 6.90%.\\nB. 7.14%.\\nC. 8.95%.\\n30. In contrast to normal distributions, lognormal distributions:\\nA. are skewed to the left.\\nB. have outcomes that cannot be negative.\\nC. are more suitable for describing asset returns than asset prices.\\n31. The lognormal distribution is a more accurate model for the distribution of stock \\nprices than the normal distribution because stock prices are:\\nA. symmetrical.\\nB. unbounded.\\nC. non-negative.\\n32. The price of a stock at t = 0 is $208.25 and at t = 1 is $186.75. The continuously \\ncompounded rate of return for the stock from t = 0 to t = 1 is closest to:\\nA. –10.90%.\\nB. –10.32%.\\nC. 11.51%.\\n33. Which one of the following statements about Student’s t-distribution is false?\\nA. It is symmetrically distributed around its mean value, like the normal \\ndistribution.\\nB. It has shorter (i.e., thinner) tails than the normal distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n295\\nC. As its degrees of freedom increase, Student’s t-distribution approaches the \\nnormal distribution.\\n34. Which one of the following statements concerning chi-square and F-distributions \\nis false?\\nA. They are both asymmetric distributions.\\nB. As their degrees of freedom increase, the shapes of their pdfs become more \\nbell curve–like.\\nC. The domains of their pdfs are positive and negative numbers.\\n35. \\nA. Define Monte Carlo simulation, and explain its use in investment \\nmanagement.\\nB. Compared with analytical methods, what are the strengths and weaknesses \\nof Monte Carlo simulation for use in valuing securities?\\n36. A Monte Carlo simulation can be used to:\\nA. directly provide precise valuations of call options.\\nB. simulate a process from historical records of returns.\\nC. test the sensitivity of a model to changes in assumptions—for example, on \\ndistributions of key variables.\\n37. A limitation of Monte Carlo simulation is:\\nA. its failure to do “what if” analysis.\\nB. that it requires historical records of returns.\\nC. its inability to independently specify cause-and-effect relationships.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n296\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 306,\n",
       "     'content': 'SOLUTIONS\\n1. \\nA. The put’s minimum value is $0. The put’s value is $0 when the stock price \\nis at or above $100 at the maturity date of the option. The put’s maximum \\nvalue is $100 = $100 (the exercise price) − $0 (the lowest possible stock \\nprice). The put’s value is $100 when the stock is worthless at the option’s \\nmaturity date. The put’s minimum price increments are $0.01. The possible \\noutcomes of terminal put value are thus $0.00, $0.01, $0.02, . . . , $100.\\nB. The price of the underlying has minimum price fluctuations of $0.01: These \\nare the minimum price fluctuations for terminal put value. For example, if \\nthe stock finishes at $98.20, the payoff on the put is $100 – $98.20 = $1.80. \\nWe can specify that the nearest values to $1.80 are $1.79 and $1.81. With a \\ncontinuous random variable, we cannot specify the nearest values. So, we \\nmust characterize terminal put value as a discrete random variable.\\nC. The probability that terminal put value is less than or equal to $24 is P(Y ≤ \\n24), or F(24) in standard notation, where F is the cumulative distribution \\nfunction for terminal put value.\\n2. C is correct. The rate of return is a random variable because the future outcomes \\nare uncertain, and it is continuous because it can take on an unlimited number of \\noutcomes.\\n3. B is correct. The function g(x) satisfies the conditions of a probability function. \\nAll of the values of g(x) are between 0 and 1, and the values of g(x) all sum to 1.\\n4. B is correct. The value of the cumulative distribution function lies between 0 and \\n1 for any x: 0 ≤ F(x) ≤ 1.\\n5. B is correct. The probability of any outcome is 0.05, P(1) = 1/20 = 0.05. The prob-\\nability that X is greater than or equal to 3 but less than 6 is expressed as P(3 ≤ X < \\n6) = P(3) + P(4) + P(5) = 0.05 + 0.05 + 0.05 = 0.15.\\n6. \\nA. The expected value of fourth-quarter sales is €14,500,000, calculated as \\n(€14,000,000 + €15,000,000)/2. With a continuous uniform random variable, \\nthe mean or expected value is the midpoint between the smallest and largest \\nvalues.\\nB. The probability that fourth-quarter sales will be less than or equal to \\n€14,125,000 is 0.125, or 12.5%, calculated as (€14,125,000 – €14,000,000)/\\n(€15,000,000 – €14,000,000).\\n7. A is correct. The probability that X will take on a value of 4 or less is F(4) = P(X \\n≤ 4) = p(1) + p(2) + p(3) + p(4) = 0.60. The probability that X will take on a value \\nof 3 or less is F(3) = P(X ≤ 3) = p(1) + p(2) + p(3) = 0.50. So, the probability that X \\nwill take on a value of 4 is F(4) – F(3) = p(4) = 0.10. The probability of X = 2 can \\nbe found using the same logic: F(2) – F(1) = p(2) = 0.25 – 0.15 = 0.10. The proba-\\nbility of X taking on a value of 2 or 4 is p(2) + p(4) = 0.10 + 0.10 = 0.20.\\n8. A is correct. The probability of generating a random number equal to any fixed \\npoint under a continuous uniform distribution is zero.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n297\\n9. A binomial random variable is defined as the number of successes in n Bernoulli \\ntrials (a trial that produces one of two outcomes). The binomial distribution is \\nused to make probability statements about a record of successes and failures or \\nabout anything with binary (twofold) outcomes.\\n10. C is correct. The binomial distribution is symmetric when the probability of suc-\\ncess on a trial is 0.50, but it is asymmetric or skewed otherwise. Here, it is given \\nthat p = 0.50.\\n11. \\nA. The probability of an earnings increase (success) in a year is estimated as \\n7/10 = 0.70, or 70%, based on the record of the past 10 years.\\nB. The probability that earnings will increase in 5 of the next 10 years is about \\n10.3%. Define a binomial random variable X, counting the number of earn-\\nings increases over the next 10 years. From Part A, the probability of an \\nearnings increase in a given year is p = 0.70 and the number of trials (years) \\nis n = 10. Equation 2 gives the probability that a binomial random variable \\nhas x successes in n trials, with the probability of success on a trial equal to \\np:\\n P  ( X = x )  =   ( x n )  p x  ( 1 − p ) \\nn−x =  \\nn\\u200a! \\n_ \\n  ( n − x )  \\u200a!x\\u200a!   p x  ( 1 − p ) \\nn−x . \\nFor this example,\\n   ( 5 \\n10 )  0.7 5  0.3 10−5 =  \\n10\\u200a! \\n_ \\n  ( 10 − 5 )  \\u200a!5\\u200a!   0.7 5  0.3 10−5   \\n \\n \\n \\n= 252 × 0.16807 × 0.00243 = 0.102919.\\n \\n \\nWe conclude that the probability that earnings will increase in exactly 5 of \\nthe next 10 years is 0.1029, or approximately 10.3%.\\nC. The expected number of yearly increases is E(X) = np = 10 × 0.70 = 7.\\nD. The variance of the number of yearly increases over the next 10 years is σ2 \\n= np(1 – p) = 10 × 0.70 × 0.30 = 2.1. The standard deviation is 1.449 (the \\npositive square root of 2.1).\\nE. You must assume that (1) the probability of an earnings increase (success) is \\nconstant from year to year and (2) earnings increases are independent trials. \\nIf current and past earnings help forecast next year’s earnings, Assumption \\n2 is violated. If the company’s business is subject to economic or industry \\ncycles, neither assumption is likely to hold.\\n12. B is correct. To calculate the probability of four years of outperformance, use the \\nformula\\n p  ( x )  = P  ( X = x )  =   ( n x  )  p x  ( 1 − p ) \\nn−x =  \\nn\\u200a! \\n_ \\n  ( n − x )  \\u200a!x\\u200a!   p x  ( 1 − p ) \\nn−x . \\nUsing this formula to calculate the probability in four of five years, n = 5, x = 4, \\nand p = 0.60.\\nTherefore,\\n p  ( 4 )  =  \\n5\\u200a! \\n_ \\n  ( 5 − 4 )  \\u200a!4\\u200a!   0.6 4  ( 1 − 0.6 ) 5−4 =   [ 120\\u200a/\\u200a24 ]   ( 0.1296 )   ( 0.40 )  = 0.2592. \\n p  ( 5 )  =  \\n5\\u200a! \\n_ \\n  ( 5 − 5 )  \\u200a!5\\u200a!   0.6 5  ( 1 − 0.6 ) 5−5 =   [ 120\\u200a/\\u200a120 ]   ( 0.0778 )   ( 1 )  = 0.0778. \\nThe probability of outperforming four or more times is p(4) + p(5) = 0.2592 + \\n0.0778 = 0.3370.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n298\\n13. The observed success rate is 4/7 = 0.571, or 57.1%. The probability of four or few-\\ner successes is F(4) = p(4) + p(3) + p(2) + p(1) + p(0), where p(4), p(3), p(2), p(1), \\nand p(0) are, respectively, the probabilities of 4, 3, 2, 1, and 0 successes, accord-\\ning to the binomial distribution with n = 7 and p = 0.70. We have the following \\nprobabilities:\\n p(4) = (7!/4!3!)(0.704)(0.303) = 35(0.006483) = 0.226895.\\n p(3) = (7!/3!4!)(0.703)(0.304) = 35(0.002778) = 0.097241.\\n p(2) = (7!/2!5!)(0.702)(0.305) = 21(0.001191) = 0.025005.\\n p(1) = (7!/1!6!)(0.701)(0.306) = 7(0.000510) = 0.003572.\\n p(0) = (7!/0!7!)(0.700)(0.307) = 1(0.000219) = 0.000219.\\nSumming all these probabilities, you conclude that F(4) = 0.226895 + 0.097241 + \\n0.025005 + 0.003572 + 0.000219 = 0.352931, or 35.3%.\\n14. C is correct. The probability that the performance is at or below the expectation \\nis calculated by finding F(3) = p(3) + p(2) + p(1) + p(0) using the formula:\\n p  ( x )  = P  ( X = x )  =   ( n x  )  p x  ( 1 − p ) \\nn−x =  \\nn\\u200a! \\n_ \\n  ( n − x )  \\u200a!x\\u200a!   p x  ( 1 − p ) \\nn−x . \\nUsing this formula,\\n p  ( 3 )  =  \\n4\\u200a! \\n_ \\n  ( 4 − 3 )  \\u200a!3\\u200a!   0.75 3  ( 1 − 0.75 ) 4−3 =   [ 24\\u200a/\\u200a6 ]   ( 0.42 )   ( 0.25 )  = 0.42. \\n p  ( 2 )  =  \\n4\\u200a! \\n_ \\n  ( 4 − 2 )  \\u200a!2\\u200a!   0.75 2  ( 1 − 0.75 ) 4−2 =   [ 24\\u200a/\\u200a4 ]   ( 0.56 )   ( 0.06 )  = 0.20. \\n p  ( 1 )  =  \\n4\\u200a! \\n_ \\n  ( 4 − 1 )  \\u200a!1\\u200a!   0.75 1  ( 1 − 0.75 ) 4−1 =   [ 24\\u200a/\\u200a6 ]   ( 0.75 )   ( 0.02 )  = 0.06. \\n p  ( 0 )  =  \\n4\\u200a! \\n_ \\n  ( 4 − 0 )  \\u200a!0\\u200a!   0.75 0  ( 1 − 0.75 ) 4−0 =   [ 24\\u200a/\\u200a24 ]   ( 1 )   ( 0.004 )  = 0.004. \\nTherefore,\\n F(3) = p(3) + p(2) + p(1) + p(0) = 0.42 + 0.20 + 0.06 + 0.004 = 0.684, or approxi-\\nmately 68%.\\n15. A is correct. A trial, such as a coin flip, will produce one of two outcomes. Such a \\ntrial is a Bernoulli trial.\\n16. C is correct. The probability of an up move (p) can be found by solving the equa-\\ntion (p)uS + (1 – p)dS = (p)105 + (1 – p)97 = 102. Solving for p gives 8p = 5, so p \\n= 0.625.\\n17. A is correct. Only the top node value of $219.9488 exceeds $200.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n299\\n$190.0000\\n$199.5000\\n$209.4750\\n$219.9488\\n$180.5000\\n$189.5250\\n$171.4750\\n$199.0013\\n$180.0488\\n$162.9013\\n18. \\nA. Approximately 68% of all outcomes of a normal random variable fall within \\nplus or minus one standard deviation of the mean.\\nB. Approximately 95% of all outcomes of a normal random variable fall within \\nplus or minus two standard deviations of the mean.\\nC. Approximately 99% of all outcomes of a normal random variable fall within \\nplus or minus three standard deviations of the mean.\\n19. \\nA. The probability of exhausting the liquidity pool is 4.7%. First, calcu-\\nlate  x = λ /   ( σ  √ \\n_\\n \\nT  )  = $2, 000 /   ( $450  √ \\n_\\n \\n5  )  = 1.987616. By using Excel’s \\nNORM.S.DIST() function, we get NORM.S.DIST(1.987616) = 0.9766. Thus, \\nthe probability of exhausting the liquidity pool is 2[1 – N(1.99)] = 2(1 – \\n0.9766) = 0.0469, or about 4.7%.\\nB. The probability of exhausting the liquidity pool is now 32.2%. The calcu-\\nlation follows the same steps as those in Part A. We calculate  x = λ /   ( σ  \\n√ \\n_\\n \\nT  )  = $2, 000 /   ( $450  √ _ \\n20  )  = 0.993808. By using Excel’s NORM.S.DIST() \\nfunction, we get NORM.S.DIST(0.993808)= 0.8398. Thus, the probability of \\nexhausting the liquidity pool is 2[1 – N(0.99)] = 2(1 – 0.8398) = 0.3203, or \\nabout 32.0%. This is a substantial probability that you will run out of funds \\nto meet marking to market.\\nIn their paper, Kolb et al. called the probability of exhausting the liquidity \\npool the probability of ruin, a traditional name for this type of calculation.\\n20. B is correct. The normal distribution has a skewness of 0, a kurtosis of 3, and a \\nmean, median, and mode that are all equal.\\n21. B is correct. Multivariate distributions specify the probabilities for a group of \\nrelated random variables. A portfolio of technology stocks represents a group \\nof related assets. Accordingly, statistical interrelationships must be considered, \\nresulting in the need to use a multivariate normal distribution.\\n22. C is correct. A bivariate normal distribution (two stocks) will have two means, \\ntwo variances, and one correlation. A multivariate normal distribution for the \\nreturns on n stocks will have n means, n variances, and n(n – 1)/2 distinct cor-\\nrelations.\\n23. A is correct. P(8% ≤ Portfolio return ≤ 11%) = N(Z corresponding to 11%) – N(Z \\ncorresponding to 8%). For the first term, NORM.S.DIST((11% – 8%)/14%) = \\n58.48%. To get the second term immediately, note that 8% is the mean, and for \\nthe normal distribution, 50% of the probability lies on either side of the mean. \\nTherefore, N(Z corresponding to 8%) must equal 50%. So, P(8% ≤ Portfolio return \\n≤ 11%) = 0.5848 – 0.50 = 0.0848, or approximately 8.5%.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n300\\n24. B is correct. By using Excel’s NORM.S.DIST() function, we get \\nNORM.S.DIST((4% – 7%)/13%) = 40.87%. The probability that the portfolio will \\nunderperform the target is about 41%.\\n25. B is correct. A normal distribution has a skewness of zero (it is symmetrical \\naround the mean). A non-zero skewness implies asymmetry in a distribution.\\n26. A is correct. The chance of a negative return falls in the area to the left of 0% \\nunder a standard normal curve. By standardizing the returns and standard de-\\nviations of the two assets, the likelihood of either asset experiencing a negative \\nreturn may be determined: Z-score (standardized value) = (X – μ)/σ.\\n Z-score for a bond return of 0% = (0 – 2)/5 = –0.40.\\n Z-score for a stock return of 0% = (0 – 10)/15 = –0.67.\\nFor bonds, a 0% return falls 0.40 standard deviations below the mean return of \\n2%. In contrast, for stocks, a 0% return falls 0.67 standard deviations below the \\nmean return of 10%. A standard deviation of 0.40 is less than a standard deviation \\nof 0.67. Negative returns thus occupy more of the left tail of the bond distribution \\nthan the stock distribution. Thus, bonds are more likely than stocks to experience \\na negative return.\\n27. \\nA. Because £50,000/£1,350,000 is 3.7%, for any return less than 3.7% the client \\nwill need to invade principal if she takes out £50,000. So RL = 3.7%.\\nB. To decide which of the allocations is safety-first optimal, select the alterna-\\ntive with the highest ratio [E(RP) − RL]/σP:\\n Allocation A: 0.5125 = (16 – 3.7)/24.\\n Allocation B: 0.488235 = (12 – 3.7)/17.\\n Allocation C: 0.525 = (10 – 3.7)/12.\\n Allocation D: 0.481818 = (9 – 3.7)/11.\\nAllocation C, with the largest ratio (0.525), is the best alternative according \\nto the safety-first criterion.\\nC. To answer this question, note that P(RC < 3.7) = N(0.037 – 0.10)/0.12) \\n= N(−0.525). By using Excel’s NORM.S.DIST() function, we get \\nNORM.S.DIST((0.037 – 0.10)/0.12) = 29.98%, or about 30%. The safety-first \\noptimal portfolio has a roughly 30% chance of not meeting a 3.7% return \\nthreshold.\\n28. B is correct. Allocation B has the highest safety-first ratio. The threshold return \\nlevel, RL, for the portfolio is £90,000/£2,000,000 = 4.5%;, thus, any return less \\nthan RL = 4.5% will invade the portfolio principal. To compute the allocation that \\nis safety-first optimal, select the alternative with the highest ratio:\\n \\n [ E  ( R P −  R L )  ]  \\n \\n___________ \\n σ P  \\n . \\n Allocation A =  6.5 − 4.5 \\n_ \\n8.35  = 0.240. \\n Allocation B =  7.5 − 4.5 \\n_ \\n10.21  = 0.294. \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n301\\n Allocation C =  8.5 − 4.5 \\n_ \\n14.34  = 0.279. \\n29. A is correct. The continuously compounded return of an asset over a period is \\nequal to the natural log of the asset’s price change during the period. In this case,\\n ln(120/112) = 6.90%.\\n30. B is correct. By definition, lognormal random variables cannot have negative \\nvalues.\\n31. C is correct. A lognormal distributed variable has a lower bound of zero. The log-\\nnormal distribution is also right skewed, which is a useful property in describing \\nasset prices.\\n32. A is correct. The continuously compounded return from t = 0 to t = 1 is r0,1 = \\nln(S1/S0) = ln(186.75/208.25) = –0.10897 = –10.90%.\\n33. A is correct, since it is false. Student’s t-distribution has longer (fatter) tails than \\nthe normal distribution and, therefore, it may provide a more reliable, more con-\\nservative downside risk estimate.\\n34. C is correct, since it is false. Both chi-square and F-distributions are bounded \\nfrom below by zero, so the domains of their pdfs are restricted to positive num-\\nbers.\\n35. \\nA. Monte Carlo simulation involves the use of computer software to repre-\\nsent the operation of a complex financial system. A characteristic feature \\nof Monte Carlo simulation is the generation of a large number of random \\nsamples from a specified probability distribution (or distributions) to \\nrepresent the role of risk in the system. Monte Carlo simulation is widely \\nused to estimate risk and return in investment applications. In this setting, \\nwe simulate the portfolio’s profit and loss performance for a specified time \\nhorizon. Repeated trials within the simulation produce a simulated fre-\\nquency distribution of portfolio returns from which performance and risk \\nmeasures are derived. Another important use of Monte Carlo simulation in \\ninvestments is as a tool for valuing complex securities for which no analytic \\npricing formula is available. It is also an important modeling resource for \\nsecurities with complex embedded options.\\nB. Strengths: Monte Carlo simulation can be used to price complex securities \\nfor which no analytic expression is available, particularly European-style \\noptions.\\nWeaknesses: Monte Carlo simulation provides only statistical estimates, not \\nexact results. Analytic methods, when available, provide more insight into \\ncause-and-effect relationships than does Monte Carlo simulation.\\n36. C is correct. A characteristic feature of Monte Carlo simulation is the generation \\nof a large number of random samples from a specified probability distribution \\nor distributions to represent the role of risk in the system. Therefore, it is very \\nuseful for investigating the sensitivity of a model to changes in assumptions—for \\nexample, on distributions of key variables.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 4 \\nCommon Probability Distributions\\n302\\n37. C is correct. Monte Carlo simulation is a complement to analytical methods. \\nMonte Carlo simulation provides statistical estimates and not exact results. \\nAnalytical methods, when available, provide more insight into cause-and-effect \\nrelationships.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling and Estimation\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity \\nManagement (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ncompare and contrast probability samples with non-probability \\nsamples and discuss applications of each to an investment problem\\nexplain sampling error\\ncompare and contrast simple random, stratified random, cluster, \\nconvenience, and judgmental sampling\\nexplain the central limit theorem and its importance\\ncalculate and interpret the standard error of the sample mean\\nidentify and describe desirable properties of an estimator\\ncontrast a point estimate and a confidence interval estimate of a \\npopulation parameter\\ncalculate and interpret a confidence interval for a population mean, \\ngiven a normal distribution with 1) a known population variance, \\n2) an unknown population variance, or 3) an unknown population \\nvariance and a large sample size\\ndescribe the use of resampling (bootstrap, jackknife) to estimate the \\nsampling distribution of a statistic\\ndescribe the issues regarding selection of the appropriate sample \\nsize, data snooping bias, sample selection bias, survivorship bias, \\nlook-ahead bias, and time-period bias\\nL E A R N I N G  M O D U L E\\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]},\n",
       "  {'title': 'Learning Module 5\\tSampling and Estimation',\n",
       "   'page_number': 313,\n",
       "   'content': 'Sampling and Estimation\\nby Richard A. DeFusco, PhD, CFA, Dennis W. McLeavey, DBA, CFA, Jerald \\nE. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.\\nRichard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. \\nMcLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, \\nCFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity \\nManagement (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ncompare and contrast probability samples with non-probability \\nsamples and discuss applications of each to an investment problem\\nexplain sampling error\\ncompare and contrast simple random, stratified random, cluster, \\nconvenience, and judgmental sampling\\nexplain the central limit theorem and its importance\\ncalculate and interpret the standard error of the sample mean\\nidentify and describe desirable properties of an estimator\\ncontrast a point estimate and a confidence interval estimate of a \\npopulation parameter\\ncalculate and interpret a confidence interval for a population mean, \\ngiven a normal distribution with 1) a known population variance, \\n2) an unknown population variance, or 3) an unknown population \\nvariance and a large sample size\\ndescribe the use of resampling (bootstrap, jackknife) to estimate the \\nsampling distribution of a statistic\\ndescribe the issues regarding selection of the appropriate sample \\nsize, data snooping bias, sample selection bias, survivorship bias, \\nlook-ahead bias, and time-period bias\\nL E A R N I N G  M O D U L E\\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n304\\nINTRODUCTION\\nEach day, we observe the high, low, and close of stock market indexes from around \\nthe world. Indexes such as the S&P 500 Index and the Nikkei 225 Stock Average \\nare samples of stocks. Although the S&P 500 and the Nikkei do not represent the \\npopulations of US or Japanese stocks, we view them as valid indicators of the whole \\npopulation’s behavior. As analysts, we are accustomed to using this sample information \\nto assess how various markets from around the world are performing. Any statistics \\nthat we compute with sample information, however, are only estimates of the under-\\nlying population parameters. A sample, then, is a subset of the population—a subset \\nstudied to infer conclusions about the population itself.\\nWe introduce and discuss sampling—the process of obtaining a sample. In \\ninvestments, we continually make use of the mean as a measure of central tendency \\nof random variables, such as return and earnings per share. Even when the probability \\ndistribution of the random variable is unknown, we can make probability statements \\nabout the population mean using the central limit theorem. We discuss and illustrate \\nthis key result. Following that discussion, we turn to statistical estimation. Estimation \\nseeks precise answers to the question “What is this parameter’s value?”\\nThe central limit theorem and estimation, the core of the body of methods pre-\\nsented in the sections that follow, may be applied in investment applications. We often \\ninterpret the results for the purpose of deciding what works and what does not work \\nin investments. We will also discuss the interpretation of statistical results based on \\nfinancial data and the possible pitfalls in this process.\\nSAMPLING METHODS\\ncompare and contrast probability samples with non-probability \\nsamples and discuss applications of each to an investment problem\\nexplain sampling error\\ncompare and contrast simple random, stratified random, cluster, \\nconvenience, and judgmental sampling\\nIn this section, we present the various methods for obtaining information on a pop-\\nulation (all members of a specified group) through samples (part of the population). \\nThe information on a population that we try to obtain usually concerns the value of a \\nparameter, a quantity computed from or used to describe a population of data. When \\nwe use a sample to estimate a parameter, we make use of sample statistics (statistics, \\nfor short). A statistic is a quantity computed from or used to describe a sample of data.\\nWe take samples for one of two reasons. In some cases, we cannot possibly exam-\\nine every member of the population. In other cases, examining every member of the \\npopulation would not be economically efficient. Thus, savings of time and money \\nare two primary factors that cause an analyst to use sampling to answer a question \\nabout a population.\\nThere are two types of sampling methods: probability sampling and non-probability \\nsampling.Probability sampling gives every member of the population an equal chance \\nof being selected. Hence it can create a sample that is representative of the popula-\\ntion. In contrast, non-probability sampling depends on factors other than probabil-\\nity considerations, such as a sampler’s judgment or the convenience to access data. \\n1\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "   'children': [{'title': 'Introduction',\n",
       "     'page_number': 314,\n",
       "     'content': 'Learning Module 5 \\nSampling and Estimation\\n304\\nINTRODUCTION\\nEach day, we observe the high, low, and close of stock market indexes from around \\nthe world. Indexes such as the S&P 500 Index and the Nikkei 225 Stock Average \\nare samples of stocks. Although the S&P 500 and the Nikkei do not represent the \\npopulations of US or Japanese stocks, we view them as valid indicators of the whole \\npopulation’s behavior. As analysts, we are accustomed to using this sample information \\nto assess how various markets from around the world are performing. Any statistics \\nthat we compute with sample information, however, are only estimates of the under-\\nlying population parameters. A sample, then, is a subset of the population—a subset \\nstudied to infer conclusions about the population itself.\\nWe introduce and discuss sampling—the process of obtaining a sample. In \\ninvestments, we continually make use of the mean as a measure of central tendency \\nof random variables, such as return and earnings per share. Even when the probability \\ndistribution of the random variable is unknown, we can make probability statements \\nabout the population mean using the central limit theorem. We discuss and illustrate \\nthis key result. Following that discussion, we turn to statistical estimation. Estimation \\nseeks precise answers to the question “What is this parameter’s value?”\\nThe central limit theorem and estimation, the core of the body of methods pre-\\nsented in the sections that follow, may be applied in investment applications. We often \\ninterpret the results for the purpose of deciding what works and what does not work \\nin investments. We will also discuss the interpretation of statistical results based on \\nfinancial data and the possible pitfalls in this process.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Sampling Methods',\n",
       "     'page_number': 314,\n",
       "     'content': 'SAMPLING METHODS\\ncompare and contrast probability samples with non-probability \\nsamples and discuss applications of each to an investment problem\\nexplain sampling error\\ncompare and contrast simple random, stratified random, cluster, \\nconvenience, and judgmental sampling\\nIn this section, we present the various methods for obtaining information on a pop-\\nulation (all members of a specified group) through samples (part of the population). \\nThe information on a population that we try to obtain usually concerns the value of a \\nparameter, a quantity computed from or used to describe a population of data. When \\nwe use a sample to estimate a parameter, we make use of sample statistics (statistics, \\nfor short). A statistic is a quantity computed from or used to describe a sample of data.\\nWe take samples for one of two reasons. In some cases, we cannot possibly exam-\\nine every member of the population. In other cases, examining every member of the \\npopulation would not be economically efficient. Thus, savings of time and money \\nare two primary factors that cause an analyst to use sampling to answer a question \\nabout a population.\\nThere are two types of sampling methods: probability sampling and non-probability \\nsampling.Probability sampling gives every member of the population an equal chance \\nof being selected. Hence it can create a sample that is representative of the popula-\\ntion. In contrast, non-probability sampling depends on factors other than probabil-\\nity considerations, such as a sampler’s judgment or the convenience to access data. \\n1\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Methods\\n305\\nConsequently, there is a significant risk that non-probability sampling might generate \\na non-representative sample. In general, all else being equal, probability sampling can \\nyield more accuracy and reliability compared with non-probability sampling.\\nWe first focus on probability sampling, particularly the widely used simple ran-\\ndom sampling and stratified random sampling. We then turn our attention to \\nnon-probability sampling.\\n',\n",
       "     'children': [{'title': 'Simple Random Sampling',\n",
       "       'page_number': 315,\n",
       "       'content': '',\n",
       "       'children': []},\n",
       "      {'title': 'Stratified Random Sampling',\n",
       "       'page_number': 316,\n",
       "       'content': 'Stratified Random Sampling\\nThe simple random sampling method just discussed may not be the best approach in \\nall situations. One frequently used alternative is stratified random sampling.\\n■ \\nDefinition of Stratified Random Sampling. In stratified random sampling, \\nthe population is divided into subpopulations (strata) based on one or more \\nclassification criteria. Simple random samples are then drawn from each \\nstratum in sizes proportional to the relative size of each stratum in the pop-\\nulation. These samples are then pooled to form a stratified random sample.\\nIn contrast to simple random sampling, stratified random sampling guarantees \\nthat population subdivisions of interest are represented in the sample. Another \\nadvantage is that estimates of parameters produced from stratified sampling have \\ngreater precision—that is, smaller variance or dispersion—than estimates obtained \\nfrom simple random sampling.\\nBond indexing is one area in which stratified sampling is frequently applied. \\nIndexing is an investment strategy in which an investor constructs a portfolio to \\nmirror the performance of a specified index. In pure bond indexing, also called the \\nfull-replication approach, the investor attempts to fully replicate an index by owning \\nall the bonds in the index in proportion to their market value weights. Many bond \\nindexes consist of thousands of issues, however, so pure bond indexing is difficult to \\nimplement. In addition, transaction costs would be high because many bonds do not \\nhave liquid markets. Although a simple random sample could be a solution to the \\ncost problem, the sample would probably not match the index’s major risk factors—\\ninterest rate sensitivity, for example. Because the major risk factors of fixed-income \\nportfolios are well known and quantifiable, stratified sampling offers a more effective \\napproach. In this approach, we divide the population of index bonds into groups of \\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Methods\\n307\\nsimilar duration (interest rate sensitivity), cash flow distribution, sector, credit quality, \\nand call exposure. We refer to each group as a stratum or cell (a term frequently used \\nin this context). Then, we choose a sample from each stratum proportional to the \\nrelative market weighting of the stratum in the index to be replicated.\\nEXAMPLE 1\\nBond Indexes and Stratified Sampling\\nSuppose you are the manager of a portfolio of bonds indexed to the Bloomberg \\nBarclays US Government/Credit Index, meaning that the portfolio returns \\nshould be similar to those of the index. You are exploring several approaches to \\nindexing, including a stratified sampling approach. You first distinguish among \\nagency bonds, US Treasury bonds, and investment-grade corporate bonds. For \\neach of these three groups, you define 10 maturity intervals—1 to 2 years, 2 \\nto 3 years, 3 to 4 years, 4 to 6 years, 6 to 8 years, 8 to 10 years, 10 to 12 years, \\n12 to 15 years, 15 to 20 years, and 20 to 30 years—and also separate the bonds \\nwith coupons (annual interest rates) of 6% or less from the bonds with coupons \\nof more than 6%.\\n1. How many cells or strata does this sampling plan entail?\\nSolution to 1:\\nWe have 3 issuer classifications, 10 maturity classifications, and 2 coupon \\nclassifications. So, in total, this plan entails 3(10)(2) = 60 different strata or \\ncells.\\n2. If you use this sampling plan, what is the minimum number of issues the \\nindexed portfolio can have?\\nSolution to 2:\\nOne cannot have less than 1 issue for each cell, so the portfolio must include \\nat least 60 issues.\\n3. Suppose that in selecting among the securities that qualify for selection \\nwithin each cell, you apply a criterion concerning the liquidity of the securi-\\nty’s market. Is the sample obtained random? Explain your answer.\\nSolution to 3:\\nApplying any additional criteria to the selection of securities for the cells, \\nnot every security that might be included has an equal probability of being \\nselected. As a result, the sampling is not random. In practice, indexing using \\nstratified sampling usually does not strictly involve random sampling be-\\ncause the selection of bond issues within cells is subject to various additional \\ncriteria. Because the purpose of sampling in this application is not to make \\nan inference about a population parameter but rather to index a portfolio, \\nlack of randomness is not in itself a problem in this application of stratified \\nsampling.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n308\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Cluster Sampling',\n",
       "       'page_number': 318,\n",
       "       'content': 'Cluster Sampling\\nAnother sampling method, cluster sampling, also requires the division or classifica-\\ntion of the population into subpopulation groups, called clusters. In this method, the \\npopulation is divided into clusters, each of which is essentially a mini-representation \\nof the entire populations Then certain clusters are chosen as a whole using simple \\nrandom sampling. If all the members in each sampled cluster are sampled, this sample \\nplan is referred to as one-stage cluster sampling. If a subsample is randomly selected \\nfrom each selected cluster, then the plan is referred as two-stage cluster sampling. \\nExhibit 1 (bottom right panel) shows how cluster sampling works and how it compares \\nwith the other probability sampling methods.\\nExhibit 1: Probability Sampling\\nSimple random sample\\nStratified sample\\nCluster sample\\nSubdivision\\n(strata)\\nSubdivision\\n(strata)\\nSystematic sample\\nClusters\\nA major difference between cluster and stratified random samples is that in cluster \\nsampling, a whole cluster is regarded as a sampling unit and only sampled clusters are \\nincluded. In stratified random sampling, however, all the strata are included and only \\nspecific elements within each stratum are then selected as sampling units.\\nCluster sampling is commonly used for market survey, and the most popular version \\nidentifies clusters based on geographic parameters. For example, a research institute \\nis looking to survey if individual investors in the United States are bullish, bearish, \\nor neutral on the stock market. It would be impossible to carry out the research by \\nsurveying all the individual investors in the country. The two-stage cluster sampling is \\na good solution in this case. At the first stage, a researcher can group the population \\nby states and all the individual investors of each state represent a cluster. A handful of \\nthe clusters are then randomly selected. At the second stage, a simple random sample \\nof individual investors is selected from each sampled cluster to conduct the survey.\\nCompared with other probability sampling methods, given equal sample size, clus-\\nter sampling usually yields lower accuracy because a sample from a cluster might be \\nless representative of the entire population. Its major advantage, however, is offering \\nthe most time-efficient and cost-efficient probability sampling plan for analyzing a \\nvast population.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Methods\\n309\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Non-Probability Sampling',\n",
       "       'page_number': 319,\n",
       "       'content': 'Non-Probability Sampling\\nNon-probability sampling methods rely not on a fixed selection process but instead \\non a researcher’s sample selection capabilities. We introduce two major types of \\nnon-probability sampling methods here.\\n■ \\nConvenience Sampling: In this method, an element is selected from the \\npopulation based on whether or not it is accessible to a researcher or on \\nhow easy it is for a researcher to access the element. Because the samples \\nare selected conveniently, they are not necessarily representative of the \\nentire population, and hence the level of the sampling accuracy could be \\nlimited. But the advantage of convenience sampling is that data can be \\ncollected quickly at a low cost. In situations such as the preliminary stage of \\nresearch or in circumstances subject to cost constraints, convenience sam-\\npling is often used as a time-efficient and cost-effective sampling plan for a \\nsmall-scale pilot study before testing a large-scale and more representative \\nsample.\\n■ \\nJudgmental Sampling: This sampling process involves selectively handpick-\\ning elements from the population based on a researcher’s knowledge and \\nprofessional judgment. Sample selection under judgmental sampling could \\nbe affected by the bias of the researcher and might lead to skewed results \\nthat do not represent the whole population. In circumstances where there \\nis a time constraint, however, or when the specialty of researchers is critical \\nto select a more representative sample than by using other probability or \\nnon-probability sampling methods, judgmental sampling allows researchers \\nto go directly to the target population of interest. For example, when audit-\\ning financial statements, seasoned auditors can apply their sound judgment \\nto select accounts or transactions that can provide sufficient audit coverage. \\nExample 2 illustrates an application of these sampling methods.\\nEXAMPLE 2\\nDemonstrating the Power of Sampling\\nTo demonstrate the power of sampling, we conduct two sampling experiments on \\na large dataset. The full dataset is the “population,” representing daily returns of \\nthe fictitious Euro-Asia-Africa (EAA) Equity Index. This dataset spans a five-year \\nperiod and consists of 1,258 observations of daily returns with a minimum value \\nof −4.1% and a maximum value of 5.0%.\\nFirst, we calculate the mean daily return of the EAA Equity Index (using the \\npopulation).\\nBy taking the average of all the data points, the mean of the entire daily return \\nseries is computed as 0.035%.\\nFirst Experiment: Random Sampling\\nThe sample size m is set to 5, 10, 20, 50, 100, 200, 500, and 1,000. At each sample \\nsize, we run random sampling multiple times (N = 100) to collect 100 samples \\nto compute mean absolute error. The aim is to compute and plot the mean error \\nversus the sample size.\\nFor a given sample size m, we use the following procedure to compute mean \\nabsolute error in order to measure sampling error:\\n1. Randomly draw m observations from the entire daily return series to \\nform a sample.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n310\\n2. Compute the mean of this sample.\\n3. Compute the absolute error, the difference between the sample’s mean \\nand the population mean. Because we treat the whole five-year daily \\nreturn series as a population, the population mean is 0.035% as we \\ncomputed in Solution 1.\\n4. We repeat the previous three steps a hundred times (N =100) to col-\\nlect 100 samples of the same size m and compute the absolute error of \\neach sample.\\n5. Compute the mean of the 100 absolute errors as the mean absolute \\nerror for the sample size m.\\nBy applying this procedure, we compute mean absolute errors for eight different \\nsample sizes: m = 5, 10, 20, 50, 100, 200, 500, and 1000. Exhibit 2 summarizes \\nthe results.\\n \\nExhibit 2: Mean Absolute Error of Random Sampling\\n \\n \\nSample \\nsize\\n5\\n10\\n20\\n50\\n100\\n200\\n500\\n1,000\\nMean \\nabsolute \\nerror\\n0.297%\\n0.218%\\n0.163%\\n0.091%\\n0.063%\\n0.039%\\n0.019%\\n0.009%\\n \\nMean absolute errors are plotted against sample size in Exhibit 3. The plot shows \\nthat the error quickly shrinks as the sample size increases. It also indicates that \\na minimum sample size is needed to limit sample error and achieve a certain \\nlevel of accuracy. After a certain size, however (e.g., 200 to 400 in this case), \\nthere is little incremental benefit from adding more observations.\\n \\nExhibit 3: Mean Absolute Error of Random Sampling vs. Sample \\nSize\\n \\nMean Absolute Deviation\\n0.30\\n0.30\\n0.25\\n0.25\\n0.20\\n0.20\\n0.15\\n0.15\\n0.10\\n0.10\\n0.05\\n0.05\\n0\\n0\\n1,000\\n1,000\\n400\\n400\\n200\\n200\\n800\\n800\\n600\\n600\\nSample Size\\nSecond Experiment: Stratified Random Sampling\\nWe now conduct stratified random sampling by dividing daily returns into groups \\nby year. The sample size m is again set to 5, 10, 20, 50, 100, 200, 500, and 1,000. \\nAt each sample size, run random sampling multiple times (N = 100) to collect \\n100 samples to compute mean absolute error.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Methods\\n311\\nWe follow the same steps as before, except for the first step. Rather than run-\\nning a simple random sampling, we conduct stratified random sampling—that \\nis, randomly selecting subsamples of equal number from daily return groups by \\nyear to generate a full sample. For example, for a sample of 50, 10 data points are \\nrandomly selected from daily returns of each year from 2014 to 2018, respectively. \\nExhibit 4 summarizes the results.\\n \\nExhibit 4: Mean Absolute Error of Stratified Random Sampling\\n \\n \\nSample \\nsize\\n5\\n10\\n20\\n50\\n100\\n200\\n500\\n1,000\\nMean \\nabsolute \\nerror\\n0.294%\\n0.205%\\n0.152%\\n0.083%\\n0.071%\\n0.051%\\n0.025%\\n0.008%\\n \\nMean absolute errors are plotted against sample size in Exhibit 5. Similar to ran-\\ndom sampling, the plot shows rapid shrinking of errors with increasing sample \\nsize, but this incremental benefit diminishes after a certain sample size is reached.\\n \\nExhibit 5: Mean Absolute Error of Stratified Random Sampling vs. \\nSample Size\\n \\nMean Absolute Deviation\\n0.30\\n0.30\\n0.25\\n0.25\\n0.20\\n0.20\\n0.15\\n0.15\\n0.10\\n0.10\\n0.05\\n0.05\\n0\\n0\\n1,000\\n1,000\\n400\\n400\\n200\\n200\\n800\\n800\\n600\\n600\\nSample Size\\nThe sampling methods introduced here are summarized in the diagram in Exhibit 6.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n312\\nExhibit 6: Summary of Sampling Methods\\nSampling\\nMethods\\nProbability\\nMethods\\nNon-Probability\\nMethods\\nSimple Random\\nSampling\\nSystematic\\nSampling\\nStratified Random\\nSampling\\nCluster\\nSampling\\nConvenience\\nSampling\\nJudgment\\nSampling\\nEXAMPLE 3\\nAn analyst is studying research and development (R&D) spending by pharma-\\nceutical companies around the world. She considers three sampling methods for \\nunderstanding a company’s level of R&D. Method 1 is to simply use all the data \\navailable to her from an internal database that she and her colleagues built while \\nresearching several dozen representative stocks in the sector. Method 2 involves \\nrelying on a commercial database provided by a data vendor. She would select \\nevery fifth pharmaceutical company on the list to pull the data. Method 3 is to \\nfirst divide pharmaceutical companies in the commercial database into three \\ngroups according to the region where a company is headquartered (e.g., Asia, \\nEurope, or North America) and then randomly select a subsample of companies \\nfrom each group, with the sample size proportional to the size of its associated \\ngroup in order to form a complete sample.\\n1. Method 1 is an example of:\\nA. simple random sampling.\\nB. stratified random sampling.\\nC. convenience sampling.\\nSolution to 1:\\nC is correct. The analyst selects the data from the internal database because \\nthey are easy and convenient to access.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Methods\\n313\\n2. Method 2 is an example of:\\nA. judgmental sampling.\\nB. systematic sampling.\\nC. cluster sampling.\\nSolution to 2:\\nB is correct. The sample elements are selected with a fixed interval (k = 5) \\nfrom the large population provided by data vendor.\\n3. Method 3 is an example of:\\nA. simple random sampling.\\nB. stratified random sampling.\\nC. cluster sampling.\\nSolution to 3:\\nB is correct. The population of pharmaceutical companies is divided into \\nthree strata by region to perform random sampling individually.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Sampling from Different Distributions',\n",
       "       'page_number': 323,\n",
       "       'content': 'Sampling from Different Distributions\\nIn practice, other than selecting appropriate sampling methods, we also need to be \\ncareful when sampling from a population that is not under one single distribution. \\nExample 4 illustrates the problems that can arise when sampling from more than one \\ndistribution.\\nEXAMPLE 4\\nCalculating Sharpe Ratios: One or Two Years of Quarterly \\nData\\n1. Analysts often use the Sharpe ratio to evaluate the performance of a man-\\naged portfolio. The Sharpe ratio is the average return in excess of the \\nrisk-free rate divided by the standard deviation of returns. This ratio mea-\\nsures the return of a fund or a security over and above the risk-free rate (the \\nexcess return) earned per unit of standard deviation of return.\\nTo compute the Sharpe ratio, suppose that an analyst collects eight quarterly \\nexcess returns (i.e., total return in excess of the risk-free rate). During the \\nfirst year, the investment manager of the portfolio followed a low-risk strat-\\negy, and during the second year, the manager followed a high-risk strategy. \\nFor each of these years, the analyst also tracks the quarterly excess returns of \\nsome benchmark against which the manager will be evaluated. For each of \\nthe two years, the Sharpe ratio for the benchmark is 0.21. Exhibit 7 gives the \\ncalculation of the Sharpe ratio of the portfolio.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n314\\nExhibit 7: Calculation of Sharpe Ratios: Low-Risk and High-Risk \\nStrategies\\n \\n \\nQuarter/Measure\\nYear 1 \\nExcess Returns\\nYear 2 \\nExcess Returns\\nQuarter 1\\n−3%\\n−12%\\nQuarter 2\\n5\\n20\\nQuarter 3\\n−3\\n−12\\nQuarter 4\\n5\\n20\\nQuarterly average\\n1%\\n4%\\nQuarterly standard deviation\\n4.62%\\n18.48%\\nSharpe ratio = 0.22 = 1/4.62 = 4/18.48\\n \\nFor the first year, during which the manager followed a low-risk strategy, the \\naverage quarterly return in excess of the risk-free rate was 1% with a stan-\\ndard deviation of 4.62%. The Sharpe ratio is thus 1/4.62 = 0.22. The second \\nyear’s results mirror the first year except for the higher average return and \\nvolatility. The Sharpe ratio for the second year is 4/18.48 = 0.22. The Sharpe \\nratio for the benchmark is 0.21 during the first and second years. Because \\nlarger Sharpe ratios are better than smaller ones (providing more return per \\nunit of risk), the manager appears to have outperformed the benchmark.\\nNow, suppose the analyst believes a larger sample to be superior to a small \\none. She thus decides to pool the two years together and calculate a Sharpe \\nratio based on eight quarterly observations. The average quarterly excess \\nreturn for the two years is the average of each year’s average excess return. \\nFor the two-year period, the average excess return is (1 + 4)/2 = 2.5% per \\nquarter. The standard deviation for all eight quarters measured from the \\nsample mean of 2.5% is 12.57%. The portfolio’s Sharpe ratio for the two-year \\nperiod is now 2.5/12.57 = 0.199; the Sharpe ratio for the benchmark remains \\n0.21. Thus, when returns for the two-year period are pooled, the manager \\nappears to have provided less return per unit of risk than the benchmark \\nand less when compared with the separate yearly results.\\nThe problem with using eight quarters of return data is that the analyst \\nhas violated the assumption that the sampled returns come from the same \\npopulation. As a result of the change in the manager’s investment strategy, \\nreturns in Year 2 followed a different distribution than returns in Year 1. \\nClearly, during Year 1, returns were generated by an underlying population \\nwith lower mean and variance than the population of the second year. Com-\\nbining the results for the first and second years yielded a sample that was \\nrepresentative of no population. Because the larger sample did not satisfy \\nmodel assumptions, any conclusions the analyst reached based on the larger \\nsample are incorrect. For this example, she was better off using a smaller \\nsample than a larger sample because the smaller sample represented a more \\nhomogeneous distribution of returns.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'The Central Limit Theorem and Distribution of the Sample Mean',\n",
       "     'page_number': 325,\n",
       "     'content': '',\n",
       "     'children': [{'title': 'The Central Limit Theorem',\n",
       "       'page_number': 325,\n",
       "       'content': 'The Central Limit Theorem and Distribution of the Sample Mean\\n315\\nTHE CENTRAL LIMIT THEOREM AND DISTRIBUTION \\nOF THE SAMPLE MEAN\\nexplain the central limit theorem and its importance\\ncalculate and interpret the ',\n",
       "       'children': []},\n",
       "      {'title': 'Standard Error of the Sample Mean',\n",
       "       'page_number': 327,\n",
       "       'content': 'Standard Error of the Sample Mean\\nThe central limit theorem states that the variance of the distribution of the sample \\nmean is σ2/n. The positive square root of variance is standard deviation. The standard \\ndeviation of a sample statistic is known as the standard error of the statistic. The \\nstandard error of the sample mean is an important quantity in applying the central \\nlimit theorem in practice.\\n■ \\nDefinition of the Standard Error of the Sample Mean. For sample mean   _\\n \\nX  \\ncalculated from a sample generated by a population with standard deviation \\nσ, the standard error of the sample mean is given by one of two expressions:\\n σ  \\n_\\n \\nX   =  σ _ \\n √ _ \\nn    \\n(1)\\nwhen we know σ, the population standard deviation, or by\\n s  \\n_\\n \\nX   =  s _ \\n √ _ \\nn    \\n(2)\\nwhen we do not know the population standard deviation and need to use \\nthe sample standard deviation, s, to estimate it.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n318\\nIn practice, we almost always need to use Equation 2. The estimate of s is given \\nby the square root of the sample variance, s2, calculated as follows:\\n s 2 =  \\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  \\n_\\n \\nX  ) \\n_ 2   \\nn − 1 \\n \\n(3)\\nIt is worth noting that although the standard error is the standard deviation of the \\nsampling distribution of the parameter, “standard deviation” in general and “standard \\nerror” are two distinct concepts, and the terms are not interchangeable. Simply put, \\nstandard deviation measures the dispersion of the data from the mean, whereas stan-\\ndard error measures how much inaccuracy of a population parameter estimate comes \\nfrom sampling. The contrast between standard deviation and standard error reflects \\nthe distinction between data description and inference. If we want to draw conclusions \\nabout how spread out the data are, standard deviation is the term to quote. If we want \\nto find out how precise the estimate of a population parameter from sampled data is \\nrelative to its true value, standard error is the metric to use.\\nWe will soon see how we can use the sample mean and its standard error to make \\nprobability statements about the population mean by using the technique of confidence \\nintervals. First, however, we provide an illustration of the central limit theorem’s force.\\nEXAMPLE 6\\nThe Central Limit Theorem\\nIt is remarkable that the sample mean for large sample sizes will be distributed \\nnormally regardless of the distribution of the underlying population. To illus-\\ntrate the central limit theorem in action, we specify in this example a distinctly \\nnon-normal distribution and use it to generate a large number of random samples \\nof size 100. We then calculate the sample mean for each sample and observe \\nthe frequency distribution of the calculated sample means. Does that sampling \\ndistribution look like a normal distribution?\\nWe return to the telecommunications analyst studying the capital expenditure \\nplans of telecom businesses. Suppose that capital expenditures for communi-\\ncations equipment form a continuous uniform random variable with a lower \\nbound equal to $0 and an upper bound equal to $100—for short, call this a \\nuniform (0, 100) random variable. The probability function of this continuous \\nuniform random variable has a rather simple shape that is anything but normal. \\nIt is a horizontal line with a vertical intercept equal to 1/100. Unlike a normal \\nrandom variable, for which outcomes close to the mean are most likely, all pos-\\nsible outcomes are equally likely for a uniform random variable.\\nTo illustrate the power of the central limit theorem, we conduct a Monte \\nCarlo simulation to study the capital expenditure plans of telecom businesses. \\nIn this simulation, we collect 200 random samples of the capital expenditures of \\n100 companies (200 random draws, each consisting of the capital expenditures \\nof 100 companies with n = 100). In each simulation trial, 100 values for capital \\nexpenditure are generated from the uniform (0, 100) distribution. For each ran-\\ndom sample, we then compute the sample mean. We conduct 200 simulation \\ntrials in total. Because we have specified the continuous random distribution \\ngenerating the samples, we know that the population mean capital expenditure \\nis equal to ($0 + $100 million)/2 = $50 million (i.e., µ = (a + b)/2) ; the popula-\\ntion variance of capital expenditures is equal to (100 − 0)2/12 = 833.33 (i.e., σ2 \\n= (b − a)2/12); thus, the standard deviation is $28.87 million and the standard \\nerror is  28.87 /  √ _ \\n100   = 2.887 under the central limit theorem.\\n© CFA Institute. For candidate use only. Not for distribution.\\nThe Central Limit Theorem and Distribution of the Sample Mean\\n319\\nThe results of this Monte Carlo experiment are tabulated in Exhibit 9 in the \\nform of a frequency distribution. This distribution is the estimated sampling \\ndistribution of the sample mean.\\n \\nExhibit 9: Frequency Distribution: 200 Random Samples of \\na Uniform (0,100) Random Variable\\n \\n \\nRange of Sample Means \\n($ million)\\nAbsolute Frequency\\n 42.5 ≤  _\\n \\nX  < 44 \\n1\\n 44 ≤  _\\n \\nX  < 45.5 \\n6\\n 45.5 ≤  _\\n \\nX  < 47 \\n22\\n 47 ≤  _\\n \\nX  < 48.5 \\n39\\n 48.5 ≤  _\\n \\nX  < 50 \\n41\\n 50 ≤  _\\n \\nX  < 51.5 \\n39\\n 51.5 ≤  _\\n \\nX  < 53 \\n23\\n 53 ≤  _\\n \\nX  < 54.5 \\n12\\n 54.5 ≤  _\\n \\nX  < 56 \\n12\\n 56 ≤  _\\n \\nX  < 57.5 \\n5\\n \\nNote:  _\\n \\nX  is the mean capital expenditure for each sample.\\nThe frequency distribution can be described as bell-shaped and centered \\nclose to the population mean of 50. The most frequent, or modal, range, with \\n41 observations, is 48.5 to 50. The overall average of the sample means is $49.92, \\nwith a standard error equal to $2.80. The calculated standard error is close to \\nthe value of 2.887 given by the central limit theorem. The discrepancy between \\ncalculated and expected values of the mean and standard deviation under the \\ncentral limit theorem is a result of random chance (sampling error).\\nIn summary, although the distribution of the underlying population is very \\nnon-normal, the simulation has shown that a normal distribution well describes \\nthe estimated sampling distribution of the sample mean, with mean and standard \\nerror consistent with the values predicted by the central limit theorem.\\nTo summarize, according to the central limit theorem, when we sample from any \\ndistribution, the distribution of the sample mean will have the following properties \\nas long as our sample size is large:\\n■ \\nThe distribution of the sample mean   _\\n \\nX  will be approximately normal.\\n■ \\nThe mean of the distribution of   _\\n \\nX   will be equal to the mean of the popula-\\ntion from which the samples are drawn.\\n■ \\nThe variance of the distribution of   _\\n \\nX  will be equal to the variance of the \\npopulation divided by the sample size.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n320\\nWe next discuss the concepts and tools related to estimating the population \\nparameters, with a special focus on the population mean. We focus on the population \\nmean because analysts are more likely to meet interval estimates for the population \\nmean than any other type of interval estimate.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Point Estimates of the Population Mean',\n",
       "     'page_number': 330,\n",
       "     'content': 'Learning Module 5 \\nSampling and Estimation\\n320\\nWe next discuss the concepts and tools related to estimating the population \\nparameters, with a special focus on the population mean. We focus on the population \\nmean because analysts are more likely to meet interval estimates for the population \\nmean than any other type of interval estimate.\\nPOINT ESTIMATES OF THE POPULATION MEAN\\nidentify and describe desirable properties of an estimator\\nStatistical inference traditionally consists of two branches, hypothesis testing and \\nestimation. Hypothesis testing addresses the question “Is the value of this parameter \\n(say, a population mean) equal to some specific value (0, for example)?” In this process, \\nwe have a hypothesis concerning the value of a parameter, and we seek to determine \\nwhether the evidence from a sample supports or does not support that hypothesis. \\nThe topic of hypothesis testing will be discussed later.\\nThe second branch of statistical inference, and what we focus on now, is estimation. \\nEstimation seeks an answer to the question “What is this parameter’s (for example, \\nthe population mean’s) value?” In estimating, unlike in hypothesis testing, we do not \\nstart with a hypothesis about a parameter’s value and seek to test it. Rather, we try \\nto make the best use of the information in a sample to form one of several types of \\nestimates of the parameter’s value. With estimation, we are interested in arriving \\nat a rule for best calculating a single number to estimate the unknown population \\nparameter (a point estimate). In addition to calculating a point estimate, we may also \\nbe interested in calculating a range of values that brackets the unknown population \\nparameter with some specified level of probability (a confidence interval). We first \\ndiscuss point estimates of parameters and then turn our attention to the formulation \\nof confidence intervals for the population mean.\\n',\n",
       "     'children': [{'title': 'Point Estimators',\n",
       "       'page_number': 330,\n",
       "       'content': 'Point Estimators\\nAn important concept introduced here is that sample statistics viewed as formulas \\ninvolving random outcomes are random variables. The formulas that we use to \\ncompute the sample mean and all the other sample statistics are examples of esti-\\nmation formulas or estimators. The particular value that we calculate from sample \\nobservations using an estimator is called an estimate. An estimator has a sampling \\ndistribution; an estimate is a fixed number pertaining to a given sample and thus has \\nno sampling distribution. To take the example of the mean, the calculated value of the \\nsample mean in a given sample, used as an estimate of the population mean, is called \\na point estimate of the population mean. As we have seen earlier, the formula for \\nthe sample mean can and will yield different results in repeated samples as different \\nsamples are drawn from the population.\\nIn many applications, we have a choice among a number of possible estimators \\nfor estimating a given parameter. How do we make our choice? We often select esti-\\nmators because they have one or more desirable statistical properties. Following is \\na brief description of three desirable properties of estimators: unbiasedness (lack of \\nbias), efficiency, and consistency.\\n■ \\nUnbiasedness. An unbiased estimator is one whose expected value (the \\nmean of its sampling distribution) equals the parameter it is intended to \\nestimate.\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nPoint Estimates of the Population Mean\\n321\\nFor example, as shown in Exhibit 10 of the sampling distribution of the sample \\nmean, the expected value of the sample mean,   _\\n \\nX  , equals μ, the population mean, so \\nwe say that the sample mean is an unbiased estimator (of the population mean). The \\nsample variance, s2, calculated using a divisor of n − 1 (Equation 3), is an unbiased \\nestimator of the population variance, σ2. If we were to calculate the sample variance \\nusing a divisor of n, the estimator would be biased: Its expected value would be smaller \\nthan the population variance. We would say that sample variance calculated with a \\ndivisor of n is a biased estimator of the population variance.\\nExhibit 10: Unbiasedness of an Estimator\\nX = μ\\nX = μ\\n–\\nWhenever one unbiased estimator of a parameter can be found, we can usually find \\na large number of other unbiased estimators. How do we choose among alternative \\nunbiased estimators? The criterion of efficiency provides a way to select from among \\nunbiased estimators of a parameter.\\n■ \\nEfficiency. An unbiased estimator is efficient if no other unbiased estimator \\nof the same parameter has a sampling distribution with smaller variance.\\nTo explain the definition, in repeated samples we expect the estimates from an \\nefficient estimator to be more tightly grouped around the mean than estimates from \\nother unbiased estimators. For example, Exhibit 11 shows the sampling distributions \\nof two different estimators of the population mean. Both estimators A and B are unbi-\\nased because their expected values are equal to the population mean (    _\\n \\nX    A   =    _\\n \\nX    B   = μ ), \\nbut estimator A is more efficient because it shows smaller variance. Efficiency is an \\nimportant property of an estimator. Sample mean   _\\n \\nX  is an efficient estimator of the \\npopulation mean; sample variance s2 is an efficient estimator of σ2.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n322\\nExhibit 11: Efficiency of an Estimator\\n–\\nXA = XB = μ\\nXA = XB = μ\\n–\\nA\\nB\\nRecall that a statistic’s sampling distribution is defined for a given sample size. Different \\nsample sizes define different sampling distributions. For example, the variance of sam-\\npling distribution of the sample mean is smaller for larger sample sizes. Unbiasedness \\nand efficiency are properties of an estimator’s sampling distribution that hold for any \\nsize sample. An unbiased estimator is unbiased equally in a sample of size 100 and \\nin a sample of size 1,000. In some problems, however, we cannot find estimators that \\nhave such desirable properties as unbiasedness in small samples. In this case, statisti-\\ncians may justify the choice of an estimator based on the properties of the estimator’s \\nsampling distribution in extremely large samples, the estimator’s so-called asymptotic \\nproperties. Among such properties, the most important is consistency.\\n■ \\nConsistency. A consistent estimator is one for which the probability of \\nestimates close to the value of the population parameter increases as sample \\nsize increases.\\nSomewhat more technically, we can define a consistent estimator as an estimator \\nwhose sampling distribution becomes concentrated on the value of the parameter it \\nis intended to estimate as the sample size approaches infinity. The sample mean, in \\naddition to being an efficient estimator, is also a consistent estimator of the population \\nmean: As sample size n goes to infinity, its standard error,  σ /  √ _ \\nn  , goes to 0 and its \\nsampling distribution becomes concentrated right over the value of population mean, \\nµ. Exhibit 12 illustrates the consistency of the sample mean, in which the standard \\nerror of the estimator narrows as the sample size increases. To summarize, we can \\nthink of a consistent estimator as one that tends to produce more and more accurate \\nestimates of the population parameter as we increase the sample’s size. If an estimator \\nis consistent, we may attempt to increase the accuracy of estimates of a population \\nparameter by calculating estimates using a larger sample. For an inconsistent esti-\\nmator, however, increasing sample size does not help to increase the probability of \\naccurate estimates.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPoint Estimates of the Population Mean\\n323\\nExhibit 12: Consistency of an Estimator\\n–X = μ\\nX = μ\\nn = 1,000\\nn = 10\\nIt is worth noting that in a Big Data world, consistency is much more crucial than effi-\\nciency, because the accuracy of a population parameter’s estimates can be increasingly \\nimproved with the availability of more sample data. In addition, given a big dataset, \\na biased but consistent estimator can offer considerably reduced error. For example, \\ns2/n is a biased estimator of variance. As n goes to infinity, the distinction between \\ns2/n and the unbiased estimator s2/(n − 1) diminish to zero.\\nEXAMPLE 7\\n \\nExhibit 13: Sampling Distributions of an Estimator\\n \\nμ\\nn = 1,000\\nn = 200\\nn = 50\\n1. Exhibit 13 plots several sampling distributions of an estimator for the pop-\\nulation mean, and the vertical dash line represents the true value of popula-\\ntion mean. \\nWhich of the following statements best describes the estimator’s properties?\\nA. The estimator is unbiased.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n324\\nB. The estimator is biased and inconsistent.\\nC. The estimator is biased but consistent.\\nSolution:\\nC is correct. The chart shows three sampling distributions of the estimator \\nat different sample sizes (n = 50, 200, and 1,000). We can observe that the \\nmeans of each sampling distribution—that is, the expected value of the \\nestimator—deviates from the population mean, so the estimator is biased. \\nAs the sample size increases, however, the mean of the sampling distribu-\\ntion draws closer to the population mean with smaller variance. So, it is a \\nconsistent estimator.\\nCONFIDENCE INTERVALS FOR THE POPULATION \\nMEAN AND SAMPLE SIZE SELECTION \\ncontrast a point estimate and a confidence interval estimate of a \\npopulation parameter\\ncalculate and interpret a confidence interval for a population mean, \\ngiven a normal distribution with 1) a known population variance, \\n2) an unknown population variance, or 3) an unknown population \\nvariance and a large sample size\\nWhen we need a single number as an estimate of a population parameter, we make \\nuse of a point estimate. However, because of sampling error, the point estimate is not \\nlikely to equal the population parameter in any given sample. Often, a more useful \\napproach than finding a point estimate is to find a range of values that we expect to \\nbracket the parameter with a specified level of probability—an interval estimate of \\nthe parameter. A confidence interval fulfills this role.\\n■ \\nDefinition of Confidence Interval. A confidence interval is a range for \\nwhich one can assert with a given probability 1 − α, called the degree of \\nconfidence, that it will contain the parameter it is intended to estimate. This \\ninterval is often referred to as the 100(1 − α)% confidence interval for the \\nparameter.\\nThe endpoints of a confidence interval are referred to as the lower and upper \\nconfidence limits. In this reading, we are concerned only with two-sided confidence \\nintervals—confidence intervals for which we calculate both lower and upper limits.\\nConfidence intervals are frequently given either a probabilistic interpretation or a \\npractical interpretation. In the probabilistic interpretation, we interpret a 95% confi-\\ndence interval for the population mean as follows. In repeated sampling, 95% of such \\nconfidence intervals will, in the long run, include or bracket the population mean. \\nFor example, suppose we sample from the population 1,000 times, and based on each \\nsample, we construct a 95% confidence interval using the calculated sample mean. \\nBecause of random chance, these confidence intervals will vary from each other, but we \\nexpect 95%, or 950, of these intervals to include the unknown value of the population \\nmean. In practice, we generally do not carry out such repeated sampling. Therefore, \\nin the practical interpretation, we assert that we are 95% confident that a single 95% \\nconfidence interval contains the population mean. We are justified in making this \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Confidence Intervals for the Population Mean and Sample Size Selection ',\n",
       "     'page_number': 334,\n",
       "     'content': 'Learning Module 5 \\nSampling and Estimation\\n324\\nB. The estimator is biased and inconsistent.\\nC. The estimator is biased but consistent.\\nSolution:\\nC is correct. The chart shows three sampling distributions of the estimator \\nat different sample sizes (n = 50, 200, and 1,000). We can observe that the \\nmeans of each sampling distribution—that is, the expected value of the \\nestimator—deviates from the population mean, so the estimator is biased. \\nAs the sample size increases, however, the mean of the sampling distribu-\\ntion draws closer to the population mean with smaller variance. So, it is a \\nconsistent estimator.\\nCONFIDENCE INTERVALS FOR THE POPULATION \\nMEAN AND SAMPLE SIZE SELECTION \\ncontrast a point estimate and a confidence interval estimate of a \\npopulation parameter\\ncalculate and interpret a confidence interval for a population mean, \\ngiven a normal distribution with 1) a known population variance, \\n2) an unknown population variance, or 3) an unknown population \\nvariance and a large sample size\\nWhen we need a single number as an estimate of a population parameter, we make \\nuse of a point estimate. However, because of sampling error, the point estimate is not \\nlikely to equal the population parameter in any given sample. Often, a more useful \\napproach than finding a point estimate is to find a range of values that we expect to \\nbracket the parameter with a specified level of probability—an interval estimate of \\nthe parameter. A confidence interval fulfills this role.\\n■ \\nDefinition of Confidence Interval. A confidence interval is a range for \\nwhich one can assert with a given probability 1 − α, called the degree of \\nconfidence, that it will contain the parameter it is intended to estimate. This \\ninterval is often referred to as the 100(1 − α)% confidence interval for the \\nparameter.\\nThe endpoints of a confidence interval are referred to as the lower and upper \\nconfidence limits. In this reading, we are concerned only with two-sided confidence \\nintervals—confidence intervals for which we calculate both lower and upper limits.\\nConfidence intervals are frequently given either a probabilistic interpretation or a \\npractical interpretation. In the probabilistic interpretation, we interpret a 95% confi-\\ndence interval for the population mean as follows. In repeated sampling, 95% of such \\nconfidence intervals will, in the long run, include or bracket the population mean. \\nFor example, suppose we sample from the population 1,000 times, and based on each \\nsample, we construct a 95% confidence interval using the calculated sample mean. \\nBecause of random chance, these confidence intervals will vary from each other, but we \\nexpect 95%, or 950, of these intervals to include the unknown value of the population \\nmean. In practice, we generally do not carry out such repeated sampling. Therefore, \\nin the practical interpretation, we assert that we are 95% confident that a single 95% \\nconfidence interval contains the population mean. We are justified in making this \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nConfidence Intervals for the Population Mean and Sample Size Selection \\n325\\nstatement because we know that 95% of all possible confidence intervals constructed \\nin the same manner will contain the population mean. The confidence intervals that \\nwe discuss in this reading have structures similar to the following basic structure:\\n■ \\nConstruction of Confidence Intervals. A 100(1 − α)% confidence interval \\nfor a parameter has the following structure:\\n Point estimate ± Reliability factor × Standard error\\nwhere\\nPoint estimate = a point estimate of the parameter (a value of a sample \\nstatistic)\\nReliability factor = a number based on the assumed distribution of the point \\nestimate and the degree of confidence (1 − α) for the confidence interval\\nStandard error = the standard error of the sample statistic providing the \\npoint estimate\\nThe quantity “Reliability factor × Standard error” is sometimes called the \\nprecision of the estimator; larger values of the product imply lower preci-\\nsion in estimating the population parameter.\\nThe most basic confidence interval for the population mean arises when we are \\nsampling from a normal distribution with known variance. The reliability factor in \\nthis case is based on the standard normal distribution, which has a mean of 0 and a \\nvariance of 1. A standard normal random variable is conventionally denoted by Z. \\nThe notation zα denotes the point of the standard normal distribution such that α \\nof the probability remains in the right tail. For example, 0.05 or 5% of the possible \\nvalues of a standard normal random variable are larger than z0.05 = 1.65. Similarly, \\n0.025 or 2.5% of the possible values of a standard normal random variable are larger \\nthan z0.025 = 1.96.\\nSuppose we want to construct a 95% confidence interval for the population mean \\nand, for this purpose, we have taken a sample of size 100 from a normally distributed \\npopulation with known variance of σ2 = 400 (so, σ = 20). We calculate a sample mean \\nof  _\\n \\nX  = 25. Our point estimate of the population mean is, therefore, 25. If we move \\n1.96 standard deviations above the mean of a normal distribution, 0.025 or 2.5% of \\nthe probability remains in the right tail; by symmetry of the normal distribution, if \\nwe move 1.96 standard deviations below the mean, 0.025 or 2.5% of the probability \\nremains in the left tail. In total, 0.05 or 5% of the probability is in the two tails and 0.95 \\nor 95% lies in between. So, z0.025 = 1.96 is the reliability factor for this 95% confidence \\ninterval. Note the relationship 100(1 − α)% for the confidence interval and the zα/2 for \\nthe reliability factor. The standard error of the sample mean, given by Equation 1, is  \\nσ  _\\n \\nX   = 20 /  √ _ \\n100   = 2. The confidence interval, therefore, has a lower limit of   _\\n \\nX  − 1.96 \\nσ  _\\n \\nX   = 25 − 1.96(2) = 25 − 3.92 = 21.08. The upper limit of the confidence interval \\nis   _\\n \\nX  + 1.96  σ  _\\n \\nX   = 25 + 1.96(2) = 25 + 3.92 = 28.92. The 95% confidence interval for the \\npopulation mean spans 21.08 to 28.92.\\n■ \\nConfidence Intervals for the Population Mean (Normally Distributed \\nPopulation with Known Variance). A 100(1 − α)% confidence interval for \\npopulation mean µ when we are sampling from a normal distribution with \\nknown variance σ2 is given by\\n \\n_\\n \\nX  ±  z α/2  σ _ \\n √ _ \\nn    \\n(4)\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n326\\nThe reliability factors for the most frequently used confidence intervals are as \\nfollows.\\n■ \\nReliability Factors for Confidence Intervals Based on the Standard \\nNormal Distribution. We use the following reliability factors when we con-\\nstruct confidence intervals based on the standard normal distribution:\\n● \\n90% confidence intervals: Use z0.05 = 1.65\\n● \\n95% confidence intervals: Use z0.025 = 1.96\\n● \\n99% confidence intervals: Use z0.005 = 2.58\\nThese reliability factors highlight an important fact about all confidence intervals. \\nAs we increase the degree of confidence, the confidence interval becomes wider and \\ngives us less precise information about the quantity we want to estimate.\\nExhibit 14 demonstrates how a confidence interval works. We again use the daily \\nreturns of the fictitious Euro-Asia-Africa Equity Index shown earlier. The dataset \\nconsists of 1,258 observations with a population mean of 0.035% and a population \\nstandard deviation of 0.834%. We conduct random sampling from the population 1,000 \\ntimes, drawing a sample of a hundred daily returns (n = 100) each time.\\nWe construct a histogram of the sample means, shown in Exhibit 14. The shape \\nappears to be that of a normal distribution, in line with the central limit theorem. \\nWe next pick one random sample to construct confidence intervals around its sample \\nmean. The mean of the selected sample is computed to be 0.103% (as plotted with a \\nsolid line). Next we construct 99%, 95%, and 50% confidence intervals around that \\nsample mean. We use Equation 4, to compute the upper and lower bounds of each \\npair of confidence intervals and plot these bounds in dashed lines.\\nThe resulting chart shows that confidence intervals narrow with decreasing con-\\nfidence level, and vice versa. For example, the narrowest confidence interval in the \\nchart corresponds to the lowest confidence level of 50%—that is, we are only 50% \\nconfident that the population mean falls within the 50% confidence interval around \\nthe sample mean. Importantly, as shown by Equation 4, given a fixed confidence level, \\nthe confidence interval narrows with smaller population deviation and greater sample \\nsize, indicating higher estimate accuracy.\\n© CFA Institute. For candidate use only. Not for distribution.\\nConfidence Intervals for the Population Mean and Sample Size Selection \\n327\\nExhibit 14: Illustration of Confidence Intervals\\n4\\n3\\n2\\n1\\n0\\nSample Mean Mass\\n–0.2\\n–0.2\\n–0.4\\n–0.4\\n0.4\\n0.4\\n0\\n0.2\\n0.2\\nSample Mean\\n50% Confidence Interval\\n99% Confidence Interval\\n95% Confidence Interval\\nIn practice, the assumption that the sampling distribution of the sample mean is at \\nleast approximately normal is frequently reasonable, either because the underlying \\ndistribution is approximately normal or because we have a large sample and the cen-\\ntral limit theorem applies. Rarely do we know the population variance in practice, \\nhowever. When the population variance is unknown but the sample mean is at least \\napproximately normally distributed, we have two acceptable ways to calculate the \\nconfidence interval for the population mean. We will soon discuss the more conserva-\\ntive approach, which is based on Student’s t-distribution (the t-distribution, for short \\nand covered earlier). In investment literature, it is the most frequently used approach \\nin both estimation and hypothesis tests concerning the mean when the population \\nvariance is not known, whether sample size is small or large.\\nA second approach to confidence intervals for the population mean, based on the \\nstandard normal distribution, is the z-alternative. It can be used only when sample \\nsize is large. (In general, a sample size of 30 or larger may be considered large.) In \\ncontrast to the confidence interval given in Equation 4, this confidence interval uses \\nthe sample standard deviation, s, in computing the standard error of the sample mean \\n(Equation 2).\\n■ \\nConfidence Intervals for the Population Mean—The z-Alternative (Large \\nSample, Population Variance Unknown). A 100(1 − α)% confidence \\ninterval for population mean µ when sampling from any distribution with \\nunknown variance and when sample size is large is given by\\n \\n_\\n \\nX  ±  z α/2  s _ \\n √ _ \\nn    \\n(5)\\nBecause this type of confidence interval appears quite often, we illustrate its cal-\\nculation in Example 8.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n328\\nEXAMPLE 8\\nConfidence Interval for the Population Mean of Sharpe \\nRatios—z-Statistic\\n1. Suppose an investment analyst takes a random sample of US equity mutual \\nfunds and calculates the average Sharpe ratio. The sample size is 100, and \\nthe average Sharpe ratio is 0.45. The sample has a standard deviation of \\n0.30. Calculate and interpret the 90% confidence interval for the population \\nmean of all US equity mutual funds by using a reliability factor based on the \\nstandard normal distribution.\\nThe reliability factor for a 90% confidence interval, as given earlier, is z0.05 = \\n1.65. The confidence interval will be\\n \\n_\\n \\nX  ±  z 0.05  s _ \\n √ _ \\nn    = 0.45 ± 1.65 0.30 \\n_ \\n √ _ \\n100    = 0.45 ± 1.65  ( 0.03 )  = 0.45 ± 0.0495 \\nThe confidence interval spans 0.4005 to 0.4995, or 0.40 to 0.50, carrying two \\ndecimal places. The analyst can say with 90% confidence that the interval \\nincludes the population mean.\\nIn this example, the analyst makes no specific assumption about the proba-\\nbility distribution describing the population. Rather, the analyst relies on the \\ncentral limit theorem to produce an approximate normal distribution for the \\nsample mean.\\nAs Example 8 shows, even if we are unsure of the underlying population distri-\\nbution, we can still construct confidence intervals for the population mean as long as \\nthe sample size is large because we can apply the central limit theorem.\\nWe now turn to the conservative alternative, using the t-distribution, for con-\\nstructing confidence intervals for the population mean when the population variance \\nis not known. For confidence intervals based on samples from normally distributed \\npopulations with unknown variance, the theoretically correct reliability factor is \\nbased on the t-distribution. Using a reliability factor based on the t-distribution is \\nessential for a small sample size. Using a t reliability factor is appropriate when the \\npopulation variance is unknown, even when we have a large sample and could use the \\ncentral limit theorem to justify using a z reliability factor. In this large sample case, \\nthe t-distribution provides more-conservative (wider) confidence intervals.\\nSuppose we sample from a normal distribution. The ratio  z =   ( _\\n \\nX  − μ )   /    ( σ /  √ _ \\nn  )  is \\ndistributed normally with a mean of 0 and standard deviation of 1; however, the ratio  \\nt =   ( _\\n \\nX  − μ )   /    ( s /  √ _ \\nn  )  follows the t-distribution with a mean of 0 and n − 1 degrees of \\nfreedom. The ratio represented by t is not normal because t is the ratio of two random \\nvariables, the sample mean and the sample standard deviation. The definition of the \\nstandard normal random variable involves only one random variable, the sample mean.\\nValues for the t-distribution are available from Excel, using the function T.INV(p,DF). \\nFor each degree of freedom, five values are given: t0.10, t0.05, t0.025, t0.01, and t0.005. The \\nvalues for t0.10, t0.05, t0.025, t0.01, and t0.005 are such that, respectively, 0.10, 0.05, 0.025, \\n0.01, and 0.005 of the probability remains in the right tail, for the specified number \\nof degrees of freedom. For example, for df = 30, t0.10 = 1.310, t0.05 = 1.697, t0.025 = \\n2.042, t0.01 = 2.457, and t0.005 = 2.750.\\n© CFA Institute. For candidate use only. Not for distribution.\\nConfidence Intervals for the Population Mean and Sample Size Selection \\n329\\nWe now give the form of confidence intervals for the population mean using the \\nt-distribution.\\n■ \\nConfidence Intervals for the Population Mean (Population Variance \\nUnknown)—t-Distribution. If we are sampling from a population with \\nunknown variance and either of the conditions below holds:\\n● \\nthe sample is large, or\\n● \\nthe sample is small, but the population is normally distributed, or \\napproximately normally distributed,\\nthen a 100(1 − α)% confidence interval for the population mean µ is given by\\n \\n_\\n \\nX  ±  t α/2  s _ \\n √ _ \\nn    \\n(6)\\nwhere the number of degrees of freedom for tα/2 is n − 1 and n is the sample \\nsize.\\nExample 9 reprises the data of Example 8 but uses the t-statistic rather than the \\nz-statistic to calculate a confidence interval for the population mean of Sharpe ratios.\\nEXAMPLE 9\\nConfidence Interval for the Population Mean of Sharpe \\nRatios—t-Statistic\\nAs in Example 8, an investment analyst seeks to calculate a 90% confidence \\ninterval for the population mean Sharpe ratio of US equity mutual funds based \\non a random sample of 100 US equity mutual funds. The sample mean Sharpe \\nratio is 0.45, and the sample standard deviation of the Sharpe ratios is 0.30. Now \\nrecognizing that the population variance of the distribution of Sharpe ratios \\nis unknown, the analyst decides to calculate the confidence interval using the \\ntheoretically correct t-statistic.\\nBecause the sample size is 100, df = 99. Using the Excel function T.INV(0.05,99), \\nt0.05 = 1.66. This reliability factor is slightly larger than the reliability factor z0.05 \\n= 1.65 that was used in Example 8. The confidence interval will be\\n \\n_\\n \\nX  ±  t 0.05  s _ \\n √ _ \\nn    = 0.45 ± 1.66 0.30 \\n_ \\n √ _ \\n100    = 0.45 ± 1.66  ( 0.03 )  = 0.45 ± 0.0498 .\\nThe confidence interval spans 0.4002 to 0.4998, or 0.40 to 0.50, carrying two \\ndecimal places. To two decimal places, the confidence interval is unchanged \\nfrom the one computed in Example 8.\\nExhibit 15 summarizes the various reliability factors that we have used.\\n \\nExhibit 15: Basis of Computing Reliability Factors\\n \\n \\nSampling from\\nStatistic for Small \\nSample Size\\nStatistic for Large \\nSample Size\\nNormal distribution with known \\nvariance\\nz\\nz\\nNormal distribution with \\nunknown variance\\nt\\nt*\\nNon-normal distribution with \\nknown variance\\nnot available\\nz\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n330\\nSampling from\\nStatistic for Small \\nSample Size\\nStatistic for Large \\nSample Size\\nNon-normal distribution with \\nunknown variance\\nnot available\\nt*\\n \\n* Use of z also acceptable.\\nExhibit 16 shows a flowchart that helps determine what statistics should be used \\nto produce confidence intervals under different conditions.\\nExhibit 16: Determining Statistics for Confidence Intervals\\nLarge Sample\\nSize?\\nPopulation\\nVariance Known?\\nNormal\\nDistribution?\\nPopulation\\nVariance Known?\\nt\\n(z is acceptable)\\nz\\nz\\nt\\nNot Available\\nNo\\nNo\\nNo\\nNo\\nYes\\nYes\\nYes\\nYes\\nSelection of Sample Size\\nWhat choices affect the width of a confidence interval? To this point we have discussed \\ntwo factors that affect width: the choice of statistic (t or z) and the choice of degree of \\nconfidence (affecting which specific value of t or z we use). These two choices deter-\\nmine the reliability factor. (Recall that a confidence interval has the structure Point \\nestimate ± Reliability factor × Standard error.)\\nThe choice of sample size also affects the width of a confidence interval. All else \\nequal, a larger sample size decreases the width of a confidence interval. Recall the \\nexpression for the standard error of the sample mean:\\n Standard error of the sample mean =  \\nSample standard deviation \\n \\n___________________ \\n \\n √ _ \\nSample size   \\n \\nWe see that the standard error varies inversely with the square root of sample \\nsize. As we increase sample size, the standard error decreases and consequently the \\nwidth of the confidence interval also decreases. The larger the sample size, the greater \\nprecision with which we can estimate the population parameter.\\nAt a given degree of confidence (1 − α), we can determine the sample size needed \\nto obtain a desired width for a confidence interval. Define E = Reliability factor × \\nStandard error; then 2E is the confidence interval’s width. The smaller E is, the smaller \\nthe width of the confidence interval. Accordingly, the sample size to obtain a desired \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': [{'title': 'Selection of Sample Size',\n",
       "       'page_number': 340,\n",
       "       'content': 'Selection of Sample Size\\nWhat choices affect the width of a confidence interval? To this point we have discussed \\ntwo factors that affect width: the choice of statistic (t or z) and the choice of degree of \\nconfidence (affecting which specific value of t or z we use). These two choices deter-\\nmine the reliability factor. (Recall that a confidence interval has the structure Point \\nestimate ± Reliability factor × Standard error.)\\nThe choice of sample size also affects the width of a confidence interval. All else \\nequal, a larger sample size decreases the width of a confidence interval. Recall the \\nexpression for the standard error of the sample mean:\\n Standard error of the sample mean =  \\nSample standard deviation \\n \\n___________________ \\n \\n √ _ \\nSample size   \\n \\nWe see that the standard error varies inversely with the square root of sample \\nsize. As we increase sample size, the standard error decreases and consequently the \\nwidth of the confidence interval also decreases. The larger the sample size, the greater \\nprecision with which we can estimate the population parameter.\\nAt a given degree of confidence (1 − α), we can determine the sample size needed \\nto obtain a desired width for a confidence interval. Define E = Reliability factor × \\nStandard error; then 2E is the confidence interval’s width. The smaller E is, the smaller \\nthe width of the confidence interval. Accordingly, the sample size to obtain a desired \\n© CFA Institute. For candidate use only. Not for distribution.\\nConfidence Intervals for the Population Mean and Sample Size Selection \\n331\\nvalue of E at a given degree of confidence (1 − α) can be derived as n = [(t × s)/E]2. \\nIt is worth noting that appropriate sample size is also needed for performing a valid \\npower analysis and determining the minimum detectable effect in hypothesis testing, \\nconcepts that will be covered at a later stage.\\nAll else equal, larger samples are good, in that sense. In practice, however, two \\nconsiderations may operate against increasing sample size. First, as we saw earlier \\nconcerning the Sharpe ratio, increasing the size of a sample may result in sampling \\nfrom more than one population. Second, increasing sample size may involve additional \\nexpenses that outweigh the value of additional precision. Thus three issues that the \\nanalyst should weigh in selecting sample size are the need for precision, the risk of \\nsampling from more than one population, and the expenses of different sample sizes.\\nEXAMPLE 10\\nA Money Manager Estimates Net Client Inflows\\nA money manager wants to obtain a 95% confidence interval for fund inflows and \\noutflows over the next six months for his existing clients. He begins by calling \\na random sample of 10 clients and inquiring about their planned additions to \\nand withdrawals from the fund. The manager then computes the change in cash \\nflow for each client sampled as a percentage change in total funds placed with \\nthe manager. A positive percentage change indicates a net cash inflow to the \\nclient’s account, and a negative percentage change indicates a net cash outflow \\nfrom the client’s account. The manager weights each response by the relative \\nsize of the account within the sample and then computes a weighted average.\\nAs a result of this process, the money manager computes a weighted average \\nof 5.5%. Thus, a point estimate is that the total amount of funds under manage-\\nment will increase by 5.5% in the next six months. The standard deviation of the \\nobservations in the sample is 10%. A histogram of past data looks fairly close to \\nnormal, so the manager assumes the population is normal.\\n1. Calculate a 95% confidence interval for the population mean and interpret \\nyour findings.\\nSolution to 1:\\nBecause the population variance is unknown and the sample size is small, \\nthe manager must use the t-statistic in Equation 6 to calculate the confi-\\ndence interval. Based on the sample size of 10, df = n − 1 = 10 − 1 = 9. For \\na 95% confidence interval, he needs to use the value of t0.025 for df = 9. This \\nvalue is 2.262, using Excel function T.INV(0.025,9). Therefore, a 95% confi-\\ndence interval for the population mean is\\n  \\n_\\n \\nX  ±  t 0.025   s _ \\n √ _ \\nn    = 5.5 %  ± 2.262  10% \\n_ \\n √ _ \\n10    \\n  \\n \\n \\n= 5.5 %    ±   2.262  ( 3.162 )    \\n \\n \\n= 5.5 %    ±   7.15%\\n \\n \\nThe confidence interval for the population mean spans −1.65% to +12.65%. \\nThe manager can be confident at the 95% level that this range includes the \\npopulation mean.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n332\\n2. The manager decides to see what the confidence interval would look like if \\nhe had used a sample size of 20 or 30 and found the same mean (5.5%) and \\nstandard deviation (10%).\\nCompute the confidence interval for sample sizes of 20 and 30. For the sam-\\nple size of 30, use Equation 6.\\nSolution to 2:\\nExhibit 17 gives the calculations for the three sample sizes.\\n \\nExhibit 17: The 95% Confidence Interval for Three Sample Sizes\\n \\n \\nDistribution\\n95% \\nConfidence Interval\\nLower \\nBound\\nUpper \\nBound\\nRelative \\nSize\\nt(n = 10)\\n5.5% ± 2.262(3.162)\\n−1.65%\\n12.65%\\n100.0%\\nt(n = 20)\\n5.5% ± 2.093(2.236)\\n0.82\\n10.18\\n65.5\\nt(n = 30)\\n5.5% ± 2.045(1.826)\\n1.77\\n9.23\\n52.2\\n \\n3. Interpret your results from Parts 1 and 2.\\nSolution to 3:\\nThe width of the confidence interval decreases as we increase the sample \\nsize. This decrease is a function of the standard error becoming smaller as n \\nincreases. The reliability factor also becomes smaller as the number of de-\\ngrees of freedom increases. The last column of Exhibit 17 shows the relative \\nsize of the width of confidence intervals based on n = 10 to be 100%. Using a \\nsample size of 20 reduces the confidence interval’s width to 65.5% of the in-\\nterval width for a sample size of 10. Using a sample size of 30 cuts the width \\nof the interval almost in half. Comparing these choices, the money manager \\nwould obtain the most precise results using a sample of 30.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Resampling',\n",
       "     'page_number': 342,\n",
       "     'content': 'RESAMPLING\\ndescribe the use of resampling (bootstrap, jackknife) to estimate the \\nsampling distribution of a statistic\\nEarlier, we demonstrated how to find the standard error of the sample mean, which can \\nbe computed using Equation 4 based on the central limit theorem. We now introduce \\na computational tool called resampling, which repeatedly draws samples from the \\noriginal observed data sample for the statistical inference of population parameters. \\nBootstrap, one of the most popular resampling methods, uses computer simulation \\nfor statistical inference without using an analytical formula such as a z-statistic or \\nt-statistic.\\nThe idea behind bootstrap is to mimic the process of performing random sampling \\nfrom a population, similar to what we have shown earlier, to construct the sampling \\ndistribution of the sample mean. The difference lies in the fact that we have no knowl-\\nedge of what the population looks like, except for a sample with size n drawn from the \\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nResampling\\n333\\npopulation. Because a random sample offers a good representation of the population, \\nwe can simulate sampling from the population by sampling from the observed sample. \\nIn other words, the bootstrap mimics the process by treating the randomly drawn \\nsample as if it were the population.\\nThe right-hand side of Exhibit 18 illustrates the process. In bootstrap, we repeat-\\nedly draw samples from the original sample, and each resample is of the same size as \\nthe original sample. Note that each item drawn is replaced for the next draw (i.e., the \\nidentical element is put back into the group so that it can be drawn more than once). \\nAssuming we are looking to find the standard error of sample mean, we take many \\nresamples and then compute the mean of each resample. Note that although some \\nitems may appear several times in the resamples, other items may not appear at all.\\nExhibit 18: Bootstrap Resampling\\nTrue Population\\nEstimated\\nPopulation\\nTrue Sampling Distribution\\nSample 1\\nEstimate 1\\nSample 2 ...\\nEstimate 2\\nSample n\\nSample\\nEstimate n\\nBootstrap Sampling Distribution\\nBootstrap\\nSample 1\\nEstimate 1\\nBootstrap\\nSample 2\\nBootstrap\\nSample “B”\\n...\\nEstimate 2\\nEstimate B\\nSubsequently, we construct a sampling distribution with these resamples. The boot-\\nstrap sampling distribution (right-hand side of Exhibit 18) will approximate the true \\nsampling distribution. We estimate the standard error of the sample mean using \\nEquation 7. Note that to distinguish the foregoing resampling process from other types \\nof resampling, it is often called model-free resampling or non-parametric resampling.\\n s  \\n_\\n \\nX   =  √ \\n_________________ \\n \\n 1 \\n_ \\nB − 1  ∑ b=1  \\nB   ( ˆ \\nθ  b −  \\n_\\n \\nθ  ) \\n2   \\n(7)\\nwhere\\n S  \\n_\\n \\nX   is the estimate of the standard error of the sample mean\\n B denotes the number of resamples drawn from the original sample.\\n ˆ \\nθ  b denotes the mean of a resample\\n \\n_\\n \\nθ  denotes the mean across all the resample means\\nBootstrap is one of the most powerful and widely used tools for statistical inference. \\nAs we have explained, it can be used to estimate the standard error of sample mean. \\nSimilarly, bootstrap can be used to find the standard error or construct confidence \\nintervals for the statistic of other population parameters, such as the median, which \\ndoes not apply to the previously discussed methodologies. Compared with conventional \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n334\\nstatistical methods, bootstrap does not rely on an analytical formula to estimate the \\ndistribution of the estimators. It is a simple but powerful method for any complicated \\nestimators and particularly useful when no analytical formula is available. In addition, \\nbootstrap has potential advantages in accuracy. Given these advantages, bootstrap \\ncan be applied widely in finance, such as for historical simulations in asset allocation \\nor in gauging an investment strategy’s performance against a benchmark.\\nEXAMPLE 11\\nBootstrap Resampling Illustration\\nThe following table displays a set of 12 monthly returns of a rarely traded stock, \\nshown in Column A. Our aim is to calculate the standard error of the sample \\nmean. Using the bootstrap resampling method, a series of bootstrap samples, \\nlabelled as “resamples” (with replacement) are drawn from the sample of 12 \\nreturns. Notice how some of the returns from data sample in Column A feature \\nmore than once in some of the resamples (for example, 0.055 features twice in \\nResample 1).\\n \\nColumn A\\n\\xa0\\nResample \\n1\\nResample \\n2\\nResample \\n3\\n\\xa0\\n\\xa0\\nResample \\n1,000\\n−0.096\\n\\xa0\\n0.055\\n−0.096\\n−0.033\\n…..\\n\\xa0\\n−0.072\\n−0.132\\n\\xa0\\n−0.033\\n0.055\\n−0.132\\n…..\\n\\xa0\\n0.255\\n−0.191\\n\\xa0\\n0.255\\n0.055\\n−0.157\\n…..\\n\\xa0\\n0.055\\n−0.096\\n\\xa0\\n−0.033\\n−0.157\\n0.255\\n…..\\n\\xa0\\n0.296\\n0.055\\n\\xa0\\n0.255\\n−0.096\\n−0.132\\n…..\\n\\xa0\\n0.055\\n−0.053\\n\\xa0\\n−0.157\\n−0.053\\n−0.191\\n…..\\n\\xa0\\n−0.096\\n−0.033\\n\\xa0\\n−0.053\\n−0.096\\n0.055\\n…..\\n\\xa0\\n0.296\\n0.296\\n\\xa0\\n−0.191\\n−0.132\\n0.255\\n…..\\n\\xa0\\n−0.132\\n0.055\\n\\xa0\\n−0.132\\n−0.132\\n0.296\\n…..\\n\\xa0\\n0.055\\n−0.072\\n\\xa0\\n−0.096\\n0.055\\n−0.096\\n…..\\n\\xa0\\n−0.096\\n0.255\\n\\xa0\\n0.055\\n−0.072\\n0.055\\n…..\\n\\xa0\\n−0.191\\n−0.157\\n\\xa0\\n−0.157\\n−0.053\\n−0.157\\n…..\\n\\xa0\\n0.055\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSample \\nmean\\n\\xa0\\n−0.019\\n−0.060\\n0.001\\n…..\\n\\xa0\\n0.040\\n \\nDrawing 1,000 such samples, we obtain 1,000 sample means. The mean across \\nall resample means is −0.01367. The sum of squares of the differences between \\neach sample mean and the mean across all resample means (  ∑ b=1  \\nB   ( ˆ \\nθ  b −  \\n_\\n \\nθ  ) \\n2  ) \\nis 1.94143. Using Equation 7, we calculate an estimate of the standard error of \\nthe sample mean:\\n s  \\n_\\n \\nX   =  √ \\n_________________ \\n \\n 1 \\n_ \\nB − 1  ∑ b=1  \\nB   ( ˆ \\nθ  b −  \\n_\\n \\nθ  ) \\n2   =  √ \\n____________ \\n \\n 1 _ \\n999  × 1.94143  = 0.04408 \\nJackknife is another resampling technique for statistical inference of population \\nparameters. Unlike bootstrap, which repeatedly draws samples with replacement, \\njackknife samples are selected by taking the original observed data sample and leaving \\nout one observation at a time from the set (and not replacing it). Jackknife method is \\noften used to reduce the bias of an estimator, and other applications include finding the \\nstandard error and confidence interval of an estimator. According to its computation \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Sampling Related  Biases',\n",
       "     'page_number': 345,\n",
       "     'content': 'Sampling Related  Biases\\n335\\nprocedure, we can conclude that jackknife produces similar results for every run, \\nwhereas bootstrap usually gives different results because bootstrap resamples are ran-\\ndomly drawn. For a sample of size n, jackknife usually requires n repetitions, whereas \\nwith bootstrap, we are left to determine how many repetitions are appropriate.\\nEXAMPLE 12\\nAn analyst in a real estate investment company is researching the housing market \\nof the Greater Boston area. From a sample of collected house sale price data \\nin the past year, she estimates the median house price of the area. To find the \\nstandard error of the estimated median, she is considering two options:\\noption 1 \\n The standard error of the sample median can be given \\nby  s _ \\n √ _ \\nn    , where s denotes the sample standard deviation and n \\ndenotes the sample size.\\noption 2 \\nApply the bootstrap method to construct the sampling \\ndistribution of the sample median, and then compute the \\nstandard error by using Equation 7.\\n1. Which of the following statements is accurate?\\nA. Option 1 is suitable to find the standard error of the sample median.\\nB. Option 2 is suitable to find the standard error of the sample median.\\nC. Both options are suitable to find the standard error of the sample \\nmedian.\\nSolution: \\nB is correct. Option 1 is valid for estimating the standard error of the sample \\nmean but not for that of the sample median, which is not based on the \\ngiven formula. Thus, both A and C are incorrect. The bootstrap method is \\na simple way to find the standard error of an estimator even if no analytical \\nformula is available or it is too complicated.\\nHaving covered many of the fundamental concepts of sampling and estimation, \\nwe are in a good position to focus on sampling issues of special concern to analysts. \\nThe quality of inferences depends on the quality of the data as well as on the quality \\nof the sampling plan used. Financial data pose special problems, and sampling plans \\nfrequently reflect one or more biases. The next section discusses these issues.\\nSAMPLING RELATED  BIASES\\ndescribe the issues regarding selection of the appropriate sample \\nsize, ',\n",
       "     'children': [{'title': 'Data Snooping Bias',\n",
       "       'page_number': 346,\n",
       "       'content': 'Data Snooping Bias\\nData snooping relates to overuse of the same or related data in ways that we shall \\ndescribe shortly. Data snooping bias refers to the errors that arise from such misuse of \\ndata. Investment strategies that reflect data snooping biases are often not successful if \\napplied in the future. Nevertheless, both investment practitioners and researchers in \\ngeneral have frequently engaged in data snooping. Analysts thus need to understand \\nand guard against this problem.\\nData snooping is the practice of determining a model by extensive searching \\nthrough a dataset for statistically significant patterns (that is, repeatedly “drilling” in \\nthe same data until finding something that appears to work). In exercises involving \\nstatistical significance, we set a significance level, which is the probability of rejecting \\nthe hypothesis we are testing when the hypothesis is in fact correct. Because rejecting \\na true hypothesis is undesirable, the investigator often sets the significance level at a \\nrelatively small number, such as 0.05 or 5%.\\nSuppose we test the hypothesis that a variable does not predict stock returns, \\nand we test in turn 100 different variables. Let us also suppose that in truth, none \\nof the 100 variables has the ability to predict stock returns. Using a 5% significance \\nlevel in our tests, we would still expect that 5 out of 100 variables would appear to \\nbe significant predictors of stock returns because of random chance alone. We have \\nmined the data to find some apparently significant variables. In essence, we have \\nexplored the same data again and again until we found some after-the-fact pattern \\nor patterns in the dataset. This is the sense in which data snooping involves overuse \\nof data. If we were to report only the significant variables without also reporting the \\ntotal number of variables tested that were unsuccessful as predictors, we would be \\npresenting a very misleading picture of our findings. Our results would appear to be \\nfar more significant than they actually were, because a series of tests such as the one \\njust described invalidates the conventional interpretation of a given significance level \\n(such as 5%), according to the theory of inference. Datasets in the Big Data space are \\noften blindly used to make statistical inferences without a proper hypothesis testing \\nframework, which may lead to inferring higher-than-justified significance.\\nHow can we investigate the presence of data snooping bias? Typically we can \\nsplit the data into three separate datasets: the training dataset, the validation dataset, \\nand the test dataset. The training dataset is used to build a model and fit the model \\nparameters. The validation dataset is used to evaluate the model fit while tuning the \\nmodel parameters. The test dataset is to provide an out-of-sample test to evaluate \\nthe final model fit. If a variable or investment strategy is the result of data snooping, \\nit should generally not be significant in out-of-sample tests.\\nA variable or investment strategy that is statistically and economically significant \\nin out-of-sample tests, and that has a plausible economic basis, may be the basis for \\na valid investment strategy. Caution is still warranted, however. The most crucial \\nout-of-sample test is future investment success. It should be noted that if the strategy \\nbecomes known to other investors, prices may adjust so that the strategy, however well \\ntested, does not work in the future. To summarize, the analyst should be aware that \\nmany apparently profitable investment strategies may reflect data snooping bias and \\nthus be cautious about the future applicability of published investment research results.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Related  Biases\\n337\\nUNTANGLING THE EXTENT OF DATA SNOOPING\\nTo assess the significance of an investment strategy, we need to know how many \\nunsuccessful strategies were tried not only by the current investigator but also by \\nprevious investigators using the same or related datasets. Much research, in practice, \\nclosely builds on what other investigators have done, and so reflects intergenerational \\ndata mining (McQueen and Thorley, 1999) that involves using information developed \\nby previous researchers using a dataset to guide current research using the same or a \\nrelated dataset. Analysts have accumulated many observations about the peculiarities of \\nmany financial datasets, and other analysts may develop models or investment strategies \\nthat will tend to be supported within a dataset based on their familiarity with the prior \\nexperience of other analysts. As a consequence, the importance of those new results may \\nbe overstated. Research has suggested that the magnitude of this type of data-mining \\nbias may be considerable.\\nMcQueen and Thorley (1999) explored data mining in the context of the popular Motley \\nFool “Foolish Four” investment strategy, a version of the Dow Dividend Strategy tuned \\nby its developers to exhibit an even higher arithmetic mean return than the original Dow \\nDividend Strategy. The Foolish Four strategy claimed to show significant investment \\nreturns over 20 years starting in 1973, and its proponents claimed that the strategy should \\nhave similar returns in the future. McQueen and Thorley highlighted the data-mining \\nissues in that research and presented two signs that can warn analysts about the potential \\nexistence of data mining:\\n■ \\nToo much digging/too little confidence. The testing of many variables by \\nthe researcher is the “too much digging” warning sign of a data-mining \\nproblem. Although the number of variables examined may not be \\nreported, we should look closely for verbal hints that the researcher \\nsearched over many variables. The use of terms such as “we noticed \\n(or noted) that” or “someone noticed (or noted) that,” with respect to a \\npattern in a dataset, should raise suspicions that the researchers were \\ntrying out variables based on their own or others’ observations of the \\ndata.\\n■ \\nNo story/no future. The absence of an explicit economic rationale \\nfor a variable or trading strategy is the “no story” warning sign of a \\ndata-mining problem. Without a plausible economic rationale or story \\nfor why a variable should work, the variable is unlikely to have predic-\\ntive power. What if we do have a plausible economic explanation for \\na significant variable? McQueen and Thorley caution that a plausible \\neconomic rationale is a necessary but not a sufficient condition for a \\ntrading strategy to have value. As we mentioned earlier, if the strategy \\nis publicized, market prices may adjust to reflect the new information \\nas traders seek to exploit it; as a result, the strategy may no longer \\nwork.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Sample Selection Bias',\n",
       "       'page_number': 347,\n",
       "       'content': 'Sample Selection Bias\\nWhen researchers look into questions of interest to analysts or portfolio managers, \\nthey may exclude certain stocks, bonds, portfolios, or periods from the analysis for \\nvarious reasons—perhaps because of data availability. When data availability leads \\nto certain assets being excluded from the analysis, we call the resulting problem \\nsample selection bias. For example, you might sample from a database that tracks \\nonly companies currently in existence. Many mutual fund databases, for instance, \\nprovide historical information about only those funds that currently exist. Databases \\nthat report historical balance sheet and income statement information suffer from \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n338\\nthe same sort of bias as the mutual fund databases: Funds or companies that are no \\nlonger in business do not appear there. So, a study that uses these types of databases \\nsuffers from a type of sample selection bias known as survivorship bias.\\nThe issue of survivorship bias has also been raised in relation to international \\nindexes, particularly those representing less established markets. Some of these markets \\nhave suffered complete loss of value as a result of hyperinflation, nationalization or \\nconfiscation of industries, or market failure. Measuring the performance of markets \\nor particular investments that survive over time will overstate returns from investing. \\nThere is, of course, no way of determining in advance which markets will fail or survive.\\nSurvivorship bias sometimes appears when we use both stock price and accounting \\ndata. For example, many studies in finance have used the ratio of a company’s market \\nprice to book equity per share (i.e., the price-to-book ratio, P/B) and found that P/B \\nis inversely related to a company’s returns. P/B is also used to create many popular \\nvalue and growth indexes. The “value” indexes, for example, would include companies \\ntrading on relatively low P/B. If the database that we use to collect accounting data \\nexcludes failing companies, however, a survivorship bias might result. It can be argued \\nthat failing stocks would be expected to have low returns and low P/Bs. If we exclude \\nfailing stocks, then those stocks with low P/Bs that are included in the index will have \\nreturns that are higher on average than if all stocks with low P/Bs were included. As \\nshown in Exhibit 19, without failing stocks (shown in the bottom left part), we can \\nfit a line with a negative slope indicating that P/B is inversely related to a company’s \\nstock return. With all the companies included, however, the fitted line (horizontal, \\ndotted) shows an insignificant slope coefficient.\\nThis bias would then be responsible for some of the traditional findings of an \\ninverse relationship between average return and P/B. Researchers should be aware \\nof any biases potentially inherent in a sample.\\nExhibit 19: Survivorship Bias\\nReturn\\n0.5\\n0.5\\n0.4\\n0.4\\n0.3\\n0.3\\n0.2\\n0.2\\n0.1\\n0.1\\n0\\n–0.1\\n–0.1\\n–0.2\\n–0.2\\n–0.3\\n–0.3\\n–0.4\\n–0.4\\n–0.5\\n–0.5\\n0\\n7\\n2\\n3\\n1\\n4\\n5\\n6\\nP/BV\\nP/BV Failing Stocks\\nP/BV Surviving Stocks\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Related  Biases\\n339\\nDELISTINGS AND BIAS\\nA sample can also be biased because of the removal (or delisting) of a company’s stock \\nfrom an exchange. For example, the Center for Research in Security Prices at the University \\nof Chicago is a major provider of return data used in academic research. When a delisting \\noccurs, CRSP attempts to collect returns for the delisted company. Many times, however, \\nit cannot do so because of the difficulty involved; CRSP must simply list delisted company \\nreturns as missing. A study in the Journal of Finance by Shumway and Warther (1999) \\ndocumented the bias caused by delisting for CRSP NASDAQ return data. The authors \\nshowed that delistings associated with poor company performance (e.g., bankruptcy) \\nare missed more often than delistings associated with good or neutral company perfor-\\nmance (e.g., merger or moving to another exchange). In addition, delistings occur more \\nfrequently for small companies.\\nSample selection bias occurs even in markets where the quality and consistency \\nof the data are quite high. Newer asset classes such as hedge funds may present even \\ngreater problems of sample selection bias. Hedge funds are a heterogeneous group of \\ninvestment vehicles typically organized so as to be free from regulatory oversight. In \\ngeneral, hedge funds are not required to publicly disclose performance (in contrast to, \\nsay, mutual funds). Hedge funds themselves decide whether they want to be included \\nin one of the various databases of hedge fund performance. Hedge funds with poor \\ntrack records clearly may not wish to make their records public, creating a problem \\nof self-selection bias in hedge fund databases. Further, as pointed out by Fung and \\nHsieh (2002), because only hedge funds with good records will volunteer to enter a \\ndatabase, in general, overall past hedge fund industry performance will tend to appear \\nbetter than it really is. Furthermore, many hedge fund databases drop funds that go out \\nof business, creating survivorship bias in the database. Even if the database does not \\ndrop defunct hedge funds, in the attempt to eliminate survivorship bias, the problem \\nremains of hedge funds that stop reporting performance because of poor results or \\nbecause successful funds no longer want new cash inflows. In some circumstances, \\nimplicit selection bias may exist because of a threshold enabling self-selection. For \\nexample, compared with smaller exchanges, the NYSE has higher stock listing require-\\nments. Choosing NYSE-listed stocks may introduce an implicit quality bias into the \\nanalysis. Although the bias is less obvious, it is important for generalizing findings.\\nA variation of selection bias is backfill bias. For example, when a new hedge \\nfund is added to a given index, the fund’s past performance may be backfilled into \\nthe index’s database, even though the fund was not included in the database in the \\nprevious year. Usually a new fund starts contributing data after a period of good \\nperformance, so adding the fund’s instant history into the index database may inflate \\nthe index performance.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Look-Ahead Bias',\n",
       "       'page_number': 349,\n",
       "       'content': 'Look-Ahead Bias\\nA test design is subject to look-ahead bias if it uses information that was not available \\non the test date. For example, tests of trading rules that use stock market returns and \\naccounting balance sheet data must account for look-ahead bias. In such tests, a com-\\npany’s book value per share is commonly used to construct the P/B variable. Although \\nthe market price of a stock is available for all market participants at the same point in \\ntime, fiscal year-end book equity per share might not become publicly available until \\nsometime in the following quarter. One solution to mitigate the look-ahead bias is to \\nuse point-in-time (PIT) data when possible. PIT data is stamped with the date when \\nit was recorded or released. In the previous example, the PIT data of P/B would be \\naccompanied with the date of company filing date or press release date, rather than \\nthe end date of the fiscal quarter the P/B data represents. It is worth noting that the \\nlook-ahead bias could also be implicitly introduced. For example, when normalizing \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n340\\ninput data by deducting the mean and dividing it by standard deviation, we must \\nensure that the standard deviation of the training data is used as the proxy for standard \\ndeviation in validation and test data sets. Using standard deviation of validation or \\ntest data to normalize them will implicitly introduce a look-ahead bias as the variance \\nof future data is inappropriately used.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Time-Period Bias',\n",
       "       'page_number': 350,\n",
       "       'content': 'Time-Period Bias\\nA test design is subject to time-period bias if it is based on a period that may make \\nthe results period specific. A short time series is likely to give period-specific results \\nthat may not reflect a longer period. A long time series may give a more accurate \\npicture of true investment performance; its disadvantage lies in the potential for a \\nstructural change occurring during the time frame that would result in two different \\nreturn distributions. In this situation, the distribution that would reflect conditions \\nbefore the change differs from the distribution that would describe conditions after \\nthe change. Regime changes, such as low versus high volatility regimes or low versus \\nhigh interest rate regimes, are highly influential to asset classes. Inferences based on \\ndata influenced by one regime, and thus not appropriately distributed, should account \\nfor how the regime may bias the inferences.\\nEXAMPLE 13\\nBiases in Investment Research\\nAn analyst is reviewing the empirical evidence on historical equity returns \\nin the Eurozone (European countries that use the euro). She finds that value \\nstocks (i.e., those with low P/Bs) outperformed growth stocks (i.e., those with \\nhigh P/Bs) in recent periods. After reviewing the Eurozone market, the analyst \\nwonders whether value stocks might be attractive in the United Kingdom. She \\ninvestigates the performance of value and growth stocks in the UK market for \\na 10-year period. To conduct this research, the analyst does the following:\\n■ \\nobtains the current composition of the Financial Times Stock \\nExchange (FTSE) All Share Index, a market-capitalization-weighted \\nindex;\\n■ \\neliminates the companies that do not have December fiscal year-ends;\\n■ \\nuses year-end book values and market prices to rank the remaining \\nuniverse of companies by P/Bs at the end of the year;\\n■ \\nbased on these rankings, divides the universe into 10 portfolios, each \\nof which contains an equal number of stocks;\\n■ \\ncalculates the equal-weighted return of each portfolio and the return \\nfor the FTSE All Share Index for the 12 months following the date \\neach ranking was made; and\\n■ \\nsubtracts the FTSE returns from each portfolio’s returns to derive \\nexcess returns for each portfolio.\\nShe discusses the research process with her supervisor, who makes two \\ncomments:\\n■ \\nThe proposed process may introduce survivorship bias into her \\nanalysis.\\n■ \\nThe proposed research should cover a longer period.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Related  Biases\\n341\\n1. Which of the following best describes the supervisor’s first comment?\\nA. The comment is false. The proposed method is designed to avoid sur-\\nvivorship bias.\\nB. The comment is true, because she is planning to use the current list \\nof FTSE stocks rather than the actual list of stocks that existed at the \\nstart of each year.\\nC. The comment is true, because the test design uses information \\nunavailable on the test date.\\nSolution to 1:\\nB is correct because the research design is subject to survivorship bias if it \\nfails to account for companies that have gone bankrupt, merged, or other-\\nwise departed the database. Using the current list of FTSE stocks rather than \\nthe actual list of stocks that existed at the start of each year means that the \\ncomputation of returns excluded companies removed from the index. The \\nperformance of the portfolios with the lowest P/B is subject to survivorship \\nbias and may be overstated. At some time during the testing period, those \\ncompanies not currently in existence were eliminated from testing. They \\nwould probably have had low prices (and low P/Bs) and poor returns.\\nA is incorrect because the method is not designed to avoid survivorship \\nbias. C is incorrect because the fact that the test design uses information un-\\navailable on the test date relates to look-ahead bias. A test design is subject \\nto look-ahead bias if it uses information unavailable on the test date. This \\nbias would make a strategy based on the information appear successful, but \\nit assumes perfect forecasting ability.\\n2. What bias is the supervisor concerned about when making the second com-\\nment?\\nA. Time period bias, because the results may be period specific\\nB. Look-ahead bias, because the bias could be reduced or eliminated if \\none uses a longer period\\nC. Survivorship bias, because the bias would become less relevant over \\nlonger periods\\nSolution to 2:\\nA is correct. A test design is subject to time-period bias if it is based on a \\nperiod that may make the results period specific. Although the research \\ncovered a period of 10 years, that period may be too short for testing an \\nanomaly. Ideally, an analyst should test market anomalies over several mar-\\nket cycles to ensure that results are not period specific. This bias can favor a \\nproposed strategy if the period chosen was favorable to the strategy.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 351,\n",
       "     'content': 'SUMMARY\\nIn this reading, we have presented basic concepts and results in sampling and esti-\\nmation. We have also emphasized the challenges faced by analysts in appropriately \\nusing and interpreting financial data. As analysts, we should always use a critical \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n342\\neye when evaluating the results from any study. The quality of the sample is of the \\nutmost importance: If the sample is biased, the conclusions drawn from the sample \\nwill be in error.\\n■ \\nTo draw valid inferences from a sample, the sample should be random.\\n■ \\nIn simple random sampling, each observation has an equal chance of being \\nselected. In stratified random sampling, the population is divided into \\nsubpopulations, called strata or cells, based on one or more classification \\ncriteria; simple random samples are then drawn from each stratum.\\n■ \\nStratified random sampling ensures that population subdivisions of interest \\nare represented in the sample. Stratified random sampling also produces \\nmore-precise parameter estimates than simple random sampling.\\n■ \\nConvenience sampling selects an element from the population on the basis \\nof whether or not it is accessible to a researcher or how easy it is to access. \\nBecause convenience sampling presents the advantage of collecting data \\nquickly at a low cost, it is a suitable sampling plan for small-scale pilot \\nstudies.\\n■ \\nJudgmental sampling may yield skewed results because of the bias of \\nresearchers, but its advantages lie in the fact that in some circumstances, the \\nspecialty of researchers and their judgmental can lead them directly to the \\ntarget population of interest within time constraints.\\n■ \\nThe central limit theorem states that for large sample sizes, for any under-\\nlying distribution for a random variable, the sampling distribution of the \\nsample mean for that variable will be approximately normal, with mean \\nequal to the population mean for that random variable and variance equal to \\nthe population variance of the variable divided by sample size.\\n■ \\nBased on the central limit theorem, when the sample size is large, we can \\ncompute confidence intervals for the population mean based on the normal \\ndistribution regardless of the distribution of the underlying population. In \\ngeneral, a sample size of 30 or larger can be considered large.\\n■ \\nAn estimator is a formula for computing a sample statistic used to estimate \\na population parameter. An estimate is a particular value that we calculate \\nfrom a sample by using an estimator.\\n■ \\nBecause an estimator or statistic is a random variable, it is described by \\nsome probability distribution. We refer to the distribution of an estimator as \\nits sampling distribution. The standard deviation of the sampling distribu-\\ntion of the sample mean is called the standard error of the sample mean.\\n■ \\nThe desirable properties of an estimator are unbiasedness (the expected \\nvalue of the estimator equals the population parameter), efficiency (the esti-\\nmator has the smallest variance), and consistency (the probability of accurate \\nestimates increases as sample size increases).\\n■ \\nThe two types of estimates of a parameter are point estimates and interval \\nestimates. A point estimate is a single number that we use to estimate a \\nparameter. An interval estimate is a range of values that brackets the popu-\\nlation parameter with some probability.\\n■ \\nA confidence interval is an interval for which we can assert with a given \\nprobability 1 − α, called the degree of confidence, that it will contain the \\nparameter it is intended to estimate. This measure is often referred to as the \\n100(1 − α)% confidence interval for the parameter.\\n■ \\nA 100(1 − α)% confidence interval for a parameter has the following struc-\\nture: Point estimate ± Reliability factor × Standard error, where the reli-\\nability factor is a number based on the assumed distribution of the point \\n© CFA Institute. For candidate use only. Not for distribution.\\nSampling Related  Biases\\n343\\nestimate and the degree of confidence (1 − α) for the confidence interval and \\nwhere standard error is the standard error of the sample statistic providing \\nthe point estimate.\\n■ \\nA 100(1 − α)% confidence interval for population mean µ when sampling \\nfrom a normal distribution with known variance σ2 is given by   _\\n \\nX  ±  z α/2  σ _ \\n √ _ \\nwhere zα/2 is the point of the standard normal distribution such that α/2 n    , \\nremains in the right tail.\\n■ \\nA random sample of size n is said to have n − 1 degrees of freedom for \\nestimating the population variance, in the sense that there are only n − 1 \\nindependent deviations from the mean on which to base the estimate.\\n■ \\nA 100(1 − α)% confidence interval for the population mean µ when sam-\\npling from a normal distribution with unknown variance (a t-distribution \\nconfidence interval) is given by   _\\n \\nX  ±  t α/2  ( s /  √ _ \\nn  )  , where tα/2 is the point of \\nthe t-distribution such that α/2 remains in the right tail and s is the sample \\nstandard deviation. This confidence interval can also be used, because of the \\ncentral limit theorem, when dealing with a large sample from a population \\nwith unknown variance that may not be normal.\\n■ \\nWe may use the confidence interval   _\\n \\nX  ±  z α/2  ( s /  √ _ \\nn  )  as an alternative to \\nthe t-distribution confidence interval for the population mean when using \\na large sample from a population with unknown variance. The confidence \\ninterval based on the z-statistic is less conservative (narrower) than the cor-\\nresponding confidence interval based on a t-distribution.\\n■ \\nBootstrap and jackknife are simple but powerful methods for statistical \\ninference, and they are particularly useful when no analytical formula is \\navailable. Bootstrap constructs the sampling distribution of an estimator by \\nrepeatedly drawing samples from the original sample to find standard error \\nand confidence interval. Jackknife draws repeated samples while leaving out \\none observation at a time from the set, without replacing it.\\n■ \\nThree issues in the selection of sample size are the need for precision, the \\nrisk of sampling from more than one population, and the expenses of differ-\\nent sample sizes.\\n■ \\nData snooping bias comes from finding models by repeatedly searching \\nthrough databases for patterns.\\n■ \\nSample selection bias occurs when data availability leads to certain assets \\nbeing excluded from the analysis, we call the resulting problem\\n■ \\nSurvivorship bias is a subset of sample selection bias and occurs if compa-\\nnies are excluded from the analysis because they have gone out of business \\nor because of reasons related to poor performance.\\n■ \\nSelf-selection bias reflects the ability of entities to decide whether or not \\nthey wish to report their attributes or results and be included in databases \\nor samples. Implicit selection bias is one type of selection bias introduced \\nthrough the presence of a threshold that filters out some unqualified \\nmembers. A subset of selection bias is backfill bias, in which past data, not \\nreported or used before, is backfilled into an existing database.\\n■ \\nLook-ahead bias exists if the model uses data not available to market partici-\\npants at the time the market participants act in the model.\\n■ \\nTime-period bias is present if the period used makes the results period spe-\\ncific or if the period used includes a point of structural change.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n344\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 354,\n",
       "     'content': 'PRACTICE PROBLEMS\\n1. Perkiomen Kinzua, a seasoned auditor, is auditing last year’s transactions for \\nConemaugh Corporation. Unfortunately, Conemaugh had a very large number \\nof transactions last year, and Kinzua is under a time constraint to finish the audit. \\nHe decides to audit only the small subset of the transaction population that is of \\ninterest and to use sampling to create that subset.\\nThe most appropriate sampling method for Kinzua to use is:\\nA. judgmental sampling.\\nB. systematic sampling.\\nC. convenience sampling.\\n2. Which one of the following statements is true about non-probability sampling?\\nA. There is significant risk that the sample is not representative of the \\npopulation.\\nB. Every member of the population has an equal chance of being selected for \\nthe sample.\\nC. Using judgment guarantees that population subdivisions of interest are rep-\\nresented in the sample.\\n3. The best approach for creating a stratified random sample of a population in-\\nvolves:\\nA. drawing an equal number of simple random samples from each \\nsubpopulation.\\nB. selecting every kth member of the population until the desired sample size \\nis reached.\\nC. drawing simple random samples from each subpopulation in sizes propor-\\ntional to the relative size of each subpopulation.\\n4. Although he knows security returns are not independent, a colleague makes \\nthe claim that because of the central limit theorem, if we diversify across a large \\nnumber of investments, the portfolio standard deviation will eventually approach \\nzero as n becomes large. Is he correct?\\n5. Why is the central limit theorem important?\\n6. What is wrong with the following statement of the central limit theorem?\\nCentral Limit Theorem. “If the random variables X1, X2, X3, …, Xn are a ran-\\ndom sample of size n from any distribution with finite mean μ and variance \\nσ2, then the distribution of   _\\n \\nX  will be approximately normal, with a standard \\ndeviation of  σ /  √ _ \\nn  .”\\n7. Peter Biggs wants to know how growth managers performed last year. Biggs as-\\nsumes that the population cross-sectional standard deviation of growth manager \\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n345\\nreturns is 6% and that the returns are independent across managers.\\nA. How large a random sample does Biggs need if he wants the standard devia-\\ntion of the sample means to be 1%?\\nB. How large a random sample does Biggs need if he wants the standard devia-\\ntion of the sample means to be 0.25%?\\n8. A population has a non-normal distribution with mean µ and variance σ2. The \\nsampling distribution of the sample mean computed from samples of large size \\nfrom that population will have:\\nA. the same distribution as the population distribution.\\nB. its mean approximately equal to the population mean.\\nC. its variance approximately equal to the population variance.\\n9. A sample mean is computed from a population with a variance of 2.45. The sam-\\nple size is 40. The standard error of the sample mean is closest to:\\nA. 0.039.\\nB. 0.247.\\nC. 0.387.\\n10. An estimator with an expected value equal to the parameter that it is intended to \\nestimate is described as:\\nA. efficient.\\nB. unbiased.\\nC. consistent.\\n11. If an estimator is consistent, an increase in sample size will increase the:\\nA. accuracy of estimates.\\nB. efficiency of the estimator.\\nC. unbiasedness of the estimator.\\n12. Petra Munzi wants to know how value managers performed last year. Munzi es-\\ntimates that the population cross-sectional standard deviation of value manager \\nreturns is 4% and assumes that the returns are independent across managers.\\nA. Munzi wants to build a 95% confidence interval for the population mean \\nreturn. How large a random sample does Munzi need if she wants the 95% \\nconfidence interval to have a total width of 1%?\\nB. Munzi expects a cost of about $10 to collect each observation. If she has \\na $1,000 budget, will she be able to construct the confidence interval she \\nwants?\\n13. Find the reliability factors based on the t-distribution for the following confi-\\ndence intervals for the population mean (df = degrees of freedom, n = sample \\nsize):\\nA. A 99% confidence interval, df = 20\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n346\\nB. A 90% confidence interval, df = 20\\nC. A 95% confidence interval, n = 25\\nD. A 95% confidence interval, n = 16\\n14. Assume that monthly returns are normally distributed with a mean of 1% and a \\nsample standard deviation of 4%. The population standard deviation is unknown. \\nConstruct a 95% confidence interval for the sample mean of monthly returns if \\nthe sample size is 24.\\n15. Explain the differences between constructing a confidence interval when sam-\\npling from a normal population with a known population variance and sampling \\nfrom a normal population with an unknown variance.\\n16. For a two-sided confidence interval, an increase in the degree of confidence will \\nresult in:\\nA. a wider confidence interval.\\nB. a narrower confidence interval.\\nC. no change in the width of the confidence interval.\\n17. For a sample size of 17, with a mean of 116.23 and a variance of 245.55, the width \\nof a 90% confidence interval using the appropriate t-distribution is closest to:\\nA. 13.23.\\nB. 13.27.\\nC. 13.68.\\n18. For a sample size of 65 with a mean of 31 taken from a normally distributed pop-\\nulation with a variance of 529, a 99% confidence interval for the population mean \\nwill have a lower limit closest to:\\nA. 23.64.\\nB. 25.41.\\nC. 30.09.\\n19. An increase in sample size is most likely to result in a:\\nA. wider confidence interval.\\nB. decrease in the standard error of the sample mean.\\nC. lower likelihood of sampling from more than one population.\\n20. Otema Chi has a spreadsheet with 108 monthly returns for shares in Marunou \\nCorporation. He writes a software program that uses bootstrap resampling to \\ncreate 200 resamples of this Marunou data by sampling with replacement. Each \\nresample has 108 data points. Chi’s program calculates the mean of each of the \\n200 resamples, and then it calculates that the mean of these 200 resample means \\nis 0.0261. The program subtracts 0.0261 from each of the 200 resample means, \\nsquares each of these 200 differences, and adds the squared differences together. \\nThe result is 0.835. The program then calculates an estimate of the standard error \\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n347\\nof the sample mean.\\nThe estimated standard error of the sample mean is closest to:\\nA. 0.0115\\nB. 0.0648\\nC. 0.0883\\n21. Compared with bootstrap resampling, jackknife resampling:\\nA. is done with replacement.\\nB. usually requires that the number of repetitions is equal to the sample size.\\nC. produces dissimilar results for every run because resamples are randomly \\ndrawn.\\n22. Suppose we take a random sample of 30 companies in an industry with 200 \\ncompanies. We calculate the sample mean of the ratio of cash flow to total debt \\nfor the prior year. We find that this ratio is 23%. Subsequently, we learn that the \\npopulation cash flow to total debt ratio (taking account of all 200 companies) is \\n26%. What is the explanation for the discrepancy between the sample mean of \\n23% and the population mean of 26%?\\nA. Sampling error.\\nB. Bias.\\nC. A lack of consistency.\\n23. Alcorn Mutual Funds is placing large advertisements in several financial publi-\\ncations. The advertisements prominently display the returns of 5 of Alcorn’s 30 \\nfunds for the past 1-, 3-, 5-, and 10-year periods. The results are indeed impres-\\nsive, with all of the funds beating the major market indexes and a few beating \\nthem by a large margin. Is the Alcorn family of funds superior to its competitors?\\n24. Julius Spence has tested several predictive models in order to identify un-\\ndervalued stocks. Spence used about 30 company-specific variables and 10 \\nmarket-related variables to predict returns for about 5,000 North American \\nand European stocks. He found that a final model using eight variables applied \\nto telecommunications and computer stocks yields spectacular results. Spence \\nwants you to use the model to select investments. Should you? What steps would \\nyou take to evaluate the model?\\n25. A report on long-term stock returns focused exclusively on all currently publicly \\ntraded firms in an industry is most likely susceptible to:\\nA. look-ahead bias.\\nB. survivorship bias.\\nC. intergenerational data mining.\\n26. Which sampling bias is most likely investigated with an out-of-sample test?\\nA. Look-ahead bias\\nB. Data-mining bias\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n348\\nC. Sample selection bias\\n27. Which of the following characteristics of an investment study most likely indi-\\ncates time-period bias?\\nA. The study is based on a short time-series.\\nB. Information not available on the test date is used.\\nC. A structural change occurred prior to the start of the study’s time series.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 359,\n",
       "     'content': 'Solutions\\n349\\nSOLUTIONS\\n1. A is correct. With judgmental sampling, Kinzua will use his knowledge and \\nprofessional judgment as a seasoned auditor to select transactions of interest \\nfrom the population. This approach will allow Kinzua to create a sample that is \\nrepresentative of the population and that will provide sufficient audit coverage. \\nJudgmental sampling is useful in cases that have a time constraint or in which the \\nspecialty of researchers is critical to select a more representative sample than by \\nusing other probability or non-probability sampling methods. Judgement sam-\\npling, however, entails the risk that Kinzua is biased in his selections, leading to \\nskewed results that are not representative of the whole population.\\n2. A is correct. Because non-probability sampling is dependent on factors other \\nthan probability considerations, such as a sampler’s judgment or the convenience \\nto access data, there is a significant risk that non-probability sampling might \\ngenerate a non-representative sample\\n3. C is correct. Stratified random sampling involves dividing a population into \\nsubpopulations based on one or more classification criteria. Then, simple random \\nsamples are drawn from each subpopulation in sizes proportional to the relative \\nsize of each subpopulation. These samples are then pooled to form a stratified \\nrandom sample.\\n4. No. First the conclusion on the limit of zero is wrong; second, the support cited \\nfor drawing the conclusion (i.e., the central limit theorem) is not relevant in this \\ncontext.\\n5. In many instances, the distribution that describes the underlying population is \\nnot normal or the distribution is not known. The central limit theorem states that \\nif the sample size is large, regardless of the shape of the underlying population, \\nthe distribution of the sample mean is approximately normal. Therefore, even in \\nthese instances, we can still construct confidence intervals (and conduct tests of \\ninference) as long as the sample size is large (generally n ≥ 30).\\n6. The statement makes the following mistakes:\\n■ \\nGiven the conditions in the statement, the distribution of   _\\n \\nX  will be approxi-\\nmately normal only for large sample sizes.\\n■ \\nThe statement omits the important element of the central limit theorem that \\nthe distribution of   _\\n \\nX  will have mean μ.\\n7. \\nA. The standard deviation or standard error of the sample mean is   σ  _\\n \\nX   = σ /  \\n√ _ \\nn  . Substituting in the values for   σ  _\\n \\nX   and σ, we have 1% =  6% /  √ _ \\nn  , or   √ _ \\nn   = 6. \\nSquaring this value, we get a random sample of n = 36.\\nB. As in Part A, the standard deviation of sample mean is   σ  _\\n \\nX   = σ /  √ _ \\nn  . \\nSubstituting in the values for   σ  _\\n \\nX   and σ, we have 0.25% =  6% /  √ _ \\nn  , or   √ _ \\nn  = 24. \\nSquaring this value, we get a random sample of n = 576, which is substan-\\ntially larger than for Part A of this question.\\n8. B is correct. Given a population described by any probability distribution (nor-\\nmal or non-normal) with finite variance, the central limit theorem states that the \\nsampling distribution of the sample mean will be approximately normal, with the \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n350\\nmean approximately equal to the population mean, when the sample size is large.\\n9. B is correct. Taking the square root of the known population variance to deter-\\nmine the population standard deviation (σ) results in\\n σ =  √ _ \\n2.45  = 1.565 \\nThe formula for the standard error of the sample mean (σX), based on a known \\nsample size (n), is\\n σ X =  σ _ \\n √ _ \\nn    \\nTherefore,\\n σ X =  1.565 \\n_ \\n √ _ \\n40    = 0.247 \\n10. B is correct. An unbiased estimator is one for which the expected value equals \\nthe parameter it is intended to estimate.\\n11. A is correct. A consistent estimator is one for which the probability of estimates \\nclose to the value of the population parameter increases as sample size increases. \\nMore specifically, a consistent estimator’s sampling distribution becomes concen-\\ntrated on the value of the parameter it is intended to estimate as the sample size \\napproaches infinity.\\n12. \\nA. Assume the sample size will be large and thus the 95% confidence interval \\nfor the population mean of manager returns is   _\\n \\nX  ± 1.96  s  _\\n \\nX   , where   s  _\\n \\nX   = s /  \\n√ _ \\nn  . Munzi wants the distance between the upper limit and lower limit in the \\nconfidence interval to be 1%, which is\\n  ( \\n_\\n \\nX  + 1.96  s  \\n_\\n \\nX   )  −   ( \\n_\\n \\nX  − 1.96  s  \\n_\\n \\nX   )  = 1% \\nSimplifying this equation, we get  2  ( 1.96  s  _\\n \\nX   )  = 1%. Finally, we have  3.92  s  _\\n \\nX   = \\n1%, which gives us the standard deviation of the sample mean,   s  _\\n \\nX   = 0.255%. \\nThe distribution of sample means is   s  _\\n \\nX   = s /  √ _ \\nn  . Substituting in the values \\nfor   s  _\\n \\nX   and s, we have 0.255% =  4% /  √ _ \\nn  , or   √ _ \\nn  = 15.69. Squaring this value, \\nwe get a random sample of n = 246.\\nB. With her budget, Munzi can pay for a sample of up to 100 observations, \\nwhich is far short of the 246 observations needed. Munzi can either proceed \\nwith her current budget and settle for a wider confidence interval or she can \\nraise her budget (to around $2,460) to get the sample size for a 1% width in \\nher confidence interval.\\n13. \\nA. For a 99% confidence interval, the reliability factor we use is t0.005; for df = \\n20, this factor is 2.845.\\nB. For a 90% confidence interval, the reliability factor we use is t0.05; for df = \\n20, this factor is 1.725.\\nC. Degrees of freedom equals n − 1, or in this case 25 − 1 = 24. For a 95% con-\\nfidence interval, the reliability factor we use is t0.025; for df = 24, this factor \\nis 2.064.\\nD. Degrees of freedom equals 16 − 1 = 15. For a 95% confidence interval, the \\nreliability factor we use is t0.025; for df = 15, this factor is 2.131.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n351\\n14. Because this is a small sample from a normal population and we have only the \\nsample standard deviation, we use the following model to solve for the confi-\\ndence interval of the population mean:\\n \\n_\\n \\nX  ±  t α/2  s _ \\n √ _ \\nn    \\nwhere we find t0.025 (for a 95% confidence interval) for df = n − 1 = 24 − 1 = 23; \\nthis value is 2.069. Our solution is 1% ± 2.069(4%)/ √ _ \\n24  = 1% ± 2.069(0.8165) = 1% \\n± 1.69. The 95% confidence interval spans the range from −0.69% to +2.69%.\\n15. If the population variance is known, the confidence interval is\\n \\n_\\n \\nX  ±  z α/2  σ _ \\n √ _ \\nn    \\nThe confidence interval for the population mean is centered at the sample \\nmean,  _\\n \\nX  . The population standard deviation is σ, and the sample size is n. The \\npopulation standard deviation divided by the square root of n is the standard \\nerror of the estimate of the mean. The value of z depends on the desired degree \\nof confidence. For a 95% confidence interval, z0.025 = 1.96 and the confidence \\ninterval estimate is\\n \\n_\\n \\nX  ± 1.96 σ _ \\n √ _ \\nn    \\nIf the population variance is not known, we make two changes to the technique \\nused when the population variance is known. First, we must use the sample stan-\\ndard deviation instead of the population standard deviation. Second, we use the \\nt-distribution instead of the normal distribution. The critical t-value will depend \\non degrees of freedom n − 1. If the sample size is large, we have the alternative of \\nusing the z-distribution with the sample standard deviation.\\n16. A is correct. As the degree of confidence increases (e.g., from 95% to 99%), a \\ngiven confidence interval will become wider. A confidence interval is a range for \\nwhich one can assert with a given probability 1 – α, called the degree of confi-\\ndence, that it will contain the parameter it is intended to estimate.\\n17. B is correct. The confidence interval is calculated using the following equation:\\n \\n_\\n \\nX  ±  t α/2  s _ \\n √ _ \\nn    \\nSample standard deviation (s) =  √ _ \\n245.55  = 15.670.\\nFor a sample size of 17, degrees of freedom equal 16, so t0.05 = 1.746.\\nThe confidence interval is calculated as\\n 116.23 ± 1.746 15.67 \\n_ \\n √ _ \\n17    = 116.23 ± 6.6357 \\nTherefore, the interval spans 109.5943 to 122.8656, meaning its width is equal to \\napproximately 13.271. (This interval can be alternatively calculated as 6.6357 × 2.)\\n18. A is correct. To solve, use the structure of Confidence interval = Point estimate ± \\nReliability factor × Standard error, which, for a normally distributed population \\nwith known variance, is represented by the following formula:\\n \\n_\\n \\nX  ±  z α/2  σ _ \\n √ _ \\nn    \\nFor a 99% confidence interval, use z0.005 = 2.58.\\nAlso, σ =  √ _ \\n529  = 23.\\nTherefore, the lower limit =  31 − 2.58 23 \\n_ \\n √ _ \\n65    = 23.6398 .\\n19. B is correct. All else being equal, as the sample size increases, the standard error of \\nthe sample mean decreases and the width of the confidence interval also decreases.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 5 \\nSampling and Estimation\\n352\\n20. B is correct.\\nThe estimate of the standard error of the sample mean with bootstrap resampling \\nis calculated as follows:\\n  s  \\n_\\n \\nX   =  √\\n \\n_______________ \\n \\n 1 \\n_ \\nB − 1  ∑ \\nb=1\\n  \\nB\\n   ( ˆ θ b −  \\n_\\n θ ) \\n2   =  √\\n \\n_____________________ \\n \\n \\n \\n1 \\n_ \\n200 − 1  ∑ \\nb=1\\n  \\n200\\n   ( ˆ θ b − 0.0261) \\n2   =  √ _ \\n 1 _ \\n 199  × 0.835    \\n \\n \\n \\n \\n s  \\n_\\n \\nX   = 0.0648\\n \\n \\n21. B is correct. For a sample of size n, jackknife resampling usually requires n repeti-\\ntions. In contrast, with bootstrap resampling, we are left to determine how many \\nrepetitions are appropriate.\\n22. A is correct. The discrepancy arises from sampling error. Sampling error exists \\nwhenever one fails to observe every element of the population, because a sample \\nstatistic can vary from sample to sample. As stated in the reading, the sample \\nmean is an unbiased estimator, a consistent estimator, and an efficient estimator \\nof the population mean. Although the sample mean is an unbiased estimator of \\nthe population mean—the expected value of the sample mean equals the popu-\\nlation mean—because of sampling error, we do not expect the sample mean to \\nexactly equal the population mean in any one sample we may take.\\n23. No, we cannot say that Alcorn Mutual Funds as a group is superior to compet-\\nitors. Alcorn Mutual Funds’ advertisement may easily mislead readers because \\nthe advertisement does not show the performance of all its funds. In particular, \\nAlcorn Mutual Funds is engaging in sample selection bias by presenting the in-\\nvestment results from its best-performing funds only.\\n24. Spence may be guilty of data mining. He has used so many possible combina-\\ntions of variables on so many stocks, it is not surprising that he found some \\ninstances in which a model worked. In fact, it would have been more surprising \\nif he had not found any. To decide whether to use his model, you should do two \\nthings: First, ask that the model be tested on out-of-sample data—that is, data \\nthat were not used in building the model. The model may not be successful with \\nout-of-sample data. Second, examine his model to make sure that the relation-\\nships in the model make economic sense, have a story, and have a future.\\n25. B is correct. A report that uses a current list of stocks does not account for firms \\nthat failed, merged, or otherwise disappeared from the public equity market in \\nprevious years. As a consequence, the report is biased. This type of bias is known \\nas survivorship bias.\\n26. B is correct. An out-of-sample test is used to investigate the presence of \\ndata-mining bias. Such a test uses a sample that does not overlap the time period \\nof the sample on which a variable, strategy, or model was developed.\\n27. A is correct. A short time series is likely to give period-specific results that may \\nnot reflect a longer time period.\\n© CFA Institute. For candidate use only. Not for distribution.\\nHypothesis Testing\\nby Pamela Peterson Drake, PhD, CFA.\\nPamela Peterson Drake, PhD, CFA, is at James Madison University (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndefine a hypothesis, describe the steps of hypothesis testing, \\nand describe and interpret the choice of the null and alternative \\nhypotheses\\ncompare and contrast one-tailed and two-tailed tests of hypotheses\\nexplain a test statistic, Type I and Type II errors, a significance level, \\nhow significance levels are used in hypothesis testing, and the power \\nof a test\\nexplain a decision rule and the relation between confidence intervals \\nand hypothesis tests, and determine whether a statistically significant \\nresult is also economically meaningful\\nexplain and interpret the p-value as it relates to hypothesis testing\\ndescribe how to interpret the significance of a test in the context of \\nmultiple tests\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning the population mean of both large and \\nsmall samples when the population is normally or approximately \\nnormally distributed and the variance is (1) known or (2) unknown\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning the equality of the population means of \\ntwo at least approximately normally distributed populations based \\non independent random samples with equal assumed variances\\nidentify the appropriate test statistic and interpret the results for \\na hypothesis test concerning the mean difference of two normally \\ndistributed populations\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning (1) the variance of a normally distributed \\npopulation and (2) the equality of the variances of two normally \\ndistributed populations based on two independent random samples\\nL E A R N I N G  M O D U L E\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]},\n",
       "  {'title': 'Learning Module 6\\tHypothesis Testing',\n",
       "   'page_number': 363,\n",
       "   'content': 'Hypothesis Testing\\nby Pamela Peterson Drake, PhD, CFA.\\nPamela Peterson Drake, PhD, CFA, is at James Madison University (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndefine a hypothesis, describe the steps of hypothesis testing, \\nand describe and interpret the choice of the null and alternative \\nhypotheses\\ncompare and contrast one-tailed and two-tailed tests of hypotheses\\nexplain a test statistic, Type I and Type II errors, a significance level, \\nhow significance levels are used in hypothesis testing, and the power \\nof a test\\nexplain a decision rule and the relation between confidence intervals \\nand hypothesis tests, and determine whether a statistically significant \\nresult is also economically meaningful\\nexplain and interpret the p-value as it relates to hypothesis testing\\ndescribe how to interpret the significance of a test in the context of \\nmultiple tests\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning the population mean of both large and \\nsmall samples when the population is normally or approximately \\nnormally distributed and the variance is (1) known or (2) unknown\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning the equality of the population means of \\ntwo at least approximately normally distributed populations based \\non independent random samples with equal assumed variances\\nidentify the appropriate test statistic and interpret the results for \\na hypothesis test concerning the mean difference of two normally \\ndistributed populations\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning (1) the variance of a normally distributed \\npopulation and (2) the equality of the variances of two normally \\ndistributed populations based on two independent random samples\\nL E A R N I N G  M O D U L E\\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n354\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ncompare and contrast parametric and nonparametric tests, and \\ndescribe situations where each is the more appropriate type of test\\nexplain parametric and nonparametric tests of the hypothesis that \\nthe population correlation coefficient equals zero, and determine \\nwhether the hypothesis is rejected at a given level of significance\\nexplain tests of independence based on contingency table data\\nINTRODUCTION\\ndefine a hypothesis, describe the steps of hypothesis testing, \\nand describe and interpret the choice of the null and alternative \\nhypotheses\\nWhy Hypothesis Testing?\\nFaced with an overwhelming amount of data, analysts must deal with the task of \\nwrangling those data into something that provides a clearer picture of what is going \\non. Consider an analyst evaluating the returns on two investments over 33 years, as \\nwe show in Exhibit 1.\\nExhibit 1: Returns for Investments One and Two over 33 Years\\nAnnual Return (%)\\n12\\n12\\n10\\n10\\n8\\n6\\n2\\n0\\n4\\n–2\\n–2\\n1\\n33\\n33\\n5\\n3\\n7\\n9\\n1111\\n13\\n13\\n15\\n15\\n19\\n19\\n21\\n21\\n23\\n23\\n25\\n25\\n29\\n29\\n31\\n31\\n17\\n17\\n27\\n27\\nYear\\nInvestment One\\nInvestment Two\\nAlthough “a picture is worth a thousand words,” what can we actually glean from this \\nplot? Can we tell if each investment’s returns are different from an average of 5%? Can \\nwe tell whether the returns are different for Investment One and Investment Two? Can \\nwe tell whether the standard deviations of the two investments are each different from \\n1\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "   'children': [{'title': 'Introduction',\n",
       "     'page_number': 364,\n",
       "     'content': 'Learning Module 6 \\nHypothesis Testing\\n354\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ncompare and contrast parametric and nonparametric tests, and \\ndescribe situations where each is the more appropriate type of test\\nexplain parametric and nonparametric tests of the hypothesis that \\nthe population correlation coefficient equals zero, and determine \\nwhether the hypothesis is rejected at a given level of significance\\nexplain tests of independence based on contingency table data\\nINTRODUCTION\\ndefine a hypothesis, describe the steps of hypothesis testing, \\nand describe and interpret the choice of the null and alternative \\nhypotheses\\n',\n",
       "     'children': [{'title': 'Why Hypothesis Testing?',\n",
       "       'page_number': 364,\n",
       "       'content': 'Why Hypothesis Testing?\\nFaced with an overwhelming amount of data, analysts must deal with the task of \\nwrangling those data into something that provides a clearer picture of what is going \\non. Consider an analyst evaluating the returns on two investments over 33 years, as \\nwe show in Exhibit 1.\\nExhibit 1: Returns for Investments One and Two over 33 Years\\nAnnual Return (%)\\n12\\n12\\n10\\n10\\n8\\n6\\n2\\n0\\n4\\n–2\\n–2\\n1\\n33\\n33\\n5\\n3\\n7\\n9\\n1111\\n13\\n13\\n15\\n15\\n19\\n19\\n21\\n21\\n23\\n23\\n25\\n25\\n29\\n29\\n31\\n31\\n17\\n17\\n27\\n27\\nYear\\nInvestment One\\nInvestment Two\\nAlthough “a picture is worth a thousand words,” what can we actually glean from this \\nplot? Can we tell if each investment’s returns are different from an average of 5%? Can \\nwe tell whether the returns are different for Investment One and Investment Two? Can \\nwe tell whether the standard deviations of the two investments are each different from \\n1\\n© CFA Institute. For candidate use only. Not for distribution.\\nIntroduction\\n355\\n2%? Can we tell whether the variability is different for the two investments? For these \\nquestions, we need to have more precise tools than simply a plot over time. What we \\nneed is a set of tools that aid us in making decisions based on the data.\\nWe use the concepts and tools of hypothesis testing to address these questions. \\nHypothesis testing is part of statistical inference, the process of making judgments \\nabout a larger group (a population) based on a smaller group of observations (that \\nis, a sample).\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Implications from a Sampling Distribution',\n",
       "       'page_number': 365,\n",
       "       'content': 'Implications from a Sampling Distribution\\nConsider a set of 1,000 asset returns with a mean of 6% and a standard deviation of \\n2%. If we draw a sample of returns from this population, what is the chance that the \\nmean of this sample will be 6%? What we know about sampling distributions is that \\nhow close any given sample mean will be to the population mean depends on the \\nsample size, the variability within the population, and the quality of our sampling \\nmethodology.\\nFor example, suppose we draw a sample of 30 observations and the sample mean \\nis 6.13%. Is this close enough to 6% to alleviate doubt that the sample is drawn from \\na population with a mean of 6%? Suppose we draw another sample of 30 and find \\na sample mean of 4.8%. Does this bring into doubt whether the population mean is \\n6%? If we keep drawing samples of 30 observations from this population, we will get \\na range of possible sample means, as we show in Exhibit 2 for 100 different samples \\nof size 30 from this population, with a range of values from 5.06 to 7.03%. All these \\nsample means are a result of sampling from the 1,000 asset returns.\\nExhibit 2: Distribution of Sample Means of 100 Samples Drawn from a \\nPopulation of 1,000 Returns\\nNumber of Means\\n20\\n20\\n18\\n18\\n16\\n16\\n14\\n14\\n12\\n12\\n10\\n10\\n8\\n6\\n4\\n2\\n0\\n5.60\\nto\\n5.78\\n5.60\\nto\\n5.78\\n5.24\\nto\\n5.42\\n5.24\\nto\\n5.42\\n5.06\\nto\\n5.24\\n5.06\\nto\\n5.24\\n5.42\\nto\\n5.60\\n5.42\\nto\\n5.60\\n5.95\\nto\\n6.13\\n5.95\\nto\\n6.13\\n5.78\\nto\\n5.95\\n5.78\\nto\\n5.95\\n6.13\\nto\\n6.31\\n6.13\\nto\\n6.31\\n6.31\\nto\\n6.49\\n6.31\\nto\\n6.49\\n6.67\\nto\\n6.85\\n6.67\\nto\\n6.85\\n6.49\\nto\\n6.67\\n6.49\\nto\\n6.67\\n6.85\\nto\\n7.03\\n6.85\\nto\\n7.03\\nRange of Means\\nMean of\\n6.020\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n356\\nAs you can see in Exhibit 2, a sample mean that is quite different from the population \\nmean can occur; this situation is not as likely as drawing a sample with a mean closer \\nto the population mean, but it can still happen. In hypothesis testing, we test to see \\nwhether a sample statistic is likely to come from a population with the hypothesized \\nvalue of the population parameter.\\nThe concepts and tools of hypothesis testing provide an objective means to gauge \\nwhether the available evidence supports the hypothesis. After applying a statistical \\ntest of a hypothesis, we should have a clearer idea of the probability that a hypothesis \\nis true or not, although our conclusion always stops short of certainty.\\nThe main focus of this reading is on the framework of hypothesis testing and \\ntests concerning mean, variance, and correlation, three quantities frequently used \\nin investments.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'The Process of Hypothesis Testing',\n",
       "     'page_number': 366,\n",
       "     'content': 'THE PROCESS OF HYPOTHESIS TESTING\\ncompare and contrast one-tailed and two-tailed tests of hypotheses\\nHypothesis testing is part of the branch of statistics known as statistical inference. In \\nstatistical inference, there is estimation and hypothesis testing. Estimation involves \\npoint estimates and interval estimates. Consider a sample mean, which is a point \\nestimate, that we can use to form a confidence interval. In hypothesis testing, the \\nfocus is examining how a sample statistic informs us about a population parameter. \\nA hypothesis is a statement about one or more populations that we test using sample \\nstatistics.\\nThe process of hypothesis testing begins with the formulation of a theory to orga-\\nnize and explain observations. We judge the correctness of the theory by its ability to \\nmake accurate predictions—for example, to predict the results of new observations. If \\nthe predictions are correct, we continue to maintain the theory as a possibly correct \\nexplanation of our observations. Risk plays a role in the outcomes of observations \\nin finance, so we can only try to make unbiased, probability-based judgments about \\nwhether the new data support the predictions. Statistical hypothesis testing fills that \\nkey role of testing hypotheses when there is uncertainty. When an analyst correctly \\nformulates the question into a testable hypothesis and carries out a test of hypotheses, \\nthe use of well-established scientific methods supports the conclusions and decisions \\nmade on the basis of this test.\\nWe organize this introduction to hypothesis testing around the six steps in Exhibit \\n3, which illustrate the standard approach to hypothesis testing.\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\nThe Process of Hypothesis Testing\\n357\\nExhibit 3: The Process of Hypothesis Testing\\nStep 1: State the hypotheses\\nStep 2: Identify the appropriate test statistic\\nStep 3: Specify the level of significance\\nStep 4: State the decision rule\\nStep 5: Collect data and calculate the test statistic\\nStep 6: Make a decision\\n',\n",
       "     'children': [{'title': 'Stating the Hypotheses',\n",
       "       'page_number': 367,\n",
       "       'content': 'The Process of Hypothesis Testing\\n357\\nExhibit 3: The Process of Hypothesis Testing\\nStep 1: State the hypotheses\\nStep 2: Identify the appropriate test statistic\\nStep 3: Specify the level of significance\\nStep 4: State the decision rule\\nStep 5: Collect data and calculate the test statistic\\nStep 6: Make a decision\\nStating the Hypotheses\\nFor each hypothesis test, we always state two hypotheses: the null hypothesis (or \\nnull), designated H0, and the alternative hypothesis, designated Ha. For example, \\nour null hypothesis may concern the value of a population mean, µ, in relation to one \\npossible value of the mean, µ0.As another example, our null hypothesis may concern \\nthe population variance, σ2, compared with a possible value of this variance, σ02. \\nThe null hypothesis is a statement concerning a population parameter or parameters \\nconsidered to be true unless the sample we use to conduct the hypothesis test gives \\nconvincing evidence that the null hypothesis is false. In fact, the null hypothesis is what \\nwe want to reject. If there is sufficient evidence to indicate that the null hypothesis is \\nnot true, we reject it in favor of the alternative hypothesis.\\nImportantly, the null and alternative hypotheses are stated in terms of population \\nparameters, and we use sample statistics to test these hypotheses.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Two-Sided vs. One-Sided Hypotheses',\n",
       "       'page_number': 367,\n",
       "       'content': 'Two-Sided vs. One-Sided Hypotheses\\nSuppose we want to test whether the population mean return is equal to 6%. We \\nwould state the hypotheses as\\n H0: µ = 6\\nand the alternative as\\n Ha: µ ≠ 6.\\nWhat we just created was a two-sided hypothesis test. We are testing whether the \\nmean is equal to 6%; it could be greater than or less than that because we are simply \\nasking whether the mean is different from 6%. If we find that the sample mean is far \\nenough away from the hypothesized value, considering the risk of drawing a sample \\nthat is not representative of the population, then we would reject the null in favor of \\nthe alternative hypothesis.\\nWhat if we wanted to test whether the mean is greater than 6%. This presents us \\nwith a one-sided hypothesis test, and we specify the hypotheses as follows:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n358\\n H0: µ ≤ 6.\\n Ha: µ > 6.\\nIf we find that the sample mean is greater than the hypothesized value of 6 by a \\nsufficient margin, then we would reject the null hypothesis. Why is the null hypoth-\\nesis stated with a “≤” sign? First, if the sample mean is less than or equal to 6%, this \\nwould not support the alternative. Second, the null and alternative hypotheses must \\nbe mutually exclusive and collectively exhaustive; in other words, all possible values \\nare contained in either the null or the alternative hypothesis.\\nDespite the different ways to formulate hypotheses, we always conduct a test of \\nthe null hypothesis at the point of equality; for example, µ = µ0. Whether the null is \\nH0: µ = µ0, H0: µ ≤ µ0, or H0: µ ≥ µ0, we actually test µ = µ0. The reasoning is straight-\\nforward: Suppose the hypothesized value of the mean is 6. Consider H0: µ ≤ 6, with a \\n“greater than” alternative hypothesis, Ha: µ > 6. If we have enough evidence to reject \\nH0: µ = 6 in favor of Ha: µ > 6, we definitely also have enough evidence to reject the \\nhypothesis that the parameter µ is some smaller value, such as 4.5 or 5.\\nUsing hypotheses regarding the population mean as an example, the three possible \\nformulations of hypotheses are as follows:\\nTwo-sided alternative: H0: μ = μ0 versusHa: μ ≠ μ0\\nOne-sided alternative (right side): H0: µ ≤ µ0 versus Ha: µ > µ0\\nOne-sided alternative (left side): H0: µ ≥ µ0 versus Ha: µ < µ0\\nThe reference to the side (right or left) refers to where we reject the null in the \\nprobability distribution. For example, if the alternative is Ha: μ > 6, this means that \\nwe will reject the null hypothesis if the sample mean is sufficiently higher than (or on \\nthe right side of the distribution of) the hypothesized value.\\nImportantly, the calculation to test the null hypothesis is the same for all three \\nformulations. What is different for the three formulations is how the calculation is \\nevaluated to decide whether to reject the null.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Selecting the Appropriate Hypotheses',\n",
       "       'page_number': 368,\n",
       "       'content': 'Selecting the Appropriate Hypotheses\\nHow do we choose the null and alternative hypotheses? The null is what we are hoping \\nto reject. The most common alternative is the “not equal to” hypothesis. However, \\neconomic or financial theory may suggest a one-sided alternative hypothesis. For \\nexample, if the population parameter is the mean risk premium, financial theory \\nmay argue that this risk premium is positive. Following the principle of stating the \\nalternative as the “hoped for” condition and using µrpfor the population mean risk \\npremium, we formulate the following hypotheses:\\n H0: µrp ≤ 0 versus Ha: µrp > 0\\nNote that the sign in the alternative hypotheses reflects the belief of the researcher \\nmore strongly than a two-sided alternative hypothesis. However, the researcher may \\nsometimes select a two-sided alternative hypothesis to emphasize an attitude of \\nneutrality when a one-sided alternative hypothesis is also reasonable. Typically, the \\neasiest way to formulate the hypotheses is to specify the alternative hypothesis first \\nand then specify the null.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Identify the Appropriate Test Statistic',\n",
       "     'page_number': 369,\n",
       "     'content': 'Identify the Appropriate Test Statistic\\n359\\nEXAMPLE 1\\nSpecifying the Hypotheses\\n1. An analyst suspects that in the most recent year excess returns on stocks \\nhave fallen below 5%. She wants to study whether the excess returns are less \\nthan 5%. Designating the population mean as μ, which hypotheses are most \\nappropriate for her analysis?\\nA. H0: µ = 5 versus Ha: µ ≠ 5\\nB. H0: µ > 5 versus Ha: µ < 5\\nC. H0: µ < 5 versus Ha: µ > 5\\nSolution\\nB is correct. The null hypothesis is what she wants to reject in favor of the \\nalternative, which is that population mean excess return is less than 5%. This \\nis a one-sided (left-side) alternative hypothesis.\\nIDENTIFY THE APPROPRIATE TEST STATISTIC\\nexplain a test statistic, Type I and Type II errors, a significance level, \\nhow significance levels are used in hypothesis testing, and the power \\nof a test\\nA test statistic is a value calculated on the basis of a sample that, when used in conjunc-\\ntion with a decision rule, is the basis for deciding whether to reject the null hypothesis.\\n',\n",
       "     'children': [{'title': 'Test Statistics',\n",
       "       'page_number': 369,\n",
       "       'content': 'Test Statistics\\nThe focal point of our statistical decision is the value of the test statistic. The test statistic \\nthat we use depends on what we are testing. As an example, let us examine the test \\nof a population mean risk premium. Consider the sample mean,   _\\n \\nX  , calculated from \\na sample of returns drawn from the population. If the population standard deviation \\nis known, the standard error of the distribution of sample means,   σ  _\\n \\nX   , is the ratio of \\nthe population standard deviation to the square root of the sample size:\\n σ  \\n_\\n \\nX   =  σ _ \\n √ _ \\nn    . \\n(1)\\nThe test statistic for the test of the mean when the population variance is known is a \\nz-distributed (that is, normally distributed) test statistic:\\n z =  \\n \\n_\\n \\nX  rp −  μ 0  \\n_ \\n σ ⁄ √ _ \\nn     . \\n(2)\\nIf the hypothesized value of the mean population risk premium is 6 (that is, μ0 = 6), \\nwe calculate this as  z =  \\n _\\n \\nX  rp − 6 \\n_ \\n σ ⁄ √ _ \\nn     .  If, however, the hypothesized value of the mean risk \\npremium is zero (that is, μ0 = 0), we can simplify this test statistic as\\n z =  \\n \\n_\\n \\nX  rp  \\n_ \\n σ ⁄ √ _ \\nn     . \\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n360\\nNotably, the key to hypothesis testing is identifying the appropriate test statistic for \\nthe hypotheses and the underlying distribution of the population.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Identifying the Distribution of the Test Statistic',\n",
       "       'page_number': 370,\n",
       "       'content': 'Learning Module 6 \\nHypothesis Testing\\n360\\nNotably, the key to hypothesis testing is identifying the appropriate test statistic for \\nthe hypotheses and the underlying distribution of the population.\\nIdentifying the Distribution of the Test Statistic\\nFollowing the identification of the appropriate test statistic, we must be concerned \\nwith the distribution of the test statistic. We show examples of the test statistics and \\ntheir corresponding distributions in Exhibit 4.\\nExhibit 4: Test Statistics and Their Distributions\\nWhat We Want to Test\\nTest Statistic\\nProbability Distribution \\nof the Statistic\\nDegrees of Freedom\\nTest of a single mean\\n t =   _\\n \\nX  −  μ 0  \\n_ \\n s ⁄ √ _ \\nn     \\nt-Distributed\\nn − 1\\nTest of the difference in means\\n t =  ( _\\n \\nX  1 −  _\\n \\nX  2 ) − ( μ 1 −  μ 2 ) \\n \\n________________ \\n \\n √ _\\n \\n \\n s p 2  \\n_ \\n n 1   +  \\n s p 2  \\n_ \\n n 2     \\n \\n \\nt-Distributed\\nn1 + n2 − 2\\nTest of the mean of differences\\n t =   \\n_\\n \\nd  −  μ d0  \\n_ \\n s  \\n_\\n \\nd     \\nt-Distributed\\nn − 1\\nTest of a single variance\\n χ 2 =   s 2 (n − 1) \\n_ \\n σ 0 2  \\n \\nChi-square distributed\\nn − 1\\nTest of the difference in variances\\n F =   s 1 2  \\n_ \\n s 2 2   \\nF-distributed\\nn1 − 1, n2 − 1\\nTest of a correlation\\n t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2     \\nt-Distributed\\nn − 2\\nTest of independence (categorical data)\\n χ 2 =  ∑ \\ni=1\\n  \\nm\\n    (O − ijE ) ij 2  \\n_ \\nE ij \\n  \\nChi-square distributed\\n(r − 1)(c − 1)\\nNote: µ0, µd0, and   σ 0 2 denote hypothesized values of the mean, mean difference, and variance, respec-\\ntively. The   _ x  ,   \\n_\\n \\nd  , s2, s, and r denote for a sample the mean, mean of the differences, variance, standard \\ndeviation, and correlation, respectively, with subscripts indicating the sample, if appropriate. The sample \\nsize is indicated as n, and the subscript indicates the sample, if appropriate. Oijand Eijare observed and \\nexpected frequencies, respectively, with r indicating the number of rows and c indicating the number of \\ncolumns in the contingency table.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Specify the Level of Significance',\n",
       "     'page_number': 370,\n",
       "     'content': 'SPECIFY THE LEVEL OF SIGNIFICANCE\\nThe level of significance reflects how much sample evidence we require to reject the \\nnull hypothesis. The required standard of proof can change according to the nature of \\nthe hypotheses and the seriousness of the consequences of making a mistake. There \\nare four possible outcomes when we test a null hypothesis, as shown in Exhibit 5. A \\nType I error is a false positive (reject when the null is true), whereas a Type II error \\nis a false negative (fail to reject when the null is false).\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nSpecify the Level of Significance\\n361\\nExhibit 5: Correct and Incorrect Decisions in Hypothesis Testing\\n\\xa0\\nTrue Situation\\nDecision\\nH0 True\\nH0 False\\nFail to reject H0\\nCorrect decision: \\nType II error:\\nDo not reject a true null \\nhypothesis.\\nFail to reject a false null \\nhypothesis.\\nFalse negative\\nReject H0\\nType I error:\\nCorrect decision:\\nReject a true null hypothesis.\\nReject a false null hypothesis.\\nFalse positive\\nWhen we make a decision in a hypothesis test, we run the risk of making either a Type \\nI or a Type II error. As you can see in Exhibit 5, these errors are mutually exclusive: \\nIf we mistakenly reject the true null, we can only be making a Type I error; if we mis-\\ntakenly fail to reject the false null, we can only be making a Type II error.\\nConsider a test of a hypothesis of whether the mean return of a population is \\nequal to 6%. How far away from 6% could a sample mean be before we believe it to \\nbe different from 6%, the hypothesized population mean? We are going to tolerate \\nsample means that are close to 6%, but we begin doubting that the population mean is \\nequal to 6% when we calculate a sample mean that is much different from 0.06. How \\ndo we determine “much different”? We do this by setting a risk tolerance for a Type I \\nerror and determining the critical value or values at which we believe that the sample \\nmean is much different from the population mean. These critical values depend on \\n(1) the alternative hypothesis, whether one sided or two sided, and (2) the probability \\ndistribution of the test statistic, which, in turn, depends on the sample size and the \\nlevel of risk tolerance in making a Type I error.\\nThe probability of a Type I error in testing a hypothesis is denoted by the lower-\\ncase Greek letter alpha, α. This probability is also known as the level of significance \\nof the test, and its complement, (1 − α), is the confidence level. For example, a level \\nof significance of 5% for a test means that there is a 5% probability of rejecting a true \\nnull hypothesis and corresponds to the 95% confidence level.\\nControlling the probabilities of the two types of errors involves a trade-off. All \\nelse equal, if we decrease the probability of a Type I error by specifying a smaller \\nsignificance level (say, 1% rather than 5%), we increase the probability of making a \\nType II error because we will reject the null less frequently, including when it is false. \\nBoth Type I and Type II errors are risks of being wrong. Whether to accept more of \\none type versus the other depends on the consequences of the errors, such as costs. \\nThe only way to reduce the probabilities of both types of errors simultaneously is to \\nincrease the sample size, n.\\nQuantifying the trade-off between the two types of errors in practice is challenging \\nbecause the probability of a Type II error is itself difficult to quantify because there \\nmay be many different possible false hypotheses. Because of this, we specify only α, \\nthe probability of a Type I error, when we conduct a hypothesis test.\\nWhereas the significance level of a test is the probability of incorrectly rejecting the \\ntrue null, the power of a test is the probability of correctly rejecting the null—that is, \\nthe probability of rejecting the null when it is false. The power of a test is, in fact, the \\ncomplement of the Type II error. The probability of a Type II error is often denoted \\nby the lowercase Greek letter beta, β. We can classify the different probabilities in \\nExhibit 6 to reflect the notation that is often used.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n362\\nExhibit 6: Probabilities Associated with Hypothesis Testing Decisions\\n\\xa0\\nTrue Situation\\nDecision\\nH0 True\\nH0 False\\nFail to reject H0\\nConfidence level\\nβ\\n(1 − α)\\n\\xa0\\nReject H0\\nLevel of significance\\nPower of the test\\nα\\n(1 − β)\\nThe standard approach to hypothesis testing involves choosing the test statistic with the \\nmost power and then specifying a level of significance. It is more appropriate to specify \\nthis significance level prior to calculating the test statistic because if we specify it after \\ncalculating the test statistic, we may be influenced by the result of the calculation. The \\nresearcher is free to specify the probability of a Type I error, but the most common \\nare 10%, 5%, and 1%.\\nEXAMPLE 2\\nSignificance Level\\n1. If a researcher selects a 5% level of significance for a hypothesis test, the \\nconfidence level is:\\nA. 2.5%.\\nB. 5%.\\nC. 95%.\\nSolution\\nC is correct. The 5% level of significance (i.e., probability of a Type I error) \\ncorresponds to 1 − 0.05 = 0.95, or a 95% confidence level (i.e., probability of \\nnot rejecting a true null hypothesis). The level of significance is the comple-\\nment to the confidence level; in other words, they sum to 1.00, or 100%.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'State the Decision Rule',\n",
       "     'page_number': 372,\n",
       "     'content': 'STATE THE DECISION RULE\\nexplain a decision rule and the relation between confidence intervals \\nand hypothesis tests, and determine whether a statistically significant \\nresult is also economically meaningful\\nThe fourth step in hypothesis testing is stating the decision rule. Before any sample \\nis drawn and before a test statistic is calculated, we need to set up a decision rule: \\nWhen do we reject the null hypothesis, and when do we not? The action we take \\nis based on comparing the calculated test statistic with a specified value or values, \\nwhich we refer to as critical values. The critical value or values we choose are based \\non the level of significance and the probability distribution associated with the test \\nstatistic. If we find that the calculated value of the test statistic is more extreme than \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nState the Decision Rule\\n363\\nthe critical value or values, then we reject the null hypothesis; we say the result is \\nstatistically significant. Otherwise, we fail to reject the null hypothesis; there is not \\nsufficient evidence to reject the null hypothesis.\\n',\n",
       "     'children': [{'title': 'Determining Critical Values',\n",
       "       'page_number': 373,\n",
       "       'content': 'Determining Critical Values\\nFor a two-tailed test, we indicate two critical values, splitting the level of significance, \\nα, equally between the left and right tails of the distribution. Using a z-distributed \\n(standard normal) test statistic, for example, we would designate these critical values \\nas ±zα/2. For a one-tailed test, we indicate a single rejection point using the symbol \\nfor the test statistic with a subscript indicating the specified probability of a Type I \\nerror— for example, zα.\\nAs we noted in our discussion of Exhibit 2, it is possible to draw a sample that \\nhas a mean different from the true population mean. In fact, it is likely that a given \\nsample mean is different from the population mean because of sampling error. The \\nissue becomes whether a given sample mean is far enough away from what is hypoth-\\nesized to be the population mean that there is doubt about whether the hypothesized \\npopulation mean is true. Therefore, we need to decide how far is too far for a sample \\nmean in comparison to a population mean. That is where the critical values come \\ninto the picture.\\nSuppose we are using a z-test and have chosen a 5% level of significance. In Exhibit \\n7, we illustrate two tests at the 5% significance level using a z-statistic: A two-sided \\nalternative hypothesis test in Panel A and a one-sided alternative hypothesis test in \\nPanel B, with the white area under the curve indicating the confidence level and the \\nshaded areas indicating the significance level. In Panel A, if the null hypothesis that \\nμ = μ0 is true, the test statistic has a 2.5% chance of falling in the left rejection region \\nand a 2.5% chance of falling in the right rejection region. Any calculated value of \\nthe test statistic that falls in either of these two regions causes us to reject the null \\nhypothesis at the 5% significance level.\\nWe determine the cut-off values for the reject and fail-to-reject regions on the basis \\nof the distribution of the test statistic. For a test statistic that is normally distributed, \\nwe determine these cut-off points on the basis of the area under the normal curve; \\nwith a Type I error (i.e., level of significance, α) of 5% and a two-sided test, there is \\n2.5% of the area on either side of the distribution. This results in rejection points of \\n−1.960 and +1.960, dividing the distribution between the rejection and fail-to-reject \\nregions. Similarly, if we have a one-sided test involving a normal distribution, we \\nwould have 5% area under the curve with a demarcation of 1.645 for a right-side test, \\nas we show in Exhibit 7 (or −1.645 for a left-side test).\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n364\\nExhibit 7: Decision Criteria Using a 5% Level of Significance\\nA. Ho: µ = µo versus Ha: µ ≠ µo\\n–1.960\\nReject the null hypothesis\\n–1.960\\nReject the null hypothesis\\n+1.960\\nReject the null hypothesis\\n+1.960\\nReject the null hypothesis\\nFail to reject the \\nnull hypothesis\\nB. Ho: µ ≤ µo versus Ha: µ > µo\\n+1.645\\nReject the null hypothesis\\n+1.645\\nReject the null hypothesis\\nFail to reject the \\nnull hypothesis\\nDetermining the cut-off points using programming\\nThe programs in Microsoft Excel, Python, and R differ slightly, depending on whether \\nthe user specifies the area to the right or the left of the cut-off in the code:\\nCut-off for . . .\\nExcel\\nPython\\nR\\nRight tail, 2.5%\\nNORM.S.INV(0.975)\\nnorm.ppf(.975)\\nqnorm(.025,lower.tail=FALSE)\\nLeft tail, 2.5%\\nNORM.S.INV(0.025)\\nnorm.ppf(.025)\\nqnorm(.025,lower.tail=TRUE)\\nRight tail, 5%\\nNORM.S.INV(0.95)\\nnorm.ppf(.95)\\nqnorm(.05,lower.tail=FALSE)\\nLeft tail, 5%\\nNORM.S.INV(0.05)\\nnorm.ppf(.05)\\nqnorm(.05,lower.tail=TRUE)\\nFor Python, install scipy.stats and import: from scipy.stats import norm.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Decision Rules and Confidence Intervals',\n",
       "       'page_number': 374,\n",
       "       'content': 'Decision Rules and Confidence Intervals\\nExhibit 7 provides an opportunity to highlight the relationship between confidence \\nintervals and hypothesis tests. A 95% confidence interval for the population mean, µ, \\nbased on sample mean,   _\\n \\nX  , is given by:\\n  { \\n_\\n \\nX  − 1.96 σ _ \\n √ _ \\nn    ,  \\n_\\n \\nX  + 1.96 σ _ \\n √ _ \\nn    }  , \\n(3)\\nor, more compactly,   _\\n \\nX  ± 1.96  σ _ \\n √ _ \\nn    .\\nNow consider the conditions for rejecting the null hypothesis:\\n  \\n_\\n \\nX  −  μ 0  \\n_ \\n σ ⁄ √ _ \\nn     < −\\u200a1.96 or  \\n \\n_\\n \\nX  −  μ 0  \\n_ \\n σ ⁄ √ _ \\nn     > 1.96, where z =  \\n \\n_\\n \\nX  −  μ 0  \\n_ \\n σ ⁄ √ _ \\nn     . \\n© CFA Institute. For candidate use only. Not for distribution.\\nState the Decision Rule\\n365\\nAs you can see by comparing these conditions with the confidence interval, we can \\naddress the question of whether   _\\n \\nX  is far enough away from μ0 by either comparing \\nthe calculated test statistic with the critical values or comparing the hypothesized \\npopulation parameter (μ = μ0) with the bounds of the confidence interval, as we show \\nin Exhibit 8. Thus, a significance level in a two-sided hypothesis test can be interpreted \\nin the same way as a (1 − α) confidence interval.\\nExhibit 8: Making a Decision Based on Critical Values and Confidence \\nIntervals for a Two-Sided Alternative Hypothesis\\nMethod\\nProcedure\\nDecision\\n1\\nCompare the calculated test \\nstatistic with the critical \\nvalues.\\nIf the calculated test statistic is less than the lower \\ncritical value or greater than the upper critical \\nvalue, reject the null hypothesis.\\n2\\nCompare the calculated test \\nstatistic with the bounds of \\nthe confidence interval.\\nIf the hypothesized value of the population param-\\neter under the null is outside the corresponding \\nconfidence interval, the null hypothesis is rejected.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Collect the Data and Calculate the Test Statistic',\n",
       "       'page_number': 375,\n",
       "       'content': 'Collect the Data and Calculate the Test Statistic\\nThe fifth step in hypothesis testing is collecting the data and calculating the test sta-\\ntistic. The quality of our conclusions depends on not only the appropriateness of the \\nstatistical model but also the quality of the data we use in conducting the test. First, \\nwe need to ensure that the sampling procedure does not include biases, such as sample \\nselection or time bias. Second, we need to cleanse the data, checking inaccuracies and \\nother measurement errors in the data. Once assured that the sample is unbiased and \\naccurate, the sample information is used to calculate the appropriate test statistic.\\nEXAMPLE 3\\nUsing a Confidence Interval in Hypothesis Testing\\n1. Consider the hypotheses H0: µ = 3 versus Ha: µ ≠ 3. If the confidence inter-\\nval based on sample information has a lower bound of 2.75 and an upper \\nbound of 4.25, the most appropriate decision is:\\nA. reject the null hypothesis.\\nB. accept the null hypothesis.\\nC. fail to reject the null hypothesis.\\nSolution\\nC is correct. Since the hypothesized population mean (µ = 3) is within the \\nbounds of the confidence interval (2.75, 4.25), the correct decision is to \\nfail to reject the null hypothesis. It is only when the hypothesized value is \\noutside these bounds that the null hypothesis is rejected. Note that the null \\nhypothesis is never accepted; either the null is rejected on the basis of the \\nevidence or there is a failure to reject the null hypothesis.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n366\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Make a Decision',\n",
       "     'page_number': 376,\n",
       "     'content': 'Learning Module 6 \\nHypothesis Testing\\n366\\nMAKE A DECISION\\nexplain a decision rule and the relation between confidence intervals \\nand hypothesis tests, and determine whether a statistically significant \\nresult is also economically meaningful\\n',\n",
       "     'children': [{'title': 'Make a Statistical Decision',\n",
       "       'page_number': 376,\n",
       "       'content': 'Learning Module 6 \\nHypothesis Testing\\n366\\nMAKE A DECISION\\nexplain a decision rule and the relation between confidence intervals \\nand hypothesis tests, and determine whether a statistically significant \\nresult is also economically meaningful\\nMake a Statistical Decision\\nThe sixth step in hypothesis testing is making the decision. Consider a test of the mean \\nrisk premium, comparing the population mean with zero. If the calculated z-statistic \\nis 2.5 and with a two-sided alternative hypothesis and a 5% level of significance, we \\nreject the null hypothesis because 2.5 is outside the bounds of ±1.96. This is a statisti-\\ncal decision: The evidence indicates that the mean risk premium is not equal to zero.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Make an Economic Decision',\n",
       "       'page_number': 376,\n",
       "       'content': 'Learning Module 6 \\nHypothesis Testing\\n366\\nMAKE A DECISION\\nexplain a decision rule and the relation between confidence intervals \\nand hypothesis tests, and determine whether a statistically significant \\nresult is also economically meaningful\\nMake a Statistical Decision\\nThe sixth step in hypothesis testing is making the decision. Consider a test of the mean \\nrisk premium, comparing the population mean with zero. If the calculated z-statistic \\nis 2.5 and with a two-sided alternative hypothesis and a 5% level of significance, we \\nreject the null hypothesis because 2.5 is outside the bounds of ±1.96. This is a statisti-\\ncal decision: The evidence indicates that the mean risk premium is not equal to zero.\\nMake an Economic Decision\\nAnother part of the decision making is making the economic or investment decision. \\nThe economic or investment decision takes into consideration not only the statistical \\ndecision but also all pertinent economic issues. If, for example, we reject the null that \\nthe risk premium is zero in favor of the alternative hypothesis that the risk premium \\nis greater than zero, we have found evidence that the US risk premium is different \\nfrom zero. The question then becomes whether this risk premium is economically \\nmeaningful. On the basis of these considerations, an investor might decide to commit \\nfunds to US equities. A range of non-statistical considerations, such as the investor’s \\ntolerance for risk and financial position, might also enter the decision-making process.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Statistically Significant but Not Economically Significant?',\n",
       "       'page_number': 376,\n",
       "       'content': 'Statistically Significant but Not Economically Significant?\\nWe frequently find that slight differences between a variable and its hypothesized value \\nare statistically significant but not economically meaningful. For example, we may be \\ntesting an investment strategy and reject a null hypothesis that the mean return to the \\nstrategy is zero based on a large sample. In the case of a test of the mean, the smaller \\nthe standard error of the mean, the larger the value of the test statistic and the greater \\nthe chance the null will be rejected, all else equal. The standard error decreases as \\nthe sample size, n, increases, so that for very large samples, we can reject the null for \\nsmall departures from it. We may find that although a strategy provides a statistically \\nsignificant positive mean return, the results may not be economically significant when \\nwe account for transaction costs, taxes, and risk. Even if we conclude that a strategy’s \\nresults are economically meaningful, we should explore the logic of why the strategy \\nmight work in the future before implementing it. Such considerations cannot be \\nincorporated into a hypothesis test.\\nEXAMPLE 4\\nDecisions and Significance\\n1. An analyst is testing whether there are positive risk-adjusted returns to a \\ntrading strategy. He collects a sample and tests the hypotheses of H0: µ ≤ 0% \\nversus Ha: µ > 0%, where µ is the population mean risk-adjusted return. The \\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'The Role of p-Values',\n",
       "     'page_number': 377,\n",
       "     'content': 'The Role of p-Values\\n367\\nmean risk-adjusted return for the sample is 0.7%. The calculated t-statistic \\nis 2.428, and the critical t-value is 2.345. He estimates that the transaction \\ncosts are 0.3%. The results are most likely:\\nA. statistically and economically significant.\\nB. statistically significant but not economically significant.\\nC. economically significant but not statistically significant.\\nSolution\\nA is correct. The results indicate that the mean risk-adjusted return is great-\\ner than 0% because the calculated test statistic of 2.428 is greater than the \\ncritical value of 2.245. The results are also economically significant because \\nthe risk-adjusted return exceeds the transaction cost associated with this \\nstrategy by 0.4% (= 0.7 − 0.3).\\nTHE ROLE OF P-VALUES\\nexplain and interpret the p-value as it relates to hypothesis testing\\nAnalysts, researchers, and statistical software often report the p-value associated with \\nhypothesis tests. The p-value is the area in the probability distribution outside the \\ncalculated test statistic; for a two-sided test, this is the area outside ± the calculated \\ntest statistic, but for a one-sided test, this is the area outside the calculated test statistic \\non the appropriate side of the probability distribution. We illustrated in Exhibit 7 the \\nrejection region, which corresponds to the probability of a Type I error. However, the \\np-value is the area under the curve (so, the probability) associated with the calculated \\ntest statistic. Stated another way, the p-value is the smallest level of significance at \\nwhich the null hypothesis can be rejected.\\nConsider the calculated z-statistic of 2.33 in a two-sided test: The p-value is the area \\nin the z-distribution that lies outside ±2.33. Calculation of this area requires a bit of \\ncalculus, but fortunately statistical programs and other software calculate the p-value \\nfor us. For the value of the test statistic of 2.33, the p-value is approximately 0.02, or 2%. \\nUsing Excel, we can get the precise value of 0.019806 [(1-NORM.S.DIST(2.33,TRUE))*2]. \\nWe can reject the null hypothesis because we were willing to tolerate up to 5% outside \\nthe calculated value. The smaller the p-value, the stronger the evidence against the \\nnull hypothesis and in favor of the alternative hypothesis; if the p-value is less than \\nthe level of significance, we reject the null hypothesis.\\nWe illustrate the comparison of the level of significance and the p-value in Exhibit \\n9. The fail-to-reject region is determined by the critical values of +1.96, as we saw in \\nExhibit 7. There is 5% of the area under the distribution in the rejection regions—2.5% \\non the left side, 2.5% on the right. But now we introduce the area outside the calculated \\ntest statistic. For the calculated z-statistic of 2.33, there is 0.01, or 1%, of the area under \\nthe normal distribution above 2.33 and 1% of the area below −2.33 (or, in other words, \\n98% of the area between ±2.33). Since we are willing to tolerate a 5% Type I error, we \\nreject the null hypothesis in the case of a calculated test statistic of 2.33 because there \\nis a p-value of 2%; there is 2% of the distribution outside the calculated test statistic.\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n368\\nExhibit 9: Comparison of the Level of Significance and the p-Value\\n–2.33\\n–2.33\\n2.33\\n2.33\\nReject\\nReject\\nReject\\nReject\\n–1.96\\n–1.96\\n1.96\\nFail to reject the 1.96\\nnull hypothesis\\na/2\\np/2\\np/2\\na/2\\nWhat if we are testing a one-sided alternative hypothesis? We focus solely on the area \\noutside the calculated value on the side indicated by the alternative hypothesis. For \\nexample, if we are testing an alternative hypothesis that the population mean risk \\npremium is greater than zero, calculate a z-statistic of 2.5, and have a level of signifi-\\ncance of 5%, the p-value is the area in the probability distribution that is greater than \\n2.5. This area is 0.00621, or 0.621%. Since this is less than what we tolerate if the α is \\n5%, we reject the null hypothesis.\\nConsider a population of 1,000 assets that has a mean return of 6% and a standard \\ndeviation of 2%. Suppose we draw a sample of 50 returns and test whether the mean \\nis equal to 6%, calculating the p-value for the calculated z-statistic. Then, suppose we \\nrepeat this process, draw 1,000 different samples, and, therefore, get 1,000 different \\ncalculated z-statistics and 1,000 different p-values. If we use a 5% level of significance, \\nwe should expect to reject the true null 5% of the time; if we use a 10% level of sig-\\nnificance, we should expect to reject the true null 10% of the time.\\nNow suppose that with this same population, whose mean return is 6%, we test \\nthe hypothesis that the population mean is 7%, with the same standard deviation. As \\nbefore, we draw 1,000 different samples and calculate 1,000 different p-values. If we \\nuse a 5% level of significance and a two-sided alternative hypothesis, we should expect \\nto reject this false null hypothesis on the basis of the power of the test.\\nPutting this together, consider the histograms of p-values for the two different tests \\nin Exhibit 10. The first bin is the p-values of 5% or less. What we see is that with the \\ntrue null hypothesis of 6%, we reject the null approximately 5% of the time. For the \\nfalse null hypothesis, that the mean is equal to 7%, we reject the null approximately \\n0.973, or 97.3%, of the time.\\n© CFA Institute. For candidate use only. Not for distribution.\\nThe Role of p-Values\\n369\\nExhibit 10: Distribution of p-values for 1,000 Different Samples of Size 50 \\nDrawn from a Population with a Mean of 6%\\nFrequency\\n1,000\\n1,000\\n800\\n800\\n900\\n900\\n700\\n700\\n600\\n600\\n500\\n500\\n400\\n400\\n200\\n200\\n300\\n300\\n100\\n100\\n0\\n30\\n30\\n5\\n55\\n55\\n85\\n85\\n100\\n95100\\n9095\\n90\\n80\\n7580\\n7075\\n6570\\n6065\\n60\\n50\\n4550\\n4045\\n3540\\n35\\n25\\n2025\\n1520\\n15\\n10\\n10\\np-Values (upper value of bins, %)\\nFalse Null Mean = 0.07\\nTrue Null Mean = 0.06\\nThe p-values for the true null hypothesis are generally uniformly distributed between \\n0% and 100% because under the null hypothesis, there is a 5% chance of the p-values \\nbeing less than 5%, a 10% chance being less than 10%, and so on. Why is it not com-\\npletely uniform? Because we took 1,000 samples of 50; taking more samples or larger \\nsamples would result in a more uniform distribution of p-values. When looking at the \\np-values for the false null hypothesis in Exhibit 10, we see that this is not a uniform \\ndistribution; rather, there is a peak around 0% and very little elsewhere. You can see \\nthe difference in the p-values for two false hypothesized means of 6.5% and 7% in \\nExhibit 11. It shows that the further the false hypothesis is away from the truth (i.e., \\nmean of 6%), the greater the power of the test and the better the ability to detect the \\nfalse hypothesis.\\nSoftware, such as Excel, Python, and R, is available for calculating p-values for \\nmost distributions.\\nExhibit 11: Comparison of the Distribution of p-Values for the False Null \\nHypotheses H0: µ = 7% and H0: µ = 6.5%\\nFrequency\\n1,000\\n1,000\\n800\\n800\\n900\\n900\\n700\\n700\\n600\\n600\\n500\\n500\\n400\\n400\\n200\\n200\\n300\\n300\\n100\\n100\\n0\\n30\\n30\\n5\\n55\\n55\\n85\\n85\\n100\\n95100\\n9095\\n90\\n80\\n7580\\n7075\\n6570\\n6065\\n60\\n50\\n4550\\n4045\\n3540\\n35\\n25\\n2025\\n1520\\n15\\n10\\n10\\np-Values (upper value of bins, %)\\nFalse Null Mean = 0.07\\nFalse Null Mean = 0.065\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n370\\nEXAMPLE 5\\nMaking a Decision Using p-Values\\n1. An analyst is testing the hypotheses H0: σ2 = 0.01 versus Ha: σ2 ≠ 0.01. Using \\nsoftware, she determines that the p-value for the test statistic is 0.03, or 3%. \\nWhich of the following statements are correct?\\nA. Reject the null hypothesis at both the 1% and 5% levels of significance.\\nB. Reject the null hypothesis at the 5% level but not at the 1% level of \\nsignificance.\\nC. Fail to reject the null hypothesis at both the 1% and 5% levels of \\nsignificance.\\nSolution\\nB is correct. Rejection of the null hypothesis requires that the p-value be less \\nthan the level of significance. On the basis of this requirement, the null is \\nrejected at the 5% level of significance but not at the 1% level of significance.\\nMULTIPLE TESTS AND SIGNIFICANCE \\nINTERPRETATION\\ndescribe how to interpret the significance of a test in the context of \\nmultiple tests\\nA Type I error is the risk of rejection of a true null hypothesis. Another way of phras-\\ning this is that it is a false positive result; that is, the null is rejected (the positive), \\nyet the null is true (hence, a false positive). The expected portion of false positives is \\nthe false discovery rate (FDR). In the previous example of drawing 1,000 samples \\nof 50 observations each, it is the case that there are samples in which we reject the \\ntrue null hypothesis of a population mean of 6%. If we draw enough samples with a \\nlevel of significance of 0.05, approximately 0.05 of the time you will reject the null \\nhypothesis, even if the null is true. In other words, if you run 100 tests and use a 5% \\nlevel of significance, you get five false positives, on average. This is referred to as the \\nmultiple testing problem.\\nThe false discovery approach to testing requires adjusting the p-value when you \\nhave a series of tests. The idea of adjusting for the likelihood of significant results \\nbeing false positives was first introduced by Benjamini and Hochberg (BH) in 1995. \\nWhat they proposed is that the researcher rank the p-values from the various tests, \\nfrom lowest to highest, and then make the following comparison, starting with the \\nlowest p-value (with k = 1), p(1):\\n p(1 ) ≤ α \\nRank\\xa0of\\xa0i \\n_____________ \\n \\nNumber\\xa0of\\xa0tests  . \\nThis comparison is repeated, such that k is determined by the highest ranked p(k) \\nfor which this is a true statement. If, say, k is 4, then the first four tests (ranked on the \\nbasis of the lowest p-values) are said to be significant.\\nSuppose we test the hypothesis that the population mean is equal to 6% and repeat \\nthe sampling process by drawing 20 samples and calculating 20 test statistics; the \\nsix test statistics with the lowest p-values are shown in  . Using a significance level of \\n8\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Multiple Tests and Significance Interpretation',\n",
       "     'page_number': 380,\n",
       "     'content': 'Multiple Tests and Significance Interpretation\\n371\\n5%, if we simply relied on each test and its p-value, then there are five tests in which \\nwe would reject the null. However, using the BH criteria, only one test is considered \\nsignificant, as shown in Exhibit 12.\\nExhibit 12: Applying the Benjamini and Hochberg Criteria\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n _\\n \\nX  \\nCalculated \\nz-Statistic\\np-Value\\nRank of p-Value \\n(lowest to highest)\\n α\\xa0 \\nRank\\xa0of\\xa0i \\n_____________ \\n \\nNumber\\xa0of\\xa0tests  \\nIs value in (3) less than or \\nequal to value in (5)?\\n0.0664\\n3.1966\\n0.0014\\n1\\n0.0025\\nYes\\n0.0645\\n2.2463\\n0.0247\\n2\\n0.0050\\nNo\\n0.0642\\n2.0993\\n0.0358\\n3\\n0.0075\\nNo\\n0.0642\\n2.0756\\n0.0379\\n4\\n0.0100\\nNo\\n0.0641\\n2.0723\\n0.0382\\n5\\n0.0125\\nNo\\n0.0637\\n1.8627\\n0.0625\\n6\\n0.0150\\nNo\\nNote: Level of significance = 5%.\\nSo, what is the conclusion from looking at p-values and the multiple testing problem?\\n■ \\nFirst, if we sample, test, and find a result that is not statistically significant, \\nthis result is not wrong; in fact, the null hypothesis may well be true.\\n■ \\nSecond, if the power of the test is low or the sample size is small, we should \\nbe cautious because there is a good chance of a false positive.\\n■ \\nThird, when we perform a hypothesis test and determine the critical values, \\nthese values are based on the assumption that the test is run once. Running \\nmultiple tests on data risks data snooping, which may result in spuri-\\nous results. Determine the dataset and perform the test, but do not keep \\nperforming tests repeatedly to search out statistically significant results, \\nbecause you may, by chance, find them (i.e., false positives).\\n■ \\nFourth, in very large samples, we will find that nearly every test is signifi-\\ncant. The approach to use to address this issue is to draw different samples; \\nif the results are similar, the results are more robust.\\nEXAMPLE 6\\nFalse Discovery and Multiple Tests\\nA researcher is examining the mean return on assets of publicly traded compa-\\nnies that constitute an index of 2,000 mid-cap stocks and is testing hypotheses \\nconcerning whether the mean is equal to 15%: H0: µROA = 15 versus Ha: µROA \\n≠ 15. She uses a 10% level of significance and collects a sample of 50 firms. She \\nwants to examine the robustness of her analysis, so she repeats the collection \\nand test of the return on assets 30 times. The results for the samples with the \\nfive lowest p-values are given in Exhibit 13.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n372\\nExhibit 13: Five Lowest p-Values of the 30 Samples Tested\\n \\n \\nSample\\nCalculated  \\nz-Statistic\\np-Value\\nRanked  \\np-Value\\n1\\n3.203\\n0.00136\\n1\\n5\\n3.115\\n0.00184\\n2\\n14\\n2.987\\n0.00282\\n3\\n25\\n2.143\\n0.03211\\n4\\n29\\n1.903\\n0.05704\\n5\\n \\n1. Of the 30 samples tested, how many should the researcher expect, on aver-\\nage, to have p-values less than the level of significance?\\nSolution to 1\\nOf the 30 samples tested, she should expect 30 × 0.10 = 3 to have significant \\nresults just by chance. Consider why she ended up with more than three. \\nThree is based on large sample sizes and large numbers of samples. Using a \\nlimited sample size (i.e., 50) and number of samples (i.e., 30), there is a risk \\nof a false discovery with repeated samples and tests.\\n2. What are the corrected p-values based on her selected level of significance, \\nand what is the effect on her decision?\\nSolution to 2\\nApplying the BH criteria, the researcher determines the adjusted p-values \\nshown in Exhibit 14.\\n \\nExhibit 14: Adjusted p-Values for Five Lowest p-Values from 30 Samples Tested\\n \\nCalculated \\nz-Statistic\\np-Value\\nRank of \\np-Value\\n α\\xa0 \\nRank\\xa0of\\xa0i \\n_____________ \\n \\nNumber\\xa0of\\xa0tests  \\nIs p-value less than or  \\nequal to adjusted p-value?\\n3.203\\n0.00136\\n1\\n0.00333\\nYes\\n3.115\\n0.00184\\n2\\n0.00667\\nYes\\n2.987\\n0.00282\\n3\\n0.01000\\nYes\\n2.143\\n0.03211\\n4\\n0.01333\\nNo\\n1.903\\n0.05704\\n5\\n0.01667\\nNo\\n \\nOn the basis of the results in Exhibit 14, there are three samples with \\np-values less than their adjusted p-values. So, the number of significant \\nsample results is the same as would be expected from chance, given the 10% \\nlevel of significance. The researcher concludes that the results for the sam-\\nples with Ranks 4 and 5 are false discoveries, and she has not uncovered any \\nevidence from her testing that supports rejecting the null hypothesis.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Tests Concerning a Single Mean',\n",
       "     'page_number': 383,\n",
       "     'content': 'Tests Concerning a Single Mean\\n373\\nTESTS CONCERNING A SINGLE MEAN\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning the population mean of both large and \\nsmall samples when the population is normally or approximately \\nnormally distributed and the variance is (1) known or (2) unknown\\nHypothesis tests concerning the mean are among the most common in practice. The \\nsampling distribution of the mean when the population standard deviation is unknown \\nis t-distributed, and when the population standard deviation is known, it is normally \\ndistributed, or z-distributed. Since the population standard deviation is unknown in \\nalmost all cases, we will focus on the use of a t-distributed test statistic.\\nThe t-distribution is a probability distribution defined by a single parameter known \\nas degrees of freedom (df). Like the standard normal distribution, a t-distribution is \\nsymmetrical with a mean of zero, but it has a standard deviation greater than 1 and \\ngenerally fatter tails. As the number of degrees of freedom increases with the sample \\nsize, the t-distribution approaches the standard normal distribution.\\nFor hypothesis tests concerning the population mean of a normally distributed pop-\\nulation with unknown variance, the theoretically correct test statistic is the t-statistic. \\nWhat if a normal distribution does not describe the population? The t-statistic is \\nrobust to moderate departures from normality, except for outliers and strong skew-\\nness. When we have large samples, departures of the underlying distribution from \\nthe normal case are of increasingly less concern. The sample mean is approximately \\nnormally distributed in large samples according to the central limit theorem, whatever \\nthe distribution describing the population. A traditional rule of thumb is that the \\nnormal distribution is used in cases when the sample size is larger than 30, but the \\nmore precise testing uses the t-distribution. Moreover, with software that aids us in \\nsuch testing, we do not need to resort to rules of thumb.\\nIf the population sampled has unknown variance, then the test statistic for hypoth-\\nesis tests concerning a single population mean, μ, is\\n t n−1 =  \\n \\n_\\n \\nX  −  μ 0  \\n_ \\n s ⁄ √ _ \\nn     , \\n(4)\\nwhere\\n \\n \\n_\\n \\nX  = sample mean\\n \\nµ0 = hypothesized value of the population mean\\n \\ns = sample standard deviation\\n \\nn = sample size\\n \\n  s  \\n_\\n \\nX   =  s ⁄ √ _ \\nn   = estimate of the sample mean standard error\\nThis test statistic is t-distributed with n − 1 degrees of freedom, which we can write \\nas tn-1. For simplicity, we often drop the subscript n − 1 because each particular test \\nstatistic has specified degrees of freedom, as we presented in Exhibit 4.\\nConsider testing whether Investment One’s returns (from Exhibit 1) are different \\nfrom 6%; that is, we are testing H0: μ = 6 versus Ha: μ ≠ 6. If the calculated t-distributed \\ntest statistic is outside the bounds of the critical values based on the level of signifi-\\ncance, we will reject the null hypothesis in favor of the alternative. If we have a sample \\n9\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n374\\nsize of 33, there are n − 1 = 32 degrees of freedom. At a 5% significance level (two \\ntailed) and 32 degrees of freedom, the critical t-values are ±2.037. We can determine \\nthe critical values from software:\\n■ \\nExcel [T.INV(0.025,32) and T.INV(0.975,32)]\\n■ \\nR [qt(c(.025,.975),32)]\\n■ \\nPython [from scipy.stats import t and t.ppf(.025,32) and t.ppf(.975,32)]\\nSuppose that the sample mean return is 5.2990% and the sample standard deviation \\nis 1.4284%. The calculated test statistic is\\n t =  5.2990 − 6 \\n_ \\n 1.4284 ⁄ √ _ \\n33     = −\\u200a2.8192 \\nwith 32 degrees of freedom. The calculated value is less than −2.037, so we reject \\nthe null that the population mean is 6%, concluding that it is different from 6%.\\nEXAMPLE 7\\nRisk and Return Characteristics of an Equity Mutual Fund\\nSuppose you are analyzing Sendar Equity Fund, a midcap growth fund that has \\nbeen in existence for 24 months. During this period, it has achieved a mean \\nmonthly return of 1.50%, with a sample standard deviation of monthly returns \\nof 3.60%. Given its level of market risk and according to a pricing model, this \\nmutual fund was expected to have earned a 1.10% mean monthly return during \\nthat time period. Assuming returns are normally distributed, are the actual results \\nconsistent with an underlying or population mean monthly return of 1.10%?\\n1. Test the hypothesis using a 5% level of significance.\\nSolution to 1\\n \\nStep 1\\nState the hypotheses.\\nH0: μ = 1.1% versus Ha: μ ≠ 1.1% \\nStep 2\\nIdentify the appropriate test statistic.\\n t =   _\\n \\nX  −  μ 0  \\n_ \\n s ⁄ √ _ \\nn      \\nwith 24 − 1 =23 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5% (two tailed).\\nStep 4\\nState the decision rule.\\nCritical t-values = ±2.069.\\nReject the null if the calculated t-statistic is less than −2.069, and reject the \\nnull if the calculated t-statistic is greater than +2.069.\\nExcel \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: T.INV(0.025,23)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: T.INV(0.975,23)\\nR qt(c(.025,.975),23)\\nPython from scipy.stats import t\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: t.ppf(.025,23)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: t.ppf(.975,23)\\nStep 5\\nCalculate the test statistic.\\n t =  1.5 − 1.1 \\n_ \\n 3.6 ⁄ √ _ \\n24     = 0.54433 \\nStep 6\\nMake a decision.\\nFail to reject the null hypothesis because the calculated t-statistic falls \\nbetween the two critical values. There is not sufficient evidence to indicate \\nthat the population mean monthly return is different from 1.10%.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nTests Concerning a Single Mean\\n375\\n. Test the hypothesis using the 95% confidence interval.\\nSolution to 2\\nThe 95% confidence interval is  _\\n \\nX  ± Critical\\xa0value  ( s _ \\n √ _ \\nn    )  , so\\n \\n  { 1.5 − 2.069  ( 3.6 ⁄ √ _ \\n24   )  ,  1.5 + 2.069  ( 3.6 ⁄ √ _ \\n24   )  }  \\n  \\n \\n \\n \\n  { 1.5 − 1.5204, 1.5 + 1.5204 }   \\n \\n \\n  { −\\u200a0.0204, 3.0204 }   \\n \\n \\nThe hypothesized value of 1.1% is within the bounds of the 95% confidence \\ninterval, so we fail to reject the null hypothesis.\\nWe stated previously that when population variance is not known, we use a t-test \\nfor tests concerning a single population mean. Given at least approximate normality, \\nthe t-distributed test statistic is always called for when we deal with small samples \\nand do not know the population variance. For large samples, the central limit theo-\\nrem states that the sample mean is approximately normally distributed, whatever the \\ndistribution of the population. The t-statistic is still appropriate, but an alternative \\ntest may be more useful when sample size is large.\\nFor large samples, practitioners sometimes use a z-test in place of a t-test for tests \\nconcerning a mean. The justification for using the z-test in this context is twofold. \\nFirst, in large samples, the sample mean should follow the normal distribution at least \\napproximately, as we have already stated, fulfilling the normality assumption of the \\nz-test. Second, the difference between the rejection points for the t-test and z-test \\nbecomes quite small when the sample size is large. Since the t-test is readily available \\nas statistical program output and theoretically correct for unknown population vari-\\nance, we present it as the test of choice.\\nIn a very limited number of cases, we may know the population variance; in such \\ncases, the z-statistic is theoretically correct. In this case, the appropriate test statistic \\nis what we used earlier (Equation 2):\\n z =  \\n \\n_\\n \\nX  −  μ 0  \\n_ \\n σ ⁄ √ _ \\nn     . \\nIn cases of large samples, a researcher may use the z-statistic, substituting the sample \\nstandard deviation (s) for the population standard deviation (σ) in the formula. When \\nwe use a z-test, we usually refer to a rejection point in Exhibit 15.\\nExhibit 15: Critical Values for Common Significance Levels for the Standard \\nNormal Distribution\\nLevel of \\nSignificance\\nAlternative\\nReject the Null if . . .\\nbelow the  \\nCritical Value\\nabove the  \\nCritical Value\\n0.01\\nTwo sided: H0: μ = μ0, Ha: μ ≠ μ0\\n−2.576\\n2.576\\nOne sided: H0: μ ≤ μ0, Ha: μ > μ0\\n\\xa0\\n2.326\\nOne sided: H0: μ ≥ μ0, Ha: μ < μ0\\n−2.326\\n\\xa0\\n0.05\\nTwo sided: H0: μ = μ0, Ha: μ ≠ μ0\\n−1.960\\n1.960\\nOne sided: H0: μ ≤ μ0, Ha: μ > μ0\\n\\xa0\\n1.645\\nOne sided: H0: μ ≥ μ0, Ha: μ < μ0\\n−1.645\\n\\xa0\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n376\\nEXAMPLE 8\\nTesting the Returns on the ACE High Yield Index\\n1. Suppose we want to test whether the daily return in the ACE High Yield \\nTotal Return Index is different from zero. Collecting a sample of 1,304 daily \\nreturns, we find a mean daily return of 0.0157%, with a standard deviation of \\n0.3157%.\\n1. Test whether the mean daily return is different from zero at the 5% level \\nof significance.\\n2. Using the z-distributed test statistic as an approximation, test whether the \\nmean daily return is different from zero at the 5% level of significance.\\nSolution to 1\\n \\nStep 1\\nState the hypotheses.\\nH0: μ = 0% versus Ha: μ ≠ 0% \\nStep 2\\nIdentify the appropriate test statistic.\\n t =   _\\n \\nX  −  μ 0  \\n_ \\n s ⁄ √ _ \\nn      \\nwith 1,304 − 1 = 1,303 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5%.\\nStep 4\\nState the decision rule.\\nCritical t-values = ±1.962.\\nReject the null if the calculated t-statistic is less than −1.962, and reject \\nthe null if the calculated t-statistic is greater than +1.962.\\nExcel\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: T.INV(0.025,1303)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: T.INV(0.975,1303)\\nR qt(c(.025,.975),1303)\\nPython from scipy.stats import t\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: t.ppf(.025,1303)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: t.ppf(.975,1303)\\nStep 5\\nCalculate the test statistic.\\n t =  0.0157 − 0 \\n_ \\n 0.3157 ⁄ √ _ \\n1, 304     = 1.79582 \\nStep 6\\nMake a decision.\\nFail to reject the null because the calculated t-statistic falls between the two \\ncritical values. There is not sufficient evidence to indicate that the mean daily \\nreturn is different from 0%.\\n \\nSolution to 2\\nStep 1\\nState the hypotheses.\\nH0: μ = 0% versus Ha: μ ≠ 0% \\nStep 2\\nIdentify the appropriate test \\nstatistic.\\n z =   _\\n \\nX  −  μ 0  \\n_ \\n s ⁄ √ _ \\nn      \\nwith 1,304 − 1 = 1,303 degrees of freedom.\\nStep 3\\nSpecify the level of \\nsignificance.\\nα = 5%.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Test Concerning Differences between Means with Independent Samples',\n",
       "     'page_number': 387,\n",
       "     'content': 'Test Concerning Differences between Means with Independent Samples\\n377\\nStep 4\\nState the decision rule.\\nCritical t-values = ±1.960.\\nReject the null if the calculated z-statistic is less than −1.960, and reject the null if \\nthe calculated z-statistic is greater than +1.960.\\nExcel\\nLower: NORM.S.INV(0.025)\\nUpper: NORM.S.INV(0.975)\\nR\\nqnorm(.025,lower.tail=TRUE)\\nqnorm(.975,lower.tail=FALSE)\\nPython from scipy.stats import norm\\nLower: norm.ppf(.025,23)\\nUpper: norm.ppf(.975,23)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 z =  0.0157 − 0 \\n_ \\n 0.3157 ⁄ √ _ \\n1, 304     = 1.79582 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Fail to reject the null because the calculated z-statistic falls between the two \\ncritical values. There is not sufficient evidence to indicate that the mean daily return \\nis different from 0%.\\nStep 5\\nCalculate the test statistic.\\n\\xa0\\nStep 6\\nMake a decision.\\n\\xa0\\n \\n TEST CONCERNING DIFFERENCES BETWEEN MEANS \\nWITH INDEPENDENT SAMPLES\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning the equality of the population means of \\ntwo at least approximately normally distributed populations based \\non independent random samples with equal assumed variances\\nWe often want to know whether a mean value—for example, a mean return—differs \\nfor two groups. Is an observed difference due to chance or to different underlying \\nvalues for the mean? We test this by drawing a sample from each group. When it is \\nreasonable to believe that the samples are from populations at least approximately \\nnormally distributed and that the samples are also independent of each other, we use \\nthe test of the differences in the means. We may assume that population variances are \\nequal or unequal. However, our focus in discussing the test of the differences of means \\nis using the assumption that the population variances are equal. In the calculation of \\nthe test statistic, we combine the observations from both samples to obtain a pooled \\nestimate of the common population variance.\\nLet µ1 and µ2 represent, respectively, the population means of the first and \\nsecond populations. Most often we want to test whether the population means are \\nequal or whether one is larger than the other. Thus, we formulate the following sets \\nof hypotheses:\\nTwo sided: \\n H0: µ1 − µ2 = 0 versus Ha: µ1 − µ2 ≠ 0,\\nor, equivalently,\\n10\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n378\\n H0: µ1 = µ2 versus Ha: µ1 ≠ µ2\\nOne sided (right side): \\n H0: µ1 − µ2 ≤ 0 versus Ha: µ1 − µ2 > 0,\\nor, equivalently,\\n H0: µ 1 ≤ µ 2 versus Ha: µ 1 > µ 2\\nOne sided (left side): \\n H0: µ 1 − µ 2 ≥ 0 versus Ha: µ 1 − µ 2 < 0,\\nor, equivalently,\\n H0: µ 1 ≥ µ 2 versus Ha: µ 1 < µ 2\\nWe can, however, formulate other hypotheses, where the difference is something \\nother than zero, such as H0: µ 1 − µ 2 = 2 versus Ha: µ 1 − µ 2 ≠ 2. The procedure is \\nthe same.\\nWhen we can assume that the two populations are normally distributed and that \\nthe unknown population variances are equal, we use a t-distributed test statistic based \\non independent random samples:\\n t =  \\n  ( \\n_\\n \\nX  1 −  \\n_\\n \\nX  2 )  −   ( μ 1 −  μ 2 )   \\n \\n__________________ \\n \\n √ \\n_\\n \\n \\n s p 2 \\n_  \\n n 1   +  \\n s p 2 \\n_  \\n n 2    \\n \\n , \\n(5)\\nWhere  s p 2 =    ( n 1 − 1 )  s 1 2 +   ( n 2 − 1 )  s 2  \\n2  \\n \\n__________________ \\n \\n n 1 +  n 2 − 2 \\n    is a pooled estimator of the common variance. As \\nyou can see, the pooled estimate is a weighted average of the two samples’ variances, \\nwith the degrees of freedom for each sample as the weight. The number of degrees \\nof freedom for this t-distributed test statistic is n1 + n2 − 2.\\nEXAMPLE 9\\nReturns on the ACE High Yield Index Compared for Two \\nPeriods\\nContinuing the example of the returns in the ACE High Yield Total Return \\nIndex, suppose we want to test whether these returns, shown in Exhibit 16, are \\ndifferent for two different time periods, Period 1 and Period 2.\\n \\nExhibit 16: Descriptive Statistics for ACE High Yield Total \\nReturn Index for Periods 1 and 2\\n \\n \\n\\xa0\\nPeriod 1\\nPeriod 2\\nMean\\n0.01775%\\n0.01134%\\nStandard deviation\\n0.31580%\\n0.38760%\\nSample size\\n445 days\\n859 days\\n \\nNote that these periods are of different lengths and the samples are independent; \\nthat is, there is no pairing of the days for the two periods.\\nTest whether there is a difference between the mean daily returns in Period 1 \\nand in Period 2 using a 5% level of significance.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Test Concerning Differences between Means with Dependent Samples',\n",
       "     'page_number': 389,\n",
       "     'content': 'Test Concerning Differences between Means with Dependent Samples\\n379\\nStep 1\\nState the hypotheses.\\nH0: μPeriod1 = μPeriod2 versus Ha: μPeriod1 ≠ μPeriod2 \\nStep 2\\nIdentify the appropriate test statistic.\\n t =  ( _\\n \\nX  Period1 −  _\\n \\nX  Period2 ) − ( μ Period1 −  μ Period2 ) \\n \\n \\n_______________________________ \\n \\n \\n √ \\n_______________\\n \\n \\n \\n s p 2  \\n_ \\n n period1   +  \\n s p 2  \\n_ \\n n period2    \\n \\n ,  \\n where   s p 2 =  \\n( n period1 − 1 )  s Period1  \\n2 \\n + ( n period2 − 1 )  s Period2  \\n2 \\n  \\n \\n \\n \\n_________________________________ \\n \\n \\n n period1 +  n period2 − 2 \\n  \\nwith 445 + 859 − 2 =1,302 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5%.\\nStep 4\\nState the decision rule.\\nCritical t-values = ±1.962.\\nReject the null if the calculated t-statistic is less than −1.962, and reject \\nthe null if the calculated t-statistic is greater than +1.962.\\n\\xa0\\n\\xa0\\nExcel\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: T.INV(0.025,1302)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: T.INV(0.975,1302)\\nR qt(c(.025,.975),1302)\\nPython from scipy.stats import t\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: t.ppf(.025,1302)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: t.ppf(.975,1302)\\nStep 5\\nCalculate the test statistic.\\n \\n s p 2 =  (445 − 1 ) 0.09973 + (859 − 1 ) 0.15023 \\n \\n \\n____________________________ \\n445 + 859 − 2  \\n = 0.1330\\n  \\n \\n \\n \\nt =  (0.01775 − 0.01134 ) − 0 \\n \\n__________________ \\n \\n √ \\n_____________ \\n 0.1330  \\n_ \\n445  +  0.1330 \\n_ \\n859   \\n \\n =  0.0064 \\n_ \\n0.0213  = 0.3009. \\n \\nStep 6\\nMake a decision.\\nFail to reject the null because the calculated t-statistic falls within the \\nbounds of the two critical values. We conclude that there is insufficient \\nevidence to indicate that the returns are different for the two time periods.\\n \\nTEST CONCERNING DIFFERENCES BETWEEN MEANS \\nWITH DEPENDENT SAMPLES\\nidentify the appropriate test statistic and interpret the results for \\na hypothesis test concerning the mean difference of two normally \\ndistributed populations\\nWhen we compare two independent samples, we use a t-distributed test statistic that \\nuses the difference in the means and a pooled variance. An assumption for the validity \\nof those tests is that the samples are independent—that is, unrelated to each other. \\nWhen we want to conduct tests on two means based on samples that we believe are \\ndependent, we use the test of the mean of the differences.\\nThe t-test in this section is based on data arranged in paired observations, and the \\ntest itself is sometimes referred to as the paired comparisons test. Paired observa-\\ntions are observations that are dependent because they have something in common. \\nFor example, we may be concerned with the dividend policy of companies before and \\nafter a change in the tax law affecting the taxation of dividends. We then have pairs \\nof observations for the same companies; these are dependent samples because we \\nhave pairs of the sample companies before and after the tax law change. We may test \\na hypothesis about the mean of the differences that we observe across companies. \\nFor example, we may be testing whether the mean returns earned by two investment \\n11\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n380\\nstrategies were equal over a study period. The observations here are dependent in \\nthe sense that there is one observation for each strategy in each month, and both \\nobservations depend on underlying market risk factors. What is being tested are \\nthe differences, and the paired comparisons test assumes that the differences are \\nnormally distributed. By calculating a standard error based on differences, we can \\nuse a t-distributed test statistic to account for correlation between the observations.\\nHow is this test of paired differences different from the test of the differences in \\nmeans in independent samples? The test of paired comparisons is more powerful than \\nthe test of the differences in the means because by using the common element (such \\nas the same periods or companies), we eliminate the variation between the samples \\nthat could be caused by something other than what we are testing.\\nSuppose we have observations for the random variables XA and XB and that the \\nsamples are dependent. We arrange the observations in pairs. Let di denote the differ-\\nence between two paired observations. We can use the notation di = xAi − xBi, where \\nxAi and xBi are the ith pair of observations, i = 1, 2, . . . , n, on the two variables. Let μd \\nstand for the population mean difference. We can formulate the following hypotheses, \\nwhere μd0 is a hypothesized value for the population mean difference:\\nTwo sided: H0: µd = µd0 versus Ha: µd ≠ µd0\\nOne sided (right side): H0: µd ≤ µd0 versus Ha: µd > µd0\\nOne sided (left side) H0: µd ≥ µd0 versus Ha: µd < µd0\\nIn practice, the most commonly used value for µd0 is zero.\\nWe are concerned with the case of normally distributed populations with unknown \\npopulation variances, and we use a t-distributed test statistic. We begin by calculating    \\n_\\n \\nd \\n, the sample mean difference, or the mean of the differences, di:\\n \\n_\\n \\nd  =  1 _ \\nn  ∑ \\ni=1\\n  \\nn\\n   d i  , \\n(6)\\nwhere n is the number of pairs of observations. The sample standard deviation, \\nsd, is the standard deviation of the differences, and the standard error of the mean \\ndifferences is   s  \\n_\\n \\nd   =   s d  \\n_ \\n √ _ \\nn    . \\nWhen we have data consisting of paired observations from samples generated \\nby normally distributed populations with unknown variances, the t-distributed test \\nstatistic is\\n t =  \\n \\n_\\n \\nd  −  μ d0  \\n_ \\n s  \\n_\\n \\nd     \\n(7)\\nwith n − 1 degrees of freedom, where n is the number of paired observations.\\nFor example, suppose we want to see if there is a difference between the returns \\nfor Investments One and Two (from Exhibit 1), for which we have returns in each of \\n33 years. Using a 1% level of significance, the critical values for a two-sided hypothesis \\ntest are ±2.7385. Lining up these returns by the years and calculating the differences, \\nwe find a sample mean difference  ( \\n_\\n \\nd  ) of 0.10353% and a standard deviation of these \\ndifferences (sd) of 2.35979%. Therefore, the calculated t-statistic for testing whether \\nthe mean of the differences is equal to zero is\\n t =  0.10353 − 0 \\n_ \\n2.35979\\u200a/\\u200a √ _ \\n33    = 0.25203 \\nwith 32 degrees of freedom. In this case, we fail to reject the null because the \\nt-statistic falls within the bounds of the two critical values. We conclude that there is \\nnot sufficient evidence to indicate that the returns for Investment One and Investment \\nTwo are different.\\nImportantly, if we think of the differences between the two samples as a single \\nsample, then the test of the mean of differences is identical to the test of a single \\nsample mean.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTest Concerning Differences between Means with Dependent Samples\\n381\\nEXAMPLE 10\\nTesting for the Mean of the Differences\\nIn Exhibit 17, we report the quarterly returns for a three-year period for two \\nactively managed portfolios specializing in precious metals. The two portfolios \\nare similar in risk and had nearly identical expense ratios. A major investment \\nservices company rated Portfolio B more highly than Portfolio A. In investigating \\nthe portfolios’ relative performance, suppose we want to test the hypothesis that \\nthe mean quarterly return on Portfolio A is equal to the mean quarterly return \\non Portfolio B during the three-year period. Since the two portfolios share \\nessentially the same set of risk factors, their returns are not independent, so a \\npaired comparisons test is appropriate. Use a 10% level of significance.\\n \\nExhibit 17: Quarterly Returns for Two Actively Managed Precious \\nMetals Portfolios\\n \\n \\nYear\\nQuarter\\nPortfolio A \\n(%)\\nPortfolio B \\n(%)\\nDifference \\n(Portfolio A − Portfolio B)\\n1\\n1\\n4.50\\n0.50\\n4.00\\n1\\n2\\n−4.10\\n−3.10\\n−1.00\\n1\\n3\\n−14.50\\n−16.80\\n2.30\\n1\\n4\\n−5.50\\n−6.78\\n1.28\\n2\\n1\\n12.00\\n−2.00\\n14.00\\n2\\n2\\n−7.97\\n−8.96\\n0.99\\n2\\n3\\n−14.01\\n−10.01\\n−4.00\\n2\\n4\\n4.11\\n−6.31\\n10.42\\n3\\n1\\n2.34\\n−5.00\\n7.34\\n3\\n2\\n26.36\\n12.77\\n13.59\\n3\\n3\\n10.72\\n9.23\\n1.49\\n3\\n4\\n3.60\\n1.20\\n2.40\\nAverage\\n1.46\\n-2.94\\n4.40083\\nStandard deviation\\n11.18\\n7.82\\n5.47434\\n \\nUsing this sample information, we can summarize the test as follows:\\n \\nStep 1\\nState the hypotheses.\\nH0: μd0 = 0 versus Ha: μd0 ≠ 0 \\nStep 2\\nIdentify the appropriate test statistic.\\n t =   \\n_\\n \\nd  − μ d0 \\n_ \\n s  \\n_\\nd     \\n \\nStep 3\\nSpecify the level of significance.\\n10%\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n382\\nStep 3\\nState the decision rule.\\nWith 12 − 1 = 11 degrees of freedom, the critical values are ±1.796.\\nWe reject the null hypothesis if the calculated test statistic is below −1.796 \\nor above +1.796.\\nExcel \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: T.INV(0.05,11)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: T.INV(0.95,11)\\nR qt(c(.05,.95),11)\\nPython from scipy.stats import t\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: t.ppf(.05,11)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: t.ppf(.95,11)\\nStep 4\\nCalculate the test statistic.\\n \\n \\n_\\n \\nd  = 4.40083\\n  \\n \\n s  \\n_\\n \\nd   =  5.47434 \\n_ \\n √ _ \\n12    = 1.58031  \\n \\n \\nt =  4.40083 − 0 \\n_ \\n1.58031  = 2.78480\\n  \\nStep 5\\nMake a decision.\\nReject the null hypothesis because the calculated t-statistic falls outside the \\nbounds of the two critical values. There is sufficient evidence to indicate \\nthat the mean of the differences of returns is not zero.\\n \\nThe following example illustrates the application of this test to evaluate two com-\\npeting investment strategies.\\nEXAMPLE 11\\nA Comparison of the Returns of Two Indexes\\n1. Suppose we want to compare the returns of the ACE High Yield Index with \\nthose of the ACE BBB Index. We collect data over 1,304 days for both index-\\nes and calculate the means and standard deviations as shown in Exhibit 18.\\n \\nExhibit 18: Mean and Standard Deviations for the ACE High Yield \\nIndex and the ACE BBB Index\\n \\n \\n\\xa0\\nACE High Yield \\nIndex \\n(%)\\nACE BBB  \\nIndex (%)\\nDifference  \\n(%)\\nMean return\\n0.0157\\n0.0135\\n−0.0021\\nStandard deviation\\n0.3157\\n0.3645\\n0.3622\\n \\nUsing a 5% level of significance, determine whether the mean of the differ-\\nences is different from zero.\\nSolution\\n \\nStep 1\\nState the hypotheses.\\nH0: μd0 = 0 versus Ha: μd0 ≠ 0 \\nStep 2\\nIdentify the appropriate test statistic.\\n t =   \\n_\\n \\nd  −  μ d0  \\n_ \\n s  \\n_\\n \\nd     \\nStep 3\\nSpecify the level of significance.\\n5%\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Testing Concerning Tests of Variances',\n",
       "     'page_number': 393,\n",
       "     'content': 'Testing Concerning Tests of Variances\\n383\\nStep 4\\nState the decision rule.\\nWith 1,304 − 1 = 1,303 degrees of freedom, the critical values are ±1.962.\\nWe reject the null hypothesis if the calculated t-statistic is less than \\n−1.962 or greater than +1.962.\\nExcel \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: T.INV(0.025,1303)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: T.INV(0.975,1303)\\nR qt(c(.025,.975),1303\\nPython from scipy.stats import t\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: t.ppf(.025,1303)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: t.ppf(.975,1303)\\nStep 5\\nCalculate the test statistic.\\n \\n \\n_\\n \\nd  = − 0.0021%\\n  \\n \\n s  \\n_\\n \\nd   =   s d  \\n_ \\n √ _ \\nn    =  0.3622 \\n_ \\n √ _ \\n1, 304    = 0.01003%  \\n \\n \\nt =  − 0.00210 − 0 \\n_ \\n0.01003  = − 0.20937\\n \\n \\nStep 6\\nMake a decision.\\nFail to reject the null hypothesis because the calculated t-statistic falls \\nwithin the bounds of the two critical values. There is insufficient evidence \\nto indicate that the mean of the differences of returns is different from \\nzero.\\n \\nTESTING CONCERNING TESTS OF VARIANCES\\nidentify the appropriate test statistic and interpret the results for a \\nhypothesis test concerning (1) the variance of a normally distributed \\npopulation and (2) the equality of the variances of two normally \\ndistributed populations based on two independent random samples\\nOften, we are interested in the volatility of returns or prices, and one approach to \\nexamining volatility is to evaluate variances. We examine two types of tests involving \\nvariance: tests concerning the value of a single population variance and tests concerning \\nthe difference between two population variances.\\n',\n",
       "     'children': [{'title': 'Tests of a Single Variance',\n",
       "       'page_number': 393,\n",
       "       'content': 'Tests of a Single Variance\\nSuppose there is a goal to keep the variance of a fund’s returns below a specified \\ntarget. In this case, we would want to compare the observed sample variance of the \\nfund with the target. Performing a test of a population variance requires specifying \\nthe hypothesized value of the variance,   σ 0 2 . We can formulate hypotheses concerning \\nwhether the variance is equal to a specific value or whether it is greater than or less \\nthan a hypothesized value:\\nTwo-sided alternative:   H 0 :  σ 2 =  σ 0 2 \\xa0versus\\xa0  H a :  σ 2 ≠  σ 0 2  \\nOne-sided alternative (right tail):   H 0 :  σ 2 ≤  σ 0 2 \\xa0versus\\xa0  H a :  σ 2 >  σ 0 2  \\nOne-sided alternative (left tail):   H 0 :  σ 2 ≥  σ 0 2 \\xa0versus\\xa0  H a :  σ 2 <  σ 0 2  \\nIn tests concerning the variance of a single normally distributed population, we \\nmake use of a chi-square test statistic, denoted χ2. The chi-square distribution, unlike \\nthe normal distribution and t-distribution, is asymmetrical. Like the t-distribution, \\nthe chi-square distribution is a family of distributions, with a different distribution \\n12\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n384\\nfor each possible value of degrees of freedom, n − 1 (n is sample size). Unlike the \\nt-distribution, the chi-square distribution is bounded below by zero; χ2 does not take \\non negative values.\\nIf we have n independent observations from a normally distributed population, \\nthe appropriate test statistic is\\n χ 2 =  (n − 1 )  s 2  \\n_ \\n σ 0 2  \\n \\n(8)\\nwith n − 1 degrees of freedom. The sample variance (s2) is in the numerator, and the \\nhypothesized variance  ( σ 0 2 ) is in the denominator.\\nIn contrast to the t-test, for example, the chi-square test is sensitive to violations \\nof its assumptions. If the sample is not random or if it does not come from a normally \\ndistributed population, inferences based on a chi-square test are likely to be faulty.\\nSince the chi-square distribution is asymmetric and bounded below by zero, we \\nno longer have the convenient ± for critical values as we have with the z- and the \\nt-distributions, so we must either use a table of chi-square values or use software \\nto generate the critical values. Consider a sample of 25 observations, so we have 24 \\ndegrees of freedom. We illustrate the rejection regions for the two- and one-sided \\ntests at the 5% significance level in Exhibit 19.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTesting Concerning Tests of Variances\\n385\\nExhibit 19: Rejection Regions (Shaded) for the Chi-Square Distribution (df = \\n24) at 5% Significance\\nA. Ho: σ2 = σ2 versus Ha: σ2 ≠ σ2\\n0\\n0\\n0\\n5.7\\n5.7\\n11.4\\n11.4\\n17.1\\n17.1\\n22.8\\n22.8\\n28.5\\n28.5\\n34.2\\n34.2\\n39.9\\n39.9\\n45.6\\n45.6\\n51.3\\n51.3\\n57.0\\n57.0\\n62.7\\n62.7\\nB. Ho: σ2 ≤ σ2 versus Ha: σ2 > σ2\\n0\\n0\\n0\\n5.7\\n5.7\\n11.4\\n11.4\\n17.1\\n17.1\\n22.8\\n22.8\\n28.5\\n28.5\\n34.2\\n34.2\\n39.9\\n39.9\\n45.6\\n45.6\\n51.3\\n51.3\\n57.0\\n57.0\\n62.7\\n62.7\\nC. Ho: σ2 ≥ σ2 versus Ha: σ2 < σ2\\n0\\n0\\n0\\n5.7\\n5.7\\n11.4\\n11.4\\n17.1\\n17.1\\n22.8\\n22.8\\n28.5\\n28.5\\n34.2\\n34.2\\n39.9\\n39.9\\n45.6\\n45.6\\n51.3\\n51.3\\n57.0\\n57.0\\n62.7\\n62.7\\nCritical values are 12.40115 and 39.36408\\nCritical value is 36.41503\\nCritical value is 13.84843\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n386\\nEXAMPLE 12\\nRisk and Return Characteristics of an Equity Mutual Fund\\n1. You continue with your analysis of Sendar Equity Fund, a midcap growth \\nfund that has been in existence for only 24 months. During this period, \\nSendar Equity achieved a mean monthly return of 1.50% and a standard \\ndeviation of monthly returns of 3.60%.\\n1. Using a 5% level of significance, test whether the standard deviation of \\nreturns is different from 4%.\\n2. Using a 5% level of significance, test whether the standard deviation of \\nreturns is less than 4%.\\nSolution to 1\\n  \\nStep 1\\nState the hypotheses.\\nH0: σ2 = 16 versus Ha: σ2 ≠ 16 \\nStep 2\\nIdentify the appropriate test statistic.\\n χ 2 =  (n − 1 )  s 2  \\n_ \\n σ 0 2  \\n \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith 24 − 1 = 23 degrees of freedom, the critical values are 11.68855 and \\n38.07563.\\nWe reject the null hypothesis if the calculated χ2 statistic is less than \\n11.68855 or greater than 38.07563.\\nExcel \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: CHISQ.INV(0.025,23)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: CHISQ.INV(0.975,23)\\nR qchisq(c(.025,.975),23)\\nPython from scipy.stats import chi2\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: chi2.ppf(.025,23)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: chi2.ppf(.975,23)\\nStep 5\\nCalculate the test statistic.\\n χ 2 =  (24 − 1 ) 12.96 \\n___________ \\n16 \\n = 18.63000 \\nStep 6\\nMake a decision\\nFail to reject the null hypothesis because the calculated χ2 statistic falls \\nwithin the bounds of the two critical values. There is insufficient evidence \\nto indicate that the variance is different from 16% (or, equivalently, that the \\nstandard deviation is different from 4%).\\n \\nSolution to 2:\\n \\nStep 1\\nState the hypotheses.\\nH0: σ2 ≥ 16 versus Ha: σ2 < 16 \\nStep 2\\nIdentify the appropriate test statistic.\\n χ 2 =  (n − 1 )  s 2  \\n_ \\n σ 0 2  \\n \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith 24 − 1 = 23 degrees of freedom, the critical value is 13.09051. \\nWe reject the null hypothesis if the calculated χ2 statistic is less than \\n13.09051.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTesting Concerning Tests of Variances\\n387\\n\\xa0\\n\\xa0\\nExcel CHISQ.INV(0.05,23)\\nR qchisq(.05,23)\\nPython from scipy.stats import chi2\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0chi2.ppf(.05,23)\\nStep 5\\nCalculate the test statistic.\\n χ 2 =  (24 − 1 ) 12.96 \\n___________ \\n16 \\n = 18.63000 \\nStep 6\\nMake a decision.\\nFail to reject the null hypothesis because the calculated χ2 statistic is \\ngreater than the critical value. There is insufficient evidence to indicate \\nthat the variance is less than 16% (or, equivalently, that the standard devia-\\ntion is less than 4%).\\n \\nTest Concerning the Equality of Two Variances (F-Test)\\nThere are many instances in which we want to compare the volatility of two samples, \\nin which case we can test for the equality of two variances. Examples include compar-\\nisons of baskets of securities against indexes or benchmarks, as well as comparisons of \\nvolatility in different periods. Suppose we have a hypothesis about the relative values \\nof the variances of two normally distributed populations with variances of  σ 1 2 and  \\nσ 2 2 , distinguishing the two populations as 1 or 2. We can formulate the hypotheses \\nas two sided or one sided:\\nTwo-sided alternative:\\n H 0 \\u200a:\\u200a σ 1 2 =  σ 2 2  versus   H a \\u200a:\\u200a σ 1 2 ≠  σ 2 2  \\nor, equivalently,\\n H 0 \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   = 1 versus   H a \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   ≠ 1 \\nOne-sided alternative (right side):\\n H 0 \\u200a:\\u200a σ 1 2 ≤  σ 2 2  versus   H a \\u200a:\\u200a σ 1 2 >  σ 2 2    \\nor, equivalently,\\n H 0 \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   ≤ 1 versus   H a \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   > 1  \\nOne-sided alternative (left side):\\n H 0 \\u200a:\\u200a σ 1 2 ≥  σ 2 2  versus   H a \\u200a:\\u200a σ 1 2 <  σ 2 2    \\nor, equivalently,\\n H 0 \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   ≥ 1 versus   H a \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   < 1  \\nGiven independent random samples from these populations, tests related to \\nthese hypotheses are based on an F-test, which is the ratio of sample variances. \\nTests concerning the difference between the variances of two populations make use \\nof the F-distribution. Like the chi-square distribution, the F-distribution is a family \\nof asymmetrical distributions bounded from below by zero. Each F-distribution is \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Test Concerning the Equality of Two Variances (F-Test)',\n",
       "       'page_number': 397,\n",
       "       'content': 'Testing Concerning Tests of Variances\\n387\\n\\xa0\\n\\xa0\\nExcel CHISQ.INV(0.05,23)\\nR qchisq(.05,23)\\nPython from scipy.stats import chi2\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0chi2.ppf(.05,23)\\nStep 5\\nCalculate the test statistic.\\n χ 2 =  (24 − 1 ) 12.96 \\n___________ \\n16 \\n = 18.63000 \\nStep 6\\nMake a decision.\\nFail to reject the null hypothesis because the calculated χ2 statistic is \\ngreater than the critical value. There is insufficient evidence to indicate \\nthat the variance is less than 16% (or, equivalently, that the standard devia-\\ntion is less than 4%).\\n \\nTest Concerning the Equality of Two Variances (F-Test)\\nThere are many instances in which we want to compare the volatility of two samples, \\nin which case we can test for the equality of two variances. Examples include compar-\\nisons of baskets of securities against indexes or benchmarks, as well as comparisons of \\nvolatility in different periods. Suppose we have a hypothesis about the relative values \\nof the variances of two normally distributed populations with variances of  σ 1 2 and  \\nσ 2 2 , distinguishing the two populations as 1 or 2. We can formulate the hypotheses \\nas two sided or one sided:\\nTwo-sided alternative:\\n H 0 \\u200a:\\u200a σ 1 2 =  σ 2 2  versus   H a \\u200a:\\u200a σ 1 2 ≠  σ 2 2  \\nor, equivalently,\\n H 0 \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   = 1 versus   H a \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   ≠ 1 \\nOne-sided alternative (right side):\\n H 0 \\u200a:\\u200a σ 1 2 ≤  σ 2 2  versus   H a \\u200a:\\u200a σ 1 2 >  σ 2 2    \\nor, equivalently,\\n H 0 \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   ≤ 1 versus   H a \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   > 1  \\nOne-sided alternative (left side):\\n H 0 \\u200a:\\u200a σ 1 2 ≥  σ 2 2  versus   H a \\u200a:\\u200a σ 1 2 <  σ 2 2    \\nor, equivalently,\\n H 0 \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   ≥ 1 versus   H a \\u200a:\\u200a \\n σ 1 2  \\n_ \\n σ 2 2   < 1  \\nGiven independent random samples from these populations, tests related to \\nthese hypotheses are based on an F-test, which is the ratio of sample variances. \\nTests concerning the difference between the variances of two populations make use \\nof the F-distribution. Like the chi-square distribution, the F-distribution is a family \\nof asymmetrical distributions bounded from below by zero. Each F-distribution is \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n388\\ndefined by two values of degrees of freedom, which we refer to as the numerator and \\ndenominator degrees of freedom. The F-test, like the chi-square test, is not robust to \\nviolations of its assumptions.\\nSuppose we have two samples, the first with n1 observations and a sample variance  \\ns 1 2 and the second with n2 observations and a sample variance   s 2 2 . The samples are \\nrandom, independent of each other, and generated by normally distributed popula-\\ntions. A test concerning differences between the variances of the two populations is \\nbased on the ratio of sample variances, as follows:\\n F =  \\n s 1 2  \\n_ \\n s 2 2   \\n(9)\\nwith df1 = (n1 − 1) numerator degrees of freedom and df2 = (n2 − 1) denominator \\ndegrees of freedom. Note that df1 and df2 are the divisors used in calculating   s 1 2 and  \\ns 2 2 , respectively.\\nWhen we rely on tables to arrive at critical values, a convention is to use the \\nlarger of the two sample variances in the numerator in Equation 9; doing so reduces \\nthe number of F-tables needed. The key is to be consistent with how the alternative \\nhypothesis is specified and the order of the sample sizes for the degrees of freedom.\\nConsider two samples, the first with 25 observations and the second with 40 \\nobservations. We show the rejection region and critical values in Exhibit 20 for two- \\nand one-sided alternative hypotheses at the 5% significance level.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTesting Concerning Tests of Variances\\n389\\nExhibit 20: Rejection Regions (Shaded) for the F-Distribution Based on \\nSample Sizes of 25 and 40 at 5% Significance\\nA. Ho: σ2 = σ2 versus Ha: σ2 ≠ σ2\\n2\\n2\\n1\\n1\\n0\\n0.29\\n0.29\\n0.57\\n0.57\\n0.85\\n0.85\\n1.14\\n1.14\\n1.42\\n1.42\\n1.71\\n1.71\\n1.99\\n1.99\\n2.28\\n2.28\\n2.56\\n2.56\\n2.85\\n2.85\\n3.13\\n3.13\\n0\\n0.29\\n0.29\\n0.57\\n0.57\\n0.85\\n0.85\\n1.14\\n1.14\\n1.42\\n1.42\\n1.71\\n1.71\\n1.99\\n1.99\\n2.28\\n2.28\\n2.56\\n2.56\\n2.85\\n2.85\\n3.13\\n3.13\\n0\\n0.29\\n0.29\\n0.57\\n0.57\\n0.85\\n0.85\\n1.14\\n1.14\\n1.42\\n1.42\\n1.71\\n1.71\\n1.99\\n1.99\\n2.28\\n2.28\\n2.56\\n2.56\\n2.85\\n2.85\\n3.13\\n3.13\\nB. Ho: σ2 ≤ σ2 versus Ha: σ2 > σ2\\n2\\n2\\n1\\n1\\nC. Ho: σ2 ≥ σ2 versus Ha: σ2 < σ2\\n2\\n2\\n1\\n1\\nCritical values are 0.49587 and 2.15095\\nCritical value is 1.89566\\nCritical value is 0.55551\\nConsider Investments One and Two (from Exhibit 1), with standard deviations of \\nreturns of 1.4284 and 2.5914, respectively, calculated over the 33-year period. If \\nwe want to know whether the variance of Investment One is different from that of \\nInvestment Two, we use the F-distributed test statistic. With 32 and 32 degrees of \\nfreedom, the critical values are 0.49389 and 2.02475 at the 5% significance level. The \\ncalculated F-statistic is\\n F =   2.5914 2  \\n_ \\n 1.4284 2   = 3.29131. \\nTherefore, we reject the null hypothesis that the variances of these two investments \\nare the same because the calculated F-statistic is outside of the critical values. We \\ncan conclude that one investment is riskier than the other.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n390\\nEXAMPLE 13\\nVolatility and Regulation\\nYou are investigating whether the population variance of returns on a stock \\nmarket index changed after a change in market regulation. The first 418 weeks \\noccurred before the regulation change, and the second 418 weeks occurred \\nafter the regulation change. You gather the data in Exhibit 21 for 418 weeks of \\nreturns both before and after the change in regulation. You have specified a 5% \\nlevel of significance.\\n \\nExhibit 21: Index Returns and Variances before and after the \\nMarket Regulation Change\\n \\n \\n\\xa0\\nn\\nMean Weekly Return \\n(%)\\nVariance of Returns\\nBefore regulation change\\n418\\n0.250\\n4.644\\nAfter regulation change\\n418\\n0.110\\n3.919\\n \\n1. Test whether the variance of returns is different before the regulation \\nchange versus after the regulation change, using a 5% level of significance.\\nSolution to 1\\n  \\nStep 1\\nState the hypotheses.\\n H 0 :  σ Before  \\n2 \\n =  σ After  \\n2 \\n \\xa0versus\\xa0  H a :  σ Before  \\n2 \\n ≠  σ After  \\n2 \\n \\nStep 2\\nIdentify the appropriate test statistic.\\n F =  \\n s Before  \\n2 \\n_   \\n s After  \\n2 \\n   \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith 418 − 1 = 417 and 418 − 1 = 417 degrees of freedom, the critical \\nvalues are 0.82512 and 1.21194.\\nReject the null if the calculated F-statistic is less than 0.82512 or greater \\nthan 1.21194.\\nExcel \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Left side: F.INV(0.025,417,417)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Right side: F.INV(0.975,417,417)\\nR qf(c(.025,.975),417,417)\\nPython from scipy.stats import f\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Left side: f.ppf(.025,417,417)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Right side: f.ppf(.975,417,417)\\nStep 5\\nCalculate the test statistic.\\n F =  4.644 \\n_ \\n3.919  = 1.18500 \\nStep 6\\nMake a decision.\\nFail to reject the null hypothesis since the calculated F-statistic falls \\nwithin the bounds of the two critical values. There is not sufficient evi-\\ndence to indicate that the weekly variances of returns are different in the \\nperiods before and after the regulation change.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTesting Concerning Tests of Variances\\n391\\n 2. \\nTest whether the variance of returns is greater before the regulation change \\nversus after the regulation change, using a 5% level of significance.\\nSolution to 2\\n \\nStep 1\\nState the hypotheses.\\n H 0 :  σ Before  \\n2 \\n ≤  σ After  \\n2 \\n \\xa0versus\\xa0  H a :  σ Before  \\n2 \\n >  σ After  \\n2 \\n \\nStep 2\\nIdentify the appropriate test statistic.\\n F =  \\n s Before  \\n2 \\n_   \\n s After  \\n2 \\n   \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith 418 − 1 = 417 and 418 − 1 = 417 degrees of freedom, the critical \\nvalue is 1.17502.\\nWe reject the null hypothesis if the calculated F-statistic is greater than \\n1.17502.\\nExcel F.INV(0.95,417,417)\\nR qf(.95,417,417)\\nPython from scipy.stats import f\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0f.ppf(.95,417,417)\\nStep 5\\nCalculate the test statistic.\\n F =  4.644 \\n_ \\n3.919  = 1.18500 \\nStep 6\\nMake a decision.\\nReject the null hypothesis since the calculated F-statistic is greater than \\n1.17502. There is sufficient evidence to indicate that the weekly variances \\nof returns before the regulation change are greater than the variances \\nafter the regulation change.\\n \\nEXAMPLE 14\\nThe Volatility of Derivatives Expiration Days\\n1. You are interested in investigating whether quadruple witching days—that \\nis, the occurrence of stock option, index option, index futures, and single \\nstock futures expirations on the same day—exhibit greater volatility than \\nnormal trading days. Exhibit 22 presents the daily standard deviation of \\nreturns for normal trading days and quadruple witching days during a \\nfour-year period.\\n \\nExhibit 22: Standard Deviation of Returns: Normal Trading Days \\nand Derivatives Expiration Days\\n \\n \\nPeriod\\nType of Day\\nn\\nStandard Deviation (%)\\n1\\nNormal trading days\\n138\\n0.821\\n2\\nQuadruple witching days\\n16\\n1.217\\n \\nTest to determine whether the variance of returns for quadruple witching \\ndays is greater than the variance for non-expiration, normal trading days. \\nUse a 5% level of significance.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n392\\nSolution\\n \\nStep 1\\nState the hypotheses.\\n H 0 :  σ Period2  \\n2 \\n ≤  σ Period1  \\n2 \\n \\xa0versus\\xa0  H a :  σ Period2  \\n2 \\n >  σ Period1  \\n2 \\n \\nStep 2\\nIdentify the appropriate test statistic.\\n F =   s Period2  \\n2 \\n_   \\n s Period1  \\n2 \\n   \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith 16 − 1 = 15 and 138 − 1 = 137 degrees of freedom, the critical value \\nis 1.73997.\\nWe reject the null hypothesis if the calculated F-statistic is greater than \\n1.73997.\\nExcel F.INV(0.95,15,137)\\nR qf(.95,15,137)\\nPython from scipy.stats import f\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0f.ppf(.95,15,137)\\nStep 5\\nCalculate the test statistic.\\n F =  1.48109 \\n_ \\n0.67404  = 2.19733 \\nStep 6\\nMake a decision.\\nReject the null hypothesis since the calculated F-statistic is greater than \\n1.73997. There is sufficient evidence to indicate that the variance of returns \\nfor quadruple witching days is greater than the variance for normal trading \\ndays.\\n \\nPARAMETRIC VS. NONPARAMETRIC TESTS\\ncompare and contrast parametric and nonparametric tests, and \\ndescribe situations where each is the more appropriate type of test\\nThe hypothesis-testing procedures we have discussed up to this point have two char-\\nacteristics in common. First, they are concerned with parameters, and second, their \\nvalidity depends on a definite set of assumptions. Mean and variance, for example, \\nare two parameters, or defining quantities, of a normal distribution. The tests also \\nmake specific assumptions—in particular, assumptions about the distribution of the \\npopulation producing the sample. Any test or procedure with either of these two \\ncharacteristics is a parametric test or procedure. In some cases, however, we are \\nconcerned about quantities other than parameters of distributions. In other cases, \\nwe may believe that the assumptions of parametric tests do not hold. In cases where \\nwe are examining quantities other than population parameters or where assumptions \\nof the parameters are not satisfied, a nonparametric test or procedure can be useful.\\nA nonparametric test is a test that is not concerned with a parameter or a test \\nthat makes minimal assumptions about the population from which the sample comes. \\nIn Exhibit 23, we give examples of nonparametric alternatives to the parametric, \\nt-distributed tests concerning means.\\n13\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Parametric vs. Nonparametric Tests',\n",
       "     'page_number': 402,\n",
       "     'content': 'PARAMETRIC VS. NONPARAMETRIC TESTS\\ncompare and contrast parametric and nonparametric tests, and \\ndescribe situations where each is the more appropriate type of test\\nThe hypothesis-testing procedures we have discussed up to this point have two char-\\nacteristics in common. First, they are concerned with parameters, and second, their \\nvalidity depends on a definite set of assumptions. Mean and variance, for example, \\nare two parameters, or defining quantities, of a normal distribution. The tests also \\nmake specific assumptions—in particular, assumptions about the distribution of the \\npopulation producing the sample. Any test or procedure with either of these two \\ncharacteristics is a parametric test or procedure. In some cases, however, we are \\nconcerned about quantities other than parameters of distributions. In other cases, \\nwe may believe that the assumptions of parametric tests do not hold. In cases where \\nwe are examining quantities other than population parameters or where assumptions \\nof the parameters are not satisfied, a nonparametric test or procedure can be useful.\\nA nonparametric test is a test that is not concerned with a parameter or a test \\nthat makes minimal assumptions about the population from which the sample comes. \\nIn Exhibit 23, we give examples of nonparametric alternatives to the parametric, \\nt-distributed tests concerning means.\\n13\\n© CFA Institute. For candidate use only. Not for distribution.\\nParametric vs. Nonparametric Tests\\n393\\nExhibit 23: Nonparametric Alternatives to Parametric Tests Concerning \\nMeans\\n\\xa0\\nParametric\\nNonparametric\\nTests concerning a single mean\\nt-distributed test \\nz-distributed test\\nWilcoxon signed-rank test\\nTests concerning differences \\nbetween means\\nt-distributed test\\nMann–Whitney U test \\n(Wilcoxon rank sum test)\\nTests concerning mean differences \\n(paired comparisons tests)\\nt-distributed test\\nWilcoxon signed-rank test \\nSign test\\n',\n",
       "     'children': [{'title': 'Uses of Nonparametric Tests',\n",
       "       'page_number': 403,\n",
       "       'content': 'Parametric vs. Nonparametric Tests\\n393\\nExhibit 23: Nonparametric Alternatives to Parametric Tests Concerning \\nMeans\\n\\xa0\\nParametric\\nNonparametric\\nTests concerning a single mean\\nt-distributed test \\nz-distributed test\\nWilcoxon signed-rank test\\nTests concerning differences \\nbetween means\\nt-distributed test\\nMann–Whitney U test \\n(Wilcoxon rank sum test)\\nTests concerning mean differences \\n(paired comparisons tests)\\nt-distributed test\\nWilcoxon signed-rank test \\nSign test\\nUses of Nonparametric Tests\\nWe primarily use nonparametric procedures in four situations: (1) when the data we \\nuse do not meet distributional assumptions, (2) when there are outliers, (3) when \\nthe data are given in ranks or use an ordinal scale, or (4) when the hypotheses we are \\naddressing do not concern a parameter.\\nThe first situation occurs when the data available for analysis suggest that the \\ndistributional assumptions of the parametric test are not satisfied. For example, we \\nmay want to test a hypothesis concerning the mean of a population but believe that \\nneither t- nor z-distributed tests are appropriate because the sample is small and may \\ncome from a markedly non-normally distributed population. In that case, we may use \\na nonparametric test. The nonparametric test will frequently involve the conversion \\nof observations (or a function of observations) into ranks according to magnitude, \\nand sometimes it will involve working with only “greater than” or “less than” relation-\\nships (using the + and − signs to denote those relationships). Characteristically, one \\nmust refer to specialized statistical tables to determine the rejection points of the \\ntest statistic, at least for small samples. Such tests, then, typically interpret the null \\nhypothesis as a hypothesis about ranks or signs.\\nSecond, whereas the underlying distribution of the population may be normal, \\nthere may be extreme values or outliers that influence the parametric statistics but \\nnot the nonparametric statistics. For example, we may want to use a nonparametric \\ntest of the median, in the case of outliers, instead of a test of the mean.\\nThird, we may have a sample in which observations are ranked. In those cases, we \\nalso use nonparametric tests because parametric tests generally require a stronger \\nmeasurement scale than ranks. For example, if our data were the rankings of invest-\\nment managers, we would use nonparametric procedures to test the hypotheses \\nconcerning those rankings.\\nA fourth situation in which we use nonparametric procedures occurs when our \\nquestion does not concern a parameter. For example, if the question concerns whether \\na sample is random or not, we use the appropriate nonparametric test (a “runs test”). \\nThe nonparametric runs test is used to test whether stock price changes can be used \\nto forecast future stock price changes—in other words, a test of the random-walk \\ntheory. Another type of question that nonparametric methods can address is whether \\na sample came from a population following a particular probability distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Nonparametric Inference: Summary',\n",
       "       'page_number': 403,\n",
       "       'content': 'Nonparametric Inference: Summary\\nNonparametric statistical procedures extend the reach of inference because they \\nmake few assumptions, can be used on ranked data, and may address questions \\nunrelated to parameters. Quite frequently, nonparametric tests are reported alongside \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n394\\nparametric tests; the user can then assess how sensitive the statistical conclusion is to \\nthe assumptions underlying the parametric test. However, if the assumptions of the \\nparametric test are met, the parametric test (where available) is generally preferred \\nover the nonparametric test because the parametric test may have more power—that \\nis, a greater ability to reject a false null hypothesis.\\nEXAMPLE 15\\nThe Use of Nonparametric Tests\\n1. A nonparametric test is most appropriate when the:\\nA. data consist of ranked values.\\nB. validity of the test depends on many assumptions.\\nC. sample sizes are large but are drawn from a population that may be \\nnon-normal.\\nSolution\\nA is correct. When the samples consist of ranked values, parametric tests \\nare not appropriate. In such cases, nonparametric tests are most appropri-\\nate.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Tests Concerning Correlation',\n",
       "     'page_number': 404,\n",
       "     'content': 'TESTS CONCERNING CORRELATION\\nexplain parametric and nonparametric tests of the hypothesis that \\nthe population correlation coefficient equals zero, and determine \\nwhether the hypothesis is rejected at a given level of significance\\nIn many contexts in investments, we want to assess the strength of the linear rela-\\ntionship between two variables; that is, we want to evaluate the correlation between \\nthem. A significance test of a correlation coefficient allows us to assess whether the \\nrelationship between two random variables is the result of chance. If we decide that \\nthe relationship does not result from chance, then we are inclined to use this infor-\\nmation in modeling or forecasting.\\nIf the correlation between two variables is zero, we conclude that there is no linear \\nrelation between the two variables. We use a test of significance to assess whether the \\ncorrelation is different from zero. After we estimate a correlation coefficient, we need \\nto ask whether the estimated correlation is significantly different from zero.\\nA correlation may be positive (that is, the two variables tend to move in the same \\ndirection at the same time) or negative (that is, the two variables tend to move in dif-\\nferent directions at the same time). The correlation coefficient is a number between \\n−1 and +1, where −1 denotes a perfect negative or inverse, straight-line relationship \\nbetween the two variables; +1 denotes a perfect positive, straight-line relationship; \\nand 0 represents the absence of any straight-line relationship (that is, no correlation).\\nThe most common hypotheses concerning correlation occur when comparing the \\npopulation correlation coefficient with zero because we are often asking whether there \\nis a relationship, which implies a null of the correlation coefficient equal to zero (that \\n14\\n© CFA Institute. For candidate use only. Not for distribution.\\nTests Concerning Correlation\\n395\\nis, no relationship). Hypotheses concerning the population correlation coefficient may \\nbe two or one sided, as we have seen in other tests. Let ρ represent the population \\ncorrelation coefficient. The possible hypotheses are as follows:\\nTwo sided: H0: ρ = 0 versus Ha: ρ ≠ 0\\nOne sided (right side): H0: ρ ≤ 0 versus Ha: ρ > 0\\nOne sided (left side): H0: ρ ≥ 0 versus Ha: ρ < 0\\nWe use the sample correlation to test these hypotheses on the population correlation.\\n',\n",
       "     'children': [{'title': 'Parametric Test of a Correlation',\n",
       "       'page_number': 405,\n",
       "       'content': 'Parametric Test of a Correlation\\nThe parametric pairwise correlation coefficient is often referred to as the Pearson \\ncorrelation, the bivariate correlation, or simply the correlation. Our focus is on \\nthe testing of the correlation and not the actual calculation of this statistic, but it \\nhelps distinguish this correlation from the nonparametric correlation if we look at \\nthe formula for the sample correlation. Consider two variables, X and Y. The sample \\ncorrelation, rXY, is\\n r XY =  \\n s XY  \\n_ \\n s X  s Y   , \\nwhere sXY is the sample covariance between the X and Y variables, sX is the standard \\ndeviation of the X variable, and sY is the standard deviation of the Y variable. We often \\ndrop the subscript to represent the correlation as simply r.\\nTherefore, you can see from this formula that each observation is compared with \\nits respective variable mean and that, because of the covariance, it matters how much \\neach observation differs from its respective variable mean. Note that the covariance \\ndrives the sign of the correlation.\\nIf the two variables are normally distributed, we can test to determine whether \\nthe null hypothesis (H0: ρ = 0) should be rejected using the sample correlation, r. The \\nformula for the t-test is\\n t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    . \\n(10)\\nThis test statistic is t-distributed with n − 2 degrees of freedom. One practical \\nobservation concerning Equation 10 is that the magnitude of r needed to reject the \\nnull hypothesis decreases as sample size n increases, for two reasons. First, as n \\nincreases, the number of degrees of freedom increases and the absolute value of the \\ncritical value of the t-statistic decreases. Second, the absolute value of the numerator \\nincreases with larger n, resulting in a larger magnitude of the calculated t-statistic. \\nFor example, with sample size n = 12, r = 0.35 results in a t-statistic of 1.182, which \\nis not different from zero at the 0.05 level (tα/2 = ±2.228). With a sample size of n = \\n32, the same sample correlation, r = 0.35, yields a t-statistic of 2.046, which is just \\nsignificant at the 0.05 level (t α/2 = ±2.042).\\nAnother way to make this point is that when sampling from the same population, \\na false null hypothesis is more likely to be rejected (that is, the power of the test \\nincreases) as we increase the sample size, all else equal, because a higher number of \\nobservations increases the numerator of the test statistic. We show this in Exhibit 24 \\nfor three different sample correlation coefficients, with the corresponding calculated \\nt-statistics and significance at the 5% level for a two-sided alternative hypothesis. As \\nthe sample size increases, significance is more likely to be indicated, but the rate of \\nachieving this significance depends on the sample correlation coefficient; the higher \\nthe sample correlation, the faster significance is achieved when increasing the sam-\\nple size. As the sample sizes increase as ever-larger datasets are examined, the null \\nhypothesis is almost always rejected and other tools of data analysis must be applied.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n396\\nExhibit 24: Calculated Test Statistics for Different Sample Sizes and Sample \\nCorrelations with a 5% Level of Significance\\nCalculated t-Statistic\\n8\\n6\\n7\\n45\\n3\\n1\\n2\\n0\\n3\\n12\\n12\\n21\\n21\\n30\\n30\\n39\\n39\\n48\\n48\\n57\\n57\\n66\\n66\\n75\\n75\\n84\\n84\\n93\\n93\\nSample Size\\nt-Statistic for Correlation = 0.2\\nt-Statistic for Correlation = 0.4\\nt-Statistic for Correlation = 0.6\\nSignificant Correlation = 0.2\\nSignificant Correlation = 0.4\\nSignificant Correlation = 0.6\\nEXAMPLE 16\\nExamining the Relationship between Returns on \\nInvestment One and Investment Two\\n1. An analyst is examining the annual returns for Investment One and Invest-\\nment Two, as displayed in Exhibit 1. Although this time series plot provides \\nsome useful information, the analyst is most interested in quantifying how \\nthe returns of these two series are related, so she calculates the correlation \\ncoefficient, equal to 0.43051, between these series.\\nIs there a significant positive correlation between these two return series if \\nshe uses a 1% level of significance?\\nSolution\\n \\nStep 1\\nState the hypotheses.\\n H 0 : ρ ≤ 0\\xa0versus\\xa0  H a : ρ > 0 \\nStep 2\\nIdentify the appropriate test statistic.\\n t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2     \\nStep 3\\nSpecify the level of significance.\\n1%\\nStep 4\\nState the decision rule\\nWith 33 − 2 = 31 degrees of freedom and a one-sided test with a 1% level \\nof significance, the critical value is 2.45282. \\nWe reject the null hypothesis if the calculated t-statistic is greater than \\n2.45282.\\nStep 5\\nCalculate the test statistic.\\n t =  0.43051  √ _ \\n 33 − 2   \\n____________ \\n \\n √ \\n___________ \\n1 − 0.18534    = 2.65568 \\nStep 6\\nMake a decision\\nReject the null hypothesis since the calculated t-statistic is greater than \\n2.45282. There is sufficient evidence to reject the H0 in favor of Ha, that \\nthe correlation between the annual returns of these two investments is \\npositive.\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nTests Concerning Correlation\\n397\\nTests Concerning Correlation: The Spearman Rank Correlation \\nCoefficient\\nWhen we believe that the population under consideration meaningfully departs from \\nnormality, we can use a test based on the Spearman rank correlation coefficient, \\nrS. The Spearman rank correlation coefficient is essentially equivalent to the usual \\ncorrelation coefficient but is calculated on the ranks of the two variables (say, X and \\nY) within their respective samples. The calculation of rS requires the following steps:\\n1. Rank the observations on X from largest to smallest. Assign the number 1 to \\nthe observation with the largest value, the number 2 to the observation with \\nsecond largest value, and so on. In case of ties, assign to each tied obser-\\nvation the average of the ranks that they jointly occupy. For example, if the \\nthird and fourth largest values are tied, we assign both observations the rank \\nof 3.5 (the average of 3 and 4). Perform the same procedure for the observa-\\ntions on Y.\\n2. Calculate the difference, di, between the ranks for each pair of observations \\non X and Y, and then calculate di2(the squared difference in ranks).\\n3. With n as the sample size, the Spearman rank correlation is given by\\n r s = 1 −  \\n6 ∑ i=1  \\nn   d i 2   \\n_ \\nn( n 2 − 1)  . \\n(11)\\nSuppose an analyst is examining the relationship between returns for two invest-\\nment funds, A and B, of similar risk over 35 years. She is concerned that the assumptions \\nfor the parametric correlation may not be met, so she decides to test Spearman rank \\ncorrelations. Her hypotheses are H0: rS = 0 and Ha: rS ≠ 0. She gathers the returns, \\nranks the returns for each fund, and calculates the difference in ranks and the squared \\ndifferences. A partial table is provided in Exhibit 25.\\nExhibit 25: Differences and Squared Differences in Ranks for Fund A and \\nFund B over 35 Years\\nYear\\nFund A\\nFund B\\nRank of A\\nRank of B\\nd\\nd2\\n1\\n2.453\\n1.382\\n27\\n31\\n−4\\n16\\n2\\n3.017\\n3.110\\n24\\n24\\n0\\n0\\n3\\n4.495\\n6.587\\n19\\n7\\n12\\n144\\n4\\n3.627\\n3.300\\n23\\n23\\n0\\n0\\n.. . \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n30\\n2.269\\n0.025\\n28\\n35\\n−7\\n49\\n31\\n6.354\\n4.428\\n10\\n19\\n−9\\n81\\n32\\n6.793\\n4.165\\n8\\n20\\n−12\\n144\\n33\\n7.300\\n7.623\\n5\\n5\\n0\\n0\\n34\\n6.266\\n4.527\\n11\\n18\\n−7\\n49\\n35\\n1.257\\n4.704\\n34\\n16\\n18\\n324\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSum =\\n2,202\\nThe Spearman rank correlation is:\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Tests Concerning Correlation: The Spearman Rank Correlation Coefficient',\n",
       "       'page_number': 407,\n",
       "       'content': 'Tests Concerning Correlation\\n397\\nTests Concerning Correlation: The Spearman Rank Correlation \\nCoefficient\\nWhen we believe that the population under consideration meaningfully departs from \\nnormality, we can use a test based on the Spearman rank correlation coefficient, \\nrS. The Spearman rank correlation coefficient is essentially equivalent to the usual \\ncorrelation coefficient but is calculated on the ranks of the two variables (say, X and \\nY) within their respective samples. The calculation of rS requires the following steps:\\n1. Rank the observations on X from largest to smallest. Assign the number 1 to \\nthe observation with the largest value, the number 2 to the observation with \\nsecond largest value, and so on. In case of ties, assign to each tied obser-\\nvation the average of the ranks that they jointly occupy. For example, if the \\nthird and fourth largest values are tied, we assign both observations the rank \\nof 3.5 (the average of 3 and 4). Perform the same procedure for the observa-\\ntions on Y.\\n2. Calculate the difference, di, between the ranks for each pair of observations \\non X and Y, and then calculate di2(the squared difference in ranks).\\n3. With n as the sample size, the Spearman rank correlation is given by\\n r s = 1 −  \\n6 ∑ i=1  \\nn   d i 2   \\n_ \\nn( n 2 − 1)  . \\n(11)\\nSuppose an analyst is examining the relationship between returns for two invest-\\nment funds, A and B, of similar risk over 35 years. She is concerned that the assumptions \\nfor the parametric correlation may not be met, so she decides to test Spearman rank \\ncorrelations. Her hypotheses are H0: rS = 0 and Ha: rS ≠ 0. She gathers the returns, \\nranks the returns for each fund, and calculates the difference in ranks and the squared \\ndifferences. A partial table is provided in Exhibit 25.\\nExhibit 25: Differences and Squared Differences in Ranks for Fund A and \\nFund B over 35 Years\\nYear\\nFund A\\nFund B\\nRank of A\\nRank of B\\nd\\nd2\\n1\\n2.453\\n1.382\\n27\\n31\\n−4\\n16\\n2\\n3.017\\n3.110\\n24\\n24\\n0\\n0\\n3\\n4.495\\n6.587\\n19\\n7\\n12\\n144\\n4\\n3.627\\n3.300\\n23\\n23\\n0\\n0\\n.. . \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n30\\n2.269\\n0.025\\n28\\n35\\n−7\\n49\\n31\\n6.354\\n4.428\\n10\\n19\\n−9\\n81\\n32\\n6.793\\n4.165\\n8\\n20\\n−12\\n144\\n33\\n7.300\\n7.623\\n5\\n5\\n0\\n0\\n34\\n6.266\\n4.527\\n11\\n18\\n−7\\n49\\n35\\n1.257\\n4.704\\n34\\n16\\n18\\n324\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSum =\\n2,202\\nThe Spearman rank correlation is:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n398\\n r s = 1 −  \\n6 ∑ i=1  \\nn   d i 2   \\n_ \\nn( n 2 − 1)  = 1 −  \\n6(2, 202) \\n_ \\n35(1, 225 − 1)  = 0.6916. \\nThe test of hypothesis for the Spearman rank correlation depends on whether the \\nsample is small or large (n > 30). For small samples, the researcher requires a spe-\\ncialized table of critical values, but for large samples, we can conduct a t-test using \\nthe test statistic in Equation 10, which is t-distributed with n − 2 degrees of freedom.\\nIn this example, for a two-tailed test with a 5% significance level, the critical values \\nfor n − 2 = 35 − 2 = 33 degrees of freedom are ±2.0345. For the sample information \\nin Exhibit 24, the calculated test statistic is\\n t =  0.6916  √ _ \\n33   \\n___________ \\n \\n √ \\n____________ \\n \\n1 − ( 0.6916 2 )    = 5.5005. \\nAccordingly, we reject the null hypothesis (H0: rS = 0), concluding that there is \\nsufficient evidence to indicate that the correlation between the returns of Fund A and \\nFund B is different from zero.\\nEXAMPLE 17\\nTesting the Exchange Rate Correlation\\n1. An analyst gathers exchange rate data for five currencies relative to the \\nUS dollar. Upon inspection of the distribution of these exchange rates, she \\nobserves a departure from normality, especially with negative skewness for \\nfour of the series and positive skewness for the fifth. Therefore, she decides \\nto examine the relationships among these currencies using Spearman rank \\ncorrelations. She calculates these correlations between the currencies over \\n180 days, which are shown in the correlogram in Exhibit 26. In this correlo-\\ngram, the lower triangle reports the pairwise correlations and the upper \\ntriangle provides a visualization of the magnitude of the correlations, with \\nlarger circles indicating larger absolute value of the correlations and darker \\ncircles indicating correlations that are negative.\\n \\nExhibit 26: Spearman Rank Correlations between Exchanges Rates \\nRelative to the US Dollar\\n \\n0.9124\\n0.6079\\n0.6816\\n–0.1973\\n–0.2654\\n0.7047\\n0.5654\\n0.3691\\n0.4889\\n–0.2046\\n1.0\\n1.0\\n0.8\\n0.8\\n0.6\\n0.6\\n0.4\\n0.4\\n0.2\\n0.2\\n0\\n–0.2\\n–0.2\\n–0.4\\n–0.4\\n–0.6\\n–0.6\\n–0.8\\n–0.8\\n–1.0\\n–1.0\\nAUD\\nCAD\\nEUR\\nGBP\\nJPY\\nFor any of these pairwise Spearman rank correlations, can we reject the null \\nhypothesis of no correlation (H0: rS = 0 and Ha: rS ≠ 0) at the 5% level of \\nsignificance?\\n© CFA Institute. For candidate use only. Not for distribution.\\nTest of Independence Using Contingency Table Data\\n399\\nSolution\\nThe critical t-values for 2.5% in each tail of the distribution are ±1.97338.\\nThere are five exchange rates, so there are 5C2, or 10, unique correlation \\npairs. Therefore, we need to calculate 10 t-statistics. For example, the \\ncorrelation between EUR/USD and AUD/USD is 0.6079. The calculated \\nt-statistic is  0.6079  √ _ \\n 180 − 2   \\n____________ \\n √ \\n___________ \\n1 −  0.6079 2    =  8.11040 \\n_ \\n0.79401  = 10.2144 . Repeating this t-statistic \\ncalculation for each pair of exchange rates yields the test statistics shown in \\nExhibit 27.\\n \\nExhibit 27: Calculated Test Statistics for Test of Spearman Rank \\nCorrelations\\n \\n \\n\\xa0\\nAUD/USD\\nCAD/USD\\nEUR/USD\\nGBP/USD\\nCAD/USD\\n29.7409\\n\\xa0\\n\\xa0\\n\\xa0\\nEUR/USD\\n10.2144\\n9.1455\\n\\xa0\\n\\xa0\\nGBP/USD\\n12.4277\\n13.2513\\n7.4773\\n\\xa0\\nJPY/USD\\n−2.6851\\n−3.6726\\n5.2985\\n−2.7887\\n \\nThe analyst should reject all 10 null hypotheses, because the calculated \\nt-statistics for all exchange rate pairs fall outside the bounds of the two crit-\\nical values. She should conclude that all the exchange rate pair correlations \\nare different from zero at the 5% level.\\nTEST OF INDEPENDENCE USING CONTINGENCY \\nTABLE DATA\\nexplain tests of independence based on contingency table data\\nWhen faced with categorical or discrete data, we cannot use the methods that we have \\ndiscussed up to this point to test whether the classifications of such data are indepen-\\ndent. Suppose we observe the following frequency table of 1,594 exchange-traded \\nfunds (ETFs) based on two classifications: size (that is, market capitalization) and \\ninvestment type (value, growth, or blend), as shown in Exhibit 28. The classification of \\nthe investment type is discrete, so we cannot use correlation to assess the relationship \\nbetween size and investment type.\\n15\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Test of Independence Using Contingency Table Data',\n",
       "     'page_number': 409,\n",
       "     'content': 'Test of Independence Using Contingency Table Data\\n399\\nSolution\\nThe critical t-values for 2.5% in each tail of the distribution are ±1.97338.\\nThere are five exchange rates, so there are 5C2, or 10, unique correlation \\npairs. Therefore, we need to calculate 10 t-statistics. For example, the \\ncorrelation between EUR/USD and AUD/USD is 0.6079. The calculated \\nt-statistic is  0.6079  √ _ \\n 180 − 2   \\n____________ \\n √ \\n___________ \\n1 −  0.6079 2    =  8.11040 \\n_ \\n0.79401  = 10.2144 . Repeating this t-statistic \\ncalculation for each pair of exchange rates yields the test statistics shown in \\nExhibit 27.\\n \\nExhibit 27: Calculated Test Statistics for Test of Spearman Rank \\nCorrelations\\n \\n \\n\\xa0\\nAUD/USD\\nCAD/USD\\nEUR/USD\\nGBP/USD\\nCAD/USD\\n29.7409\\n\\xa0\\n\\xa0\\n\\xa0\\nEUR/USD\\n10.2144\\n9.1455\\n\\xa0\\n\\xa0\\nGBP/USD\\n12.4277\\n13.2513\\n7.4773\\n\\xa0\\nJPY/USD\\n−2.6851\\n−3.6726\\n5.2985\\n−2.7887\\n \\nThe analyst should reject all 10 null hypotheses, because the calculated \\nt-statistics for all exchange rate pairs fall outside the bounds of the two crit-\\nical values. She should conclude that all the exchange rate pair correlations \\nare different from zero at the 5% level.\\nTEST OF INDEPENDENCE USING CONTINGENCY \\nTABLE DATA\\nexplain tests of independence based on contingency table data\\nWhen faced with categorical or discrete data, we cannot use the methods that we have \\ndiscussed up to this point to test whether the classifications of such data are indepen-\\ndent. Suppose we observe the following frequency table of 1,594 exchange-traded \\nfunds (ETFs) based on two classifications: size (that is, market capitalization) and \\ninvestment type (value, growth, or blend), as shown in Exhibit 28. The classification of \\nthe investment type is discrete, so we cannot use correlation to assess the relationship \\nbetween size and investment type.\\n15\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n400\\nExhibit 28: Size and Investment Type Classifications of 1,594 ETFs\\n\\xa0\\nSize Based on Market Capitalization\\n\\xa0\\nInvestment Type\\nSmall\\nMedium\\nLarge\\nTotal\\nValue\\n50\\n110\\n343\\n503\\nGrowth\\n42\\n122\\n202\\n366\\nBlend\\n56\\n149\\n520\\n725\\nTotal\\n148\\n381\\n1,065\\n1,594\\nThis table is referred to as a contingency table or a two-way table (because there \\nare two classifications, or classes—size and investment type).\\nIf we want to test whether there is a relationship between the size and investment \\ntype, we can perform a test of independence using a nonparametric test statistic that \\nis chi-square distributed:\\n χ 2 =  ∑ i=1  \\nm   \\n ( O ij −  E ij ) \\n_   2\\n E ij  \\n  , \\n(12)\\nwhere\\n \\nm = the number of cells in the table, which is the number of groups in the \\nfirst class multiplied by the number of groups in the second class\\n \\nOij = the number of observations in each cell of row i and column j (i.e., \\nobserved frequency)\\n \\nEij = the expected number of observations in each cell of row i and column j, \\nassuming independence (i.e., expected frequency)\\nThis test statistic has (r − 1)(c − 1) degrees of freedom, where r is the number of rows \\nand c is the number of columns.\\nIn Exhibit 28, size class has three groups (small, medium, and large) and invest-\\nment type class has three groups (value, growth, and blend), so m is 9 (= 3 × 3). The \\nnumber of ETFs in each cell (Oij), the observed frequency, is given, so to calculate \\nthe chi-square test statistic, we need to estimate Eij, the expected frequency, which \\nis the number of ETFs we would expect to be in each cell if size and investment type \\nare completely independent. The expected number of ETFs (Eij) is calculated using\\n E ij =  \\n(Total row i ) × (Total column j) \\n \\n \\n______________________ \\nOverall total  \\n . \\n(13)\\nConsider one combination of size and investment type, small-cap value:\\n E ij =  503 × 148 \\n_ \\n1,594  = 46.703. \\nWe repeat this calculation for each combination of size and investment type (i.e., m \\n= 9 pairs) to arrive at the expected frequencies, shown in Panel A of Exhibit 29.\\nNext, we calculate    \\n  (   O  ij   −  E  ij   )     \\n_ 2  \\n E  ij   \\n  , the squared difference between observed and expected \\nfrequencies scaled by expected frequency, for each cell as shown in Panel B of Exhibit \\n29. Finally, by summing the values of    \\n  (   O  ij   −  E  ij   )     \\n_ 2  \\n E  ij   \\n   for each of the m cells, we calculate the \\nchi-square statistic as 32.08025.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTest of Independence Using Contingency Table Data\\n401\\nExhibit 29: Inputs to Chi-Square Test Statistic Calculation for 1,594 ETFs \\nAssuming Independence of Size and Investment Type\\nA. Expected Frequency of ETFs by Size and Investment Type\\n\\xa0\\nSize Based on Market Capitalization\\nInvestment Type\\nSmall\\nMedium\\nLarge\\nValue\\n46.703\\n120.228\\n336.070\\nGrowth\\n33.982\\n87.482\\n244.536\\nBlend\\n67.315\\n173.290\\n484.395\\nTotal\\n148.000\\n381.000\\n1,065.000\\nB. Scaled Squared Deviation for Each Combination of Size and Investment Type\\n\\xa0\\nSize Based on Market Capitalization\\nInvestment Type\\nSmall\\nMedium\\nLarge\\nValue\\n0.233\\n0.870\\n0.143\\nGrowth\\n1.892\\n13.620\\n7.399\\nBlend\\n1.902\\n3.405\\n2.617\\nIn our ETF example, we test the null hypothesis of independence between the two \\nclasses (i.e., no relationship between size and investment type) versus the alternative \\nhypothesis of dependence (i.e., a relationship between size and investment type) using \\na 5% level of significance, as shown in Exhibit 30. If, on the one hand, the observed \\nvalues are equal to the expected values, the calculated test statistic would be zero. If, \\non the other hand, there are differences between the observed and expected values, \\nthese differences are squared, so the calculated chi-square statistic will be positive. \\nTherefore, for the test of independence using a contingency table, there is only one \\nrejection region, on the right side.\\nExhibit 30: Test of Independence of Size and Investment Type for 1,594 ETFs\\nStep 1\\nState the hypotheses.\\nH0: ETF size and investment type are not related, so these classifications \\nare independent;  \\nHa : ETF size and investment type are related, so these classifications are \\nnot independent. \\nStep 2\\nIdentify the appropriate test statistic.\\n χ 2 =  ∑ \\ni=1\\n  \\nm\\n   \\n ( O ij −  E ij ) \\n_ 2  \\n E ij   \\n  \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith (3 − 1) × (3 − 1) = 4 degrees of freedom and a one-sided test with a \\n5% level of significance, the critical value is 9.4877.\\nWe reject the null hypothesis if the calculated χ2 statistic is greater than \\n9.4877.\\nExcel\\nCHISQ.INV(0.95,4)\\nR \\nqchisq(.95,4)\\nPython\\nfrom scipy.stats import chi2\\nchi2.ppf(.95,4)\\nStep 5\\nCalculate the test statistic.\\nχ2 = 32.08025\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n402\\nStep 6\\nMake a decision.\\nReject the null hypothesis of independence because the calculated χ2 test \\nstatistic is greater than 9.4877. There is sufficient evidence to conclude \\nthat ETF size and investment type are related (i.e., not independent).\\nWe can visualize the contingency table in a graphic referred to as a mosaic. In a \\nmosaic, a grid reflects the comparison between the observed and expected frequencies. \\nConsider Exhibit 31, which represents the ETF contingency table.\\nExhibit 31: Mosaic of the ETF Contingency Table\\nSmall\\nValue\\nGrowth\\nInvestment Type\\nStandardized\\nResiduals:\\nBlend\\nMedium\\nLarge\\nSize\\n>4\\n2 to 4\\n–4- to –2–2 to 00 to 2\\n<–4\\nThe width of the rectangles in Exhibit 31 reflect the proportion of ETFs that are small, \\nmedium, and large, whereas the height reflects the proportion that are value, growth, \\nand blend. The darker shading indicates whether there are more observations than \\nexpected under the null hypothesis of independence, whereas the lighter shading \\nindicates that there are fewer observations than expected, with “more” and “fewer” \\ndetermined by reference to the standardized residual boxes. The standardized residual, \\nalso referred to as a Pearson residual, is\\n Standardized residual =  \\n O ij −  E ij  \\n_ \\n √ _ \\n E ij    . \\n(14)\\nThe interpretation for this ETF example is that there are more medium-size growth \\nETFs (standardized residual of 3.69) and fewer large-size growth ETFs (standardized \\nresidual of −2.72) than would be expected if size and investment type were independent.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTest of Independence Using Contingency Table Data\\n403\\nEXAMPLE 18\\nUsing Contingency Tables to Test for Independence\\nConsider the contingency table in Exhibit 32, which classifies 500 randomly \\nselected companies on the basis of two environmental, social, and governance \\n(ESG) rating dimensions: environmental rating and governance rating.\\n \\nExhibit 32: Classification of 500 Randomly Selected Companies \\nBased on Environmental and Governance Ratings\\n \\n \\n\\xa0\\nGovernance Rating\\n\\xa0\\nEnvironmental Rating\\nProgressive\\nAverage\\nPoor\\nTotal\\nProgressive\\n35\\n40\\n5\\n80\\nAverage\\n80\\n130\\n50\\n260\\nPoor\\n40\\n60\\n60\\n160\\nTotal\\n155\\n230\\n115\\n500\\n \\n1. What are the expected frequencies for these two ESG rating dimensions if \\nthese categories are independent?\\nSolution to 1\\nThe expected frequencies based on independence of the governance rating \\nand the environmental rating are shown in Panel A of Exhibit 33. For ex-\\nample, using Equation 12, the expected frequency for the combination of \\nprogressive governance and progressive environmental ratings is\\n E ij =  155 × 80 \\n_ \\n500  = 24.80. \\n \\nExhibit 33: Inputs to Chi-Square Test Statistic Calculation Assuming \\nIndependence of Environmental and Governance Ratings\\n \\n \\nA. Expected Frequencies of Environmental and Governance Ratings Assuming \\nIndependence\\n\\xa0\\nGovernance Rating\\nEnvironmental Rating\\nProgressive\\nAverage\\nPoor\\nProgressive\\n24.8\\n36.8\\n18.4\\nAverage\\n80.6\\n119.6\\n59.8\\nPoor\\n49.6\\n73.6\\n36.8\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n404\\n \\n \\nB. Scaled Squared Deviation for Each Combination of Environmental and \\nGovernance Ratings\\n\\xa0\\nGovernance Rating\\nEnvironmental Rating\\nProgressive\\nAverage\\nPoor\\nProgressive\\n4.195\\n0.278\\n9.759\\nAverage\\n0.004\\n0.904\\n1.606\\nPoor\\n1.858\\n2.513\\n14.626\\n \\n2. Using a 5% level of significance, determine whether these two ESG rating \\ndimensions are independent of one another.\\nSolution to 2\\n \\nStep 1\\nState the hypotheses.\\nH0: Governance and environmental ratings are not related, so these rat-\\nings are independent; \\nHa: Governance and environmental ratings are related, so these ratings \\nare not independent. \\nStep 2\\nIdentify the appropriate test statistic.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  χ 2 =  ∑ \\ni=1\\n  \\nm\\n   \\n ( O ij −  E ij ) \\n_ 2  \\n E ij   \\n  \\nStep 3\\nSpecify the level of significance.\\n5%\\nStep 4\\nState the decision rule.\\nWith (3 − 1) × (3 − 1) = 4 degrees of freedom and a one-sided test with a \\n5% level of significance, the critical value is 9.487729.\\nWe reject the null hypothesis if the calculated χ2 statistic is greater than \\n9.487729.\\nExcel CHISQ.INV(0.95,4)\\nR qchisq(.95,4)\\nPython from scipy.stats import chi2\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0chi2.ppf(.95,4)\\nStep 5\\nCalculate the test statistic.\\nχ2 = 35.74415\\nTo calculate the test statistic, we first calculate the squared difference \\nbetween observed and expected frequencies scaled by expected fre-\\nquency for each cell, as shown in Panel B of Exhibit 33. Then, summing \\nthe values in each of the m cells (see Equation 11), we calculate the \\nchi-square statistic as 35.74415.\\nStep 6\\nMake a decision.\\nReject the null hypothesis because the calculated χ2 test statistic is greater \\nthan 9.487729. There is sufficient evidence to indicate that the environ-\\nmental and governance ratings are related, so they are not independent.\\n \\n',\n",
       "     'children': []},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 414,\n",
       "     'content': 'SUMMARY\\nIn this reading, we have presented the concepts and methods of statistical inference \\nand hypothesis testing.\\n■ \\nA hypothesis is a statement about one or more populations.\\n■ \\nThe steps in testing a hypothesis are as follows:\\n© CFA Institute. For candidate use only. Not for distribution.\\nTest of Independence Using Contingency Table Data\\n405\\n1. State the hypotheses.\\n2. Identify the appropriate test statistic and its probability distribution.\\n3. Specify the significance level.\\n4. State the decision rule.\\n5. Collect the data and calculate the test statistic.\\n6. Make a decision.\\n■ \\nWe state two hypotheses: The null hypothesis is the hypothesis to be tested; \\nthe alternative hypothesis is the hypothesis accepted if the null hypothesis is \\nrejected.\\n■ \\nThere are three ways to formulate hypotheses. Let θ indicate the population \\nparameters:\\n1. Two-sided alternative: H0: θ = θ0 versus Ha: θ ≠ θ0\\n2. One-sided alternative (right side): H0: θ ≤ θ0 versus Ha: θ > θ0\\n3. One-sided alternative (left side): H0: θ ≥ θ0 versus Ha: θ < θ0\\nwhere θ0 is a hypothesized value of the population parameter and θ is the \\ntrue value of the population parameter.\\n■ \\nWhen we have a “suspected” or “hoped for” condition for which we want to \\nfind supportive evidence, we frequently set up that condition as the alter-\\nnative hypothesis and use a one-sided test. However, the researcher may \\nselect a “not equal to” alternative hypothesis and conduct a two-sided test to \\nemphasize a neutral attitude.\\n■ \\nA test statistic is a quantity, calculated using a sample, whose value is the \\nbasis for deciding whether to reject or not reject the null hypothesis. We \\ncompare the computed value of the test statistic to a critical value for \\nthe same test statistic to decide whether to reject or not reject the null \\nhypothesis.\\n■ \\nIn reaching a statistical decision, we can make two possible errors: We may \\nreject a true null hypothesis (a Type I error, or false positive), or we may fail \\nto reject a false null hypothesis (a Type II error, or false negative).\\n■ \\nThe level of significance of a test is the probability of a Type I error that we \\naccept in conducting a hypothesis test. The standard approach to hypothesis \\ntesting involves specifying only a level of significance (that is, the probability \\nof a Type I error). The complement of the level of significance is the confi-\\ndence level.\\n■ \\nThe power of a test is the probability of correctly rejecting the null (reject-\\ning the null when it is false). The complement of the power of the test is the \\nprobability of a Type II error.\\n■ \\nA decision rule consists of determining the critical values with which to \\ncompare the test statistic to decide whether to reject or not reject the null \\nhypothesis. When we reject the null hypothesis, the result is said to be sta-\\ntistically significant.\\n■ \\nThe (1 − α) confidence interval represents the range of values of the test \\nstatistic for which the null hypothesis is not be rejected.\\n■ \\nThe statistical decision consists of rejecting or not rejecting the null hypoth-\\nesis. The economic decision takes into consideration all economic issues \\npertinent to the decision.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n406\\n■ \\nThe p-value is the smallest level of significance at which the null hypothesis \\ncan be rejected. The smaller the p-value, the stronger the evidence against \\nthe null hypothesis and in favor of the alternative hypothesis. The p-value \\napproach to hypothesis testing involves computing a p-value for the test \\nstatistic and allowing the user of the research to interpret the implications \\nfor the null hypothesis.\\n■ \\nFor hypothesis tests concerning the population mean of a normally dis-\\ntributed population with unknown variance, the theoretically correct test \\nstatistic is the t-statistic.\\n■ \\nWhen we want to test whether the observed difference between two means \\nis statistically significant, we must first decide whether the samples are inde-\\npendent or dependent (related). If the samples are independent, we conduct \\na test concerning differences between means. If the samples are dependent, \\nwe conduct a test of mean differences (paired comparisons test).\\n■ \\nWhen we conduct a test of the difference between two population means \\nfrom normally distributed populations with unknown but equal variances, \\nwe use a t-test based on pooling the observations of the two samples to esti-\\nmate the common but unknown variance. This test is based on an assump-\\ntion of independent samples.\\n■ \\nIn tests concerning two means based on two samples that are not indepen-\\ndent, we often can arrange the data in paired observations and conduct a \\ntest of mean differences (a paired comparisons test). When the samples are \\nfrom normally distributed populations with unknown variances, the appro-\\npriate test statistic is t-distributed.\\n■ \\nIn tests concerning the variance of a single normally distributed population, \\nthe test statistic is chi-square with n − 1 degrees of freedom, where n is \\nsample size.\\n■ \\nFor tests concerning differences between the variances of two normally \\ndistributed populations based on two random, independent samples, the \\nappropriate test statistic is based on an F-test (the ratio of the sample vari-\\nances). The degrees of freedom for this F-test are n1 − 1 and n2 − 1, where \\nn1 corresponds to the number of observations in the calculation of the \\nnumerator and n2 is the number of observations in the calculation of the \\ndenominator of the F-statistic.\\n■ \\nA parametric test is a hypothesis test concerning a population parameter or \\na hypothesis test based on specific distributional assumptions. In contrast, a \\nnonparametric test either is not concerned with a parameter or makes mini-\\nmal assumptions about the population from which the sample comes.\\n■ \\nA nonparametric test is primarily used when data do not meet distributional \\nassumptions, when there are outliers, when data are given in ranks, or when \\nthe hypothesis we are addressing does not concern a parameter.\\n■ \\nIn tests concerning correlation, we use a t-statistic to test whether a popula-\\ntion correlation coefficient is different from zero. If we have n observations \\nfor two variables, this test statistic has a t-distribution with n − 2 degrees of \\nfreedom.\\n■ \\nThe Spearman rank correlation coefficient is calculated on the ranks of two \\nvariables within their respective samples.\\n© CFA Institute. For candidate use only. Not for distribution.\\nTest of Independence Using Contingency Table Data\\n407\\n■ \\nA chi-square distributed test statistic is used to test for independence of two \\ncategorical variables. This nonparametric test compares actual frequencies \\nwith those expected on the basis of independence. This test statistic has \\ndegrees of freedom of (r − 1)(c − 2), where r is the number of categories for \\nthe first variable and c is the number of categories of the second variable.\\nREFERENCES\\nBenjamini, Y., Y. Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and \\nPowerful Approach to Multiple Testing.” Journal of the Royal Statistical Society. Series B. \\nMethodological, 57: 289–300.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n408\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 418,\n",
       "     'content': 'PRACTICE PROBLEMS\\n1. Which of the following statements about hypothesis testing is correct? \\nA. The null hypothesis is the condition a researcher hopes to support.\\nB. The alternative hypothesis is the proposition considered true without con-\\nclusive evidence to the contrary.\\nC. The alternative hypothesis exhausts all potential parameter values not \\naccounted for by the null hypothesis.\\n2. Willco is a manufacturer in a mature cyclical industry. During the most recent \\nindustry cycle, its net income averaged $30 million per year with a standard \\ndeviation of $10 million (n = 6 observations). Management claims that Willco’s \\nperformance during the most recent cycle results from new approaches and that \\nWillco’s profitability will exceed the $24 million per year observed in prior cycles.\\nA. With μ as the population value of mean annual net income, formulate null \\nand alternative hypotheses consistent with testing Willco management’s \\nclaim.\\nB. Assuming that Willco’s net income is at least approximately normally dis-\\ntributed, identify the appropriate test statistic and calculate the degrees of \\nfreedom.\\nC. Based on critical value of 2.015, determine whether to reject the null \\nhypothesis.\\n3. Which of the following statements is correct with respect to the null hypothesis?\\nA. It can be stated as “not equal to” provided the alternative hypothesis is \\nstated as “equal to.”\\nB. Along with the alternative hypothesis, it considers all possible values of the \\npopulation parameter.\\nC. In a two-tailed test, it is rejected when evidence supports equality between \\nthe hypothesized value and the population parameter.\\n4. Which of the following statements regarding a one-tailed hypothesis test is cor-\\nrect?\\nA. The rejection region increases in size as the level of significance becomes \\nsmaller.\\nB. A one-tailed test more strongly reflects the beliefs of the researcher than a \\ntwo-tailed test.\\nC. The absolute value of the rejection point is larger than that of a two-tailed \\ntest at the same level of significance.\\n5. A hypothesis test for a normally distributed population at a 0.05 significance level \\nimplies a:\\nA. 95% probability of rejecting a true null hypothesis.\\nB. 95% probability of a Type I error for a two-tailed test.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n409\\nC. 5% critical value rejection region in a tail of the distribution for a one-tailed \\ntest. \\n6. The value of a test statistic is best described as the basis for deciding whether to:\\nA. reject the null hypothesis.\\nB. accept the null hypothesis.\\nC. reject the alternative hypothesis.\\n7. Which of the following is a Type I error?\\nA. Rejecting a true null hypothesis\\nB. Rejecting a false null hypothesis\\nC. Failing to reject a false null hypothesis\\n8. A Type II error is best described as:\\nA. rejecting a true null hypothesis.\\nB. failing to reject a false null hypothesis.\\nC. failing to reject a false alternative hypothesis.\\n9. The level of significance of a hypothesis test is best used to:\\nA. calculate the test statistic.\\nB. define the test’s rejection points.\\nC. specify the probability of a Type II error.\\n10. All else equal, is specifying a smaller significance level in a hypothesis test likely \\nto increase the probability of a: \\n\\xa0\\n\\xa0\\nType I error?\\n\\xa0\\nType II error?\\n\\xa0\\nA.\\n\\xa0\\nNo\\n\\xa0\\nNo\\n\\xa0\\nB.\\n\\xa0\\nNo\\n\\xa0\\nYes\\n\\xa0\\nC.\\n\\xa0\\nYes\\n\\xa0\\nNo\\n\\xa0\\n11. The probability of correctly rejecting the null hypothesis is the:\\nA. p-value.\\nB. power of a test.\\nC. level of significance.\\n12. The power of a hypothesis test is:\\nA. equivalent to the level of significance.\\nB. the probability of not making a Type II error. \\nC. unchanged by increasing a small sample size.\\n13. For each of the following hypothesis tests concerning the population mean, μ, \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n410\\nstate the conclusion regarding the test of the hypotheses.\\nA. H0: μ = 10 versus Ha: μ ≠ 10, with a calculated t-statistic of 2.05 and critical \\nt-values of ±1.984.\\nB. H0: μ ≤ 10 versus Ha: μ > 10, with a calculated t-statistic of 2.35 and a criti-\\ncal t-value of +1.679\\nC. H0: μ = 10 versus Ha: μ ≠ 10, with a calculated t-statistic of 2.05, a p-value of \\n4.6352%, and a level of significance of 5%.\\nD. H0: μ ≤ 10 versus Ha: μ > 10, with a 2% level of significance and a calculated \\ntest statistic with a p-value of 3%. \\n14. In the step “stating a decision rule” in testing a hypothesis, which of the following \\nelements must be specified?\\nA. Critical value\\nB. Power of a test\\nC. Value of a test statistic\\n15. When making a decision about investments involving a statistically significant \\nresult, the:\\nA. economic result should be presumed to be meaningful.\\nB. statistical result should take priority over economic considerations.\\nC. economic logic for the future relevance of the result should be further \\nexplored.\\n16. An analyst tests the profitability of a trading strategy with the null hypothesis \\nthat the average abnormal return before trading costs equals zero. The calculat-\\ned t-statistic is 2.802, with critical values of ±2.756 at significance level α = 0.01. \\nAfter considering trading costs, the strategy’s return is near zero. The results are \\nmost likely:\\nA. statistically but not economically significant.\\nB. economically but not statistically significant.\\nC. neither statistically nor economically significant.\\n17. Which of the following statements is correct with respect to the p-value?\\nA. It is a less precise measure of test evidence than rejection points.\\nB. It is the largest level of significance at which the null hypothesis is rejected.\\nC. It can be compared directly with the level of significance in reaching test \\nconclusions.\\n18. Which of the following represents a correct statement about the p-value?\\nA. The p-value offers less precise information than does the rejection points \\napproach.\\nB. A larger p-value provides stronger evidence in support of the alternative \\nhypothesis.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n411\\nC. A p-value less than the specified level of significance leads to rejection of the \\nnull hypothesis.\\n19. Which of the following statements on p-value is correct?\\nA. The p-value indicates the probability of making a Type II error.\\nB. The lower the p-value, the weaker the evidence for rejecting the H0.\\nC. The p-value is the smallest level of significance at which H0 can be rejected.\\n20. The following table shows the significance level (α) and the p-value for two hy-\\npothesis tests. \\n\\xa0\\nα\\np-Value\\nTest 1\\n0.02\\n0.05\\nTest 2\\n0.05\\n0.02\\nIn which test should we reject the null hypothesis?\\nA. Test 1 only\\nB. Test 2 only\\nC. Both Test 1 and Test 2\\n21. Identify the appropriate test statistic or statistics for conducting the following hy-\\npothesis tests. (Clearly identify the test statistic and, if applicable, the number of \\ndegrees of freedom. For example, “We conduct the test using an x-statistic with y \\ndegrees of freedom.”)\\nA. H0: μ = 0 versus Ha: μ ≠ 0, where μ is the mean of a normally distributed \\npopulation with unknown variance. The test is based on a sample of 15 \\nobservations.\\nB. H0: μ = 5 versus Ha: μ ≠ 5, where μ is the mean of a normally distributed \\npopulation with unknown variance. The test is based on a sample of 40 \\nobservations.\\nC. H0: μ ≤ 0 versus Ha: μ > 0, where μ is the mean of a normally distributed \\npopulation with known variance σ2. The sample size is 45.\\nD. H0: σ2 = 200 versus Ha: σ2 ≠ 200, where σ2 is the variance of a normally \\ndistributed population. The sample size is 50.\\nE.  H 0 :  σ 1 2 =  σ 2 2 \\xa0versus\\xa0  H a :  σ 1 2 ≠  σ 2 2 , where σ 1 2 is the variance of one normally \\ndistributed population and   σ 2 2 is the variance of a second normally distrib-\\nuted population. The test is based on two independent samples, with the \\nfirst sample of size 30 and the second sample of size 40.\\nF. H0: μ1 − μ2 = 0 versus Ha: μ1 − μ2 ≠ 0, where the samples are drawn from \\nnormally distributed populations with unknown but assumed equal vari-\\nances. The observations in the two samples (of size 25 and 30, respectively) \\nare independent.\\n22. For each of the following hypothesis tests concerning the population mean, state \\nthe conclusion.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n412\\nA. H0: σ2 = 0.10 versus Ha: σ2 ≠ 0.10, with a calculated chi-square test statistic \\nof 45.8 and critical chi-square values of 42.950 and 86.830.\\nB. H0: σ2 = 0.10 versus Ha: σ2 ≠ 0.10, with a 5% level of significance and a \\np-value for this calculated chi-square test statistic of 4.463%.\\nC. H0: σ12 = σ22 versus Ha: σ12 ≠ σ22, with a calculated F-statistic of 2.3. With \\n40 and 30 degrees of freedom, the critical F-values are 0.498 and 1.943.\\nD. H0: σ2 ≤ 10 versus Ha: μσ2 > 10, with a calculated test statistic of 32 and a \\ncritical chi-square value of 26.296.\\nThe following information relates to questions \\n23-24\\nPerformance in Forecasting Quarterly Earnings per Share\\n\\xa0\\nNumber of \\nForecasts\\nMean Forecast Error \\n(Predicted − Actual)\\nStandard Deviation of \\nForecast Errors\\nAnalyst A\\n10\\n0.05\\n0.10\\nAnalyst B\\n15\\n0.02\\n0.09\\nCritical t-values:\\n\\xa0\\nArea in the Right-Side Rejection Area \\nDegrees of Freedom\\np = 0.05\\np = 0.025\\n8\\n1.860\\n2.306\\n9\\n1.833\\n2.262\\n10\\n1.812\\n2.228\\n11\\n1.796\\n2.201\\n12\\n1.782\\n2.179\\n13\\n1.771\\n2.160\\n14\\n1.761\\n2.145\\n15\\n1.753\\n2.131\\n16\\n1.746\\n2.120\\n17\\n1.740\\n2.110\\n18\\n1.734\\n2.101\\n19\\n1.729\\n2.093\\n20\\n1.725\\n2.086\\n21\\n1.721\\n2.080\\n22\\n1.717\\n2.074\\n23\\n1.714\\n2.069\\n24\\n1.711\\n2.064\\n25\\n1.708\\n2.060\\n26\\n1.706\\n2.056\\n27\\n1.703\\n2.052\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n413\\n23. Investment analysts often use earnings per share (EPS) forecasts. One test of \\nforecasting quality is the zero-mean test, which states that optimal forecasts \\nshould have a mean forecasting error of zero. The forecasting error is the differ-\\nence between the predicted value of a variable and the actual value of the vari-\\nable.\\nYou have collected data (shown in the previous table) for two analysts who cover \\ntwo different industries: Analyst A covers the telecom industry; Analyst B covers \\nautomotive parts and suppliers.\\nA. With μ as the population mean forecasting error, formulate null and alterna-\\ntive hypotheses for a zero-mean test of forecasting quality.\\nB. For Analyst A, determine whether to reject the null at the 0.05 level of \\nsignificance.\\nC. For Analyst B, determine whether to reject the null at the 0.05 level of \\nsignificance.\\n24. Reviewing the EPS forecasting performance data for Analysts A and B, you want \\nto investigate whether the larger average forecast errors of Analyst A relative to \\nAnalyst B are due to chance or to a higher underlying mean value for Analyst A. \\nAssume that the forecast errors of both analysts are normally distributed and that \\nthe samples are independent.\\nA. Formulate null and alternative hypotheses consistent with determining \\nwhether the population mean value of Analyst A’s forecast errors (μ1) are \\nlarger than Analyst B’s (μ2).\\nB. Identify the test statistic for conducting a test of the null hypothesis formu-\\nlated in Part A.\\nC. Identify the rejection point or points for the hypotheses tested in Part A at \\nthe 0.05 level of significance.\\nD. Determine whether to reject the null hypothesis at the 0.05 level of \\nsignificance.\\n25. An analyst is examining a large sample with an unknown population variance. \\nWhich of the following is the most appropriate test to test the hypothesis that the \\nhistorical average return on an index is less than or equal to 6%?\\nA. One-sided t-test\\nB. Two-sided t-test\\nC. One-sided chi-square test\\n26. Which of the following tests of a hypothesis concerning the population mean is \\nmost appropriate?\\nA. A z-test if the population variance is unknown and the sample is small\\nB. A z-test if the population is normally distributed with a known variance\\nC. A t-test if the population is non-normally distributed with unknown vari-\\nance and a small sample\\n27. For a small sample from a normally distributed population with unknown vari-\\nance, the most appropriate test statistic for the mean is the:\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n414\\nA. z-statistic.\\nB. t-statistic.\\nC. χ2 statistic.\\n28. An investment consultant conducts two independent random samples of \\nfive-year performance data for US and European absolute return hedge funds. \\nNoting a return advantage of 50 bps for US managers, the consultant decides \\nto test whether the two means are different from one another at a 0.05 level of \\nsignificance. The two populations are assumed to be normally distributed with \\nunknown but equal variances. Results of the hypothesis test are contained in the \\nfollowing tables.\\n\\xa0\\nSample Size\\nMean Return \\n(%)\\nStandard Deviation\\nUS managers\\n50\\n4.7\\n5.4\\nEuropean managers\\n50\\n4.2\\n4.8\\nNull and alternative hypotheses\\nH0: μUS − μE = 0; Ha: μUS − μE ≠ 0\\nCalculated test statistic\\n0.4893\\nCritical value rejection points\\n±1.984\\nThe mean return for US funds is μUS, and μE is the mean return for European funds.\\nThe results of the hypothesis test indicate that the:\\nA. null hypothesis is not rejected. \\nB. alternative hypothesis is statistically confirmed.\\nC. difference in mean returns is statistically different from zero. \\n29. A pooled estimator is used when testing a hypothesis concerning the:\\nA. equality of the variances of two normally distributed populations.\\nB. difference between the means of two at least approximately normally dis-\\ntributed populations with unknown but assumed equal variances.\\nC. difference between the means of two at least approximately normally dis-\\ntributed populations with unknown and assumed unequal variances.\\n30. The following table gives data on the monthly returns on the S&P 500 Index and \\nsmall-cap stocks for a 40-year period and provides statistics relating to their \\nmean differences. Further, the entire sample period is split into two subperiods of \\n20 years each, and the return data for these subperiods is also given in the table.\\nMeasure\\nS&P 500 Return \\n(%)\\nSmall-Cap \\nStock Return \\n(%)\\nDifferences \\n(S&P 500 − Small-Cap \\nStock)\\nEntire sample period, 480 months\\nMean\\n1.0542\\n1.3117\\n−0.258\\nStandard deviation\\n4.2185\\n5.9570\\n3.752\\nFirst subperiod, 240 months\\nMean\\n0.6345\\n1.2741\\n−0.640\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n415\\nMeasure\\nS&P 500 Return \\n(%)\\nSmall-Cap \\nStock Return \\n(%)\\nDifferences \\n(S&P 500 − Small-Cap \\nStock)\\nStandard deviation\\n4.0807\\n6.5829\\n4.096\\nSecond subperiod, 240 months\\nMean\\n1.4739\\n1.3492\\n0.125\\nStandard deviation\\n4.3197\\n5.2709\\n3.339\\nUse a significance level of 0.05 and assume that mean differences are approxi-\\nmately normally distributed.\\nA. Formulate null and alternative hypotheses consistent with testing whether \\nany difference exists between the mean returns on the S&P 500 and \\nsmall-cap stocks.\\nB. Determine whether to reject the null hypothesis for the entire sample period \\nif the critical values are ±1.96.\\nC. Determine whether to reject the null hypothesis for the first subperiod if the \\ncritical values are ±1.96.\\nD. Determine whether to reject the null hypothesis for the second subperiod if \\nthe critical values are ±1.96.\\n31. When evaluating mean differences between two dependent samples, the most \\nappropriate test is a:\\nA. z-test.\\nB. chi-square test.\\nC. paired comparisons test.\\n32. A chi-square test is most appropriate for tests concerning:\\nA. a single variance.\\nB. differences between two population means with variances assumed to be \\nequal.\\nC. differences between two population means with variances assumed to not \\nbe equal.\\n33. During a 10-year period, the standard deviation of annual returns on a portfolio \\nyou are analyzing was 15% a year. You want to see whether this record is suffi-\\ncient evidence to support the conclusion that the portfolio’s underlying variance \\nof return was less than 400, the return variance of the portfolio’s benchmark.\\nA. Formulate null and alternative hypotheses consistent with your objective.\\nB. Identify the test statistic for conducting a test of the hypotheses in Part A, \\nand calculate the degrees of freedom.\\nC. Determine whether the null hypothesis is rejected or not rejected at the 0.05 \\nlevel of significance using a critical value of 3.325.\\n34. You are investigating whether the population variance of returns on an index \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n416\\nchanged subsequent to a market disruption. You gather the following data for 120 \\nmonths of returns before the disruption and for 120 months of returns after the \\ndisruption. You have specified a 0.05 level of significance.\\nTime Period\\nn\\nMean Monthly Return \\n(%)\\nVariance of Returns\\nBefore disruption\\n120\\n1.416\\n22.367\\nAfter disruption\\n120\\n1.436\\n15.795\\nA. Formulate null and alternative hypotheses consistent with the research goal.\\nB. Identify the test statistic for conducting a test of the hypotheses in Part A, \\nand calculate the degrees of freedom.\\nC. Determine whether to reject the null hypothesis at the 0.05 level of signifi-\\ncance if the critical values are 0.6969 and 1.4349. \\n35. Which of the following should be used to test the difference between the varianc-\\nes of two normally distributed populations?\\nA. t-test\\nB. F-test\\nC. Paired comparisons test\\n36. In which of the following situations would a nonparametric test of a hypothesis \\nmost likely be used?\\nA. The sample data are ranked according to magnitude.\\nB. The sample data come from a normally distributed population.\\nC. The test validity depends on many assumptions about the nature of the \\npopulation.\\n37. An analyst is examining the monthly returns for two funds over one year. Both \\nfunds’ returns are non-normally distributed. To test whether the mean return of \\none fund is greater than the mean return of the other fund, the analyst can use:\\nA. a parametric test only.\\nB. a nonparametric test only.\\nC. both parametric and nonparametric tests.\\n38. The following table shows the sample correlations between the monthly returns \\nfor four different mutual funds and the S&P 500. The correlations are based on 36 \\nmonthly observations. The funds are as follows:\\nFund 1\\nLarge-cap fund\\nFund 2\\nMid-cap fund\\nFund 3\\nLarge-cap value fund\\nFund 4\\nEmerging market fund\\nS&P 500\\nUS domestic stock index\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n417\\n\\xa0\\nFund 1\\nFund 2\\nFund 3\\nFund 4\\nS&P 500\\nFund 1\\n1\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nFund 2\\n0.9231\\n1\\n\\xa0\\n\\xa0\\n\\xa0\\nFund 3\\n0.4771\\n0.4156\\n1\\n\\xa0\\n\\xa0\\nFund 4\\n0.7111\\n0.7238\\n0.3102\\n1\\n\\xa0\\nS&P 500\\n0.8277\\n0.8223\\n0.5791\\n0.7515\\n1\\nTest the null hypothesis that each of these correlations, individually, is equal to \\nzero against the alternative hypothesis that it is not equal to zero. Use a 5% signif-\\nicance level and critical t-values of ±2.032.\\n39. You are interested in whether excess risk-adjusted return (alpha) is correlated \\nwith mutual fund expense ratios for US large-cap growth funds. The following \\ntable presents the sample.\\nMutual Fund \\nAlpha \\nExpense Ratio\\n1\\n−0.52\\n1.34\\n2\\n−0.13\\n0.40\\n3\\n−0.50\\n1.90\\n4\\n−1.01\\n1.50\\n5\\n−0.26\\n1.35\\n6\\n−0.89\\n0.50\\n7\\n−0.42\\n1.00\\n8\\n−0.23\\n1.50\\n9\\n−0.60\\n1.45\\nA. Formulate null and alternative hypotheses consistent with the verbal \\ndescription of the research goal.\\nB. Identify and justify the test statistic for conducting a test of the hypotheses \\nin Part A.\\nC. Determine whether to reject the null hypothesis at the 0.05 level of signifi-\\ncance if the critical values are ±2.306.\\n40. Jill Batten is analyzing how the returns on the stock of Stellar Energy Corp. are \\nrelated with the previous month’s percentage change in the US Consumer Price \\nIndex for Energy (CPIENG). Based on 248 observations, she has computed the \\nsample correlation between the Stellar and CPIENG variables to be −0.1452. She \\nalso wants to determine whether the sample correlation is significantly different \\nfrom zero. The critical value for the test statistic at the 0.05 level of significance \\nis approximately 1.96. Batten should conclude that the statistical relationship \\nbetween Stellar and CPIENG is:\\nA. significant, because the calculated test statistic is outside the bounds of the \\ncritical values for the test statistic.\\nB. significant, because the calculated test statistic has a lower absolute value \\nthan the critical value for the test statistic.\\nC. insignificant, because the calculated test statistic is outside the bounds of \\nthe critical values for the test statistic.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n418\\n41. An analyst group follows 250 firms and classifies them in two dimensions. First, \\nthey use dividend payment history and earnings forecasts to classify firms into \\none of three groups, with 1 indicating the dividend stars and 3 the dividend \\nlaggards. Second, they classify firms on the basis of financial leverage, using debt \\nratios, debt features, and corporate governance to classify the firms into three \\ngroups, with 1 indicating the least risky firms based on financial leverage and 3 \\nindicating the riskiest. The classification of the 250 firms is as follows:\\nFinancial Leverage \\nGroup\\nDividend Group\\n1\\n2\\n3\\n1\\n40\\n40\\n40\\n2\\n30\\n10\\n20\\n3\\n10\\n50\\n10\\nA. What are the null and alternative hypotheses to test whether the dividend \\nand financial leverage groups are independent of one another?\\nB. What is the appropriate test statistic to use in this type of test?\\nC. If the critical value for the 0.05 level of significance is 9.4877, what is your \\nconclusion?\\n42. Which of the following statements is correct regarding the chi-square test of \\nindependence?\\nA. The test has a one-sided rejection region.\\nB. The null hypothesis is that the two groups are dependent.\\nC. If there are two categories, each with three levels or groups, there are six \\ndegrees of freedom.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 429,\n",
       "     'content': 'Solutions\\n419\\nSOLUTIONS\\n1. C is correct. Together, the null and alternative hypotheses account for all possible \\nvalues of the parameter. Any possible values of the parameter not covered by the \\nnull must be covered by the alternative hypothesis (e.g., H0: μ ≤ 5 versus Ha: μ > \\n5). \\n2. \\nA. As stated in the text, we often set up the “hoped for” or “suspected” condi-\\ntion as the alternative hypothesis. Here, that condition is that the population \\nvalue of Willco’s mean annual net income exceeds $24 million. Thus, we \\nhave H0: μ ≤ 24 versus Ha: μ > 24.\\nB. Given that net income is normally distributed with unknown variance, the \\nappropriate test statistic is  t =   _\\n \\nX  −  μ 0  \\n_ \\n s ⁄ √ _ \\nn     = 1.469694 with n − 1 = 6 − 1 = 5 \\ndegrees of freedom.\\nC. We reject the null if the calculated t-statistic is greater than 2.015. The \\ncalculated t-statistic is  t =  30 − 24 \\n_ \\n 10 ⁄ √ \\n_\\n \\n6     = 1.469694 . Because the calculated test \\nstatistic does not exceed 2.015, we fail to reject the null hypothesis. There is \\nnot sufficient evidence to indicate that the mean net income is greater than \\n$24 million.\\n3. A is correct. The null hypothesis and the alternative hypothesis are complements \\nof one another and together are exhaustive; that is, the null and alternative hy-\\npotheses combined consider all the possible values of the population parameter.\\n4. B is correct. One-tailed tests in which the alternative is “greater than” or “less \\nthan” represent the beliefs of the researcher more firmly than a “not equal to” \\nalternative hypothesis.\\n5. C is correct. For a one-tailed hypothesis test, there is a 5% rejection region in one \\ntail of the distribution. \\n6. A is correct. Calculated using a sample, a test statistic is a quantity whose value is \\nthe basis for deciding whether to reject the null hypothesis.\\n7. A is correct. The definition of a Type I error is when a true null hypothesis is \\nrejected.\\n8. B is correct. A Type II error occurs when a false null hypothesis is not rejected.\\n9. B is correct. The level of significance is used to establish the rejection points of \\nthe hypothesis test.\\n10. B is correct. Specifying a smaller significance level decreases the probability of \\na Type I error (rejecting a true null hypothesis) but increases the probability of \\na Type II error (not rejecting a false null hypothesis). As the level of significance \\ndecreases, the null hypothesis is less frequently rejected.\\n11. B is correct. The power of a test is the probability of rejecting the null hypothesis \\nwhen it is false.\\n12. B is correct. The power of a hypothesis test is the probability of correctly reject-\\ning the null when it is false. Failing to reject the null when it is false is a Type II \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n420\\nerror. Thus, the power of a hypothesis test is the probability of not committing a \\nType II error. \\n13. We make the decision either by comparing the calculated test statistic with the \\ncritical values or by comparing the p-value for the calculated test statistic with \\nthe level of significance.\\nA. Reject the null hypothesis because the calculated test statistic is outside the \\nbounds of the critical values.\\nB. The calculated t-statistic is in the rejection region that is defined by +1.679, \\nso we reject the null hypothesis.\\nC. The p-value corresponding to the calculated test statistic is less than the \\nlevel of significance, so we reject the null hypothesis.\\nD. We fail to reject because the p-value for the calculated test statistic is \\ngreater than what is tolerated with a 2% level of significance.\\n14. B is correct. The critical value in a decision rule is the rejection point for the test. \\nIt is the point with which the test statistic is compared to determine whether to \\nreject the null hypothesis, which is part of the fourth step in hypothesis testing.\\n15. C is correct. When a statistically significant result is also economically mean-\\ningful, one should further explore the logic of why the result might work in the \\nfuture.\\n16. A is correct. The hypothesis is a two-tailed formulation. The t-statistic of 2.802 \\nfalls outside the critical rejection points of less than −2.756 and greater than \\n2.756. Therefore, the null hypothesis is rejected; the result is statistically signif-\\nicant. However, despite the statistical results, trying to profit on the strategy is \\nnot likely to be economically meaningful because the return is near zero after \\ntransaction costs.\\n17. C is correct. When directly comparing the p-value with the level of significance, \\nit can be used as an alternative to using rejection points to reach conclusions on \\nhypothesis tests. If the p-value is smaller than the specified level of significance, \\nthe null hypothesis is rejected. Otherwise, the null hypothesis is not rejected.\\n18. C is correct. The p-value is the smallest level of significance at which the null hy-\\npothesis can be rejected for a given value of the test statistic. The null hypothesis \\nis rejected when the p-value is less than the specified significance level.\\n19. C is correct. The p-value is the smallest level of significance (α) at which the null \\nhypothesis can be rejected. \\n20. B is correct. The p-value is the smallest level of significance (α) at which the null \\nhypothesis can be rejected. If the p-value is less than α, the null is rejected. In \\nTest 1, the p-value exceeds the level of significance, whereas in Test 2, the p-value \\nis less than the level of significance. \\n21. \\nA. The appropriate test statistic is a t-statistic,  t =   _\\n \\nX  −  μ 0  \\n_ \\n s _ \\n √ _ \\nn      , with n − 1 = 15 − 1 = \\n14 degrees of freedom. A t-statistic is correct when the sample comes from \\nan approximately normally distributed population with unknown variance. \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n421\\nB. The appropriate test statistic is a t-statistic,  t =   _\\n \\nX  −  μ 0  \\n_ \\n s _ \\n √ _ \\nn      , with 40 − 1 = 39 \\ndegrees of freedom. A t-statistic is theoretically correct when the sample \\ncomes from a normally distributed population with unknown variance. \\nC. The appropriate test statistic is a z-statistic,  z =   _\\n \\nX  −  μ 0  \\n_ \\n σ _ \\n √ _ \\nn      , because the sample \\ncomes from a normally distributed population with a known variance. \\nD. The appropriate test statistic is chi-square,   x 2 =   s 2 (n − 1) \\n_ \\n σ 0 2  \\n , with 50 − 1 = 49 \\ndegrees of freedom.\\nE. The appropriate test statistic is the F-statistic,  F =  σ12 ⁄ σ22 , with 29 and 39 \\ndegrees of freedom.\\nF. The appropriate test statistic is a t-statistic using a pooled esti-\\nmate of the population variance:  t =    ( _\\n \\nX  1 −  _\\n \\nX  2 )  −   ( μ 1 −  μ 2 )   \\n \\n__________________ \\n \\n √ _\\n \\n \\n s p 2  \\n_ \\n n 1   +  \\n s p 2  \\n_ \\n n 2    \\n \\n , where  \\ns p 2 =    ( n 1 − 1 )  s 1 2 +   ( n 2 − 1 )  s 2 2  \\n \\n__________________ \\n \\n n 1 +  n 2 − 2 \\n . The t-statistic has 25 + 30 − 2 = 53 degrees of \\nfreedom. This statistic is appropriate because the populations are normally \\ndistributed with unknown variances; because the variances are assumed to \\nbe equal, the observations can be pooled to estimate the common variance. \\nThe requirement of independent samples for using this statistic has been \\nmet. \\n22. We make the decision either by comparing the calculated test statistic with the \\ncritical values or by comparing the p-value for the calculated test statistic with \\nthe level of significance.\\nA. The calculated chi-square falls between the two critical values, so we fail to \\nreject the null hypothesis.\\nB. The p-value for the calculated test statistic is less than the level of signifi-\\ncance (the 5%), so we reject the null hypothesis.\\nC. The calculated F-statistic falls outside the bounds of the critical F-values, so \\nwe reject the null hypothesis.\\nD. The calculated chi-square exceeds the critical value for this right-side test, \\nso we reject the null hypothesis.\\n23. \\nA. H0: μ = 0 versus Ha: μ ≠ 0.\\nB. The t-test is based on  t =   _\\n \\nX  −  μ 0  \\n_ \\ns /  √ _ \\nn    with n − 1 = 10 − 1 = 9 degrees of freedom. \\nAt the 0.05 significance level, we reject the null if the calculated t-statistic is \\noutside the bounds of ±2.262 (from the table for 9 degrees of freedom and \\n0.025 in the right side of the distribution). For Analyst A, we have a calcu-\\nlated test statistic of  t =  0.05 − 0 \\n_ \\n 0.10 ⁄ √ _ \\n10     = 1.58114 . We, therefore, fail to reject the \\nnull hypothesis at the 0.05 level.\\nC. For Analyst B, the t-test is based on t with 15 − 1 = 14 degrees of freedom. \\nAt the 0.05 significance level, we reject the null if the calculated t-statistic \\nis outside the bounds of ±2.145 (from the table for 14 degrees of freedom). \\nThe calculated test statistic is  t =  0.02 − 0 \\n_ \\n 0.09 ⁄ √ _ \\n10     = 0.86066 . Because 0.86066 is \\nwithin the range of ±2.145, we fail to reject the null at the 0.05 level.\\n24. \\nA. Stating the suspected condition as the alternative hypothesis, we have\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n422\\n H0: μA − μB ≤ 0 versus Ha: μA − μB > 0, \\nwhere\\nμA = the population mean value of Analyst A’s forecast errors\\nμB = the population mean value of Analyst B’s forecast errors\\nB. We have two normally distributed populations with unknown variances. \\nBased on the samples, it is reasonable to assume that the population vari-\\nances are equal. The samples are assumed to be independent; this assump-\\ntion is reasonable because the analysts cover different industries. The \\nappropriate test statistic is t using a pooled estimate of the common vari-\\nance:  t =  ( _\\n \\nX  1 −  _\\n \\nX  2 ) − ( μ 1 −  μ 2 ) \\n \\n________________ \\n \\n √ _\\n \\n \\n s p 2  \\n_ \\n n 1   +  \\n s p 2  \\n_ \\n n 2    \\n \\n , where   s p 2 =  ( n 1 − 1 )  s 1 2 + ( n 2 − 1 )  s 2 2  \\n \\n__________________ \\n \\n n 1 +  n 2 − 2 \\n . The number \\nof degrees of freedom is nA + nB − 2 = 10 +15 − 2 = 23.\\nC. For df = 23, according to the table, the rejection point for a one-sided (right \\nside) test at the 0.05 significance level is 1.714.\\nD. We first calculate the pooled estimate of variance:\\nE.  s p 2 =  (10 − 1 ) 0.01 + (15 − 1 ) 0.0081 \\n \\n \\n_______________________ \\n10 + 15 − 2  \\n = 0.0088435 . \\nWe then calculate the t-distributed test statistic:\\n t =  \\n(0.05 − 0.02 ) −\\u200a0 \\n \\n_________________ \\n \\n √ \\n__________________ \\n 0.0088435  \\n_ \\n10 \\n +  0.0088435 \\n_ \\n15 \\n  \\n  =  \\n0.03 \\n_ \\n0.0383916  = 0.78142. \\nBecause 0.78142 < 1.714, we fail to reject the null hypothesis. There is not \\nsufficient evidence to indicate that the mean for Analyst A exceeds that for \\nAnalyst B.\\n25. A is correct. If the population sampled has unknown variance and the sample \\nis large, a z-test may be used. Hypotheses involving “greater than” or “less than” \\npostulations are one sided (one tailed). In this situation, the null and alternative \\nhypotheses are stated as H0: μ ≤ 6% and Ha: μ > 6%, respectively. A one-tailed \\nt-test is also acceptable in this case, and the rejection region is on the right side of \\nthe distribution.\\n26. B is correct. The z-test is theoretically the correct test to use in those limited cas-\\nes when testing the population mean of a normally distributed population with \\nknown variance.\\n27. B is correct. A t-statistic is the most appropriate for hypothesis tests of the \\npopulation mean when the variance is unknown and the sample is small but the \\npopulation is normally distributed.\\n28. A is correct. The calculated t-statistic value of 0.4893 falls within the bounds of \\nthe critical t-values of ±1.984. Thus, H0 cannot be rejected; the result is not statis-\\ntically significant at the 0.05 level. \\n29. B is correct. The assumption that the variances are equal allows for the combin-\\ning of both samples to obtain a pooled estimate of the common variance.\\n30. \\nA. We test H0: μd = 0 versus Ha: μd ≠ 0, where μd is the population mean \\ndifference.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n423\\nB. This is a paired comparisons t-test with n − 1 = 480 − 1 = 479 degrees of \\nfreedom. At the 0.05 significance level, we reject the null hypothesis if the \\ncalculated t is less than −1.96 or greater than 1.96.\\n t =  \\n \\n_\\n \\nd  −  μ d0  \\n_ \\ns \\n_\\n \\nd    =  −\\u200a0.258 − 0 \\n_ \\n3.752\\u200a/\\u200a √ _ \\n480    =  −\\u200a0.258 \\n_ \\n0.171255  = −\\u200a1.506529,   or − 1.51. \\nBecause the calculate t-statistic is between ±1.96, we do not reject the null \\nhypothesis that the mean difference between the returns on the S&P 500 \\nand small-cap stocks during the entire sample period was zero.\\nC. This t-test now has n − 1 = 240 − 1 = 239 degrees of freedom. At the 0.05 \\nsignificance level, we reject the null hypothesis if the calculated t is less than \\n−1.96 or greater than 1.96. \\n t =  \\n \\n_\\n \\nd  −  μ d0  \\n_ \\ns \\n_\\n \\nd    =  −\\u200a0.640 − 0 \\n_ \\n4.096\\u200a/\\u200a √ _ \\n240    =  −\\u200a0.640 \\n_ \\n0.264396  = −\\u200a2.420615,   or − 2.42. \\nBecause −2.42 < −1.96, we reject the null hypothesis at the 0.05 significance \\nlevel. We conclude that during this subperiod, small-cap stocks significantly \\noutperformed the S&P 500.\\nD. This t-test has n − 1 = 240 − 1 = 239 degrees of freedom. At the 0.05 signif-\\nicance level, we reject the null hypothesis if the calculated t-statistic is less \\nthan −1.96 or greater than 1.96. The calculated test statistic is\\n t =  \\n \\n_\\n \\nd  −  μ d0  \\n_ \\ns \\n_\\n \\nd    =  0.125 − 0 \\n_ \\n3.339\\u200a/\\u200a √ _ \\n240    =  0.125 \\n_ \\n0.215532  = 0.579962,  or 0.58. \\nAt the 0.05 significance level, because the calculated test statistic of 0.58 \\nis between ±1.96, we fail to reject the null hypothesis for the second \\nsubperiod.\\n31. C is correct. A paired comparisons test is appropriate to test the mean differences \\nof two samples believed to be dependent.\\n32. A is correct. A chi-square test is used for tests concerning the variance of a single \\nnormally distributed population.\\n33. \\nA. We have a “less than” alternative hypothesis, where σ2 is the variance of \\nreturn on the portfolio. The hypotheses are H0: σ2 ≥ 400 versus Ha: σ2 < 400, \\nwhere 400 is the hypothesized value of variance,   σ 0 2 . This means that the \\nrejection region is on the left side of the distribution.\\nB. The test statistic is chi-square distributed with 10 − 1 = 9 degrees of free-\\ndom:   χ 2 =    ( n − 1 )  s 2  \\n_ \\n σ 0 \\n2  \\n .\\nC. The test statistic is calculated as\\n χ 2 =    ( n − 1 )  s 2  \\n_ \\n σ 0 2  \\n =  9 ×  15 2  \\n_ \\n400  =  2, 025 \\n_ \\n400  = 5.0625 ,  or  5.06. \\nBecause 5.06 is not less than 3.325, we do not reject the null hypothesis; \\nthe calculated test statistic falls to the right of the critical value, where the \\ncritical value separates the left-side rejection region from the region where \\nwe fail to reject.\\nWe can determine the critical value for this test using software: \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n424\\nExcel [CHISQ.INV(0.05,9)]\\nR [qchisq(.05,9)]\\nPython [from scipy.stats import chi2 and chi2.ppf(.05,9)]\\nWe can determine the p-value for the calculated test statistic of 17.0953 \\nusing software: \\nExcel [CHISQ.DIST(5.06,9,TRUE)]\\nR [pchisq(5.06,9,lower.tail=TRUE)]\\nPython [from scipy.stats import chi2 and chi2.cdf(5.06,9)]\\n34. \\nA. We have a “not equal to” alternative hypothesis:\\n H 0 \\u200a\\u200a:\\u200a σ Before  \\n2 \\n =  σ After  \\n2 \\n  versus   H a \\u200a\\u200a:\\u200a σ Before  \\n2 \\n ≠  σ After  \\n2 \\n \\nB. To test a null hypothesis of the equality of two variances, we use an F-test:\\n F =  \\n s 1 2  \\n_ \\n s 2 2   . \\nF = 22.367/15.795 = 1.416, with 120 − 1 = 119 numerator and 120 − 1 = 119 \\ndenominator degrees of freedom. Because this is a two-tailed test, we use \\ncritical values for the 0.05/2 = 0.025 level. The calculated test statistic falls \\nwithin the bounds of the critical values (that is, between 0.6969 and 1.4349), \\nso we fail to reject the null hypothesis; there is not enough evidence to \\nindicate that the variances are different before and after the disruption. Note \\nthat we could also have formed the F-statistic as 15.796/22.367 = 0.706 and \\ndraw the same conclusion.\\nWe could also use software to calculate the critical values: \\nExcel [F.INV(0.025,119,119) and F.INV(0.975,119,119)]\\nR [qf(c(.025,.975),119,119)]\\nPython from scipy.stats import f and f.ppf\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[(.025,119,119) and\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0f.ppf(.975,119,119)]\\nAdditionally, we could use software to calculate the p-value of the calcu-\\nlated test statistic, which is 5.896% and then compare it with the level of \\nsignificance: \\nExcel [(1-F.DIST(22.367/15.796,119,119,TRUE))*2 or\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0F.DIST(15.796/22.367,119,119,TRUE)*2] \\nR [(1-pf(22.367/15.796,119,119))*2 or\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pf(15.796/22.367,119,119)*2 ]\\nPython from scipy.stats import f and f.cdf\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[(15.796/22.367,119,119)*2 or\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0(1-f.cdf(22.367/15.796,119,119))*2]\\n35. B is correct. An F-test is used to conduct tests concerning the difference between \\nthe variances of two normally distributed populations with random independent \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n425\\nsamples.\\n36. A is correct. A nonparametric test is used when the data are given in ranks.\\n37. B is correct. There are only 12 (monthly) observations over the one year of \\nthe sample and thus the samples are small. Additionally, the funds’ returns are \\nnon-normally distributed. Therefore, the samples do not meet the distributional \\nassumptions for a parametric test. The Mann–Whitney U test (a nonparametric \\ntest) could be used to test the differences between population means.\\n38. The hypotheses are H0: ρ = 0 and Ha: ρ ≠ 0. The calculated test statistics are based \\non the formula  t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    . For example, the calculated t-statistic for the correla-\\ntion of Fund 3 and Fund 4 is \\n t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    =  0.3102  √ _ \\n 36 − 2   \\n___________ \\n \\n √ _ \\n1 −  0.3102 2    = 1.903. \\nRepeating this calculation for the entire matrix of correlations gives the following:\\n\\xa0\\nCalculated t-Statistics for Correlations\\n\\xa0\\nFund 1\\nFund 2\\nFund 3\\nFund 4\\nS&P 500\\nFund 1\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nFund 2\\n13.997\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nFund 3\\n3.165\\n2.664\\n\\xa0\\n\\xa0\\n\\xa0\\nFund 4\\n5.897\\n6.116\\n1.903\\n\\xa0\\n\\xa0\\nS&P 500\\n8.600\\n8.426\\n4.142\\n6.642\\n\\xa0\\nWith critical values of ±2.032, with the exception of the correlation between \\nFund 3 and Fund 4 returns, we reject the null hypothesis for these correlations. In \\nother words, there is sufficient evidence to indicate that the correlations are dif-\\nferent from zero, with the exception of the correlation of returns between Fund 3 \\nand Fund 4.\\nWe could use software to determine the critical values: \\nExcel [T.INV(0.025,34) and T.INV(0.975,34)]\\nR [qt(c(.025,.975),34)]\\nPython [from scipy.stats import t and t.ppf(.025,34) and t.ppf(.975,34)]\\nWe could also use software to determine the p-value for the calculated test sta-\\ntistic to enable a comparison with the level of significance. For example, for t = \\n2.664, the p-value is 0.01172: \\nExcel [(1-T.DIST(2.664,34,TRUE))*2]\\nR [(1-pt(2.664,34))*2]\\nPython [from scipy.stats import t and (1-t.cdf(2.664,34))*2]\\n39. \\nA. We have a “not equal to” alternative hypothesis:\\n H0: ρ = 0 versus Ha: ρ ≠ 0\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 6 \\nHypothesis Testing\\n426\\nB. Mutual fund expense ratios are bounded from above and below; in practice, \\nthere is at least a lower bound on alpha (as any return cannot be less than \\n−100%), and expense ratios cannot be negative. These variables may not be \\nnormally distributed, and the assumptions of a parametric test are not likely \\nto be fulfilled. Thus, a nonparametric test appears to be appropriate.\\nWe would use the nonparametric Spearman rank correlation coefficient to \\nconduct the test:   r s = 1 −  6 ∑ i=1  \\nn   d i 2   \\n_ \\nn( n 2 − 1)   with the t-distributed test statistic of  \\nt =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    .\\nC. The calculation of the Spearman rank correlation coefficient is given in the \\nfollowing table.\\nMutual \\nFund\\nAlpha\\nExpense \\nRatio\\nRank \\nby \\nAlpha\\nRank by \\nExpense \\nRatio\\nDifference \\nin Rank\\nDifference \\nSquared\\n1\\n−0.52\\n1.34\\n6\\n6\\n0\\n0\\n2\\n−0.13\\n0.40\\n1\\n9\\n−8\\n64\\n3\\n−0.50\\n1.90\\n5\\n1\\n4\\n16\\n4\\n−1.01\\n1.50\\n9\\n2\\n7\\n49\\n5\\n−0.26\\n1.35\\n3\\n5\\n−2\\n4\\n6\\n−0.89\\n0.50\\n8\\n8\\n0\\n0\\n7\\n−0.42\\n1.00\\n4\\n7\\n−3\\n9\\n8\\n−0.23\\n1.50\\n2\\n2\\n0\\n0\\n9\\n−0.60\\n1.45\\n7\\n4\\n3\\n9\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n151\\n r s = 1 −  6(151) \\n_ \\n9(80)  = −\\u200a0.25833. \\nThe calculated test statistic, using the t-distributed test statistic t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2     \\nis  t =  − 0.25833  √ \\n_\\n7    \\n___________ \\n \\n √ \\n____________ \\n \\n1 − 0.066736    =  − 0.683486 \\n_ \\n0.9332638  = − 0.7075 . On the basis of this value fall-\\ning within the range of +2.306, we fail to reject the null hypothesis that the \\nSpearman rank correlation coefficient is zero. \\n40. A is correct. The calculated test statistic is\\n \\nt =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    \\n  \\n=  −\\u200a0.1452  √ _ \\n 248 − 2   \\n_____________ \\n \\n √ \\n_____________ \\n \\n1 −  ( −\\u200a0.1452 ) 2    = −\\u200a2.30177.\\n  \\nBecause the value of t = −2.30177 is outside the bounds of ±1.96, we reject the \\nnull hypothesis of no correlation and conclude that there is enough evidence to \\nindicate that the correlation is different from zero. \\n41. \\nA. The hypotheses are as follows:\\nH0: Dividend and financial leverage ratings are not related, so these \\ngroupings are independent.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n427\\nHa: Dividend and financial leverage ratings are related, so these group-\\nings are not independent.\\nB. The appropriate test statistic is   χ 2 =  ∑ i=1  \\nm   \\n ( O ij −  E ij ) \\n_ 2  \\n E ij  \\n  , where Oij represents \\nthe observed frequency for the i and j group and Eij represents the expected \\nfrequency for the i and j group if the groupings are independent.\\nC. The expected frequencies based on independence are as follows:\\nFinancial Leverage Group\\nDividend Group\\n\\xa0\\n1\\n2\\n3\\nSum\\n1\\n38.4\\n48\\n33.6\\n120\\n2\\n19.2\\n24\\n16.8\\n60\\n3\\n22.4\\n28\\n19.6\\n70\\nSum\\n80\\n100\\n70\\n250\\nThe scaled squared deviations for each combination of financial leverage \\nand dividend grouping are:\\nFinancial Leverage Group\\nDividend Group\\n1\\n2\\n3\\n1\\n0.06667\\n1.33333\\n1.21905\\n2\\n6.07500\\n8.16667\\n0.60952\\n3\\n6.86429\\n17.28571\\n4.70204\\nThe sum of these scaled squared deviations is the calculated chi-square sta-\\ntistic of 46.3223. Because this calculated value exceeds the critical value of \\n9.4877, we reject the null hypothesis that these groupings are independent.\\n42. A is correct. The test statistic comprises squared differences between the ob-\\nserved and expected values, so the test involves only one side, the right side. B is \\nincorrect because the null hypothesis is that the groups are independent, and C \\nis incorrect because with three levels of groups for the two categorical variables, \\nthere are four degrees of freedom.\\n© CFA Institute. For candidate use only. Not for distribution.\\n© CFA Institute. For candidate use only. Not for distribution.\\nIntroduction to Linear Regression\\nby Pamela Peterson Drake, PhD, CFA.\\nPamela Peterson Drake, PhD, CFA, is at James Madison University (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndescribe a simple linear regression model and the roles of the \\ndependent and independent variables in the model\\ndescribe the least squares criterion, how it is used to estimate \\nregression coefficients, and their interpretation\\nexplain the assumptions underlying the simple linear regression \\nmodel, and describe how residuals and residual plots indicate if these \\nassumptions may have been violated\\ncalculate and interpret the coefficient of determination and the \\nF-statistic in a simple linear regression\\ndescribe the use of analysis of variance (ANOVA) in regression \\nanalysis, interpret ANOVA results, and calculate and interpret the \\nstandard error of estimate in a simple linear regression\\nformulate a null and an alternative hypothesis about a population \\nvalue of a regression coefficient, and determine whether the null \\nhypothesis is rejected at a given level of significance\\ncalculate and interpret the predicted value for the dependent \\nvariable, and a prediction interval for it, given an estimated linear \\nregression model and a value for the independent variable\\ndescribe different functional forms of simple linear regressions\\nSIMPLE LINEAR REGRESSION\\ndescribe a simple linear regression model and the roles of the \\ndependent and independent variables in the model\\n1\\nL E A R N I N G  M O D U L E\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]},\n",
       "  {'title': 'Learning Module 7\\tIntroduction to Linear Regression',\n",
       "   'page_number': 439,\n",
       "   'content': 'Introduction to Linear Regression\\nby Pamela Peterson Drake, PhD, CFA.\\nPamela Peterson Drake, PhD, CFA, is at James Madison University (USA).\\nLEARNING OUTCOME\\nMastery\\nThe candidate should be able to:\\ndescribe a ',\n",
       "   'children': [{'title': 'Simple Linear Regression',\n",
       "     'page_number': 439,\n",
       "     'content': 'simple linear regression model and the roles of the \\ndependent and independent variables in the model\\ndescribe the least squares criterion, how it is used to estimate \\nregression coefficients, and their interpretation\\nexplain the assumptions underlying the simple linear regression \\nmodel, and describe how residuals and residual plots indicate if these \\nassumptions may have been violated\\ncalculate and interpret the coefficient of determination and the \\nF-statistic in a simple linear regression\\ndescribe the use of analysis of variance (ANOVA) in regression \\nanalysis, interpret ANOVA results, and calculate and interpret the \\nstandard error of estimate in a simple linear regression\\nformulate a null and an alternative hypothesis about a population \\nvalue of a regression coefficient, and determine whether the null \\nhypothesis is rejected at a given level of significance\\ncalculate and interpret the predicted value for the dependent \\nvariable, and a prediction interval for it, given an estimated linear \\nregression model and a value for the independent variable\\ndescribe different functional forms of simple linear regressions\\nSIMPLE LINEAR REGRESSION\\ndescribe a simple linear regression model and the roles of the \\ndependent and independent variables in the model\\n1\\nL E A R N I N G  M O D U L E\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n430\\nFinancial analysts often need to examine whether a variable is useful for explaining \\nanother variable. For example, the analyst may want to know whether earnings growth, \\nor perhaps cash flow growth, helps explain the company’s value in the marketplace. \\nRegression analysis is a tool for examining this type of issue.\\nSuppose an analyst is examining the return on assets (ROA) for an industry and \\nobserves the ROA for the six companies shown in Exhibit 1. The average of these \\nROAs is 12.5%, but the range is from 4% to 20%.\\nExhibit 1: Return on Assets of Selected Companies\\nCompany\\nROA (%)\\nA\\n6\\nB\\n4\\nC\\n15\\nD\\n20\\nE\\n10\\nF\\n20\\nIn trying to understand why the ROAs differ among these companies, we could look \\nat why the ROA of Company A differs from that of Company B, why the ROA of \\nCompany A differs from that of Company D, why the ROA of Company F differs from \\nthat of Company C, and so on, comparing each pair of ROAs. A way to make this a \\nsimpler exploration is to try to understand why each company’s ROA differs from the \\nmean ROA of 12.5%. We look at the sum of squared deviations of the observations \\nfrom the mean to capture variations in ROA from their mean. Let Y represent the \\nvariable that we would like to explain, which in this case is the return on assets. Let  \\nY i represent an observation of a company’s ROA, and let   _\\n \\nY  represent the mean ROA \\nfor the sample of size n. We can describe the variation of the ROAs as\\n Variation of Y =  ∑ i=1  \\nn   ( Y i −  \\n_\\n \\nY  ) 2 . \\n(1)\\nOur goal is to understand what drives these returns on assets or, in other words, \\nwhat explains the variation of Y. The variation of Y is often referred to as the sum of \\nsquares total (SST), or the total sum of squares.\\nWe now ask whether it is possible to explain the variation of the ROA using \\nanother variable that also varies among the companies; note that if this other variable \\nis constant or random, it would not serve to explain why the ROAs differ from one \\nanother. Suppose the analyst believes that the capital expenditures in the previous \\nperiod, scaled by the prior period’s beginning property, plant, and equipment, are a \\ndriver for the ROA variable. Let us represent this scaled capital expenditures variable \\nas CAPEX, as we show in Exhibit 2.\\nExhibit 2: Return on Assets and Scaled Capital \\nExpenditures\\nCompany\\nROA \\n(%)\\nCAPEX \\n(%)\\nA\\n6.0\\n0.7\\nB\\n4.0\\n0.4\\nC\\n15.0\\n5.0\\n© CFA Institute. For candidate use only. Not for distribution.\\nSimple Linear Regression\\n431\\nCompany\\nROA \\n(%)\\nCAPEX \\n(%)\\nD\\n20.0\\n10.0\\nE\\n10.0\\n8.0\\nF\\n20.0\\n12.5\\n\\xa0\\n\\xa0\\n\\xa0\\nArithmetic mean\\n12.50\\n6.10\\nThe variation of X, in this case CAPEX, is calculated as\\n Variation of X =  ∑ i=1  \\nn   ( X i −  \\n_\\n \\nX  ) 2 . \\n(2)\\nWe can see the relation between ROA and CAPEX in the scatter plot (or scatter-\\ngram) in Exhibit 3, which represents the two variables in two dimensions. Typically, \\nwe present the variable whose variation we want to explain along the vertical axis \\nand the variable whose variation we want to use to explain that variation along the \\nhorizontal axis. Each point in this scatter plot represents a paired observation that \\nconsists of CAPEX and ROA. From a casual visual inspection, there appears to be a \\npositive relation between ROA and CAPEX: Companies with higher CAPEX tend to \\nhave a higher ROA.\\nExhibit 3: Scatter Plot of ROA and CAPEX\\nROA (%)\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\n0\\n14\\n14\\n4\\n2\\n6\\n10\\n10\\n8\\n12\\n12\\nCAPEX (%)\\n0.7, 6.0\\n0.4, 4.0\\n5.0, 15.0\\n8.0, 10.0\\n10.0, 20.0\\n12.5, 20.0\\nIn the ROA example, we use the capital expenditures to explain the returns on assets. \\nWe refer to the variable whose variation is being explained as the dependent vari-\\nable, or the explained variable; it is typically denoted by Y. We refer to the variable \\nwhose variation is being used to explain the variation of the dependent variable as \\nthe independent variable, or the explanatory variable; it is typically denoted by X. \\nTherefore, in our example, the ROA is the dependent variable (Y) and CAPEX is the \\nindependent variable (X).\\nA common method for relating the dependent and independent variables is through \\nthe estimation of a linear relationship, which implies describing the relation between \\nthe two variables as represented by a straight line. If we have only one independent \\nvariable, we refer to the method as simple linear regression (SLR); if we have more \\nthan one independent variable, we refer to the method as multiple regression.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n432\\nLinear regression allows us to test hypotheses about the relationship between two \\nvariables, by quantifying the strength of the relationship between the two variables, \\nand to use one variable to make predictions about the other variable. Our focus is on \\nlinear regression with a single independent variable—that is, simple linear regression.\\nEXAMPLE 1\\nIdentifying the Dependent and Independent Variables in \\na Regression\\n1. An analyst is researching the relationship between corporate earnings \\ngrowth and stock returns. Specifically, she is interested in whether earnings \\nrevisions affect stock price returns in the same period. She collects five years \\nof monthly data on “Wall Street” EPS revisions for a sample of 100 compa-\\nnies and on their monthly stock price returns over the five-year period.\\nWhat are the dependent and independent variables in her model?\\nSolution\\nThe dependent variable is monthly stock price returns, and the indepen-\\ndent variable is Wall Street EPS revisions, since in the analyst’s model, the \\nvariation in monthly stock price returns is being explained by the variation \\nin EPS revisions.\\nESTIMATING THE PARAMETERS OF A SIMPLE LINEAR \\nREGRESSION\\ndescribe the least squares criterion, how it is used to estimate \\nregression coefficients, and their interpretation\\nThe Basics of Simple Linear Regression\\nRegression analysis begins with the dependent variable, the variable whose variation \\nyou are seeking to explain. The independent variable is the variable whose variation \\nyou are using to explain changes in the dependent variable. For example, you might \\ntry to explain small-stock returns (the dependent variable) using returns to the S&P \\n500 Index (the independent variable). Or you might try to explain a country’s infla-\\ntion rate (the dependent variable) as a function of growth in its money supply (the \\nindependent variable).\\nAs the name implies, linear regression assumes a linear relationship between the \\ndependent and the independent variables. The goal is to fit a line to the observations \\non Y and X to minimize the squared deviations from the line; this is the least squares \\ncriterion—hence, the name least squares regression. Because of its common use, linear \\nregression is often referred to as ordinary least squares (OLS) regression.\\nUsing notation, the linear relation between the dependent and independent vari-\\nables is described as\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Estimating the Parameters of a Simple Linear Regression',\n",
       "     'page_number': 442,\n",
       "     'content': 'Learning Module 7 \\nIntroduction to Linear Regression\\n432\\nLinear regression allows us to test hypotheses about the relationship between two \\nvariables, by quantifying the strength of the relationship between the two variables, \\nand to use one variable to make predictions about the other variable. Our focus is on \\nlinear regression with a single independent variable—that is, simple linear regression.\\nEXAMPLE 1\\nIdentifying the Dependent and Independent Variables in \\na Regression\\n1. An analyst is researching the relationship between corporate earnings \\ngrowth and stock returns. Specifically, she is interested in whether earnings \\nrevisions affect stock price returns in the same period. She collects five years \\nof monthly data on “Wall Street” EPS revisions for a sample of 100 compa-\\nnies and on their monthly stock price returns over the five-year period.\\nWhat are the dependent and independent variables in her model?\\nSolution\\nThe dependent variable is monthly stock price returns, and the indepen-\\ndent variable is Wall Street EPS revisions, since in the analyst’s model, the \\nvariation in monthly stock price returns is being explained by the variation \\nin EPS revisions.\\nESTIMATING THE PARAMETERS OF A SIMPLE LINEAR \\nREGRESSION\\ndescribe the least squares criterion, how it is used to estimate \\nregression coefficients, and their interpretation\\n',\n",
       "     'children': [{'title': 'The Basics of Simple Linear Regression',\n",
       "       'page_number': 442,\n",
       "       'content': 'The Basics of Simple Linear Regression\\nRegression analysis begins with the dependent variable, the variable whose variation \\nyou are seeking to explain. The independent variable is the variable whose variation \\nyou are using to explain changes in the dependent variable. For example, you might \\ntry to explain small-stock returns (the dependent variable) using returns to the S&P \\n500 Index (the independent variable). Or you might try to explain a country’s infla-\\ntion rate (the dependent variable) as a function of growth in its money supply (the \\nindependent variable).\\nAs the name implies, linear regression assumes a linear relationship between the \\ndependent and the independent variables. The goal is to fit a line to the observations \\non Y and X to minimize the squared deviations from the line; this is the least squares \\ncriterion—hence, the name least squares regression. Because of its common use, linear \\nregression is often referred to as ordinary least squares (OLS) regression.\\nUsing notation, the linear relation between the dependent and independent vari-\\nables is described as\\n2\\n© CFA Institute. For candidate use only. Not for distribution.\\nEstimating the Parameters of a Simple Linear Regression\\n433\\n Yi = b0 + b1Xi + εi, i = 1, . . . , n. \\n(3)\\nEquation 3 is a model that does not require that every (Y, X) pair for an observa-\\ntion fall on the regression line. This equation states that the dependent variable, Y, is \\nequal to the intercept, b0, plus a slope coefficient, b1, multiplied by the independent \\nvariable, X, plus an error term, ε. The error term, or simply the error, represents the \\ndifference between the observed value of Y and that expected from the true underly-\\ning population relation between Y and X. We refer to the intercept, b0, and the slope \\ncoefficient, b1, as the regression coefficients. A way that we often describe this simple \\nlinear regression relation is that Y is regressed on X.\\nConsider the ROA and CAPEX scatter diagram from Exhibit 3, which we elaborate \\non in Exhibit 4 by including the fitted regression line. This line represents the average \\nrelationship between ROA and CAPEX; not every observation falls on the line, but \\nthe line describes the mean relation between ROA and CAPEX.\\nExhibit 4: Fitted Regression Line of ROA and CAPEX\\nROA (Y,%)\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\n0\\n14\\n14\\n4\\n2\\n6\\n10\\n10\\n8\\n12\\n12\\nCAPEX (X,%)\\nRegression Line\\nObserved Values\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Estimating the Regression Line',\n",
       "       'page_number': 443,\n",
       "       'content': 'Estimating the Regression Line\\nWe cannot observe the population parameter values b0 and b1 in a regression model. \\nInstead, we observe only   ˆ \\nb  0   and   ˆ \\nb  1 , which are estimates (as indicated by the “hats” \\nabove the coefficients) of the population parameters based on the sample. Thus, pre-\\ndictions must be based on the parameters’ estimated values, and testing is based on \\nestimated values in relation to the hypothesized population values.\\nWe estimate the regression line as the line that best fits the observations. In simple \\nlinear regression, the estimated intercept,   ˆ \\nb  0  , and slope,   ˆ \\nb  1 , are such that the sum of \\nthe squared vertical distances from the observations to the fitted line is minimized. The \\nfocus is on the sum of the squared differences between the observations on Yiand the \\ncorresponding estimated value,   ˆ \\nY  i , on the regression line.\\nWe represent the value of the dependent variable for the ith observation that falls \\non the line as     ˆ \\nY    i   , which is equal to     ˆ \\nb    0   +    ˆ \\nb    1    X  i . The     ˆ \\nY    i    is what the estimated value of the \\nY variable would be for the ith observation based on the mean relationship between Y \\nand X. The residual for the ith observation,   e i , is how much the observed value of   Y i \\ndiffers from the   ˆ \\nY  i estimated using the regression line:   e i =  Y i −  ˆ \\nY  i . Note the subtle \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n434\\ndifference between the error term and the residual: The error term refers to the true \\nunderlying population relationship, whereas the residual refers to the fitted linear \\nrelation based on the sample.\\nFitting the line requires minimizing the sum of the squared residuals, the sum of \\nsquares error (SSE), also known as the residual sum of squares:\\n \\nSum of squares error =  ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n2  \\n  \\n \\n \\n=  ∑ i=1   \\nn   [ Y i −   ( ˆ \\nb  0 +  ˆ \\n b  1  X i )  ]  2   \\n \\n=  ∑ i=1  \\nn   e i 2 . \\n \\n \\n(4)\\nUsing least squares regression to estimate the values of the population parameters \\nof   b 0 and   b 1 , we can fit a line through the observations of X and Y that explains the \\nvalue that Y takes for any particular value of X.\\nAs seen in Exhibit 5, the residuals are represented by the vertical distances from \\nthe fitted line (see the third and fifth observations, Companies C and E, respectively) \\nand are, therefore, in the units of measurement represented by the dependent vari-\\nable. The residual is in the same unit of measurement as the dependent variable: If \\nthe dependent variable is in euros, the error term is in euros, and if the dependent \\nvariable is in growth rates, the error term is in growth rates.\\nExhibit 5: Residuals of the Linear Regression\\nROA (Y,%)\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\n0\\n14\\n14\\n4\\n2\\n6\\n10\\n10\\n8\\n12\\n12\\nCAPEX (X,%)\\nCompany C Residual:\\ne3 = Y3 – (b0 – b1X3)\\nCompany E Residual:\\ne5 = Y5 – (b0 – b1X5)\\nRegression Line\\nObserved Values of Y\\nPredicted Values of Y\\n^\\n^\\n^\\n^\\nHow do we calculate the intercept (  ˆ \\nb  0  ) and the slope (  ˆ \\nb  1 ) for a given sample of (Y, X) \\npairs of observations? The slope is the ratio of the covariance between Y and X to the \\nvariance of X, where   _\\n \\nY  is the mean of the Y variable and   _\\n \\nX  is the mean of X variable:\\n ˆ \\nb  1 =  Covariance of Y and X \\n \\n________________ \\n \\nVariance of X \\n =  \\n \\n ∑ i=1  \\nn  ( Y i −  \\n_\\n \\nY  ) ( X i −  \\n_\\n X  )   \\n________________ \\nn − 1 \\n  \\n \\n______________ \\n \\n \\n ∑ i=1  \\nn   ( X i −  \\n_\\n \\n X  ) 2   \\n___________ \\nn − 1 \\n  \\n . \\nSimplifying,\\n ˆ \\nb  1 =  \\n ∑ i=1  \\nn  ( Y i −  \\n_\\n \\nY  ) ( X i −  \\n_\\n X  )   \\n________________ \\n \\n ∑ i=1  \\nn   ( X i −  \\n_\\n \\nX  ) 2   \\n . \\n(5)\\n© CFA Institute. For candidate use only. Not for distribution.\\nEstimating the Parameters of a Simple Linear Regression\\n435\\nOnce we estimate the slope, we can then estimate the intercept using the mean of Y \\nand the mean of X:\\n ˆ \\nb  0 =  \\n_\\n \\nY  −  ˆ \\nb  1 \\n_\\nX  .  \\n(6)\\nWe show the calculation of the slope and the intercept in Exhibit 6.\\nExhibit 6: Estimating Slope and Intercept for the ROA Model\\nCompany\\nROA (Yi)\\nCAPEX (Xi)\\n ( Y i −  \\n_\\n \\nY  ) 2  \\n ( X i −  \\n_\\n \\nX  ) 2  \\n  ( Y i −  \\n_\\n \\nY  )   ( X i −  _\\n \\nX )   \\nA\\n6.0\\n0.7\\n42.25\\n29.16\\n35.10\\nB\\n4.0\\n0.4\\n72.25\\n32.49\\n48.45\\nC\\n15.0\\n5.0\\n6.25\\n1.21\\n−2.75\\nD\\n20.0\\n10.0\\n56.25\\n15.21\\n29.25\\nE\\n10.0\\n8.0\\n6.25\\n3.61\\n−4.75\\nF\\n20.0\\n12.5\\n56.25\\n40.96\\n48.00\\nSum\\n75.0\\n36.6\\n239.50\\n122.64\\n153.30\\nArithmetic mean\\n12.5\\n6.100\\n\\xa0\\n\\xa0\\n\\xa0\\nSlope coefficient:  ˆ \\nb  1 =  153.30 \\n_ \\n122.64  = 1.25. \\nIntercept:  ˆ \\nb  0 = 12.5 − (1.25 × 6.10 ) = 4.875 \\nROA regression model:   ˆ \\nY  i = 4.875 + 1.25  X i +  ε i . \\nNotice the similarity of the formula for the slope coefficient and that of the pairwise \\ncorrelation. The sample correlation, r, is the ratio of the covariance to the product of \\nthe standard deviations:\\n r =  \\nCovariance of Y and X \\n \\n \\n________________________________ \\n \\n \\n \\n  ( Standard deviation  \\n \\nof Y \\n )   ( Standard deviation  \\n \\nof X \\n )  \\n  . \\nThe subtle difference between the slope and the correlation formulas is in the denom-\\ninator: For the slope, this is the variance of the independent variable, but for the \\ncorrelation, the denominator is the product of the standard deviations. For our ROA \\nand CAPEX analysis,\\n Covariance of Y and X:  cov XY =  \\n ∑ i=1  \\nn  ( Y i −  \\n_\\n \\nY   ) ( X i −  \\n_\\n X  )  \\n________________ \\nn − 1 \\n =  153.30 \\n_ \\n5  = 30.66. \\n(7)\\nStandard deviation of Y and X:\\n \\n S Y =  √ \\n___________\\n \\n \\n \\n ∑ i=1  \\nn  ( Y i −  \\n_\\n \\n Y    ) 2  \\n___________ \\nn − 1 \\n  =  √ _ \\n 239.50 \\n_ \\n5   = 6.9210 ;\\n  \\n \\n \\n \\n S X =  √ \\n___________\\n \\n \\n \\n ∑ i=1  \\nn  ( X i −  \\n_\\n \\n X    ) 2  \\n___________ \\nn − 1 \\n  =  √ _ \\n 122.64 \\n_ \\n5   = 4.9526.\\n  \\n(8)\\n r =  \\n30.66 \\n_____________ \\n \\n(6.9210 ) (4.9526)  = 0.89458945. \\nBecause the denominators of both the slope and the correlation are positive, the sign \\nof the slope and the correlation are driven by the numerator: If the covariance is pos-\\nitive, both the slope and the correlation are positive, and if the covariance is negative, \\nboth the slope and the correlation are negative.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n436\\nHOW DO ANALYSTS PERFORM SIMPLE LINEAR REGRESSION? \\nTypically, an analyst will use the data analysis functions on a spreadsheet, such as Microsoft \\nExcel, or a statistical package in the R or Python programming languages to perform linear \\nregression analysis. The following are some of the more common choices in practice.\\nSimple Linear Regression: Intercept and Slope\\n■ \\nExcel: Use the INTERCEPT, SLOPE functions.\\n■ \\nR: Use the lm function.\\n■ \\nPython: Use the sm.OLS function in the statsmodels package.\\nCorrelations\\n■ \\nExcel: Use the CORREL function.\\n■ \\nR: Use the cor function in the stats library.\\n■ \\nPython: Use the corrcoef function in the numpy library.\\nNote that in R and Python, there are many choices for regression and correlation analysis.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Interpreting the Regression Coefficients',\n",
       "       'page_number': 446,\n",
       "       'content': 'Interpreting the Regression Coefficients\\nWhat is the meaning of the regression coefficients? The intercept is the value of the \\ndependent variable if the value of the independent variable is zero. Importantly, this \\ndoes not make sense in some contexts, especially if it is unrealistic that the independent \\nvariable would be zero. For example, if we have a model where money supply explains \\nGDP growth, the intercept has no meaning because, practically speaking, zero money \\nsupply is not possible. If the independent variable were money supply growth, however, \\nthe intercept is meaningful. The slope is the change in the dependent variable for a \\none-unit change in the independent variable. If the slope is positive, then the change \\nin the independent variable and that of the dependent variable will be in the same \\ndirection; if the slope is negative, the change in the independent variable and that of \\nthe dependent variable will be in opposite directions.\\nINTERPRETING POSITIVE AND NEGATIVE SLOPES\\nSuppose the dependent variable (Y) is in millions of euros and the independent variable \\n(X) is in millions of US dollars.\\nIf the slope is positive 1.2, then\\n↑ USD1 million → ↑ EUR1.2 million\\n↓ USD1 million → ↓ EUR1.2 million\\nIf the slope is negative 1.2, then\\n↑ USD1 million → ↓ EUR1.2 million\\n↓ USD1 million → ↑ EUR1.2 million\\nUsing the ROA regression model from Exhibit 6, we would interpret the estimated \\ncoefficients as follows:\\n■ \\nThe return on assets for a company is 4.875% if the company makes no cap-\\nital expenditures.\\n© CFA Institute. For candidate use only. Not for distribution.\\nEstimating the Parameters of a Simple Linear Regression\\n437\\n■ \\nIf CAPEX increases by one unit—say, from 4% to 5%—ROA increases by \\n1.25%.\\nUsing the estimated regression coefficients, we can determine the values of the \\ndependent variable if they follow the average relationship between the dependent \\nand independent variables. A result of the mathematics of the least squares fitting \\nof the regression line is that the expected value of the residual term is zero: E(ε) = 0.\\nWe show the calculation of the predicted dependent variable and residual term \\nfor each observation in the ROA example in Exhibit 7. Note that the sum and average \\nof  Y i and   ˆ \\nY  1 are the same, and the sum of the residuals is zero.\\nExhibit 7: Calculation of the Dependent Variable and Residuals for the ROA \\nand CAPEX Model\\n\\xa0\\n(1)\\n(2)\\n(3)\\n(4)\\nCompany\\nROA (Yi)\\nCAPEX \\n(Xi)\\nPredicted ROA \\n ( ˆ \\nY  i ) \\n(1) − (3) \\nResidual (ei)\\nA\\n6.0\\n0.7\\n5.750\\n0.250\\nB\\n4.0\\n0.4\\n5.375\\n−1.375\\nC\\n15.0\\n5.0\\n11.125\\n3.875\\nD\\n20.0\\n10.0\\n17.375\\n2.625\\nE\\n10.0\\n8.0\\n14.875\\n−4.875\\nF\\n20.0\\n12.5\\n20.500\\n−0.500\\nSum\\n75.0\\n36.6\\n75.000\\n0.000\\nAverage\\n12.5\\n6.1\\n12.5\\n0.000\\nFor Company C (i = 3),\\n ˆ \\nY  i =  ˆ \\nb  0 +  ˆ \\nb  1  X i +  ε i = 4.875 + 1.25  X i +  ε i  \\n ˆ \\nY  i = 4.875 + (1.25 × 5.0 ) = 4.875 + 6.25 = 11.125 \\n Y i −  ˆ \\nY  i =  e i = 15.0 − 11.125 = 3.875,\\xa0the\\xa0vertical\\xa0distance\\xa0in\\xa0Exhibit\\xa05. \\nWhereas the sum of the residuals must equal zero by design, the focus of fitting the \\nregression line in a simple linear regression is minimizing the sum of the squared \\nresidual terms.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Cross-Sectional vs. Time-Series Regressions',\n",
       "       'page_number': 447,\n",
       "       'content': 'Cross-Sectional vs. Time-Series Regressions\\nRegression analysis uses two principal types of data: cross sectional and time series. \\nA cross-sectional regression involves many observations of X and Y for the same \\ntime period. These observations could come from different companies, asset classes, \\ninvestment funds, countries, or other entities, depending on the regression model. \\nFor example, a cross-sectional model might use data from many companies to test \\nwhether predicted EPS growth explains differences in price-to-earnings ratios during \\na specific time period. Note that if we use cross-sectional observations in a regression, \\nwe usually denote the observations as i = 1, 2, . . . , n.\\nTime-series data use many observations from different time periods for the same \\ncompany, asset class, investment fund, country, or other entity, depending on the \\nregression model. For example, a time-series model might use monthly data from \\nmany years to test whether a country’s inflation rate determines its short-term interest \\nrates. If we use time-series data in a regression, we usually denote the observations \\nas t = 1, 2, . . . , T. Note that in the sections that follow, we primarily use the notation \\ni = 1, 2, . . . , n, even for time series.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n438\\nEXAMPLE 2\\nEstimating a Simple Linear Regression Model\\nAn analyst is exploring the relationship between a company’s net profit margin \\nand research and development expenditures. He collects data for an industry \\nand calculates the ratio of research and development expenditures to revenues \\n(RDR) and the net profit margin (NPM) for eight companies. Specifically, he \\nwants to explain the variation that he observes in the net profit margin by using \\nthe variation he observes in the companies’ research and development spending. \\nHe reports the data in Exhibit 8.\\n \\nExhibit 8: Observations on NPM and RDR for Eight \\nCompanies\\n \\n \\nCompany\\nNPM \\n(%)\\nRDR \\n(%)\\n1\\n4\\n8\\n2\\n5\\n10\\n3\\n10\\n6\\n4\\n9\\n5\\n5\\n5\\n7\\n6\\n6\\n9\\n7\\n12\\n5\\n8\\n3\\n10\\n \\n1. What is the slope coefficient for this simple linear regression model?\\nSolution to 1\\nThe slope coefficient for the regression model is −1.3, and the details for the \\ninputs to this calculation are in Exhibit 9.\\n \\nExhibit 9: Details of Calculation of Slope of NPM Regressed on RDR\\n \\n \\nCompany\\nNPM \\n(%)\\n(Yi)\\nRDR \\n(%) \\n(Xi)\\n Y i −  \\n_\\n \\nY  \\n X i −  \\n_\\nX   \\n ( Y i −  \\n_\\n \\nY  ) 2  \\n ( X i −  \\n_\\n \\nX  ) 2  \\n ( Y i −  \\n_\\n \\nY  ) ( \\nX i −  \\n_\\n \\nX  ) \\n1\\n4\\n8\\n−2.8\\n0.5\\n7.5625\\n0.25\\n−1.375\\n2\\n5\\n10\\n−1.8\\n2.5\\n3.0625\\n6.25\\n−4.375\\n3\\n10\\n6\\n3.3\\n−1.5\\n10.5625\\n2.25\\n−4.875\\n4\\n9\\n5\\n2.3\\n−2.5\\n5.0625\\n6.25\\n−5.625\\n5\\n5\\n7\\n−1.8\\n−0.5\\n3.0625\\n0.25\\n0.875\\n6\\n6\\n9\\n−0.8\\n1.5\\n0.5625\\n2.25\\n−1.125\\n7\\n12\\n5\\n5.3\\n−2.5\\n27.5625\\n6.25\\n−13.125\\n8\\n3\\n10\\n−3.8\\n2.5\\n14.0625\\n6.25\\n−9.375\\nSum\\n54.0\\n60.0\\n0.0\\n0.0\\n71.5000\\n30.00\\n−39.0\\nAverage\\n6.75\\n7.5\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\nEstimating the Parameters of a Simple Linear Regression\\n439\\nSlope coefficient:  ˆ \\nb  1 =  − 39 \\n_ \\n30  = − 1.3. \\n2. What is the intercept for this regression model?\\nSolution to 2\\nThe intercept of the regression model is 16.5:\\n Intercept:  ˆ \\nb  0 = 6.75 − (−\\u200a1.3 × 7.5 ) = 6.75 + 9.75 = 16.5 \\n3. How is this estimated linear regression model represented?\\nSolution to 3\\nThe regression model is represented by  ˆ \\nY  i = 16.5 − 1.3  X i +  ε i . \\n4. What is the pairwise correlation between NPM and RDR?\\nSolution to 4\\nThe pairwise correlation is −0.8421:\\n r =  \\n −\\u200a39 ⁄ 7  \\n_ \\n √ _ \\n 71.5 ⁄ 7   √ _ \\n 30 ⁄ 7    =  \\n−\\u200a5.5714 \\n_____________ \\n \\n(3.1960 ) (2.0702)  = −\\u200a0.8421. \\nEXAMPLE 3\\nInterpreting Regression Coefficients\\nAn analyst has estimated a model that regresses a company’s return on equity \\n(ROE) against its growth opportunities (GO), defined as the company’s three-year \\ncompounded annual growth rate in sales, over 20 years and produces the fol-\\nlowing estimated simple linear regression:\\n ROEi = 4 + 1.8 GOi + εi.\\nBoth variables are stated in percentages, so a GO observation of 5% is included \\nas 5.\\n1. The predicted value of the company’s ROE if its GO is 10% is closest to:\\nA. 1.8%.\\nB. 15.8%.\\nC. 22.0%.\\nSolution to 1\\nC is correct. The predicted value of ROE = 4 + (1.8 × 10) = 22.\\n2. The change in ROE for a change in GO from 5% to 6% is closest to:\\nA. 1.8%.\\nB. 4.0%.\\nC. 5.8%.\\nSolution to 2\\nA is correct. The slope coefficient of 1.8 is the expected change in the depen-\\ndent variable (ROE) for a one-unit change in the independent variable (GO).\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n440\\n3. The residual in the case of a GO of 8% and an observed ROE of 21% is clos-\\nest to:\\nA. −1.8%.\\nB. 2.6%.\\nC. 12.0%.\\nSolution to 3\\nB is correct. The predicted value is ROE = 4 + (1.8 × 8) = 18.4. The observed \\nvalue of ROE is 21, so the residual is 2.6 = 21.0 − 18.4.\\nASSUMPTIONS OF THE SIMPLE LINEAR REGRESSION \\nMODEL\\nexplain the assumptions underlying the simple linear regression \\nmodel, and describe how residuals and residual plots indicate if these \\nassumptions may have been violated\\nWe have discussed how to interpret the coefficients in a simple linear regression model. \\nNow we turn to the statistical assumptions underlying this model. Suppose that we \\nhave n observations of both the dependent variable, Y, and the independent variable, \\nX, and we want to estimate the simple linear regression of Y regressed on X. We need \\nto make the following four key assumptions to be able to draw valid conclusions from \\na simple linear regression model:\\n1. Linearity: The relationship between the dependent variable, Y, and the inde-\\npendent variable, X, is linear.\\n2. Homoskedasticity: The variance of the regression residuals is the same for \\nall observations.\\n3. Independence: The observations, pairs of Ys and Xs, are independent of \\none another. This implies the regression residuals are uncorrelated across \\nobservations.\\n4. Normality: The regression residuals are normally distributed.\\nNow we take a closer look at each of these assumptions and introduce the “best \\npractice” of examining residual plots of regression results to identify potential viola-\\ntions of these key assumptions.\\nAssumption 1: Linearity\\nWe are fitting a linear model, so we must assume that the true underlying relationship \\nbetween the dependent and independent variables is linear. If the relationship between \\nthe independent and dependent variables is nonlinear in the parameters, estimating \\nthat relation with a simple linear regression model will produce invalid results: The \\nmodel will be biased, because it will under- and overestimate the dependent variable \\nat certain points. For example,   Y i =  b 0  e  b 1 X i  +  ε i is nonlinear in b1, so we should not \\napply the linear regression model to it. Exhibit 10 shows an example of this exponential \\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Assumptions of the Simple Linear Regression Model',\n",
       "     'page_number': 450,\n",
       "     'content': 'Learning Module 7 \\nIntroduction to Linear Regression\\n440\\n3. The residual in the case of a GO of 8% and an observed ROE of 21% is clos-\\nest to:\\nA. −1.8%.\\nB. 2.6%.\\nC. 12.0%.\\nSolution to 3\\nB is correct. The predicted value is ROE = 4 + (1.8 × 8) = 18.4. The observed \\nvalue of ROE is 21, so the residual is 2.6 = 21.0 − 18.4.\\nASSUMPTIONS OF THE SIMPLE LINEAR REGRESSION \\nMODEL\\nexplain the assumptions underlying the simple linear regression \\nmodel, and describe how residuals and residual plots indicate if these \\nassumptions may have been violated\\nWe have discussed how to interpret the coefficients in a simple linear regression model. \\nNow we turn to the statistical assumptions underlying this model. Suppose that we \\nhave n observations of both the dependent variable, Y, and the independent variable, \\nX, and we want to estimate the simple linear regression of Y regressed on X. We need \\nto make the following four key assumptions to be able to draw valid conclusions from \\na simple linear regression model:\\n1. Linearity: The relationship between the dependent variable, Y, and the inde-\\npendent variable, X, is linear.\\n2. Homoskedasticity: The variance of the regression residuals is the same for \\nall observations.\\n3. Independence: The observations, pairs of Ys and Xs, are independent of \\none another. This implies the regression residuals are uncorrelated across \\nobservations.\\n4. Normality: The regression residuals are normally distributed.\\nNow we take a closer look at each of these assumptions and introduce the “best \\npractice” of examining residual plots of regression results to identify potential viola-\\ntions of these key assumptions.\\n',\n",
       "     'children': [{'title': 'Assumption 1: Linearity',\n",
       "       'page_number': 450,\n",
       "       'content': 'Assumption 1: Linearity\\nWe are fitting a linear model, so we must assume that the true underlying relationship \\nbetween the dependent and independent variables is linear. If the relationship between \\nthe independent and dependent variables is nonlinear in the parameters, estimating \\nthat relation with a simple linear regression model will produce invalid results: The \\nmodel will be biased, because it will under- and overestimate the dependent variable \\nat certain points. For example,   Y i =  b 0  e  b 1 X i  +  ε i is nonlinear in b1, so we should not \\napply the linear regression model to it. Exhibit 10 shows an example of this exponential \\n3\\n© CFA Institute. For candidate use only. Not for distribution.\\nAssumptions of the Simple Linear Regression Model\\n441\\nmodel, with a regression line indicated. You can see that this line does not fit this \\nrelationship well: For lower and higher values of X, the linear model underestimates \\nthe Y, whereas for the middle values, the linear model overestimates Y.\\nExhibit 10: Illustration of Nonlinear Relationship Estimated as a Linear \\nRelationship\\nY\\nX\\nAnother implication of this assumption is that the independent variable, X, must \\nnot be random; that is, it is non-stochastic. If the independent variable is random, \\nthere would be no linear relation between the dependent and independent variables. \\nAlthough we may initially assume that the independent variable in the regression \\nmodel is not random, that assumption may not always be true.\\nWhen we look at the residuals of a model, what we would like to see is that the \\nresiduals are random. The residuals should not exhibit a pattern when plotted against \\nthe independent variable. As we show in Exhibit 11, the residuals from the Exhibit \\n10 linear regression do not appear to be random but, rather, exhibit a relationship \\nwith the independent variable, X, falling for some range of X and rising in another.\\nExhibit 11: Illustration of Residuals in a Nonlinear Relationship Estimated \\nas a Linear Relationship\\nY\\nX\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n442\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Assumption 2: Homoskedasticity',\n",
       "       'page_number': 452,\n",
       "       'content': 'Assumption 2: Homoskedasticity\\nAssumption 2, that the variance of the residuals is the same for all observations, is \\nknown as the homoskedasticity assumption. In terms of notation, this assumption \\nrelates to the squared residuals:\\n E  ( ε i 2 )  =  σ ε 2 , i = 1, . . . , n. \\n(9)\\nIf the residuals are not homoscedastic, that is, if the variance of residuals differs across \\nobservations, then we refer to this as heteroskedasticity.\\nSuppose you are examining a time series of short-term interest rates as the depen-\\ndent variable and inflation rates as the independent variable over 16 years. We may \\nbelieve that short-term interest rates (Y) and inflation rates (X) should be related (that \\nis, interest rates are higher with higher rates of inflation. If this time series spans many \\nyears, with different central bank actions that force short-term interest rates to be \\n(artificially) low for the last eight years of the series, then it is likely that the residuals \\nin this estimated model will appear to come from two different models. We will refer \\nto the first eight years as Regime 1 (normal rates) and the second eight years as Regime \\n2 (low rates). If the model fits differently in the two regimes, the residuals and their \\nvariances will be different.\\nYou can see this situation in Exhibit 12, which shows a scatter plot with an estimated \\nregression line. The slope of the regression line over all 16 years is 1.1979.\\nExhibit 12: Scatter Plot of Interest Rates (Y) and Inflation Rates (X)\\nShort-Term Interest Rate (Y,%)\\n5.0\\n5.0\\n4.0\\n4.0\\n3.0\\n3.0\\n2.0\\n2.0\\n1.0\\n1.0\\n0.5\\n0.5\\n4.5\\n4.5\\n3.5\\n3.5\\n2.5\\n2.5\\n1.5\\n1.5\\n0\\n0\\n3.0\\n3.0\\n1.0\\n1.0\\n0.5\\n0.5\\n2.0\\n2.0\\n1.5\\n1.5\\n2.5\\n2.5\\nRate of Inflation (X,%)\\nY = 0.9954 + 1.1979X\\nRegression Line: All Years\\nShort-Term Interest Rate (Y,%)\\nWe plot the residuals of this model in Exhibit 13 against the years. In this plot, we \\nindicate the distance that is two standard deviations from zero (the mean of the \\nresiduals) for the first eight years’ residuals and then do the same for the second \\neight years. As you can see, the residuals appear different for the two regimes: the \\nvariation in the residuals for the first eight years is much smaller than the variation \\nfor the second eight years.\\n© CFA Institute. For candidate use only. Not for distribution.\\nAssumptions of the Simple Linear Regression Model\\n443\\nExhibit 13: Residual Plot for Interest Rates (Y) vs. Inflation Rates (X) Model\\n0\\nResidual (%)\\n1.0\\n0.6\\n0.6\\n0.8\\n0.8\\n0.4\\n0.4\\n0.2\\n0.2\\n–0.2\\n–0.2\\n–0.4\\n–0.4\\n–0.6\\n–0.6\\n–0.8\\n–0.8\\n–1.0\\n–1.0\\n1\\n16\\n16\\n5\\n2\\n3\\n4\\n6\\n7\\n8\\n10\\n10\\n1111\\n12\\n12\\n14\\n14\\n15\\n15\\n9\\n13\\n13\\nYear\\n2 Standard Deviations below the Zero\\n2 Standard Deviations above the Zero\\nResiduals Regime 1\\nResiduals Regime 2\\nWhy does this happen? The model seems appropriate, but when we examine the \\nresiduals (Exhibit 13), an important step in assessing the model fit, we see that the \\nmodel fits better in some years compared with others. The difference in variance of \\nresiduals between the two regimes is apparent from the much wider band around \\nresiduals for Regime 2 (the low-rate period). This indicates a clear violation of the \\nhomoskedasticity assumption.\\nIf we estimate a regression line for each regime, we can see that the model for the \\ntwo regimes is quite different, as we show in Exhibit 14. In the case of Regime 1 (nor-\\nmal rates), the slope is 1.0247, whereas in Regime 2 (low rates) the slope is −0.2805. \\nIn sum, the clustering of residuals in two groups with much different variances clearly \\nindicates the existence of distinct regimes for the relationship between short-term \\ninterest rates and the inflation rate.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n444\\nExhibit 14: Fitted Regression Lines for the Two Regimes\\nShort-Term Interest Rate (Y,%)\\n5.0\\n5.0\\n4.0\\n4.0\\n3.0\\n3.0\\n2.0\\n2.0\\n1.0\\n1.0\\n0.5\\n0.5\\n4.5\\n4.5\\n3.5\\n3.5\\n2.5\\n2.5\\n1.5\\n1.5\\n0\\n0\\n3.0\\n3.0\\n1.0\\n1.0\\n0.5\\n0.5\\n2.0\\n2.0\\n1.5\\n1.5\\n2.5\\n2.5\\nRate of Inflation (X,%)\\nRegime 2:\\nY = 1.6440 – 0.2805X\\nRegime 1:\\nY = 1.4372 + 1.0247X\\nRegime 1 Regression Line\\nRegime 1\\nRegime 2 Regression Line\\nRegime 2\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Assumption 3: Independence',\n",
       "       'page_number': 454,\n",
       "       'content': 'Assumption 3: Independence\\nWe assume that the observations (Y and X pairs) are uncorrelated with one another, \\nmeaning they are independent. If there is correlation between observations (that is, \\nautocorrelation), they are not independent and the residuals will be correlated. The \\nassumption that the residuals are uncorrelated across observations is also necessary \\nfor correctly estimating the variances of the estimated parameters of   b 0 and   b 1 (i.e., \\n ˆ \\nb  0  and  ˆ \\nb  1 ) that we use in hypothesis tests of the intercept and slope, respectively. It is \\nimportant to examine whether the residuals exhibit a pattern, suggesting a violation of \\nthis assumption. Therefore, we need to visually and statistically examine the residuals \\nfor a regression model.\\nConsider the quarterly revenues of a company regressed over 40 quarters, as shown \\nin Exhibit 15, with the regression line included. It is clear that these revenues display \\na seasonal pattern, an indicator of autocorrelation.\\n© CFA Institute. For candidate use only. Not for distribution.\\nAssumptions of the Simple Linear Regression Model\\n445\\nExhibit 15: Regression of Quarterly Revenues vs. Time (40 Quarters)\\nRevenues (millions, €)(Y)\\n140,000\\n130,000\\n130,000\\n120,000\\n120,000\\n115,000\\n115,000\\n110,000\\n110,000\\n135,000\\n135,000\\n125,000\\n125,000\\n100,000\\n100,000\\n95,000\\n95,000\\n105,000\\n105,000\\n90,000\\n90,000\\n1\\n5\\n17\\n17\\n3\\n19\\n19\\n21\\n21\\n7\\n23\\n23\\n25\\n25\\n1111\\n27\\n27\\n37\\n37\\n35\\n35\\n33\\n33\\n31\\n31\\n29\\n29\\n39\\n39\\n15\\n15\\n9\\n13\\n13\\nQuarter (X)\\nY = 108,502.3 + 609.6427\\nObserved Quarterly Revenues: 1st-3rd Quarters\\nObserved Quarterly Revenues: 4th Quarter\\nIn Exhibit 16, we plot the residuals from this model and see that there is a pattern. \\nThese residuals are correlated, specifically jumping up in Quarter 4 and then falling \\nback the subsequent quarter. In sum, the patterns in both Exhibits 15 and 16 indicate \\na violation of the assumption of independence.\\nExhibit 16: Residual Plot for Quarterly Revenues vs. Time Model\\n0\\nResiduals (millions, €)\\n15,000\\n10,000\\n10,000\\n5,000\\n5,000\\n–5,000\\n–5,000\\n–10,000\\n–10,000\\n1\\n5\\n17\\n17\\n3\\n19\\n19\\n21\\n21\\n7\\n23\\n23\\n25\\n25\\n1111\\n27\\n27\\n37\\n37\\n35\\n35\\n33\\n33\\n31\\n31\\n29\\n29\\n39\\n39\\n15\\n15\\n9\\n13\\n13\\nQuarter\\nResiduals: 1st-3rd Quarters\\nResiduals: 4th Quarter\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Assumption 4: Normality',\n",
       "       'page_number': 455,\n",
       "       'content': 'Assumption 4: Normality\\nThe assumption of normality requires that the residuals be normally distributed. \\nThis does not mean that the dependent and independent variables must be normally \\ndistributed; it only means that the residuals from the model are normally distributed. \\nHowever, in estimating any model, it is good practice to understand the distribution of \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n446\\nthe dependent and independent variables to explore for outliers. An outlier in either \\nor both variables can substantially influence the fitted line such that the estimated \\nmodel will not fit well for most of the other observations.\\nWith normally distributed residuals, we can test a particular hypothesis about a \\nlinear regression model. For large sample sizes, we may be able to drop the assumption \\nof normality by appealing to the central limit theorem; asymptotic theory (which deals \\nwith large samples) shows that in many cases, the test statistics produced by standard \\nregression programs are valid even if the model’s residuals are not normally distributed.\\nEXAMPLE 4\\nAssumptions of Simple Linear Regression\\n1. An analyst is investigating a company’s revenues and estimates a simple lin-\\near time-series model by regressing revenues against time, where time—1, \\n2, . . . , 15—is measured in years. She plots the company’s observed revenues \\nand the estimated regression line, as shown in Exhibit 17. She also plots the \\nresiduals from this regression model, as shown in Exhibit 18.\\n \\nExhibit 17: Revenues vs. Time Using Simple Linear Regression\\n \\nRevenues (millions, €)(Y)\\n900\\n700\\n700\\n500\\n500\\n400\\n400\\n300\\n300\\n800\\n800\\n600\\n600\\n100\\n100\\n200\\n200\\n0\\n1\\n5\\n3\\n2\\n4\\n7\\n6\\n8\\n1111\\n10\\n10\\n14\\n14\\n12\\n12\\n15\\n15\\n9\\n13\\n13\\nYear (X)\\nObserved Revenues\\nLinear Prediction of Revenues\\n \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Analysis of Variance',\n",
       "     'page_number': 457,\n",
       "     'content': 'Analysis of Variance\\n447\\nExhibit 18: Residual Plot for Revenues vs. Time\\n \\nRevenues (millions, €)(Y)\\n150\\n100\\n100\\n50\\n50\\n–50\\n–50\\n0\\n–100\\n–100\\n1\\n5\\n3\\n2\\n4\\n7\\n6\\n8\\n1111\\n10\\n10\\n14\\n14\\n12\\n12\\n15\\n15\\n9\\n13\\n13\\nYear (X)\\nBased on Exhibits 17 and 18, describe which assumption(s) of simple linear \\nregression the analyst’s model may be violating.\\nSolution\\nThe correct model is not linear, as evident from the pattern of the revenues \\nin Exhibit 17. In the earlier years (i.e., 1 and 2) and later years (i.e., 14 and \\n15), the linear model underestimates revenues, whereas for the middle years \\n(i.e., 7–11), the linear model overestimates revenues. Moreover, the curved \\npattern of residuals in Exhibit 18 indicates potential heteroskedasticity \\n(residuals have unequal variances), lack of independence of observations, \\nand non-normality (a concern given the small sample size of n = 15). In sum, \\nthe analyst should be concerned that her model violates all the assumptions \\ngoverning simple linear regression (linearity, homoskedasticity, indepen-\\ndence, and normality).\\nANALYSIS OF VARIANCE\\ncalculate and interpret the coefficient of determination and the \\nF-statistic in a simple linear regression\\ndescribe the use of analysis of variance (ANOVA) in regression \\nanalysis, interpret ANOVA results, and calculate and interpret the \\nstandard error of estimate in a simple linear regression\\nThe simple linear regression model sometimes describes the relationship between \\ntwo variables quite well, but sometimes it does not. We must be able to distinguish \\nbetween these two cases to use regression analysis effectively. Remember our goal \\nis to explain the variation of the dependent variable. So, how well has this goal been \\nachieved, given our choice of independent variable?\\n4\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n448\\n',\n",
       "     'children': [{'title': 'Breaking down the Sum of Squares Total into Its Components',\n",
       "       'page_number': 458,\n",
       "       'content': 'Breaking down the Sum of Squares Total into Its Components\\nWe begin with the sum of squares total and then break it down into two parts: the \\nsum of squares error and the sum of squares regression (SSR). The sum of squares \\nregression is the sum of the squared differences between the predicted value of the \\ndependent variable,   ˆ \\nY  i , based on the estimated regression line, and the mean of the \\ndependent variable,   _\\n \\nY  :\\n ∑ i=1  \\nn   ( ˆ \\nY  i −  \\n_\\n \\nY  ) \\n2 . \\n(10)\\nWe have already defined the sum of squares total, which is the total variation in Y, \\nand the sum of squares error, the unexplained variation in Y. Note that the sum of \\nsquares regression is the explained variation in Y. So, as illustrated in Exhibit 19, \\nSST = SSR + SSE, meaning total variation in Y equals explained variation in Y plus \\nunexplained variation in Y.\\nExhibit 19: Breakdown of Variation of Dependent Variable\\nSum of Squares Total (SST)\\nni=1(Yi – Y)2\\n–\\nΣ\\nSum of Squares Error (SSE) \\nni=1(Yi – Yi)2\\n^\\nΣ\\nSum of Squares Regression (SSR)\\n(Yi – Y)2\\ni=1\\nn\\n–\\nΣ\\n^\\nWe show the breakdown of the sum of squares total formula for our ROA regression \\nexample in Exhibit 20. The total variation of ROA that we want to explain (SST) is \\n239.50. This number comprises the variation unexplained (SSE), 47.88, and the vari-\\nation explained (SSR), 191.63. These sum of squares values are important inputs into \\nmeasures of the fit of the regression line.\\nExhibit 20: Breakdown of Sum of Squares Total for ROA Model\\nCompany\\nROA \\n(Yi)\\nCAPEX \\n(Xi)\\nPredicted \\nROA \\n ( ˆ \\nY  ) \\nVariation \\nto Be \\nExplained \\n ( Y i −  \\n_\\n \\nY  ) 2  \\nVariation \\nUnexplained \\n ( Y i −  ˆ \\nY  i ) \\n2  \\nVariation \\nExplained \\n ( ˆ \\nY  i −  \\n_\\n \\nY  ) \\n2  \\nA\\n6.0\\n0.7\\n5.750\\n42.25\\n0.063\\n45.563\\nB\\n4.0\\n0.4\\n5.375\\n72.25\\n1.891\\n50.766\\nC\\n15.0\\n5.0\\n11.125\\n6.25\\n15.016\\n1.891\\n© CFA Institute. For candidate use only. Not for distribution.\\nAnalysis of Variance\\n449\\nCompany\\nROA \\n(Yi)\\nCAPEX \\n(Xi)\\nPredicted \\nROA \\n ( ˆ \\nY  ) \\nVariation \\nto Be \\nExplained \\n ( Y i −  \\n_\\n \\nY  ) 2  \\nVariation \\nUnexplained \\n ( Y i −  ˆ \\nY  i ) \\n2  \\nVariation \\nExplained \\n ( ˆ \\nY  i −  \\n_\\n \\nY  ) \\n2  \\nD\\n20.0\\n10.0\\n17.375\\n56.25\\n6.891\\n23.766\\nE\\n10.0\\n8.0\\n14.875\\n6.25\\n23.766\\n5.641\\nF\\n20.0\\n12.5\\n20.500\\n56.25\\n0.250\\n64.000\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n239.50\\n47.88\\n191.625\\nMean\\n12.50\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSum of squares total = 239.50.\\nSum of squares error = 47.88.\\nSum of squares regression = 191.63.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Measures of Goodness of Fit',\n",
       "       'page_number': 459,\n",
       "       'content': 'Measures of Goodness of Fit\\nThere are several measures that we can use to evaluate goodness of fit—that is, how \\nwell the regression model fits the data. These include the coefficient of determination, \\nthe F-statistic for the test of fit, and the standard error of the regression.\\nThe coefficient of determination, also referred to as the R-squared or R2, is the \\npercentage of the variation of the dependent variable that is explained by the inde-\\npendent variable:\\n \\nCoefficient of determination =  \\nSum of squares regression \\n \\n___________________ \\n \\nSum of squares total  \\n  \\n \\n \\n \\n \\nCoefficient of determination =  \\n ∑ i=1  \\nn  ( ˆ \\n Y i −  \\n_\\n \\n Y    ) 2  \\n___________ \\n ∑ i=1   \\nn  ( Y i −  \\n_\\n \\nY    ) 2   .\\n \\n \\n(11)\\nBy construction, the coefficient of determination ranges from 0% to 100%. In our \\nROA example, the coefficient of determination is 191.625 ÷ 239.50, or 0.8001, so \\n80.01% of the variation in ROA is explained by CAPEX. In a simple linear regression, \\nthe square of the pairwise correlation is equal to the coefficient of determination:\\n r 2 =  \\n ∑ i=1  \\nn   ( ˆ \\n Y i −  \\n_\\n \\nY  ) \\n 2  \\n___________ \\n \\n ∑ i=1  \\nn   ( Y i −  \\n_\\n \\nY  ) \\n2 \\n  =  R 2 . \\nIn our earlier ROA regression analysis, r = 0.8945, so we now see that r2 is indeed \\nequal to the coefficient of determination (R2), since (0.8945)2 = 0.8001.\\nWhereas the coefficient of determination—the portion of the variation of the \\ndependent variable explained by the independent variable—is descriptive, it is not a \\nstatistical test. To see if our regression model is likely to be statistically meaningful, \\nwe will need to construct an F-distributed test statistic.\\nIn general, we use an F-distributed test statistic to compare two variances. In \\nregression analysis, we can use an F-distributed test statistic to test whether the \\nslopes in a regression are equal to zero, with the slopes designated as bi, against the \\nalternative hypothesis that at least one slope is not equal to zero:\\n H0: b1 = b2 = b3 =. . . = bk= 0.\\n Ha: At least one bkis not equal to zero.\\nFor simple linear regression, these hypotheses simplify to\\n H0: b1 = 0.\\n H a: b1 ≠ 0.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n450\\nThe F-distributed test statistic is constructed by using the sum of squares regression \\nand the sum of squares error, each adjusted for degrees of freedom; in other words, it \\nis the ratio of two variances. We divide the sum of squares regression by the number \\nof independent variables, represented by k. In the case of a simple linear regression, \\nk = 1, so we arrive at the mean square regression (MSR), which is the same as the \\nsum of squares regression:\\n MSR =  \\nSum of squares regression \\n \\n___________________ \\nk \\n =  \\n ∑ i=1  \\nn  ( ˆ \\n Y i −  \\n_\\n \\n Y    ) 2  \\n___________ \\n1 \\n . \\nSo, for simple linear regression,\\n MSR =  ∑ i=1  \\nn  ( ˆ \\n Y i −  \\n_\\n \\nY    ) 2 . \\n(12)\\nNext, we calculate the mean square error (MSE), which is the sum of squares error \\ndivided by the degrees of freedom, which are n − k − 1. In simple linear regression, \\nn − k − 1 becomes n − 2:\\n MSE =  Sum of squares error \\n \\n_______________ \\nn − k − 1  \\n . \\n MSE =  \\n ∑ i=1  \\nn  ( Y i −  ˆ \\n  Y i   ) 2  \\n___________ \\nn − 2 \\n . \\n(13)\\nTherefore, the F-distributed test statistic (MSR/MSE) is\\n F =  \\n \\nSum of squares regression \\n \\n___________________ \\nk \\n  \\n \\n________________ \\n \\n \\nSum of squares error \\n \\n_______________ \\nn − k − 1  \\n \\n  =  MSR \\n_ \\nMSE  \\n F =  \\n \\n ∑ i=1  \\nn   ( ˆ \\nY  i −  \\n_\\n \\nY  ) \\n 2   \\n___________ \\n1 \\n  \\n_ \\n \\n ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n 2   \\n___________ \\nn − 2 \\n  ,  \\n(14)\\nwhich is distributed with 1 and n − 2 degrees of freedom in simple linear regression. \\nThe F-statistic in regression analysis is one sided, with the rejection region on the right \\nside, because we are interested in whether the variation in Y explained (the numerator) \\nis larger than the variation in Y unexplained (the denominator).\\nANOVA and Standard Error of Estimate in Simple Linear \\nRegression\\nWe often represent the sums of squares from a regression model in an analysis of \\nvariance (ANOVA) table, as shown in Exhibit 21, which presents the sums of squares, \\nthe degrees of freedom, the mean squares, and the F-statistic. Notice that the variance \\nof the dependent variable is the ratio of the sum of squares total to n − 1.\\nExhibit 21: Analysis of Variance Table for Simple Linear Regression\\nSource\\nSum of Squares\\nDegrees of \\nFreedom\\nMean Square\\nF-Statistic\\nRegression\\n SSR =  ∑ i=1  \\nn   ( ˆ \\nY  i −  _\\n \\nY  ) \\n2  \\n1\\n MSR =   ∑ i=1  \\nn   ( ˆ \\nY  i −  _\\n \\nY  ) \\n 2   \\n___________ \\n1 \\n \\n F =  MSR \\n_ \\nMSE  =  \\n  ∑ i=1  \\nn   ( ˆ \\nY  i −  _\\n \\nY  ) \\n 2   \\n___________ \\n1 \\n  \\n_ \\n  ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n 2   \\n____________ \\nn − 2 \\n \\n  \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'ANOVA and Standard Error of Estimate in Simple Linear Regression',\n",
       "       'page_number': 460,\n",
       "       'content': 'Learning Module 7 \\nIntroduction to Linear Regression\\n450\\nThe F-distributed test statistic is constructed by using the sum of squares regression \\nand the sum of squares error, each adjusted for degrees of freedom; in other words, it \\nis the ratio of two variances. We divide the sum of squares regression by the number \\nof independent variables, represented by k. In the case of a simple linear regression, \\nk = 1, so we arrive at the mean square regression (MSR), which is the same as the \\nsum of squares regression:\\n MSR =  \\nSum of squares regression \\n \\n___________________ \\nk \\n =  \\n ∑ i=1  \\nn  ( ˆ \\n Y i −  \\n_\\n \\n Y    ) 2  \\n___________ \\n1 \\n . \\nSo, for simple linear regression,\\n MSR =  ∑ i=1  \\nn  ( ˆ \\n Y i −  \\n_\\n \\nY    ) 2 . \\n(12)\\nNext, we calculate the mean square error (MSE), which is the sum of squares error \\ndivided by the degrees of freedom, which are n − k − 1. In simple linear regression, \\nn − k − 1 becomes n − 2:\\n MSE =  Sum of squares error \\n \\n_______________ \\nn − k − 1  \\n . \\n MSE =  \\n ∑ i=1  \\nn  ( Y i −  ˆ \\n  Y i   ) 2  \\n___________ \\nn − 2 \\n . \\n(13)\\nTherefore, the F-distributed test statistic (MSR/MSE) is\\n F =  \\n \\nSum of squares regression \\n \\n___________________ \\nk \\n  \\n \\n________________ \\n \\n \\nSum of squares error \\n \\n_______________ \\nn − k − 1  \\n \\n  =  MSR \\n_ \\nMSE  \\n F =  \\n \\n ∑ i=1  \\nn   ( ˆ \\nY  i −  \\n_\\n \\nY  ) \\n 2   \\n___________ \\n1 \\n  \\n_ \\n \\n ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n 2   \\n___________ \\nn − 2 \\n  ,  \\n(14)\\nwhich is distributed with 1 and n − 2 degrees of freedom in simple linear regression. \\nThe F-statistic in regression analysis is one sided, with the rejection region on the right \\nside, because we are interested in whether the variation in Y explained (the numerator) \\nis larger than the variation in Y unexplained (the denominator).\\nANOVA and Standard Error of Estimate in Simple Linear \\nRegression\\nWe often represent the sums of squares from a regression model in an analysis of \\nvariance (ANOVA) table, as shown in Exhibit 21, which presents the sums of squares, \\nthe degrees of freedom, the mean squares, and the F-statistic. Notice that the variance \\nof the dependent variable is the ratio of the sum of squares total to n − 1.\\nExhibit 21: Analysis of Variance Table for Simple Linear Regression\\nSource\\nSum of Squares\\nDegrees of \\nFreedom\\nMean Square\\nF-Statistic\\nRegression\\n SSR =  ∑ i=1  \\nn   ( ˆ \\nY  i −  _\\n \\nY  ) \\n2  \\n1\\n MSR =   ∑ i=1  \\nn   ( ˆ \\nY  i −  _\\n \\nY  ) \\n 2   \\n___________ \\n1 \\n \\n F =  MSR \\n_ \\nMSE  =  \\n  ∑ i=1  \\nn   ( ˆ \\nY  i −  _\\n \\nY  ) \\n 2   \\n___________ \\n1 \\n  \\n_ \\n  ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n 2   \\n____________ \\nn − 2 \\n \\n  \\n© CFA Institute. For candidate use only. Not for distribution.\\nAnalysis of Variance\\n451\\nSource\\nSum of Squares\\nDegrees of \\nFreedom\\nMean Square\\nF-Statistic\\nError\\n SSE =  ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n2  \\nn − 2\\n MSE =   ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n 2   \\n____________ \\nn − 2 \\n \\n\\xa0\\nTotal\\n SST =  ∑ i=1  \\nn   ( Y i −  _\\n \\nY  ) 2  \\nn − 1\\n\\xa0\\n\\xa0\\nFrom the ANOVA table, we can also calculate the standard error of the estimate (se), \\nwhich is also known as the standard error of the regression or the root mean square \\nerror. The se is a measure of the distance between the observed values of the depen-\\ndent variable and those predicted from the estimated regression; the smaller the se, \\nthe better the fit of the model. The se, along with the coefficient of determination and \\nthe F-statistic, is a measure of the goodness of the fit of the estimated regression line. \\nUnlike the coefficient of determination and the F-statistic, which are relative measures \\nof fit, the standard error of the estimate is an absolute measure of the distance of the \\nobserved dependent variable from the regression line. Thus, the se is an important \\nstatistic used to evaluate a regression model and is used in calculating prediction inter-\\nvals and performing tests on the coefficients. The calculation of se is straightforward \\nonce we have the ANOVA table because it is the square root of the MSE:\\n Standard error of the estimate  ( s e ) =  √ _ \\nMSE  =  √ \\n___________\\n \\n \\n \\n ∑ i=1  \\nn   ( Y i −  ˆ \\nY  i ) \\n 2   \\n___________ \\nn − 2 \\n  . \\n(15)\\nWe show the ANOVA table for our ROA regression example in Exhibit 22, using the \\ninformation from Exhibit 20. For a 5% level of significance, the critical F-value for the \\ntest of whether the model is a good fit (that is, whether the slope coefficient is different \\nfrom zero) is 7.71. We can get this critical value in the following ways:\\n■ \\nExcel [F.INV(0.95,1,4)]\\n■ \\nR [qf(.95,1,4)]\\n■ \\nPython [from scipy.stats import f and f.ppf(.95,1,4)]\\nWith a calculated F-statistic of 16.0104 and a critical F-value of 7.71, we reject the \\nnull hypothesis and conclude that the slope of our simple linear regression model for \\nROA is different from zero.\\nExhibit 22: ANOVA Table for ROA Regression Model\\nSource\\nSum of Squares\\nDegrees \\nof \\nFreedom\\nMean Square\\nF-Statistic\\nRegression\\n191.625\\n1\\n191.625\\n16.0104\\nError\\n47.875\\n4\\n11.96875\\n\\xa0\\nTotal\\n239.50\\n5\\n\\xa0\\n\\xa0\\nThe calculations to derive the ANOVA table and ultimately to test the goodness of \\nfit of the regression model can be time consuming, especially for samples with many \\nobservations. However, statistical packages, such as SAS, SPSS Statistics, and Stata, \\nas well as software, such as Excel, R, and Python, produce the ANOVA table as part \\nof the output for regression analysis. \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n452\\nEXAMPLE 5\\nUsing ANOVA Table Results to Evaluate a Simple Linear \\nRegression\\nSuppose you run a cross-sectional regression for 100 companies, where the \\ndependent variable is the annual return on stock and the independent variable \\nis the lagged percentage of institutional ownership (INST). The results of this \\nsimple linear regression estimation are shown in Exhibit 23. Evaluate the model \\nby answering the questions below.\\n \\nExhibit 23: ANOVA Table for Annual Stock Return Regressed on \\nInstitutional Ownership\\n \\n \\nSource\\nSum of Squares\\nDegrees of \\nFreedom\\nMean Square\\nRegression\\n576.1485\\n1\\n576.1485\\nError\\n1,873.5615\\n98\\n19.1180\\nTotal\\n2,449.7100\\n\\xa0\\n\\xa0\\n \\n1. What is the coefficient of determination for this regression model?\\nSolution to 1\\nThe coefficient of determination is sum of squares regression/sum of \\nsquares total: 576.148 ÷ 2,449.71 = 0.2352, or 23.52%.\\n2. What is the standard error of the estimate for this regression model?\\nSolution to 2\\nThe standard error of the estimate is the square root of the mean square \\nerror:  √ _ \\n19.1180  = 4.3724 . \\n3. At a 5% level of significance, do we reject the null hypothesis of the slope \\ncoefficient equal to zero if the critical F-value is 3.938?\\nSolution to 3\\nUsing a six-step process for testing hypotheses, we get the following:\\n \\nStep 1\\nState the hypotheses.\\nH0: b1 = 0 versus Ha: b1 ≠ 0\\nStep 2\\nIdentify the appropriate test \\nstatistic.\\n F =  MSR \\n_ \\nMSE  \\nwith 1 and 98 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5% (one tail, right side).\\nStep 4\\nState the decision rule.\\nCritical F-value = 3.938. \\nReject the null hypothesis if the calculated F-statistic is greater than 3.938.\\nStep 5\\nCalculate the test statistic.\\n F =  576.1485 \\n_ \\n19.1180  = 30.1364 \\nStep 6\\nMake a decision.\\nReject the null hypothesis because the calculated F-statistic is greater than \\nthe critical F-value. There is sufficient evidence to indicate that the slope \\ncoefficient is different from 0.0.\\n© CFA Institute. For candidate use only. Not for distribution.\\nHypothesis Testing of Linear Regression Coefficients\\n453\\n 4. Based on your answers to the preceding questions, evaluate this simple \\nlinear regression model.\\nSolution to 4\\nThe coefficient of determination indicates that variation in the independent \\nvariable explains 23.52% of the variation in the dependent variable. Also, \\nthe F-statistic test confirms that the model’s slope coefficient is different \\nfrom 0 at the 5% level of significance. In sum, the model seems to fit the data \\nreasonably well.\\nHYPOTHESIS TESTING OF LINEAR REGRESSION \\nCOEFFICIENTS\\nformulate a null and an alternative hypothesis about a population \\nvalue of a regression coefficient, and determine whether the null \\nhypothesis is rejected at a given level of significance\\nHypothesis Tests of the Slope Coefficient\\nWe can use the F-statistic to test for the significance of the slope coefficient (that is, \\nwhether it is significantly different from zero), but we also may want to perform other \\nhypothesis tests for the slope coefficient—for example, testing whether the population \\nslope is different from a specific value or whether the slope is positive. We can use \\na t-distributed test statistic to test such hypotheses about a regression coefficient.\\nSuppose we want to check a stock’s valuation using the market model; we hypoth-\\nesize that the stock has an average systematic risk (i.e., risk similar to that of the \\nmarket), as represented by the coefficient on the market returns variable. Or we may \\nwant to test the hypothesis that economists’ forecasts of the inflation rate are unbiased \\n(that is, on average, not overestimating or underestimating actual inflation rates). In \\neach case, does the evidence support the hypothesis? Such questions as these can be \\naddressed with hypothesis tests on the regression slope. To test a hypothesis about a \\nslope, we calculate the test statistic by subtracting the hypothesized population slope \\n(B1) from the estimated slope coefficient (  ˆ \\nstandard error of the slope coefficient,   s  ˆ b  1 ) and then dividing this difference by the \\nb   1   :\\n t =  \\n ˆ \\nb  1 −  B 1  \\n_ \\n s  ˆ \\nb  1    . \\n(16)\\nThis test statistic is t-distributed with n − k − 1 or n − 2 degrees of freedom because \\ntwo parameters (an intercept and a slope) were estimated in the regression.\\nThe standard error of the slope coefficient (  s  ˆ \\nb   1   ) for a simple linear regression \\nis the ratio of the model’s standard error of the estimate (se) to the square root of the \\nvariation of the independent variable:\\n s  ˆ \\nb   1   =  \\n s e  \\n____________ \\n \\n √ \\n_____________ \\n ∑ i=1   \\nn   ( X i −  \\n_\\n \\nX  ) 2     . \\n(17)\\nWe compare the calculated t-statistic with the critical values to test hypotheses. Note \\nthat the greater the variability of the independent variable, the lower the standard error \\nof the slope (Equation 16) and hence the greater the calculated t-statistic (Equation \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Hypothesis Testing of Linear Regression Coefficients',\n",
       "     'page_number': 463,\n",
       "     'content': 'Hypothesis Testing of Linear Regression Coefficients\\n453\\n 4. Based on your answers to the preceding questions, evaluate this simple \\nlinear regression model.\\nSolution to 4\\nThe coefficient of determination indicates that variation in the independent \\nvariable explains 23.52% of the variation in the dependent variable. Also, \\nthe F-statistic test confirms that the model’s slope coefficient is different \\nfrom 0 at the 5% level of significance. In sum, the model seems to fit the data \\nreasonably well.\\nHYPOTHESIS TESTING OF LINEAR REGRESSION \\nCOEFFICIENTS\\nformulate a null and an alternative hypothesis about a population \\nvalue of a regression coefficient, and determine whether the null \\nhypothesis is rejected at a given level of significance\\n',\n",
       "     'children': [{'title': 'Hypothesis Tests of the Slope Coefficient',\n",
       "       'page_number': 463,\n",
       "       'content': 'Hypothesis Tests of the Slope Coefficient\\nWe can use the F-statistic to test for the significance of the slope coefficient (that is, \\nwhether it is significantly different from zero), but we also may want to perform other \\nhypothesis tests for the slope coefficient—for example, testing whether the population \\nslope is different from a specific value or whether the slope is positive. We can use \\na t-distributed test statistic to test such hypotheses about a regression coefficient.\\nSuppose we want to check a stock’s valuation using the market model; we hypoth-\\nesize that the stock has an average systematic risk (i.e., risk similar to that of the \\nmarket), as represented by the coefficient on the market returns variable. Or we may \\nwant to test the hypothesis that economists’ forecasts of the inflation rate are unbiased \\n(that is, on average, not overestimating or underestimating actual inflation rates). In \\neach case, does the evidence support the hypothesis? Such questions as these can be \\naddressed with hypothesis tests on the regression slope. To test a hypothesis about a \\nslope, we calculate the test statistic by subtracting the hypothesized population slope \\n(B1) from the estimated slope coefficient (  ˆ \\nstandard error of the slope coefficient,   s  ˆ b  1 ) and then dividing this difference by the \\nb   1   :\\n t =  \\n ˆ \\nb  1 −  B 1  \\n_ \\n s  ˆ \\nb  1    . \\n(16)\\nThis test statistic is t-distributed with n − k − 1 or n − 2 degrees of freedom because \\ntwo parameters (an intercept and a slope) were estimated in the regression.\\nThe standard error of the slope coefficient (  s  ˆ \\nb   1   ) for a simple linear regression \\nis the ratio of the model’s standard error of the estimate (se) to the square root of the \\nvariation of the independent variable:\\n s  ˆ \\nb   1   =  \\n s e  \\n____________ \\n \\n √ \\n_____________ \\n ∑ i=1   \\nn   ( X i −  \\n_\\n \\nX  ) 2     . \\n(17)\\nWe compare the calculated t-statistic with the critical values to test hypotheses. Note \\nthat the greater the variability of the independent variable, the lower the standard error \\nof the slope (Equation 16) and hence the greater the calculated t-statistic (Equation \\n5\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n454\\n15). If the calculated t-statistic is outside the bounds of the critical t-values, we reject \\nthe null hypothesis, but if the calculated t-statistic is within the bounds of the critical \\nvalues, we fail to reject the null hypothesis. Similar to tests of the mean, the alternative \\nhypothesis can be two sided or one sided.\\nConsider our previous simple linear regression example with ROA as the dependent \\nvariable and CAPEX as the independent variable. Suppose we want to test whether \\nthe slope coefficient of CAPEX is different from zero to confirm our intuition of a \\nsignificant relationship between ROA and CAPEX. We can test the hypothesis concern-\\ning the slope using the six-step process, as we show in Exhibit 24. As a result of this \\ntest, we conclude that the slope is different from zero; that is, CAPEX is a significant \\nexplanatory variable of ROA.\\nExhibit 24: Test of the Slope for the Regression of ROA on CAPEX\\nStep 1\\nState the hypotheses.\\nH0: b1 = 0 versus Ha: b1 ≠ 0\\nStep 2\\nIdentify the appropriate test \\nstatistic.\\n t =   ˆ \\nb  − 1  B 1  \\n_ \\n s  ˆ \\nb  1      \\nwith 6 − 2 = 4 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5%.\\nStep 4\\nState the decision rule.\\nCritical t-values = ±2.776. \\nWe can determine this from\\nExcel \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: T.INV(0.025,4)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: T.INV(0.975,4)\\nR qt(c(.025,.975),4)\\nPython from scipy.stats import t \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower: t.ppf(.025,4)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Upper: t.ppf(.975,4)\\nWe reject the null hypothesis if the calculated t-statistic is less than −2.776 \\nor greater than +2.776.\\nStep 5\\nCalculate the test statistic.\\nThe slope coefficient is 1.25 (Exhibit 6). \\nThe mean square error is 11.96875 (Exhibit 22). \\nThe variation of CAPEX is 122.640 (Exhibit 6). \\n s e =  √ _ \\n11.96875  = 3.459588.  \\n s  ˆ \\nb      1   =  3.459588 \\n_ \\n √ _ \\n122.640    = 0.312398.  \\n t =  1.25 − 0 \\n_ \\n0.312398  = 4.00131. \\n\\xa0\\n\\xa0\\n\\xa0\\nStep 6\\nMake a decision.\\nReject the null hypothesis of a zero slope. There is sufficient evidence to \\nindicate that the slope is different from zero.\\nA feature of simple linear regression is that the t-statistic used to test whether the \\nslope coefficient is equal to zero and the t-statistic to test whether the pairwise cor-\\nrelation is zero (that is, H0: ρ = 0 versus Ha: ρ ≠ 0) are the same value. Just as with a \\ntest of a slope, both two-sided and one-sided alternatives are possible for a test of a \\ncorrelation—for example, H0: ρ ≤ 0 versus Ha: ρ > 0. The test-statistic to test whether \\nthe correlation is equal to zero is\\n t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    . \\n© CFA Institute. For candidate use only. Not for distribution.\\nHypothesis Testing of Linear Regression Coefficients\\n455\\nIn our example of ROA regressed on CAPEX, the correlation (r) is 0.8945. To test \\nwhether this correlation is different from zero, we perform a test of hypothesis, shown \\nin Exhibit 25. As you can see, we draw a conclusion similar to that for our test of the \\nslope, but it is phrased in terms of the correlation between ROA and CAPEX: There \\nis a significant correlation between ROA and CAPEX.\\nExhibit 25: Test of the Correlation between ROA and CAPEX\\nStep 1\\nState the hypotheses.\\nH0: ρ = 0 versus Ha: ρ ≠ 0\\nStep 2\\nIdentify the appropriate test \\nstatistic.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    .  \\nwith 6 − 2 = 4 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5%.\\nStep 4\\nState the decision rule.\\nCritical t-values = ±2.776. \\nReject the null if the calculated t-statistic is less than −2.776 or greater \\nthan +2.776.\\nStep 5\\nCalculate the test statistic.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 t =  0.8945  √ \\n_\\n4    \\n_ \\n √ _ \\n1 − 0.8001    = 4.00131. \\nStep 6\\nMake a decision.\\nReject the null hypothesis of no correlation. There is sufficient evidence to \\nindicate that the correlation between ROA and CAPEX is different from \\nzero.\\nAnother interesting feature of simple linear regression is that the test-statistic used \\nto test the fit of the model (that is, the F-distributed test statistic) is related to the \\ncalculated t-statistic used to test whether the slope coefficient is equal to zero: t2 = F; \\ntherefore, 4.001312 = 16.0104. \\nWhat if instead we want to test whether there is a one-to-one relationship between \\nROA and CAPEX, implying a slope coefficient of 1.0. The hypotheses become H0: b1 \\n= 1 and Ha: b1 ≠ 1. The calculated t-statistic is\\n t =  1.25 − 1 \\n_ \\n0.312398  = 0.80026. \\nThis calculated test statistic falls within the bounds of the critical values, ±2.776, so \\nwe fail to reject the null hypothesis: There is not sufficient evidence to indicate that \\nthe slope is different from 1.0.\\nWhat if instead we want to test whether there is a positive slope or positive cor-\\nrelation, as our intuition suggests? In this case, all the steps are the same as in Exhibits \\n24 and 25 except the critical values because the tests are one sided. For a test of a \\npositive slope or positive correlation, the critical value for a 5% level of significance is \\n+2.132. We show the test of hypotheses for a positive slope and a positive correlation \\nin Exhibit 26. Our conclusion is that there is sufficient evidence supporting both a \\npositive slope and a positive correlation.\\nExhibit 26: One-Sided Tests for the Slope and Correlation\\n\\xa0\\n\\xa0\\nTest of the Slope\\nTest of the Correlation\\nStep 1\\nState the hypotheses.\\nH0: b1 ≤ 0 versus Ha: b1 > 0\\nH0: ρ ≤ 0 versus Ha: ρ > 0\\nStep 2\\nIdentify the appropri-\\nate test statistic.\\n t =   ˆ \\nb  − 1  B 1  \\n_ \\n s  ˆ \\nb  1      \\nwith 6 − 2 = 4 degrees of freedom.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 t =  r  √ _ \\nn − 2   \\n_ \\n √ _ \\n1 −  r 2    .  \\nwith 6 − 2 = 4 degrees of freedom.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n456\\n\\xa0\\n\\xa0\\nTest of the Slope\\nTest of the Correlation\\nStep 3\\nSpecify the level of \\nsignificance.\\nα = 5%.\\nα = 5%.\\nStep 4\\nState the decision rule.\\nCritical t-value = 2.132. \\nReject the null if the calculated t-statistic \\nis greater than 2.132.\\nCritical t-value = 2.132. \\nReject the null if the calculated t-statistic is \\ngreater than 2.132.\\nStep 5\\nCalculate the test \\nstatistic.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 t =  1.25 − 0 \\n_ \\n0.312398  = 4.00131 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 t =  0.8945  √ \\n_\\n4    \\n_ \\n √ _ \\n1 − 0.8001    = 4.00131 \\nStep 6\\nMake a decision.\\nReject the null hypothesis. There is suffi-\\ncient evidence to indicate that the slope \\nis greater than zero.\\nReject the null hypothesis. There is suffi-\\ncient evidence to indicate that the correla-\\ntion is greater than zero.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Hypothesis Tests of the Intercept',\n",
       "       'page_number': 466,\n",
       "       'content': 'Hypothesis Tests of the Intercept\\nThere are occasions when we want to test whether the population intercept is a spe-\\ncific value. As a reminder on how to interpret the intercept, consider the simple linear \\nregression with a company’s revenue growth rate as the dependent variable (Y) and the \\nGDP growth rate of its home country as the independent variable (X). The intercept \\nis the company’s revenue growth rate if the GDP growth rate is 0%.\\nThe equation for the standard error of the intercept,   s  ˆ \\nb   0   , is\\n s  ˆ \\nb   0   =  √ \\n______________ \\n \\n 1 _ \\nn  +  \\n \\n_\\nX  2   \\n___________ \\n ∑ i=1   \\nn   ( X i −  \\n_\\n \\nX  ) 2     . \\n(18)\\nWe can test whether the intercept is different from the hypothesized value, B0, by \\ncomparing the estimated intercept ( ˆ \\nb   0   ) with the hypothesized intercept and then \\ndividing the difference by the standard error of the intercept:\\n t intercept =  \\n ˆ \\nb   0  −  B 0  \\n_ \\n s  ˆ \\nb      0     =  \\n ˆ \\nb   0  −  B 0  \\n______________ \\n \\n √ \\n______________ \\n \\n 1 _ \\nn  +  \\n \\n_\\nX  2   \\n___________ \\n ∑ i=1   \\nn   ( X i −  \\n_\\n \\nX  ) 2     \\n  . \\nIn the ROA regression example, the intercept is 4.875%. Suppose we want to test \\nwhether the intercept is greater than 3%. The one-sided hypothesis test is shown \\nin Exhibit 27. As you can see, we reject the null hypothesis. In other words, there \\nis sufficient evidence that if there are no capital expenditures (CAPEX = 0), ROA is \\ngreater than 3%.\\nExhibit 27: Test of Hypothesis for Intercept for Regression of ROA on CAPEX\\nStep 1\\nState the hypotheses.\\nH0: b0 ≤ 3% versus Ha: b0 > 3%\\nStep 2\\nIdentify the appropriate test \\nstatistic.\\n t intercept =  \\n ˆ \\nb   0  −  B 0  \\n_ \\n s  ˆ \\nb      0       \\nwith 6 − 2 = 4 degrees of freedom.\\nStep 3\\nSpecify the level of significance.\\nα = 5%.\\nStep 4\\nState the decision rule.\\nCritical t-value = 2.132. \\nReject the null if the calculated t-statistic is greater than 2.132.\\nStep 5\\nCalculate the test statistic.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  t intercept =  4.875 − 3.0 \\n_ \\n √ _ \\n 1 _ \\n6  +   6.1  2  \\n_ \\n122.64   \\n  =  1.875 \\n_ \\n0.68562  = 2.73475 \\n© CFA Institute. For candidate use only. Not for distribution.\\nHypothesis Testing of Linear Regression Coefficients\\n457\\nStep 6\\nMake a decision.\\nReject the null hypothesis. There is sufficient evidence to indicate that the \\nintercept is greater than 3%.\\nHypothesis Tests of Slope When Independent Variable Is an \\nIndicator Variable\\nSuppose we want to examine whether a company’s quarterly earnings announcements \\ninfluence its monthly stock returns. In this case, we could use an indicator variable, \\nor dummy variable, that takes on only the values 0 or 1 as the independent variable. \\nConsider the case of a company’s monthly stock returns over a 30-month period. A \\nsimple linear regression model for investigating this question would be monthly returns, \\nRET, regressed on the indicator variable, EARN, that takes on a value of 0 if there is \\nno earnings announcement that month and 1 if there is an earnings announcement:\\n RET i =  b 0 +  b 1  EARN i +  ε i . \\nThis regression setup allows us to test whether there are different returns for \\nearnings-announcement months versus non-earnings-announcement months. The \\nobservations and regression results are shown graphically in Exhibit 28.\\nExhibit 28: Earnings Announcements, Dummy Variable, and Stock Returns\\nMonthly Return (%)\\n2.5\\n2.5\\n1.5\\n1.5\\n0.5\\n0.5\\n2.0\\n2.0\\n1.0\\n1.0\\n0\\nMonth\\nMean Return for Announcement Months\\nMean Return for Non-Announcement Months\\nReturns for Months without Announcements\\nReturns for Announcement Months\\n1\\n5\\n17\\n17\\n3\\n19\\n19\\n21\\n21\\n7\\n23\\n23\\n25\\n25\\n1111\\n27\\n27\\n8\\n6\\n4\\n2\\n29\\n29\\n10\\n10\\n18\\n18\\n16\\n16\\n14\\n14\\n12\\n12\\n20\\n20\\n28\\n28\\n26\\n26\\n24\\n24\\n22\\n22\\n30\\n30\\n15\\n15\\n9\\n13\\n13\\nClearly there are some months in which the returns are different from other months, \\nand these correspond to months in which there was an earnings announcement. We \\nestimate the simple linear regression model and perform hypothesis testing in the \\nsame manner as if the independent variable were a continuous variable. In a simple \\nlinear regression, the interpretation of the intercept is the predicted value of the \\ndependent variable if the indicator variable is zero. Moreover, the slope, when the \\nindicator variable is 1, is the difference in the means if we grouped the observations by \\nthe indicator variable. The results of the regression are given in Panel A of Exhibit 29.\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Hypothesis Tests of Slope When Independent Variable Is an Indicator Variable',\n",
       "       'page_number': 467,\n",
       "       'content': 'Hypothesis Testing of Linear Regression Coefficients\\n457\\nStep 6\\nMake a decision.\\nReject the null hypothesis. There is sufficient evidence to indicate that the \\nintercept is greater than 3%.\\nHypothesis Tests of Slope When Independent Variable Is an \\nIndicator Variable\\nSuppose we want to examine whether a company’s quarterly earnings announcements \\ninfluence its monthly stock returns. In this case, we could use an indicator variable, \\nor dummy variable, that takes on only the values 0 or 1 as the independent variable. \\nConsider the case of a company’s monthly stock returns over a 30-month period. A \\nsimple linear regression model for investigating this question would be monthly returns, \\nRET, regressed on the indicator variable, EARN, that takes on a value of 0 if there is \\nno earnings announcement that month and 1 if there is an earnings announcement:\\n RET i =  b 0 +  b 1  EARN i +  ε i . \\nThis regression setup allows us to test whether there are different returns for \\nearnings-announcement months versus non-earnings-announcement months. The \\nobservations and regression results are shown graphically in Exhibit 28.\\nExhibit 28: Earnings Announcements, Dummy Variable, and Stock Returns\\nMonthly Return (%)\\n2.5\\n2.5\\n1.5\\n1.5\\n0.5\\n0.5\\n2.0\\n2.0\\n1.0\\n1.0\\n0\\nMonth\\nMean Return for Announcement Months\\nMean Return for Non-Announcement Months\\nReturns for Months without Announcements\\nReturns for Announcement Months\\n1\\n5\\n17\\n17\\n3\\n19\\n19\\n21\\n21\\n7\\n23\\n23\\n25\\n25\\n1111\\n27\\n27\\n8\\n6\\n4\\n2\\n29\\n29\\n10\\n10\\n18\\n18\\n16\\n16\\n14\\n14\\n12\\n12\\n20\\n20\\n28\\n28\\n26\\n26\\n24\\n24\\n22\\n22\\n30\\n30\\n15\\n15\\n9\\n13\\n13\\nClearly there are some months in which the returns are different from other months, \\nand these correspond to months in which there was an earnings announcement. We \\nestimate the simple linear regression model and perform hypothesis testing in the \\nsame manner as if the independent variable were a continuous variable. In a simple \\nlinear regression, the interpretation of the intercept is the predicted value of the \\ndependent variable if the indicator variable is zero. Moreover, the slope, when the \\nindicator variable is 1, is the difference in the means if we grouped the observations by \\nthe indicator variable. The results of the regression are given in Panel A of Exhibit 29.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n458\\nExhibit 29: Regression and Test of Differences Using an Indicator Variable\\nA. Regression Estimation Results\\n\\xa0\\nEstimated \\nCoefficients\\nStandard Error of \\nCoefficients\\nCalculated Test \\nStatistic\\nIntercept\\n0.5629\\n0.0560\\n10.0596\\nEARN\\n1.2098\\n0.1158\\n10.4435\\nDegrees of freedom = 28.\\nCritical t-values = +2.0484 (5% significance).\\nB. Test of Differences in Means\\n\\xa0\\nRET for \\nEarnings-Announcement \\nMonths\\nRET for \\nNon-Earnings-Announcement \\nMonths\\nDifference in \\nMeans\\nMean\\n1.7727\\n0.5629\\n1.2098\\nVariance\\n0.1052\\n0.0630\\n\\xa0\\nObservations\\n7\\n23\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nPooled \\nvariance\\n\\xa0\\n\\xa0\\n0.07202\\nCalculated test \\nstatistic\\n\\xa0\\n\\xa0\\n10.4435\\nDegrees of freedom = 28.\\nCritical t-values = +2.0484 (5% significance).\\nWe can see the following from Panel A of Exhibit 29:\\n■ \\nThe intercept (0.5629) is the mean of the returns for \\nnon-earnings-announcement months.\\n■ \\nThe slope coefficient (1.2098) is the difference in means of returns between \\nearnings-announcement and non-announcement months.\\n■ \\nWe reject the null hypothesis that the slope coefficient on EARN is equal to \\nzero. We also reject the null hypothesis that the intercept is zero. The reason \\nis that in both cases, the calculated test statistic exceeds the critical t-value.\\nWe could also test whether the mean monthly return is the same for both the \\nnon-earnings-announcement months and the earnings-announcement months by \\ntesting the following:\\n H 0 \\u200a:\\u200a μ RETearnings =  μ RETNon−earnings  and   H a \\u200a:\\u200a μ RETearnings ≠  μ RETNon−earnings  \\nThe results of this hypothesis test are gleaned from Panel B of Exhibit 29. As you can \\nsee, we reject the null hypothesis that there is no difference in the mean RET for the \\nearnings-announcement and non-earnings-announcements months at the 5% level \\nof significance, since the calculated test statistic (10.4435) exceeds the critical value \\n(2.0484).\\n© CFA Institute. For candidate use only. Not for distribution.\\nHypothesis Testing of Linear Regression Coefficients\\n459\\nTest of Hypotheses: Level of Significance and p-Values\\nThe choice of significance level in hypothesis testing is always a matter of judgment. \\nAnalysts often choose the 0.05 level of significance, which indicates a 5% chance of \\nrejecting the null hypothesis when, in fact, it is true (a Type I error, or false positive). \\nOf course, decreasing the level of significance from 0.05 to 0.01 decreases the proba-\\nbility of Type I error, but it also increases the probability of Type II error—failing to \\nreject the null hypothesis when, in fact, it is false (that is, a false negative).\\nThe p-value is the smallest level of significance at which the null hypothesis can \\nbe rejected. The smaller the p-value, the smaller the chance of making a Type I error \\n(i.e., rejecting a true null hypothesis), so the greater the likelihood the regression \\nmodel is valid. For example, if the p-value is 0.005, we reject the null hypothesis that \\nthe true parameter is equal to zero at the 0.5% significance level (99.5% confidence). \\nIn most software packages, the p-values provided for regression coefficients are for a \\ntest of null hypothesis that the true parameter is equal to zero against the alternative \\nthat the parameter is not equal to zero.\\nIn our ROA regression example, the calculated t-statistic for the test of whether the \\nslope coefficient is zero is 4.00131. The p-value corresponding to this test statistic is \\n0.008, which means there is just a 0.8% chance of rejecting the null hypotheses when it \\nis true. Comparing this p-value with the level of significance of 5% (and critical values \\nof ±2.776) leads us to easily reject the null hypothesis of H0: b1 = 0. \\nHow do we determine the p-values? Since this is the area in the distribution out-\\nside the calculated test statistic, we need to resort to software tools. For the p-value \\ncorresponding to the t = 4.00131 from the ROA regression example, we could use \\nthe following:\\n■ \\nExcel 1-T.DIST(4.00131,4,TRUE))*2\\n■ \\nR (1-pt(4.00131,4))*2\\n■ \\nPython from scipy.stats import t and (1 - t.cdf(4.00131,4))*2\\nEXAMPLE 6\\nHypothesis Testing of Simple Linear Regression Results\\nAn analyst is interested in interpreting the results of and performing tests of \\nhypotheses for the market model estimation that regresses the daily return on \\nABC stock on the daily return on the fictitious Europe–Asia–Africa (EAA) \\nEquity Index, his proxy for the stock market. He has generated the regression \\nresults presented in Exhibit 30.\\n \\nExhibit 30: Selected Results of Estimation of Market Model for ABC \\nStock\\n \\n \\nStandard error of the estimate (se)\\n1.26\\nStandard deviation of ABC stock returns\\n0.80\\nStandard deviation of EAA Equity Index returns\\n0.70\\nNumber of observations\\n1,200\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nIntercept\\n0.010\\nSlope of EAA Equity Index returns\\n0.982\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Test of Hypotheses: Level of Significance and p-Values',\n",
       "       'page_number': 469,\n",
       "       'content': 'Test of Hypotheses: Level of Significance and p-Values\\nThe choice of significance level in hypothesis testing is always a matter of judgment. \\nAnalysts often choose the 0.05 level of significance, which indicates a 5% chance of \\nrejecting the null hypothesis when, in fact, it is true (a Type I error, or false positive). \\nOf course, decreasing the level of significance from 0.05 to 0.01 decreases the proba-\\nbility of Type I error, but it also increases the probability of Type II error—failing to \\nreject the null hypothesis when, in fact, it is false (that is, a false negative).\\nThe p-value is the smallest level of significance at which the null hypothesis can \\nbe rejected. The smaller the p-value, the smaller the chance of making a Type I error \\n(i.e., rejecting a true null hypothesis), so the greater the likelihood the regression \\nmodel is valid. For example, if the p-value is 0.005, we reject the null hypothesis that \\nthe true parameter is equal to zero at the 0.5% significance level (99.5% confidence). \\nIn most software packages, the p-values provided for regression coefficients are for a \\ntest of null hypothesis that the true parameter is equal to zero against the alternative \\nthat the parameter is not equal to zero.\\nIn our ROA regression example, the calculated t-statistic for the test of whether the \\nslope coefficient is zero is 4.00131. The p-value corresponding to this test statistic is \\n0.008, which means there is just a 0.8% chance of rejecting the null hypotheses when it \\nis true. Comparing this p-value with the level of significance of 5% (and critical values \\nof ±2.776) leads us to easily reject the null hypothesis of H0: b1 = 0. \\nHow do we determine the p-values? Since this is the area in the distribution out-\\nside the calculated test statistic, we need to resort to software tools. For the p-value \\ncorresponding to the t = 4.00131 from the ROA regression example, we could use \\nthe following:\\n■ \\nExcel 1-T.DIST(4.00131,4,TRUE))*2\\n■ \\nR (1-pt(4.00131,4))*2\\n■ \\nPython from scipy.stats import t and (1 - t.cdf(4.00131,4))*2\\nEXAMPLE 6\\nHypothesis Testing of Simple Linear Regression Results\\nAn analyst is interested in interpreting the results of and performing tests of \\nhypotheses for the market model estimation that regresses the daily return on \\nABC stock on the daily return on the fictitious Europe–Asia–Africa (EAA) \\nEquity Index, his proxy for the stock market. He has generated the regression \\nresults presented in Exhibit 30.\\n \\nExhibit 30: Selected Results of Estimation of Market Model for ABC \\nStock\\n \\n \\nStandard error of the estimate (se)\\n1.26\\nStandard deviation of ABC stock returns\\n0.80\\nStandard deviation of EAA Equity Index returns\\n0.70\\nNumber of observations\\n1,200\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nIntercept\\n0.010\\nSlope of EAA Equity Index returns\\n0.982\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n460\\n \\n1. If the critical t-values are ±1.96 (at the 5% significance level), is the slope \\ncoefficient different from zero?\\nSolution to 1\\nFirst, we calculate the variation of the independent variable using the stan-\\ndard deviation of the independent variable:\\n ∑  \\ni=1\\n  \\nn   ( X i −  \\n_\\n \\nX  ) \\n2 =  \\n ∑ i=1  \\nn   ( X i −  \\n_\\n \\nX  ) \\n 2  \\n___________ \\nn − 1 \\n ×   ( n − 1 )  . \\nSo,\\n ∑  \\ni=1\\n  \\nn   ( X i −  \\n_\\n \\nX  ) \\n2 =  0.70 2 × 1, 199 = 587.51. \\nNext, the standard error of the estimated slope coefficient is\\n s  ˆ \\nb   1   =  \\n s e  \\n____________ \\n \\n √ \\n_____________ \\n ∑ i=1   \\nn   ( X i −  \\n_\\n \\nX  ) 2     =  1.26 \\n_ \\n √ _ \\n587.51    = 0.051983, \\nand the test statistic is\\n t =  \\n ˆ \\nb  1 −  B 1  \\n_ \\n s  ˆ \\nb   1     =  0.982 − 0 \\n_ \\n0.051983  = 18.89079 \\nThe calculated test statistic is outside the bounds of ±1.96, so we reject the \\nnull hypothesis of a slope coefficient equal to zero.\\n2. If the critical t-values are ±1.96 (at the 5% significance level), is the slope \\ncoefficient different from 1.0?\\nSolution to 2\\nThe calculated test statistic for the test of whether the slope coefficient is \\nequal to 1.0 is\\n t =  0.982 − 1 \\n_ \\n0.051983  = −\\u200a0.3463. \\nThe calculated test statistic is within the bounds of ±1.96, so we fail to reject \\nthe null hypothesis of a slope coefficient equal to 1.0, which is evidence that \\nthe true population slope may be 1.0.\\nPREDICTION USING SIMPLE LINEAR REGRESSION \\nAND PREDICTION INTERVALS\\ncalculate and interpret the predicted value for the dependent \\nvariable, and a prediction interval for it, given an estimated linear \\nregression model and a value for the independent variable\\nFinancial analysts often want to use regression results to make predictions about \\na dependent variable. For example, we might ask, “How fast will the sales of XYZ \\nCorporation grow this year if real GDP grows by 4%?” But we are not merely interested \\n6\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Prediction Using Simple Linear Regression and Prediction Intervals',\n",
       "     'page_number': 470,\n",
       "     'content': 'Prediction Using Simple Linear Regression and Prediction Intervals\\n461\\nin making these forecasts; we also want to know how certain we can be about the \\nforecasts’ results. A forecasted value of the dependent variable,  ˆ \\nY  f  , is determined using \\nthe estimated intercept and slope, as well as the expected or forecasted independent \\nvariable, Xf :\\n ˆ \\nY  f =  ˆ \\nb  0 +  ˆ \\nb  1  X f . \\n(19)\\nIn our ROA regression model, if we forecast a company’s CAPEX to be 6%, the fore-\\ncasted ROA based on our estimated equation is 12.375%:\\n ˆ \\nY  f = 4.875 +   ( 1.25 × 6 )  = 12.375 .\\nHowever, we need to consider that the estimated regression line does not describe the \\nrelation between the dependent and independent variables perfectly; it is an average \\nof the relation between the two variables. This is evident because the residuals are \\nnot all zero.\\nTherefore, an interval estimate of the forecast is needed to reflect this uncertainty. \\nThe estimated variance of the prediction error,   s f 2  , of Y, given X, is\\n s f 2 =  s e 2  [\\n 1 +  1 _ \\nn  +  \\n ( X f −  \\n_\\n \\nX  ) \\n_   2\\n  ( n − 1 )  s X \\n2    ]\\n  =  s e 2  [\\n 1 +  1 _ \\nn  +  \\n ( X f −  \\n_\\n \\nX  ) \\n  2\\n___________ \\n \\n ∑ i=1  \\nn   ( X i −  \\n_\\n \\nX  ) \\n2 \\n  ]\\n  , \\nand the standard error of the forecast is\\n s f =  s e  √ \\n__________________ \\n \\n1 +  1 _ \\nn  +  \\n ( X f −  \\n_\\n \\nX  ) 2  \\n___________ \\n ∑ i=1   \\nn   ( X i −  \\n_\\n \\nX  ) 2    .  \\n(20)\\nThe standard error of the forecast depends on\\n■ \\nthe standard error of the estimate, se;\\n■ \\nthe number of observations, n;\\n■ \\nthe forecasted value of the independent variable, Xf, used to predict the \\ndependent variable and its deviation from the estimated mean,   _\\n \\nX  ; and\\n■ \\nthe variation of the independent variable.\\nWe can see the following from the equation for the standard error of the forecast:\\n1. The better the fit of the regression model, the smaller the standard error of \\nthe estimate (se) and, therefore, the smaller standard error of the forecast.\\n2. The larger the sample size (n) in the regression estimation, the smaller the \\nstandard error of the forecast.\\n3. The closer the forecasted independent variable (Xf) is to the mean of the \\nindependent variable  ( _\\n \\nX  ) used in the regression estimation, the smaller the \\nstandard error of the forecast.\\nOnce we have this estimate of the standard error of the forecast, determining \\na prediction interval around the predicted value of the dependent variable  ( ˆ \\nY  f )  is \\nvery similar to estimating a confidence interval around an estimated parameter. The \\nprediction interval is\\n ˆ \\nY  f ±  t critical for α/2  s f . \\n(21)\\nWe outline the steps for developing the prediction interval in Exhibit 31.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n462\\nExhibit 31: Creating a Prediction Interval around the Predicted Dependent \\nVariable\\nPredict the value of Y, Yf, given the forecasted value of X, Xf\\n^\\nChoose a significance level, α, for the prediction interval\\nDetermine the critical value for the prediction interval based on the degrees of \\nfreedom and the significance level\\nCompute the standard error of the forecast\\nCompute the (1 – α) percent prediction interval for the prediction as: Yf ± t critical for α/2 sf\\n^\\nFor our ROA regression model, given that the forecasted value of CAPEX is 6.0, the \\npredicted value of Y is 12.375:\\n ˆ \\nY  f = 4.875 + 1.25  X f = 4.875 + (1.25 × 6.0 ) = 12.375. \\nAssuming a 5% significance level (α), two sided, with n − 2 degrees of freedom (so, df \\n= 4), the critical values for the prediction interval are ±2.776. \\nThe standard error of the forecast is\\n s f = 3.459588  √ \\n______________ \\n \\n1 +  1 _ \\n6  +   ( 6 − 6.1 ) 2  \\n_ \\n122.640   = 3.459588  √ _ \\n1.166748  = 3.736912. \\nThe 95% prediction interval then becomes\\n 12.375 ± 2.776  ( 3.736912 )   \\n 12.375 ± 10.3737 \\n  { 2.0013 <  ˆ \\nY  f < 22.7487 }   \\nFor our ROA regression example, we can see how the standard error of the forecast \\n(sf) changes as our forecasted value of the independent variable gets farther from the \\nmean of the independent variable  ( X  f   −   _\\n \\nX  )  in Exhibit 32. The mean of CAPEX is 6.1%, \\nand the band that represents one standard error of the forecast, above and below the \\nforecast, is minimized at that point and increases as the independent variable gets \\nfarther from  _\\n \\nX  .\\n© CFA Institute. For candidate use only. Not for distribution.\\nPrediction Using Simple Linear Regression and Prediction Intervals\\n463\\nExhibit 32: ROA Forecasts and Standard Error of the Forecast\\nForecasted ROA (%)\\n45\\n45\\n40\\n40\\n35\\n35\\n30\\n30\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\n–5\\n–5\\n1\\n20\\n20\\n6\\n12\\n12\\n17\\n17\\n7\\n13\\n13\\n18\\n18\\n8\\n14\\n14\\n19\\n19\\n2\\n3\\n4\\n5\\n9\\n10\\n10\\n1111\\n15\\n15\\n16\\n16\\nForecasted CAPEX (%)\\nUpper Bound of 95% Prediction Interval\\nLower Bound of 95% Prediction Interval\\nOne Standard Error of the Forecast above the Forecast\\nOne Standard Error of the Forecast below the Forecast\\nForecasted Y\\nEXAMPLE 7\\nPredicting Net Profit Margin Using R&D Spending\\nSuppose we want to forecast a company’s net profit margin (NPM) based on \\nits research and development expenditures scaled by revenues (RDR), using \\nthe model estimated in Example 2 and the details provided in Exhibit 8. The \\nregression model was estimated using data on eight companies as\\n ˆ \\nY  f = 16.5 − 1.3  X f , \\nwith a standard error of the estimate (se) of 1.8618987 and variance of RDR, \\n  ∑ i=1  \\nn  ( X i −  _\\n X  ) 2   \\n___________ \\n(n − 1) \\n , of 4.285714, as given.\\n1. What is the predicted value of NPM if the forecasted value of RDR is 5?\\nSolution to 1\\nThe predicted value of NPM is 10: 16.5 − (1.3 × 5) = 10.\\n2. What is the standard error of the forecast (sf) if the forecasted value of RDR \\nis 5?\\nSolution to 2\\nTo derive the standard error of the forecast (sf), we first have to calculate the \\nvariation of RDR. Then, we have all the pieces to calculate sf :\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n464\\n ∑  \\ni=1\\n  \\nn   ( X i −  \\n_\\n \\nX  ) \\n2 = 4.285714 × 7 = 30. \\n s f = 1.8618987  √ \\n______________ \\n \\n1 +  1 _ \\n8  +   (5 − 7.5) 2  \\n_ \\n30 \\n  = 2.1499. \\n3. What is the 95% prediction interval for the predicted value of NPM using \\ncritical t-values (df = 6) of ±2.447?\\nSolution to 3\\nThe 95% prediction interval for the predicted value of NPM is\\n  { 10 ± 2.447  ( 2.1499 )  }   \\n  { 4.7392 <  ˆ \\nY  f < 15.2608 }   \\n4. What is the predicted value of NPM if the forecasted value of RDR is 15?\\nSolution to 4\\nThe predicted value of NPM is −3: 16.5 − (1.3 × 15) = −3.\\n5. What is the standard error of the forecast if the forecasted value of RDR is \\n15?\\nSolution to 5\\nTo derive the standard error of the forecast, we first must calculate the vari-\\nation of RDR. Then, we can calculate sf:\\n ∑  \\ni=1\\n  \\nn   ( X i −  \\n_\\n \\nX  ) \\n2 = 4.285714 × 7 = 30. \\n s f = 1.8618987  √ \\n_______________ \\n \\n1 +  1 _ \\n8  +   ( 15 − 7.5 ) 2  \\n_ \\n30 \\n  = 3.2249. \\n6. What is the 95% prediction interval for the predicted value of NPM using \\ncritical t-values (df = 6) of ±2.447?\\nSolution to 6\\nThe 95% prediction interval for the predicted value of NPM is\\n  { −\\u200a3 ± 2.447  ( 3.2249 )  }   \\n  { −\\u200a10.8913 <  ˆ \\nY  f < 4.8913 }   \\nFUNCTIONAL FORMS FOR SIMPLE LINEAR \\nREGRESSION\\ndescribe different functional forms of simple linear regressions\\n7\\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Functional Forms for Simple Linear Regression',\n",
       "     'page_number': 474,\n",
       "     'content': 'Functional Forms for Simple Linear Regression\\n465\\nNot every set of independent and dependent variables has a linear relation. In fact, \\nwe often see non-linear relationships in economic and financial data. Consider the \\nrevenues of a company over time illustrated in Exhibit 33, with revenues as the depen-\\ndent (Y) variable and time as the independent (X) variable. Revenues grow at a rate \\nof 15% per year for several years, but then the growth rate eventually declines to just \\n5% per year. Estimating this relationship as a simple linear model would understate \\nthe dependent variable, revenues, for some ranges of the independent variable, time, \\nand would overstate it for other ranges of the independent variable.\\nExhibit 33: Company Revenues over Time\\nRevenues (Y)\\n2,500\\n2,500\\n2,000\\n2,000\\n1,500\\n1,500\\n1,000\\n1,000\\n500\\n500\\n0\\n–500\\n–500\\n1\\n10\\n10\\n4\\n7\\n13\\n13\\n16\\n16\\n22\\n22\\n19\\n19\\n25\\n25\\n28\\n28\\nY = –287.61 + 76.578 X\\nR2 = 0.9478\\nF–statistic = 508.9017\\nTime (X)\\nLinear Prediction of Y\\nObserved Y\\nWe can still use the simple linear regression model, but we need to modify either \\nthe dependent or the independent variables to make it work well. This is the case \\nwith many different financial or economic data that you might use as dependent and \\nindependent variables in your regression analysis.\\nThere are several different functional forms that can be used to potentially transform \\nthe data to enable their use in linear regression. These transformations include using \\nthe log (i.e., natural logarithm) of the dependent variable, the log of the independent \\nvariable, the reciprocal of the independent variable, the square of the independent \\nvariable, or the differencing of the independent variable. We illustrate and discuss \\nthree often-used functional forms, each of which involves log transformation:\\n1. ',\n",
       "     'children': [{'title': 'The Log-Lin Model',\n",
       "       'page_number': 475,\n",
       "       'content': 'the log-lin model, in which the dependent variable is logarithmic but the \\nindependent variable is linear;\\n2. ',\n",
       "       'children': []},\n",
       "      {'title': 'The Lin-Log Model',\n",
       "       'page_number': 476,\n",
       "       'content': 'The Lin-Log Model\\nThe lin-log model is similar to the log-lin model, but only the independent variable \\nis in logarithmic form:\\n Yi = b0 + b1 lnXi, \\n(23)\\nThe slope coefficient in this regression model provides the absolute change in the \\ndependent variable for a relative change in the independent variable.\\nSuppose an analyst is examining the cross-sectional relationship between operating \\nprofit margin, the dependent variable (Y), and unit sales, the independent variable \\n(X), and gathers data on a sample of 30 companies. The scatter plot and regression \\nline for these observations are shown in Exhibit 35. Although the slope is different \\nfrom zero at the 5% level (the calculated t-statistic on the slope is 5.8616, compared \\nwith critical t-values of ±2.048), given the R2 of 55.10%, the issue is whether we can \\nget a better fit by using a different functional form.\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n467\\nExhibit 35: Relationship between Operating Profit Margin and Unit Sales\\nOperating Profit Margin (Y)\\n20\\n20\\n16\\n16\\n18\\n18\\n14\\n14\\n12\\n12\\n10\\n10\\n8\\n6\\n4\\n2\\n0\\n0\\n50,000\\n50,000\\n100,000\\n100,000\\n150,000\\n150,000\\nY = 10.3665 + 0.000045X\\nR2 = 0.5510\\nSe = 2.2528\\nF–statistic = 33.8259\\nUnit Sales (X)\\nRegression Line\\nOperating Profit Margin\\nIf instead we use the natural log of the unit sales as the independent variable in our \\nmodel, we get a very different picture, as shown in Exhibit 36. The R2 for the model \\nof operating profit margin regressed on the natural log of unit sales jumps to 97.17%. \\nSince the dependent variable is the same in both the original and transformed models, \\nwe can compare the standard error of the estimate: 2.2528 with the original indepen-\\ndent variable and a much lower 0.5629 with the transformed independent variable. \\nClearly the log-transformed explanatory variable has resulted in a better fitting model.\\nExhibit 36: Relationship Between Operating Profit Margin and Natural \\nLogarithm of Unit Sales\\nOperating Profit Margin (Y)\\n20\\n20\\n16\\n16\\n18\\n18\\n14\\n14\\n12\\n12\\n10\\n10\\n02468\\n0\\n2\\n4\\n8\\n10\\n10\\n6\\n12\\n12\\n14\\n14\\nY = 2.6286 + 1.1797 LnX\\nR2 = 0.9717\\nSe = 0.5659\\nF–statistic = 960.1538\\nLn of Unit Sales (Ln X)\\nLin-Log Regression Line\\nOperating Profit Margin\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n468\\n',\n",
       "       'children': []},\n",
       "      {'title': 'The Log-Log Model',\n",
       "       'page_number': 478,\n",
       "       'content': 'The Log-Log Model\\nThe log-log model, in which both the dependent variable and the independent vari-\\nable are linear in their logarithmic forms, is also referred to as the double-log model.\\n ln Yi = b0 + b1 ln Xi. \\n(24)\\nThis model is useful in calculating elasticities because the slope coefficient is the relative \\nchange in the dependent variable for a relative change in the independent variable. \\nConsider a cross-sectional model of company revenues (the Y variable) regressed on \\nadvertising spending as a percentage of selling, general, and administrative expenses, \\nADVERT (the X variable). As shown in Exhibit 37, a simple linear regression model \\nresults in a shallow regression line, with a coefficient of determination of just 20.89%.\\nExhibit 37: Fitting a Linear Relation Between Revenues and Advertising \\nSpending\\nRevenues (Y)\\n5,000\\n5,000\\n4,500\\n4,500\\n4,000\\n4,000\\n3,500\\n3,500\\n3,000\\n3,000\\n2,000\\n2,000\\n1,500\\n1,500\\n1,000\\n1,000\\n500\\n500\\n2,500\\n2,500\\n0\\n0\\n4.0\\n4.0\\n1.0\\n1.0\\n2.0\\n2.0\\n3.0\\n3.0\\n0.5\\n0.5\\n1.5\\n1.5\\n2.5\\n2.5\\n3.5\\n3.5\\nAdvertising as a Percentage of SG&A  (%, X)\\nRegression Line\\nRevenues\\nY = 70.7139 + 475.3665 X\\nR2 = 0.2089\\nF–statistic = 7.3924\\nHowever, if instead we use the natural logarithms of both the revenues and ADVERT, \\nwe get a much different picture of this relationship. As shown in Exhibit 38, the esti-\\nmated regression line has a significant positive slope; the log-log model’s R2 increases \\nby more than four times, from 20.89% to 84.91%; and the F-statistic jumps from 7.39 \\nto 157.52. So, using the log-log transformation dramatically improves the regression \\nmodel fit relative to our data.\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n469\\nExhibit 38: Fitting a Log-Log Model of Revenues and Advertising Spending\\nLn Revenues (Ln Y)\\n10\\n10\\n8\\n6\\n4\\n0\\n–4\\n–4\\n–2\\n–2\\n2\\n–6\\n–6\\n–8\\n–8\\n2\\n–4\\n–4\\n0\\n–6\\n–6\\n–2\\n–2\\nLn Advertising as a Percentage of SG&A  (Ln X)\\nLog-Log Regression Line\\nNatural Log of Revenues\\nLnY = 5.3109 + 1.3686 LnX\\nR2 = 0.8491\\nF–statistic = 157.5208\\n',\n",
       "       'children': []},\n",
       "      {'title': 'Selecting the Correct Functional Form',\n",
       "       'page_number': 479,\n",
       "       'content': 'Selecting the Correct Functional Form\\nThe key to fitting the appropriate functional form of a simple linear regression is \\nexamining the goodness of fit measures—the coefficient of determination (R2), the \\nF-statistic, and the standard error of the estimate (se)—as well as examining whether \\nthere are patterns in the residuals. In addition to fit statistics, most statistical pack-\\nages provide plots of residuals as part of the regression output, which enables you to \\nvisually inspect the residuals. To reiterate an important point, what you want to see \\nin these plots is random residuals.\\nAs an example, consider the relationship between the monthly returns on DEF \\nstock and the monthly returns of the EAA Equity Index, as depicted in Panel A of \\nExhibit 39, with the regression line indicated. Using the equation for this regression \\nline, we calculate the residuals and plot them against the EAA Equity Index, as shown \\nin Panel B of Exhibit 39. The residuals appear to be random, bearing no relation to the \\nindependent variable. The distribution of the residuals, shown in Panel C of Exhibit \\n39, shows that the residuals are approximately normal. Using statistical software, we \\ncan investigate further by examining the distribution of the residuals, including using \\na normal probability plot or statistics to test for normality of the residuals.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n470\\nExhibit 39: Monthly Returns on DEF Stock Regressed on Returns on the EAA \\nIndex\\nReturn on DEF Stock (%)\\nA. Scatterplot of Returns on DEF Stock and Return on the EAA Index\\n3.0\\n3.0\\n2.0\\n2.0\\n1.0\\n1.0\\n0\\n–1.0\\n–1.0\\n–2.0\\n–2.0\\n–3.0\\n–3.0\\n–4.0\\n–4.0\\n–3.0\\n–3.0\\n2.0\\n2.0\\n–1.0\\n–1.0\\n–2.0\\n–2.0\\n1.0\\n1.0\\n0\\nReturn on the EAA Index (%)\\nY = 0.0062 + 1.1905X\\nResidual Return on DEF Stock (%)\\nB. Scatterplot of Residuals and the Returns on the EAA Index\\n0.3\\n0.3\\n0.2\\n0.2\\n0.1\\n0.1\\n0\\n–0.1\\n–0.1\\n–0.2\\n–0.2\\n–0.3\\n–0.3\\n–0.4\\n–0.4\\n–3.0\\n–3.0\\n3.0\\n3.0\\n–1.0\\n–1.0\\n–2.0\\n–2.0\\n1.0\\n1.0\\n2.0\\n2.0\\n0\\nReturn on the EAA Index (%)\\nNumber of Observations\\nC. Histogram of Residuals\\n35\\n35\\n30\\n30\\n25\\n25\\n20\\n20\\n15\\n15\\n10\\n10\\n5\\n0\\nResidual Range (%)\\n–0.16\\nto\\n–0.10\\n–0.16\\nto\\n–0.10\\n–0.10\\nto\\n–0.04\\n–0.10\\nto\\n–0.04\\n–0.22\\nto\\n–0.16\\n–0.22\\nto\\n–0.16\\n–0.28 \\nto\\n–0.22\\n–0.28 \\nto\\n–0.22\\n–0.04\\nto\\n0.02\\n–0.04\\nto\\n0.02\\n0.02\\nto\\n0.08\\n0.02\\nto\\n0.08\\n0.08\\nto\\n0.14\\n0.08\\nto\\n0.14\\n0.14\\nto\\n0.20\\n0.14\\nto\\n0.20\\n0.20\\nto\\n0.26\\n0.20\\nto\\n0.26\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n471\\nEXAMPLE 8\\nComparing Functional Forms\\nAn analyst is investigating the relationship between the annual growth in con-\\nsumer spending (CONS) in a country and the annual growth in the country’s \\nGDP (GGDP). The analyst estimates the following two models:\\n \\n\\xa0\\nModel 1\\nModel 2\\n\\xa0\\nGGDPi = b0 + b1CONSi \\n+ εi\\nGGDPi = b0 + b1ln(CONSi) + εi\\nIntercept\\n1.040\\n1.006\\nSlope\\n0.669\\n1.994\\nR2\\n0.788\\n0.867\\nStandard error of \\nthe estimate\\n0.404\\n0.320\\nF-statistic\\n141.558\\n247.040\\n \\n1. Identify the functional form used in these models.\\nSolution to 1\\nModel 1 is the simple linear regression with no variable transformation, \\nwhereas Model 2 is a lin-log model with the natural log of the variable \\nCONS as the independent variable.\\n2. Explain which model has better goodness-of-fit with the sample data.\\nSolution to 2\\nThe lin-log model, Model 2, fits the data better. Since the dependent variable \\nis the same for the two models, we can compare the fit of the models using \\neither the relative measures (R2 or F-statistic) or the absolute measure of \\nfit, the standard error of the estimate. The standard error of the estimate \\nis lower for Model 2, whereas the R2 and F-statistic are higher for Model 2 \\ncompared with Model 1.\\n',\n",
       "       'children': []}]},\n",
       "    {'title': 'Summary',\n",
       "     'page_number': 481,\n",
       "     'content': 'SUMMARY\\n■ \\nThe dependent variable in a linear regression is the variable whose variabil-\\nity the regression model tries to explain. The independent variable is the \\nvariable whose variation the researcher uses to explain the variation of the \\ndependent variable.\\n■ \\nIf there is one independent variable in a linear regression and there are n \\nobservations of the dependent and independent variables, the regression \\nmodel is Yi = b0 + b1Xi + εi, i = 1, . . . , n, where Yi is the dependent variable, \\nXi is the independent variable, and εi is the error term. In this model, the \\ncoefficients   b 0 and   b 1 are the population intercept and slope, respectively.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n472\\n■ \\nThe intercept is the expected value of the dependent variable when the inde-\\npendent variable has a value of zero. The slope coefficient is the estimate of \\nthe population slope of the regression line and is the expected change in the \\ndependent variable for a one-unit change in the independent variable.\\n■ \\nThe assumptions of the classic simple linear regression model are as follows:\\n● \\nLinearity: A linear relation exists between the dependent variable and \\nthe independent variable.\\n● \\nHomoskedasticity: The variance of the error term is the same for all \\nobservations.\\n● \\nIndependence: The error term is uncorrelated across observations.\\n● \\nNormality: The error term is normally distributed.\\n■ \\nThe estimated parameters in a simple linear regression model minimize the \\nsum of the squared errors.\\n■ \\nThe coefficient of determination, or R2, measures the percentage of the total \\nvariation in the dependent variable explained by the independent variable.\\n■ \\nTo test the fit of the simple linear regression, we can calculate an \\nF-distributed test statistic and test the hypotheses H0: b1 = 0 versus Ha: b1 ≠ \\n0, with 1 and n − 2 degrees of freedom.\\n■ \\nThe standard error of the estimate is an absolute measure of the fit of the \\nmodel calculated as the square root of the mean square error.\\n■ \\nWe can evaluate a regression model by testing whether the population value \\nof a regression coefficient is equal to a particular hypothesized value. We do \\nthis by calculating a t-distributed test statistic that compares the estimated \\nparameter with the hypothesized parameter, dividing this difference by the \\nstandard error of the coefficient.\\n■ \\nAn indicator (or dummy) variable takes on only the values 0 or 1 and can \\nbe used as the independent variable in a simple linear regression. In such \\na model, the interpretation of the intercept is the predicted value of the \\ndependent variable if the indicator variable is 0, and when the indicator vari-\\nable is 1, the slope is the difference in the means if we grouped the observa-\\ntions by the indicator variable.\\n■ \\nWe calculate a prediction interval for a regression coefficient using the \\nestimated coefficient, the standard error of the estimated coefficient, and the \\ncritical value for the t-distributed test statistic based on the level of signif-\\nicance and the appropriate degrees of freedom, which are n − 2 for simple \\nregression.\\n■ \\nWe can make predictions for the dependent variable using an estimated lin-\\near regression by inserting the forecasted value of the independent variable \\ninto the estimated model.\\n■ \\nThe standard error of the forecast is the product of the standard error of the \\nestimate and a term that reflects the sample size of the regression, the vari-\\nation of the independent variable, and the deviation between the forecasted \\nvalue of the independent variable and the mean of the independent variable \\nin the regression.\\n■ \\nThe prediction interval for a particular forecasted value of the dependent \\nvariable is formed by using the forecasted value of the dependent variable \\nand extending above and below this value a quantity that reflects the critical \\nt-value corresponding to the degrees of freedom, the level of significance, \\nand the standard error of the forecast.\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n473\\n■ \\nIf the relationship between the independent variable and the dependent \\nvariable is not linear, we can often transform one or both of these variables \\nto convert this relation to a linear form, which then allows the use of simple \\nlinear regression.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n474\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Practice Problems',\n",
       "     'page_number': 484,\n",
       "     'content': 'PRACTICE PROBLEMS\\n1. Homoskedasticity is best described as the situation in which the variance of the \\nresiduals of a regression is:\\nA. zero.\\nB. normally distributed.\\nC. constant across observations.\\n2. Julie Moon is an energy analyst examining electricity, oil, and natural gas con-\\nsumption in different regions over different seasons. She ran a simple regression \\nexplaining the variation in energy consumption as a function of temperature. The \\ntotal variation of the dependent variable was 140.58, and the explained variation \\nwas 60.16. She had 60 monthly observations.\\nA. Calculate the coefficient of determination.\\nB. Calculate the F-statistic to test the fit of the model.\\nC. Calculate the standard error of the estimate of the regression estimation.\\nD. Calculate the sample standard deviation of monthly energy consumption.\\n3. An economist collected the monthly returns for KDL’s portfolio and a diversified \\nstock index. The data collected are shown in the following table:\\nMonth \\nPortfolio Return (%)\\nIndex Return (%)\\n1\\n1.11\\n−0.59\\n2\\n72.10\\n64.90\\n3\\n5.12\\n4.81\\n4\\n1.01\\n1.68\\n5\\n−1.72\\n−4.97\\n6\\n4.06\\n−2.06\\nThe economist calculated the correlation between the two returns and found it \\nto be 0.996. The regression results with the KDL return as the dependent variable \\nand the index return as the independent variable are given as follows:\\nRegression Statistics\\n\\xa0\\n\\xa0\\nR2\\n0.9921\\n\\xa0\\nStandard error\\n2.8619\\n\\xa0\\nObservations\\n6\\n\\xa0\\nSource\\ndf\\nSum of \\nSquares\\nMean \\nSquare\\nF\\np-Value\\nRegression\\n1\\n4,101.6205\\n4,101.6205\\n500.7921\\n0.0000\\nResidual\\n4\\n32.7611\\n8.1903\\n\\xa0\\n\\xa0\\nTotal\\n5\\n4,134.3815\\n\\xa0\\n\\xa0\\n\\xa0\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n475\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Statistic\\np-Value\\nIntercept\\n2.2521\\n1.2739\\n1.7679\\n0.1518\\nIndex return (%)\\n1.0690\\n0.0478\\n22.3784\\n0.0000\\nWhen reviewing the results, Andrea Fusilier suspected that they were unreliable. \\nShe found that the returns for Month 2 should have been 7.21% and 6.49%, in-\\nstead of the large values shown in the first table. Correcting these values resulted \\nin a revised correlation of 0.824 and the following revised regression results:\\nRegression Statistics\\n\\xa0\\n\\xa0\\nR2\\n0.6784\\n\\xa0\\nStandard error\\n2.0624\\n\\xa0\\nObservations\\n6\\n\\xa0\\nSource\\ndf\\nSum of \\nSquares\\nMean \\nSquare\\nF\\np-Value\\nRegression\\n1\\n35.8950\\n35.8950\\n8.4391\\n0.044\\nResidual\\n4\\n17.0137\\n4.2534\\n\\xa0\\n\\xa0\\nTotal\\n5\\n52.91\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Statistic\\np-Value\\nIntercept\\n2.2421\\n0.8635\\n2.5966\\n0.060\\nSlope\\n0.6217\\n0.2143\\n2.9050\\n0.044\\nExplain how the bad data affected the results.\\nThe following information relates to questions \\n4-7\\nAn analyst is examining the annual growth of the money supply for a country \\nover the past 30 years. This country experienced a central bank policy shift 15 \\nyears ago, which altered the approach to the management of the money supply. \\nThe analyst estimated a model using the annual growth rate in the money supply \\nregressed on the variable (SHIFT) that takes on a value of 0 before the policy shift \\nand 1 after. She estimated the following:\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Stat.\\nIntercept\\n5.767264\\n0.445229\\n12.95348\\nSHIFT\\n−5.13912\\n0.629649\\n−8.16188\\nCritical t-values, level of significance of 0.05:\\nOne-sided, left side: −1.701\\nOne-sided, right side: +1.701\\nTwo-sided: ±2.048\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n476\\n4. The variable SHIFT is best described as:\\nA. an indicator variable.\\nB. a dependent variable.\\nC. a continuous variable.\\n5. The interpretation of the intercept is the mean of the annual growth rate of the \\nmoney supply:\\nA. over the enter entire period.\\nB. after the shift in policy.\\nC. before the shift in policy.\\n6. The interpretation of the slope is the:\\nA. change in the annual growth rate of the money supply per year.\\nB. average annual growth rate of the money supply after the shift in policy.\\nC. difference in the average annual growth rate of the money supply from \\nbefore to after the shift in policy.\\n7. Testing whether there is a change in the money supply growth after the shift in \\npolicy, using a 0.05 level of significance, we conclude that there is:\\nA. sufficient evidence that the money supply growth changed.\\nB. not enough evidence that the money supply growth is different from zero.\\nC. not enough evidence to indicate that the money supply growth changed.\\n8. You are examining the results of a regression estimation that attempts to explain \\nthe unit sales growth of a business you are researching. The analysis of variance \\noutput for the regression is given in the following table. The regression was based \\non five observations (n = 5).\\nSource\\ndf\\nSum of \\nSquares\\nMean \\nSquare\\nF\\np-Value\\nRegression\\n1\\n88.0\\n88.0\\n36.667\\n0.00904\\nResidual\\n3\\n7.2\\n2.4\\n\\xa0\\n\\xa0\\nTotal\\n4\\n95.2\\n\\xa0\\n\\xa0\\n\\xa0\\nA. Calculate the sample variance of the dependent variable using information \\nin the table.\\nB. Calculate the coefficient of determination for this estimated model.\\nC. What hypothesis does the F-statistic test?\\nD. Is the F-test significant at the 0.05 significance level?\\nE. Calculate the standard error of the estimate.\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n477\\nThe following information relates to questions \\n9-12\\nKenneth McCoin, CFA, is a challenging interviewer. Last year, he handed each \\njob applicant a sheet of paper with the information in the following table, and \\nhe then asked several questions about regression analysis. Some of McCoin’s \\nquestions, along with a sample of the answers he received to each, are given \\nbelow. McCoin told the applicants that the independent variable is the ratio of \\nnet income to sales for restaurants with a market cap of more than $100 million \\nand the dependent variable is the ratio of cash flow from operations to sales for \\nthose restaurants. Which of the choices provided is the best answer to each of \\nMcCoin’s questions?\\nRegression Statistics\\n\\xa0\\n\\xa0\\nR2\\n0.7436\\n\\xa0\\nStandard error\\n0.0213\\n\\xa0\\nObservations\\n24\\n\\xa0\\nSource\\ndf\\nSum of \\nSquares\\nMean \\nSquare\\nF\\np-Value\\nRegression\\n1\\n0.029\\n0.029000\\n63.81\\n0\\nResidual\\n22\\n0.010\\n0.000455\\n\\xa0\\n\\xa0\\nTotal\\n23\\n0.040\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Statistic\\np-Value\\nIntercept\\n0.077\\n0.007\\n11.328\\n0\\nNet income to \\nsales (%)\\n0.826\\n0.103\\n7.988\\n0\\n9. The coefficient of determination is closest to:\\nA. 0.7436.\\nB. 0.8261.\\nC. 0.8623.\\n10. The correlation between X and Y is closest to:\\nA. −0.7436.\\nB. 0.7436.\\nC. 0.8623.\\n11. If the ratio of net income to sales for a restaurant is 5%, the predicted ratio of \\ncash flow from operations (CFO) to sales is closest to:\\nA. −4.054.\\nB. 0.524.\\nC. 4.207.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n478\\n12. Is the relationship between the ratio of cash flow to operations and the ratio of \\nnet income to sales significant at the 0.05 level?\\nA. No, because the R2 is greater than 0.05\\nB. No, because the p-values of the intercept and slope are less than 0.05\\nC. Yes, because the p-values for F and t for the slope coefficient are less than \\n0.05\\nThe following information relates to questions \\n13-17\\nHoward Golub, CFA, is preparing to write a research report on Stellar Energy \\nCorp. common stock. One of the world’s largest companies, Stellar is in the \\nbusiness of refining and marketing oil. As part of his analysis, Golub wants to \\nevaluate the sensitivity of the stock’s returns to various economic factors. For \\nexample, a client recently asked Golub whether the price of Stellar Energy Corp. \\nstock has tended to rise following increases in retail energy prices. Golub believes \\nthe association between the two variables is negative, but he does not know the \\nstrength of the association.\\nGolub directs his assistant, Jill Batten, to study the relationships between (1) \\nStellar monthly common stock returns and the previous month’s percentage \\nchange in the US Consumer Price Index for Energy (CPIENG) and (2) Stellar \\nmonthly common stock returns and the previous month’s percentage change in \\nthe US Producer Price Index for Crude Energy Materials (PPICEM). Golub wants \\nBatten to run both a correlation and a linear regression analysis. In response, \\nBatten compiles the summary statistics shown in Exhibit 1 for 248 months. All \\nthe data are in decimal form, where 0.01 indicates a 1% return. Batten also runs \\na regression analysis using Stellar monthly returns as the dependent variable and \\nthe monthly change in CPIENG as the independent variable. Exhibit 2 displays \\nthe results of this regression model.\\nExhibit 1: Descriptive Statistics\\n\\xa0\\nStellar Common \\nStock Monthly Return\\nLagged Monthly Change\\n\\xa0\\nCPIENG\\nPPICEM\\nMean\\n0.0123\\n0.0023\\n0.0042\\nStandard deviation\\n0.0717\\n0.0160\\n0.0534\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCovariance, Stellar vs. CPIENG\\n−0.00017\\n\\xa0\\n\\xa0\\nCovariance, Stellar vs. PPICEM\\n−0.00048\\n\\xa0\\n\\xa0\\nCovariance, CPIENG vs. PPICEM\\n0.00044\\n\\xa0\\n\\xa0\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n479\\n\\xa0\\nStellar Common \\nStock Monthly Return\\nLagged Monthly Change\\n\\xa0\\nCPIENG\\nPPICEM\\nCorrelation, Stellar vs. CPIENG\\n−0.1452\\n\\xa0\\n\\xa0\\nExhibit 2: Regression Analysis with CPIENG\\nRegression Statistics\\nR2\\n0.0211\\n\\xa0\\nStandard error of the estimate\\n0.0710\\n\\xa0\\nObservations\\n248\\n\\xa0\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Statistic\\nIntercept\\n0.0138\\n0.0046\\n3.0275\\nCPIENG (%)\\n−0.6486\\n0.2818\\n−2.3014\\nCritical t-values\\nOne-sided, left side: −1.651\\nOne-sided, right side: +1.651\\nTwo-sided: ±1.967\\n13. Which of the following best describes Batten’s regression?\\nA. Time-series regression\\nB. Cross-sectional regression\\nC. Time-series and cross-sectional regression\\n14. Based on the regression, if the CPIENG decreases by 1.0%, the expected return \\non Stellar common stock during the next period is closest to:\\nA. 0.0073 (0.73%).\\nB. 0.0138 (1.38%).\\nC. 0.0203 (2.03%).\\n15. Based on Batten’s regression model, the coefficient of determination indicates \\nthat:\\nA. Stellar’s returns explain 2.11% of the variability in CPIENG.\\nB. Stellar’s returns explain 14.52% of the variability in CPIENG.\\nC. changes in CPIENG explain 2.11% of the variability in Stellar’s returns.\\n16. For Batten’s regression model, 0.0710 is the standard deviation of:\\nA. the dependent variable.\\nB. the residuals from the regression.\\nC. the predicted dependent variable from the regression.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n480\\n17. For the analysis run by Batten, which of the following is an incorrect conclusion \\nfrom the regression output?\\nA. The estimated intercept from Batten’s regression is statistically different \\nfrom zero at the 0.05 level of significance.\\nB. In the month after the CPIENG declines, Stellar’s common stock is expected \\nto exhibit a positive return.\\nC. Viewed in combination, the slope and intercept coefficients from Batten’s \\nregression are not statistically different from zero at the 0.05 level of \\nsignificance.\\nThe following information relates to questions \\n18-26\\nAnh Liu is an analyst researching whether a company’s debt burden affects inves-\\ntors’ decision to short the company’s stock. She calculates the short interest ratio \\n(the ratio of short interest to average daily share volume, expressed in days) for \\n50 companies as of the end of 2016 and compares this ratio with the companies’ \\ndebt ratio (the ratio of total liabilities to total assets, expressed in decimal form).\\nLiu provides a number of statistics in Exhibit 1. She also estimates a simple \\nregression to investigate the effect of the debt ratio on a company’s short inter-\\nest ratio. The results of this simple regression, including the analysis of variance \\n(ANOVA), are shown in Exhibit 2.\\nIn addition to estimating a regression equation, Liu graphs the 50 observations \\nusing a scatter plot, with the short interest ratio on the vertical axis and the debt \\nratio on the horizontal axis.\\nExhibit 1: Summary Statistics\\nStatistic\\nDebt Ratio \\nXi\\n\\xa0\\nShort Interest Ratio \\nYi\\nSum\\n19.8550\\n\\xa0\\n192.3000\\nSum of squared deviations \\nfrom the mean\\n ∑ \\ni=1\\n  \\nn\\n   ( X i −  _\\n \\nX  ) 2  = 2.2225. \\xa0\\n ∑ \\ni=1\\n  \\nn\\n   ( Y i −  _\\n \\nY  ) 2  = 412.2042. \\nSum of cross-products of devi-\\nations from the mean\\n ∑ \\ni=1\\n  \\nn\\n    ( X i −  _\\n \\nX  )   ( Y i −  _\\n \\nY  )   = − 9.2430. \\nExhibit 2: Regression of the Short Interest Ratio on the Debt Ratio\\nANOVA\\nDegrees of \\nFreedom (df)\\nSum of Squares\\nMean Square\\nRegression\\n1\\n38.4404\\n38.4404\\nResidual\\n48\\n373.7638\\n7.7867\\nTotal\\n49\\n412.2042\\n\\xa0\\nRegression Statistics\\n\\xa0\\n\\xa0\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n481\\nANOVA\\nDegrees of \\nFreedom (df)\\nSum of Squares\\nMean Square\\nR2\\n0.0933\\n\\xa0\\n\\xa0\\nStandard error of \\nestimate\\n2.7905\\n\\xa0\\n\\xa0\\nObservations\\n50\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Statistic\\nIntercept\\n5.4975\\n0.8416\\n6.5322\\nDebt ratio (%)\\n−4.1589\\n1.8718\\n−2.2219\\nCritical t-values for a 0.05 level of significance:\\nOne-sided, left side: −1.677\\nOne-sided, right side: +1.677\\nTwo-sided: ±2.011\\nLiu is considering three interpretations of these results for her report on the rela-\\ntionship between debt ratios and short interest ratios:\\nInterpretation 1 Companies’ higher debt ratios cause lower short interest \\nratios.\\nInterpretation 2 Companies’ higher short interest ratios cause higher debt \\nratios.\\nInterpretation 3 Companies with higher debt ratios tend to have lower short \\ninterest ratios.\\nShe is especially interested in using her estimation results to predict the short \\ninterest ratio for MQD Corporation, which has a debt ratio of 0.40.\\n18. Based on Exhibits 1 and 2, if Liu were to graph the 50 observations, the scatter \\nplot summarizing this relation would be best described as:\\nA. horizontal.\\nB. upward sloping.\\nC. downward sloping.\\n19. Based on Exhibit 1, the sample covariance is closest to:\\nA. −9.2430.\\nB. −0.1886.\\nC. 8.4123.\\n20. Based on Exhibits 1 and 2, the correlation between the debt ratio and the short \\ninterest ratio is closest to:\\nA. −0.3054.\\nB. 0.0933.\\nC. 0.3054.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n482\\n21. Which of the interpretations best describes Liu’s findings?\\nA. Interpretation 1\\nB. Interpretation 2\\nC. Interpretation 3\\n22. The dependent variable in Liu’s regression analysis is the:\\nA. intercept.\\nB. debt ratio.\\nC. short interest ratio.\\n23. Based on Exhibit 2, the degrees of freedom for the t-test of the slope coefficient \\nin this regression are:\\nA. 48.\\nB. 49.\\nC. 50.\\n24. Which of the following should Liu conclude from the results shown in Exhibit 2?\\nA. The average short interest ratio is 5.4975.\\nB. The estimated slope coefficient is different from zero at the 0.05 level of \\nsignificance.\\nC. The debt ratio explains 30.54% of the variation in the short interest ratio.\\n25. Based on Exhibit 2, the short interest ratio expected for MQD Corporation is \\nclosest to:\\nA. 3.8339.\\nB. 5.4975.\\nC. 6.2462.\\n26. Based on Liu’s regression results in Exhibit 2, the F-statistic for testing whether \\nthe slope coefficient is equal to zero is closest to:\\nA. −2.2219.\\nB. 3.5036.\\nC. 4.9367.\\nThe following information relates to questions \\n27-31\\nElena Vasileva recently joined EnergyInvest as a junior portfolio analyst. Vasile-\\nva’s supervisor asks her to evaluate a potential investment opportunity in Amtex, \\na multinational oil and gas corporation based in the United States. Vasileva’s \\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n483\\nsupervisor suggests using regression analysis to examine the relation between \\nAmtex shares and returns on crude oil.\\nVasileva notes the following assumptions of regression analysis:\\nAssumption 1 \\nThe error term is uncorrelated across observations.\\nAssumption 2 \\nThe variance of the error term is the same for all \\nobservations.\\nAssumption 3 \\nThe dependent variable is normally distributed.\\nVasileva runs a regression of Amtex share returns on crude oil returns using the \\nmonthly data she collected. Selected data used in the regression are presented in \\nExhibit 1, and selected regression output is presented in Exhibit 2. She uses a 1% \\nlevel of significance in all her tests.\\nExhibit 1: Selected Data for Crude Oil Returns and Amtex Share Returns\\n\\xa0\\nOil Return \\n(Xi)\\nAmtex Return \\n(Yi)\\nCross-Product \\n  ( X i −  \\n_\\n \\nX  )   ( Y i −  \\n_\\n \\nY  )   \\nPredicted \\nAmtex Return \\n ˆ \\nY  i  \\nRegression \\nResidual \\n Y i −  ˆ \\nY  i  \\nSquared Residual \\n ( Y i −  ˆ \\nY  i ) \\n2  \\nMonth 1\\n−0.032000\\n0.033145\\n−0.000388\\n0.002011\\n−0.031134\\n0.000969\\n⋮\\n⋮\\n⋮\\n⋮\\n⋮\\n⋮\\n⋮\\nMonth 36\\n0.028636\\n0.062334\\n0.002663\\n0.016282\\n−0.046053\\n0.002121\\nSum\\n\\xa0\\n\\xa0\\n0.085598\\n\\xa0\\n\\xa0\\n0.071475\\nAverage\\n−0.018056\\n0.005293\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nExhibit 2: Selected Regression Output, Dependent \\nVariable: Amtex Share Return\\n\\xa0\\nCoefficient\\nStandard Error\\nIntercept\\n0.0095\\n0.0078\\nOil return\\n0.2354\\n0.0760\\nCritical t-values for a 1% level of significance:\\nOne-sided, left side: −2.441\\nOne-sided, right side: +2.441\\nTwo-sided: ±2.728\\nVasileva expects the crude oil return next month, Month 37, to be −0.01. She \\ncomputes the standard error of the forecast to be 0.0469.\\n27. Which of Vasileva’s assumptions regarding regression analysis is incorrect?\\nA. Assumption 1\\nB. Assumption 2\\nC. Assumption 3\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n484\\n28. Based on Exhibit 1, the standard error of the estimate is closest to:\\nA. 0.04456.\\nB. 0.04585.\\nC. 0.05018.\\n29. Based on Exhibit 2, Vasileva should reject the null hypothesis that:\\nA. the slope is less than or equal to 0.15.\\nB. the intercept is less than or equal to zero.\\nC. crude oil returns do not explain Amtex share returns.\\n30. Based on Exhibit 2 and Vasileva’s prediction of the crude oil return for Month 37, \\nthe estimate of Amtex share return for Month 37 is closest to:\\nA. −0.0024.\\nB. 0.0071.\\nC. 0.0119.\\n31. Using information from Exhibit 2, the 99% prediction interval for Amtex share \\nreturn for Month 37 is best described as:\\nA.  ˆ \\nY  f ± 0.0053 .\\nB.  ˆ \\nY  f ± 0.0469 .\\nC.  ˆ \\nY  f ± 0.1279 .\\nThe following information relates to questions \\n32-34\\nDoug Abitbol is a portfolio manager for Polyi Investments, a hedge fund that \\ntrades in the United States. Abitbol manages the hedge fund with the help of \\nRobert Olabudo, a junior portfolio manager.\\nAbitbol looks at economists’ inflation forecasts and would like to examine the \\nrelationship between the US Consumer Price Index (US CPI) consensus forecast \\nand the actual US CPI using regression analysis. Olabudo estimates regression \\ncoefficients to test whether the consensus forecast is unbiased. If the consensus \\nforecasts are unbiased, the intercept should be 0.0 and the slope will be equal to \\n1.0. Regression results are presented in Exhibit 1. Additionally, Olabudo calcu-\\nlates the 95% prediction interval of the actual CPI using a US CPI consensus \\nforecast of 2.8.\\nExhibit 1: Regression Output: Estimating US CPI\\nRegression Statistics\\n\\xa0\\n\\xa0\\n\\xa0\\nR2\\n0.9859\\n\\xa0\\n\\xa0\\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n485\\nRegression Statistics\\n\\xa0\\n\\xa0\\n\\xa0\\nStandard error of estimate\\n0.0009\\n\\xa0\\n\\xa0\\nObservations\\n60\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nStandard Error\\nt-Statistic\\nIntercept\\n0.0001\\n0.0002\\n0.5000\\nUS CPI consensus forecast\\n0.9830\\n0.0155\\n63.4194\\nNotes:\\n1. The absolute value of the critical value for the t-statistic is 2.002 at the 5% \\nlevel of significance.\\n2. The standard deviation of the US CPI consensus forecast is sx = 0.7539.\\n3. The mean of the US CPI consensus forecast is   _\\n \\nX  = 1.3350.\\nFinally, Abitbol and Olabudo discuss the forecast and forecast interval:\\nObservation 1 \\nFor a given confidence level, the forecast interval is the same \\nno matter the US CPI consensus forecast.\\nObservation 2 \\nA larger standard error of the estimate will result in a wider \\nconfidence interval.\\n32. Based on Exhibit 1, Olabudo should:\\nA. conclude that the inflation predictions are unbiased.\\nB. reject the null hypothesis that the slope coefficient equals one.\\nC. reject the null hypothesis that the intercept coefficient equals zero.\\n33. Based on Exhibit 1, Olabudo should calculate a prediction interval for the actual \\nUS CPI closest to:\\nA. 2.7506 to 2.7544.\\nB. 2.7521 to 2.7529.\\nC. 2.7981 to 2.8019.\\n34. Which of Olabudo’s observations of forecasting is correct?\\nA. Only Observation 1\\nB. Only Observation 2\\nC. Both Observation 1 and Observations 2\\nThe following information relates to questions \\n35-38\\nEspey Jones is examining the relation between the net profit margin (NPM) of \\ncompanies, in percent, and their fixed asset turnover (FATO). He collected a \\nsample of 35 companies for the most recent fiscal year and fit several different \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n486\\nfunctional forms, settling on the following model:\\n lnNPM i =  b 0 +  b 1  FATO i . \\nThe results of this estimation are provided in Exhibit 1.\\nExhibit 1: Results of Regressing NPM on FATO\\nSource\\ndf\\nSum of \\nSquares\\nMean \\nSquare\\nF\\np-Value\\nRegression\\n1\\n102.9152\\n102.9152\\n1,486.7079\\n0.0000\\nResidual\\n32\\n2.2152\\n0.0692\\n\\xa0\\n\\xa0\\nTotal\\n33\\n105.1303\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCoefficients\\nStandard \\nError\\nt- Statistic\\np-Value\\n\\xa0\\nIntercept\\n0.5987\\n0.0561\\n10.6749\\n0.0000\\n\\xa0\\nFATO\\n0.2951\\n0.0077\\n38.5579\\n0.0000\\n\\xa0\\n35. The coefficient of determination is closest to:\\nA. 0.0211.\\nB. 0.9789.\\nC. 0.9894.\\n36. The standard error of the estimate is closest to:\\nA. 0.2631.\\nB. 1.7849.\\nC. 38.5579.\\n37. At a 0.01 level of significance, Jones should conclude that:\\nA. the mean net profit margin is 0.5987%.\\nB. the variation of the fixed asset turnover explains the variation of the natural \\nlog of the net profit margin.\\nC. a change in the fixed asset turnover from 3 to 4 times is likely to result in a \\nchange in the net profit margin of 0.5987%.\\n38. The predicted net profit margin for a company with a fixed asset turnover of 2 \\ntimes is closest to:\\nA. 1.1889%.\\nB. 1.8043%.\\nC. 3.2835%\\n39. Using the same information as in Question 8, what would Accent’s cost of goods \\n© CFA Institute. For candidate use only. Not for distribution.\\nPractice Problems\\n487\\nsold be under the weighted average cost method?\\nA. ₤120,000.\\nB. ₤122,000.\\nC. ₤124,000.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n488\\n',\n",
       "     'children': []},\n",
       "    {'title': 'Solutions',\n",
       "     'page_number': 498,\n",
       "     'content': 'Learning Module 7 \\nIntroduction to Linear Regression\\n488\\nSOLUTIONS\\n1. C is correct. Homoskedasticity is the situation in which the variance of the resid-\\nuals is constant across the observations.\\n2. \\nA. The coefficient of determination is 0.4279:\\n \\nExplained variation \\n \\n______________ \\n \\nTotal variation  =  60.16 \\n_ \\n140.58  = 0.4279. \\nB.  F =  \\n 60.16 ⁄ 1  \\n_______________ \\n \\n  ( 140.58 − 60.16 ) ⁄ ( 60 − 2 )    =  60.16 \\n_ \\n1.3866  = 43.3882. \\nC. Begin with the sum of squares error of 140.58 − 60.16 = 80.42. Then calcu-\\nlate the mean square error of 80.42 ÷ (60 − 2) = 1.38655. The standard error \\nof the estimate is the square root of the mean square error:   s e =  √ _ \\n1.38655   = \\n1.1775.\\nD. The sample variance of the dependent variable uses the total variation of the \\ndependent variable and divides it by the number of observations less one:\\n ∑ \\ni=1\\n  \\nn\\n   \\n ( Y i −  \\n_\\n \\nY  ) \\n_ 2  \\nn − 1  =  Total variation \\n \\n___________ \\nn − 1 \\n =  140.58 \\n_ \\n60 − 1   = 2.3827. \\nThe sample standard deviation of the dependent variable is the square root \\nof the variance, or   √ _ \\n2.3827  = 1.544 .\\n3. The Month 2 data point is an outlier, lying far away from the other data values. \\nBecause this outlier was caused by a data entry error, correcting the outlier \\nimproves the validity and reliability of the regression. In this case, revised R2 is \\nlower (from 0.9921 to 0.6784). The outliers created the illusion of a better fit from \\nthe higher R2; the outliers altered the estimate of the slope. The standard error of \\nthe estimate is lower when the data error is corrected (from 2.8619 to 2.0624), as \\na result of the lower mean square error. However, at a 0.05 level of significance, \\nboth models fit well. The difference in the fit is illustrated in Exhibit 1.\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n489\\nExhibit 1: The Fit of the Model with and without Data Errors\\n0\\n0\\nPortfolio Return\\nA. Before the Data Errors Are Corrected\\n80\\n80\\n60\\n60\\n70\\n70\\n30\\n30\\n40\\n40\\n10\\n10\\n50\\n50\\n20\\n20\\n–10\\n–10\\n–20\\n–20\\n80\\n80\\n40\\n40\\n60\\n60\\n20\\n20\\nIndex Return\\nPortfolio Return (%)\\nB. After the Data Errors Are Corrected\\n8\\n4\\n6\\n–2\\n–2\\n0\\n2\\n–4\\n–4\\n–10\\n–10\\n10\\n10\\n0\\n5\\n–5\\n–5\\nIndex Return (%)\\n4. A is correct. SHIFT is an indicator or dummy variable because it takes on only \\nthe values 0 and 1.\\n5. C is correct. In a simple regression with a single indicator variable, the intercept \\nis the mean of the dependent variable when the indicator variable takes on a \\nvalue of zero, which is before the shift in policy in this case.\\n6. C is correct. Whereas the intercept is the average of the dependent variable when \\nthe indicator variable is zero (that is, before the shift in policy), the slope is the \\ndifference in the mean of the dependent variable from before to after the change \\nin policy.\\n7. A is correct. The null hypothesis of no difference in the annual growth rate is \\nrejected at the 0.05 level: The calculated test statistic of −8.16188 is outside the \\nbounds of ±2.048.\\n8. \\nA. The sample variance of the dependent variable is the sum of squares total \\ndivided by its degrees of freedom (n − 1 = 5 − 1 = 4, as given). Thus, the \\nsample variance of the dependent variable is 95.2 ÷ 4 = 23.8.\\nB. The coefficient of determination = 88.0 ÷ 95.2 = 0.92437.\\nC. The F-statistic tests whether all the slope coefficients in a linear regression \\nare equal to zero.\\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n490\\nD. The calculated value of the F-statistic is 36.667, as shown in the table. The \\ncorresponding p-value is less than 0.05, so you reject the null hypothesis of a \\nslope equal to zero.\\nE. The standard error of the estimate is the square root of the mean square \\nerror:   s e =  √ _ \\n2.4   = 1.54919.\\n9. A is correct. The coefficient of determination is the same as R2, which is 0.7436 in \\nthe table.\\n10. C is correct. Because the slope is positive, the correlation between X and Y is sim-\\nply the square root of the coefficient of determination:   √ _ \\n0.7436  =\\xa00.8623. \\n11. C is correct. To make a prediction using the regression model, multiply the slope \\ncoefficient by the forecast of the independent variable and add the result to the \\nintercept. Expected value of CFO to sales = 0.077 + (0.826 × 5) = 4.207.\\n12. C is correct. The p-value is the smallest level of significance at which the null hy-\\npotheses concerning the slope coefficient can be rejected. In this case, the p-value \\nis less than 0.05, and thus the regression of the ratio of cash flow from operations \\nto sales on the ratio of net income to sales is significant at the 5% level.\\n13. A is correct. The data are observations over time.\\n14. C is correct. From the regression equation, Expected return = 0.0138 + (−0.6486 \\n× −0.01) = 0.0138 + 0.006486 = 0.0203, or 2.03%.\\n15. C is correct. R2 is the coefficient of determination. In this case, it shows that \\n2.11% of the variability in Stellar’s returns is explained by changes in CPIENG.\\n16. B is correct. The standard error of the estimate is the standard deviation of the \\nregression residuals.\\n17. C is the correct response because it is a false statement. The slope and intercept \\nare both statistically different from zero at the 0.05 level of significance.\\n18. C is correct. The slope coefficient (shown in Exhibit 2) is negative. We could also \\ndetermine this by looking at the cross-product (Exhibit 1), which is negative.\\n19. B is correct. The sample covariance is calculated as\\n \\n ∑ \\ni=1\\n  \\nn\\n    ( X i −  \\n_\\n \\nX  )   ( Y i −  \\n_\\n \\n Y  )    \\n________________ \\nn − 1 \\n = −\\u200a9.2430 ÷ 49 = −\\u200a0.1886 .\\n20. A is correct. In simple regression, the R2 is the square of the pairwise correlation. \\nBecause the slope coefficient is negative, the correlation is the negative of the \\nsquare root of 0.0933, or −0.3054.\\n21. C is correct. Conclusions cannot be drawn regarding causation; they can be \\ndrawn only about association; therefore, Interpretations 1 and 2 are incorrect.\\n22. C is correct. Liu explains the variation of the short interest ratio using the varia-\\ntion of the debt ratio.\\n23. A is correct. The degrees of freedom are the number of observations minus the \\nnumber of parameters estimated, which equals 2 in this case (the intercept and \\nthe slope coefficient). The number of degrees of freedom is 50 − 2 = 48.\\n24. B is correct. The t-statistic is −2.2219, which is outside the bounds created by \\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n491\\nthe critical t-values of ±2.011 for a two-tailed test with a 5% significance level. \\nThe value of 2.011 is the critical t-value for the 5% level of significance (2.5% in \\none tail) for 48 degrees of freedom. A is incorrect because the mean of the short \\ninterest ratio is 192.3 ÷ 50 = 3.846. C is incorrect because the debt ratio explains \\n9.33% of the variation of the short interest ratio.\\n25. A is correct. The predicted value of the short interest ratio = 5.4975 + (−4.1589 × \\n0.40) = 5.4975 − 1.6636 = 3.8339.\\n26. C is correct because  F =  Mean\\xa0square\\xa0regression \\n \\n__________________ \\n \\nMean\\xa0square\\xa0error  =  38.4404 \\n_ \\n7.7867  = 4.9367 .\\n27. C is correct. The assumptions of the linear regression model are that (1) the rela-\\ntionship between the dependent variable and the independent variable is linear \\nin the parameters b0 and b1, (2) the residuals are independent of one another, (3) \\nthe variance of the error term is the same for all observations, and (4) the error \\nterm is normally distributed. Assumption 3 is incorrect because the dependent \\nvariable need not be normally distributed.\\n28. B is correct. The standard error of the estimate for a linear regression model with \\none independent variable is calculated as the square root of the mean square error:\\n s e =  √ _ \\n 0.071475 \\n_ \\n34 \\n  = 0.04585. \\n29. C is correct. Crude oil returns explain the Amtex share returns if the slope coef-\\nficient is statistically different from zero. The slope coefficient is 0.2354, and the \\ncalculated t-statistic is\\n t =  0.2354 − 0.0000 \\n \\n____________ \\n0.0760 \\n = 3.0974, \\nwhich is outside the bounds of the critical values of ±2.728.\\nTherefore, Vasileva should reject the null hypothesis that crude oil returns do not \\nexplain Amtex share returns, because the slope coefficient is statistically different \\nfrom zero.\\nA is incorrect because the calculated t-statistic for testing the slope against 0.15 is  \\nt =  0.2354 − 0.1500 \\n \\n____________ \\n0.0760 \\n = 1.1237 , which is less than the critical value of +2.441.\\nB is incorrect because the calculated t-statistic is  t =  0.0095 − 0.0000 \\n \\n____________ \\n0.0078 \\n = 1.2179 , \\nwhich is less than the critical value of +2.441.\\n30. B is correct. The predicted value of the dependent variable, Amtex share return, \\ngiven the value of the independent variable, crude oil return, −0.01, is calculated \\nas  ˆ \\nY  =  ˆ \\nb  0 +  ˆ \\nb  1  X i = 0.0095 +   [ 0.2354 ×   ( − 0.01 )  ]  = 0.0071. \\n31. C is correct. The predicted share return is 0.0095 + [0.2354 × (−0.01)] = 0.0071. \\nThe lower limit for the prediction interval is 0.0071 − (2.728 × 0.0469) = −0.1208, \\nand the upper limit for the prediction interval is 0.0071 + (2.728 × 0.0469) = \\n0.1350.\\nA is incorrect because the bounds of the interval should be based on the standard \\nerror of the forecast and the critical t-value, not on the mean of the dependent \\nvariable.\\nB is incorrect because bounds of the interval are based on the product of the \\nstandard error of the forecast and the critical t-value, not simply the standard \\nerror of the forecast.\\n32. A is correct. We fail to reject the null hypothesis of a slope equal to one, and we \\nfail to reject the null hypothesis of an intercept equal to zero. The test of the slope \\nequal to 1.0 is  t =  0.9830 − 1.000 \\n___________ \\n0.0155 \\n = − 1.09677 . The test of the intercept equal to \\n© CFA Institute. For candidate use only. Not for distribution.\\nLearning Module 7 \\nIntroduction to Linear Regression\\n492\\n0.0 is  t =  0.0001 − 0.0000 \\n \\n____________ \\n.00002 \\n = 0.5000 . Therefore, we conclude that the forecasts are \\nunbiased.\\n33. A is correct. The forecast interval for inflation is calculated in three steps:\\nStep 1. Make the prediction given the US CPI forecast of 2.8:\\n \\n Y  \\n⌢ =  b 0 +  b 1 X\\n  \\n \\n= 0.0001 +   ( 0.9830 × 2.8 )    \\n \\n \\n= 2.7525.\\n \\n \\nStep 2. Compute the variance of the prediction error:\\n \\n s f 2 =  s e 2  { 1 +   ( 1\\u200a/\\u200an )  +   [ ( X f −  \\n_\\n \\nX  ) \\n2\\n ]  \\u200a/\\u200a  [  ( n − 1 )  ×  s x 2 ]  }  .\\n  \\n \\n \\n \\n \\n s f 2 =  0.0009 2  { 1 +   ( 1\\u200a/\\u200a60 )  +   [ ( 2.8 − 1.3350 ) 2 ]  \\u200a/\\u200a  [  ( 60 − 1 )  ×  0.7539 2 ]  }  .  \\n \\n \\n \\n \\n \\n s f 2 = 0.00000088.\\n  \\n \\n s f = 0.0009.\\n \\n \\nStep 3. Compute the prediction interval:\\n ˆ \\nY  ±  t c ×  s f  \\n 2.7525 ± (2.0 × 0.0009)\\n Lower bound: 2.7525 − (2.0 × 0.0009) = 2.7506.\\n Upper bound: 2.7525 + (2.0 × 0.0009) = 2.7544.\\nSo, given the US CPI forecast of 2.8, the 95% prediction interval is 2.7506 to 2.7544.\\n34. B is correct. The confidence level influences the width of the forecast interval \\nthrough the critical t-value that is used to calculate the distance from the fore-\\ncasted value: The larger the confidence level, the wider the interval. Therefore, \\nObservation 1 is not correct.\\nObservation 2 is correct. The greater the standard error of the estimate, the \\ngreater the standard error of the forecast.\\n35. B is correct. The coefficient of determination is 102.9152 ÷ 105.1303 = 0.9789.\\n36. A is correct. The standard error is the square root of the mean square error, or  \\n√ _ \\n0.0692  = 0.2631 .\\n37. B is correct. The p-value corresponding to the slope is less than 0.01, so we reject \\nthe null hypothesis of a zero slope, concluding that the fixed asset turnover ex-\\nplains the natural log of the net profit margin.\\n38. C is correct. The predicted natural log of the net profit margin is 0.5987 + (2 × \\n0.2951) = 1.1889. The predicted net profit margin is  e 1.1889 = 3.2835 %.\\n39. C is correct. Under the weighted average cost method:\\nOctober purchases \\n10,000 units\\n\\xa0\\n$100,000\\nNovember purchases\\n5,000 units\\n\\xa0\\n$55,000\\n\\xa0\\xa0\\xa0Total\\n15,000 units\\n\\xa0\\n$155,000\\n$155,000/15,000 units = $10.3333 \\n$10.3333 × 12,000 units = $124,000\\n© CFA Institute. For candidate use only. Not for distribution.\\nSolutions\\n493\\nAPPENDICES\\nAppendix A \\nCumulative Probabilities for a Standard Normal Distribution\\nAppendix B\\nTable of the Student’s t-Distribution (One-Tailed Probabilities) \\nAppendix C\\nValues of X2 (Degrees of Freedom, Level of Significance) \\nAppendix D\\nTable of the F-Distribution \\nAppendix E\\nCritical Values for the Durbin-Watson Statistic (α = .05)\\n© CFA Institute. For candidate use only. Not for distribution.\\n494\\n\\xa0Appendix A \\n\\xa0Cumulative Probabilities for a Standard Normal Distribution  \\n\\xa0P(Z ≤ x) = N(x) for x ≥ 0 or P(Z ≤ z) = N(z) for z ≥ 0\\n x or z\\n 0\\n 0.01\\n 0.02\\n 0.03\\n 0.04\\n 0.05\\n 0.06\\n 0.07\\n 0.08\\n 0.09\\n 0.00\\n 0.5000\\n 0.5040\\n 0.5080\\n 0.5120\\n 0.5160\\n 0.5199\\n 0.5239\\n 0.5279\\n 0.5319\\n 0.5359\\n 0.10\\n 0.5398\\n 0.5438\\n 0.5478\\n 0.5517\\n 0.5557\\n 0.5596\\n 0.5636\\n 0.5675\\n 0.5714\\n 0.5753\\n 0.20\\n 0.5793\\n 0.5832\\n 0.5871\\n 0.5910\\n 0.5948\\n 0.5987\\n 0.6026\\n 0.6064\\n 0.6103\\n 0.6141\\n 0.30\\n 0.6179\\n 0.6217\\n 0.6255\\n 0.6293\\n 0.6331\\n 0.6368\\n 0.6406\\n 0.6443\\n 0.6480\\n 0.6517\\n 0.40\\n 0.6554\\n 0.6591\\n 0.6628\\n 0.6664\\n 0.6700\\n 0.6736\\n 0.6772\\n 0.6808\\n 0.6844\\n 0.6879\\n 0.50\\n 0.6915\\n 0.6950\\n 0.6985\\n 0.7019\\n 0.7054\\n 0.7088\\n 0.7123\\n 0.7157\\n 0.7190\\n 0.7224\\n 0.60\\n 0.7257\\n 0.7291\\n 0.7324\\n 0.7357\\n 0.7389\\n 0.7422\\n 0.7454\\n 0.7486\\n 0.7517\\n 0.7549\\n 0.70\\n 0.7580\\n 0.7611\\n 0.7642\\n 0.7673\\n 0.7704\\n 0.7734\\n 0.7764\\n 0.7794\\n 0.7823\\n 0.7852\\n 0.80\\n 0.7881\\n 0.7910\\n 0.7939\\n 0.7967\\n 0.7995\\n 0.8023\\n 0.8051\\n 0.8078\\n 0.8106\\n 0.8133\\n 0.90\\n 0.8159\\n 0.8186\\n 0.8212\\n 0.8238\\n 0.8264\\n 0.8289\\n 0.8315\\n 0.8340\\n 0.8365\\n 0.8389\\n 1.00\\n 0.8413\\n 0.8438\\n 0.8461\\n 0.8485\\n 0.8508\\n 0.8531\\n 0.8554\\n 0.8577\\n 0.8599\\n 0.8621\\n 1.10\\n 0.8643\\n 0.8665\\n 0.8686\\n 0.8708\\n 0.8729\\n 0.8749\\n 0.8770\\n 0.8790\\n 0.8810\\n 0.8830\\n 1.20\\n 0.8849\\n 0.8869\\n 0.8888\\n 0.8907\\n 0.8925\\n 0.8944\\n 0.8962\\n 0.8980\\n 0.8997\\n 0.9015\\n 1.30\\n 0.9032\\n 0.9049\\n 0.9066\\n 0.9082\\n 0.9099\\n 0.9115\\n 0.9131\\n 0.9147\\n 0.9162\\n 0.9177\\n 1.40\\n 0.9192\\n 0.9207\\n 0.9222\\n 0.9236\\n 0.9251\\n 0.9265\\n 0.9279\\n 0.9292\\n 0.9306\\n 0.9319\\n 1.50\\n 0.9332\\n 0.9345\\n 0.9357\\n 0.9370\\n 0.9382\\n 0.9394\\n 0.9406\\n 0.9418\\n 0.9429\\n 0.9441\\n 1.60\\n 0.9452\\n 0.9463\\n 0.9474\\n 0.9484\\n 0.9495\\n 0.9505\\n 0.9515\\n 0.9525\\n 0.9535\\n 0.9545\\n 1.70\\n 0.9554\\n 0.9564\\n 0.9573\\n 0.9582\\n 0.9591\\n 0.9599\\n 0.9608\\n 0.9616\\n 0.9625\\n 0.9633\\n 1.80\\n 0.9641\\n 0.9649\\n 0.9656\\n 0.9664\\n 0.9671\\n 0.9678\\n 0.9686\\n 0.9693\\n 0.9699\\n 0.9706\\n 1.90\\n 0.9713\\n 0.9719\\n 0.9726\\n 0.9732\\n 0.9738\\n 0.9744\\n 0.9750\\n 0.9756\\n 0.9761\\n 0.9767\\n 2.00\\n 0.9772\\n 0.9778\\n 0.9783\\n 0.9788\\n 0.9793\\n 0.9798\\n 0.9803\\n 0.9808\\n 0.9812\\n 0.9817\\n 2.10\\n 0.9821\\n 0.9826\\n 0.9830\\n 0.9834\\n 0.9838\\n 0.9842\\n 0.9846\\n 0.9850\\n 0.9854\\n 0.9857\\n 2.20\\n 0.9861\\n 0.9864\\n 0.9868\\n 0.9871\\n 0.9875\\n 0.9878\\n 0.9881\\n 0.9884\\n 0.9887\\n 0.9890\\n 2.30\\n 0.9893\\n 0.9896\\n 0.9898\\n 0.9901\\n 0.9904\\n 0.9906\\n 0.9909\\n 0.9911\\n 0.9913\\n 0.9916\\n 2.40\\n 0.9918\\n 0.9920\\n 0.9922\\n 0.9925\\n 0.9927\\n 0.9929\\n 0.9931\\n 0.9932\\n 0.9934\\n 0.9936\\n 2.50\\n 0.9938\\n 0.9940\\n 0.9941\\n 0.9943\\n 0.9945\\n 0.9946\\n 0.9948\\n 0.9949\\n 0.9951\\n 0.9952\\n 2.60\\n 0.9953\\n 0.9955\\n 0.9956\\n 0.9957\\n 0.9959\\n 0.9960\\n 0.9961\\n 0.9962\\n 0.9963\\n 0.9964\\n 2.70\\n 0.9965\\n 0.9966\\n 0.9967\\n 0.9968\\n 0.9969\\n 0.9970\\n 0.9971\\n 0.9972\\n 0.9973\\n 0.9974\\n 2.80\\n 0.9974\\n 0.9975\\n 0.9976\\n 0.9977\\n 0.9977\\n 0.9978\\n 0.9979\\n 0.9979\\n 0.9980\\n 0.9981\\n 2.90\\n 0.9981\\n 0.9982\\n 0.9982\\n 0.9983\\n 0.9984\\n 0.9984\\n 0.9985\\n 0.9985\\n 0.9986\\n 0.9986\\n 3.00\\n 0.9987\\n 0.9987\\n 0.9987\\n 0.9988\\n 0.9988\\n 0.9989\\n 0.9989\\n 0.9989\\n 0.9990\\n 0.9990\\n 3.10\\n 0.9990\\n 0.9991\\n 0.9991\\n 0.9991\\n 0.9992\\n 0.9992\\n 0.9992\\n 0.9992\\n 0.9993\\n 0.9993\\n 3.20\\n 0.9993\\n 0.9993\\n 0.9994\\n 0.9994\\n 0.9994\\n 0.9994\\n 0.9994\\n 0.9995\\n 0.9995\\n 0.9995\\n 3.30\\n 0.9995\\n 0.9995\\n 0.9995\\n 0.9996\\n 0.9996\\n 0.9996\\n 0.9996\\n 0.9996\\n 0.9996\\n 0.9997\\n 3.40\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9997\\n 0.9998\\n 3.50\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 0.9998\\n 3.60\\n 0.9998\\n 0.9998\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 3.70\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 3.80\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 0.9999\\n 3.90\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 4.00\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n 1.0000\\n For example, to ﬁ nd the z-value leaving 2.5\\xa0percent of the area/probability in the upper tail, ﬁ nd the element 0.9750 in the body of the table. Read \\n1.90 at the left end of the element’s row and 0.06 at the top of the element’s column, to give 1.90\\xa0+ 0.06\\xa0= 1.96. Table generated with Excel.\\n Quantitative Methods for Investment Analysis, Second Edition, by Richard A. DeFusco, CFA, Dennis W. McLeavey, CFA, Jerald E. Pinto, CFA, and \\nDavid E. Runkle, CFA. Copyright © 2004 by CFA Institute.\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n495\\n\\xa0Appendix A (continued) \\n\\xa0Cumulative Probabilities for a Standard Normal Distribution  \\n\\xa0P(Z ≤ x) = N(x) for x ≤ 0 or P(Z ≤ z) = N(z) for z ≤ 0\\n x or z\\n 0\\n 0.01\\n 0.02\\n 0.03\\n 0.04\\n 0.05\\n 0.06\\n 0.07\\n 0.08\\n 0.09\\n 0.0\\n 0.5000\\n 0.4960\\n 0.4920\\n 0.4880\\n 0.4840\\n 0.4801\\n 0.4761\\n 0.4721\\n 0.4681\\n 0.4641\\n −0.10\\n 0.4602\\n 0.4562\\n 0.4522\\n 0.4483\\n 0.4443\\n 0.4404\\n 0.4364\\n 0.4325\\n 0.4286\\n 0.4247\\n −0.20\\n 0.4207\\n 0.4168\\n 0.4129\\n 0.4090\\n 0.4052\\n 0.4013\\n 0.3974\\n 0.3936\\n 0.3897\\n 0.3859\\n −0.30\\n 0.3821\\n 0.3783\\n 0.3745\\n 0.3707\\n 0.3669\\n 0.3632\\n 0.3594\\n 0.3557\\n 0.3520\\n 0.3483\\n −0.40\\n 0.3446\\n 0.3409\\n 0.3372\\n 0.3336\\n 0.3300\\n 0.3264\\n 0.3228\\n 0.3192\\n 0.3156\\n 0.3121\\n −0.50\\n 0.3085\\n 0.3050\\n 0.3015\\n 0.2981\\n 0.2946\\n 0.2912\\n 0.2877\\n 0.2843\\n 0.2810\\n 0.2776\\n −0.60\\n 0.2743\\n 0.2709\\n 0.2676\\n 0.2643\\n 0.2611\\n 0.2578\\n 0.2546\\n 0.2514\\n 0.2483\\n 0.2451\\n −0.70\\n 0.2420\\n 0.2389\\n 0.2358\\n 0.2327\\n 0.2296\\n 0.2266\\n 0.2236\\n 0.2206\\n 0.2177\\n 0.2148\\n −0.80\\n 0.2119\\n 0.2090\\n 0.2061\\n 0.2033\\n 0.2005\\n 0.1977\\n 0.1949\\n 0.1922\\n 0.1894\\n 0.1867\\n −0.90\\n 0.1841\\n 0.1814\\n 0.1788\\n 0.1762\\n 0.1736\\n 0.1711\\n 0.1685\\n 0.1660\\n 0.1635\\n 0.1611\\n −1.00\\n 0.1587\\n 0.1562\\n 0.1539\\n 0.1515\\n 0.1492\\n 0.1469\\n 0.1446\\n 0.1423\\n 0.1401\\n 0.1379\\n −1.10\\n 0.1357\\n 0.1335\\n 0.1314\\n 0.1292\\n 0.1271\\n 0.1251\\n 0.1230\\n 0.1210\\n 0.1190\\n 0.1170\\n −1.20\\n 0.1151\\n 0.1131\\n 0.1112\\n 0.1093\\n 0.1075\\n 0.1056\\n 0.1038\\n 0.1020\\n 0.1003\\n 0.0985\\n −1.30\\n 0.0968\\n 0.0951\\n 0.0934\\n 0.0918\\n 0.0901\\n 0.0885\\n 0.0869\\n 0.0853\\n 0.0838\\n 0.0823\\n −1.40\\n 0.0808\\n 0.0793\\n 0.0778\\n 0.0764\\n 0.0749\\n 0.0735\\n 0.0721\\n 0.0708\\n 0.0694\\n 0.0681\\n −1.50\\n 0.0668\\n 0.0655\\n 0.0643\\n 0.0630\\n 0.0618\\n 0.0606\\n 0.0594\\n 0.0582\\n 0.0571\\n 0.0559\\n −1.60\\n 0.0548\\n 0.0537\\n 0.0526\\n 0.0516\\n 0.0505\\n 0.0495\\n 0.0485\\n 0.0475\\n 0.0465\\n 0.0455\\n −1.70\\n 0.0446\\n 0.0436\\n 0.0427\\n 0.0418\\n 0.0409\\n 0.0401\\n 0.0392\\n 0.0384\\n 0.0375\\n 0.0367\\n −1.80\\n 0.0359\\n 0.0351\\n 0.0344\\n 0.0336\\n 0.0329\\n 0.0322\\n 0.0314\\n 0.0307\\n 0.0301\\n 0.0294\\n −1.90\\n 0.0287\\n 0.0281\\n 0.0274\\n 0.0268\\n 0.0262\\n 0.0256\\n 0.0250\\n 0.0244\\n 0.0239\\n 0.0233\\n −2.00\\n 0.0228\\n 0.0222\\n 0.0217\\n 0.0212\\n 0.0207\\n 0.0202\\n 0.0197\\n 0.0192\\n 0.0188\\n 0.0183\\n −2.10\\n 0.0179\\n 0.0174\\n 0.0170\\n 0.0166\\n 0.0162\\n 0.0158\\n 0.0154\\n 0.0150\\n 0.0146\\n 0.0143\\n −2.20\\n 0.0139\\n 0.0136\\n 0.0132\\n 0.0129\\n 0.0125\\n 0.0122\\n 0.0119\\n 0.0116\\n 0.0113\\n 0.0110\\n −2.30\\n 0.0107\\n 0.0104\\n 0.0102\\n 0.0099\\n 0.0096\\n 0.0094\\n 0.0091\\n 0.0089\\n 0.0087\\n 0.0084\\n −2.40\\n 0.0082\\n 0.0080\\n 0.0078\\n 0.0075\\n 0.0073\\n 0.0071\\n 0.0069\\n 0.0068\\n 0.0066\\n 0.0064\\n −2.50\\n 0.0062\\n 0.0060\\n 0.0059\\n 0.0057\\n 0.0055\\n 0.0054\\n 0.0052\\n 0.0051\\n 0.0049\\n 0.0048\\n −2.60\\n 0.0047\\n 0.0045\\n 0.0044\\n 0.0043\\n 0.0041\\n 0.0040\\n 0.0039\\n 0.0038\\n 0.0037\\n 0.0036\\n −2.70\\n 0.0035\\n 0.0034\\n 0.0033\\n 0.0032\\n 0.0031\\n 0.0030\\n 0.0029\\n 0.0028\\n 0.0027\\n 0.0026\\n −2.80\\n 0.0026\\n 0.0025\\n 0.0024\\n 0.0023\\n 0.0023\\n 0.0022\\n 0.0021\\n 0.0021\\n 0.0020\\n 0.0019\\n −2.90\\n 0.0019\\n 0.0018\\n 0.0018\\n 0.0017\\n 0.0016\\n 0.0016\\n 0.0015\\n 0.0015\\n 0.0014\\n 0.0014\\n −3.00\\n 0.0013\\n 0.0013\\n 0.0013\\n 0.0012\\n 0.0012\\n 0.0011\\n 0.0011\\n 0.0011\\n 0.0010\\n 0.0010\\n −3.10\\n 0.0010\\n 0.0009\\n 0.0009\\n 0.0009\\n 0.0008\\n 0.0008\\n 0.0008\\n 0.0008\\n 0.0007\\n 0.0007\\n −3.20\\n 0.0007\\n 0.0007\\n 0.0006\\n 0.0006\\n 0.0006\\n 0.0006\\n 0.0006\\n 0.0005\\n 0.0005\\n 0.0005\\n −3.30\\n 0.0005\\n 0.0005\\n 0.0005\\n 0.0004\\n 0.0004\\n 0.0004\\n 0.0004\\n 0.0004\\n 0.0004\\n 0.0003\\n −3.40\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0003\\n 0.0002\\n −3.50\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n 0.0002\\n −3.60\\n 0.0002\\n 0.0002\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n −3.70\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n −3.80\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n 0.0001\\n −3.90\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n −4.00\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n 0.0000\\n For example, to ﬁ nd the z-value leaving 2.5\\xa0percent of the area/probability in the lower tail, ﬁ nd the element 0.0250 in the body of the table. Read \\n–1.90 at the left end of the element’s row and 0.06 at the top of the element’s column, to give –1.90 – 0.06\\xa0= –1.96. Table generated with Excel.\\n© CFA Institute. For candidate use only. Not for distribution.\\n496\\n\\xa0Appendix B  \\n\\xa0Table of the Student’s t-Distribution (One-Tailed Probabilities) \\n df\\n p = 0.10\\n p = 0.05\\n p = 0.025\\n p = 0.01\\n p = 0.005\\n \\n df\\n p = 0.10\\n p = 0.05\\n p = 0.025\\n p = 0.01\\n p = 0.005\\n 1\\n 3.078\\n 6.314\\n 12.706\\n 31.821\\n 63.657\\n \\n 31\\n 1.309\\n 1.696\\n 2.040\\n 2.453\\n 2.744\\n 2\\n 1.886\\n 2.920\\n 4.303\\n 6.965\\n 9.925\\n \\n 32\\n 1.309\\n 1.694\\n 2.037\\n 2.449\\n 2.738\\n 3\\n 1.638\\n 2.353\\n 3.182\\n 4.541\\n 5.841\\n \\n 33\\n 1.308\\n 1.692\\n 2.035\\n 2.445\\n 2.733\\n 4\\n 1.533\\n 2.132\\n 2.776\\n 3.747\\n 4.604\\n \\n 34\\n 1.307\\n 1.691\\n 2.032\\n 2.441\\n 2.728\\n 5\\n 1.476\\n 2.015\\n 2.571\\n 3.365\\n 4.032\\n \\n 35\\n 1.306\\n 1.690\\n 2.030\\n 2.438\\n 2.724\\n 6\\n 1.440\\n 1.943\\n 2.447\\n 3.143\\n 3.707\\n \\n 36\\n 1.306\\n 1.688\\n 2.028\\n 2.434\\n 2.719\\n 7\\n 1.415\\n 1.895\\n 2.365\\n 2.998\\n 3.499\\n \\n 37\\n 1.305\\n 1.687\\n 2.026\\n 2.431\\n 2.715\\n 8\\n 1.397\\n 1.860\\n 2.306\\n 2.896\\n 3.355\\n \\n 38\\n 1.304\\n 1.686\\n 2.024\\n 2.429\\n 2.712\\n 9\\n 1.383\\n 1.833\\n 2.262\\n 2.821\\n 3.250\\n \\n 39\\n 1.304\\n 1.685\\n 2.023\\n 2.426\\n 2.708\\n 10\\n 1.372\\n 1.812\\n 2.228\\n 2.764\\n 3.169\\n \\n 40\\n 1.303\\n 1.684\\n 2.021\\n 2.423\\n 2.704\\n 11\\n 1.363\\n 1.796\\n 2.201\\n 2.718\\n 3.106\\n \\n 41\\n 1.303\\n 1.683\\n 2.020\\n 2.421\\n 2.701\\n 12\\n 1.356\\n 1.782\\n 2.179\\n 2.681\\n 3.055\\n \\n 42\\n 1.302\\n 1.682\\n 2.018\\n 2.418\\n 2.698\\n 13\\n 1.350\\n 1.771\\n 2.160\\n 2.650\\n 3.012\\n \\n 43\\n 1.302\\n 1.681\\n 2.017\\n 2.416\\n 2.695\\n 14\\n 1.345\\n 1.761\\n 2.145\\n 2.624\\n 2.977\\n \\n 44\\n 1.301\\n 1.680\\n 2.015\\n 2.414\\n 2.692\\n 15\\n 1.341\\n 1.753\\n 2.131\\n 2.602\\n 2.947\\n \\n 45\\n 1.301\\n 1.679\\n 2.014\\n 2.412\\n 2.690\\n 16\\n 1.337\\n 1.746\\n 2.120\\n 2.583\\n 2.921\\n \\n 46\\n 1.300\\n 1.679\\n 2.013\\n 2.410\\n 2.687\\n 17\\n 1.333\\n 1.740\\n 2.110\\n 2.567\\n 2.898\\n \\n 47\\n 1.300\\n 1.678\\n 2.012\\n 2.408\\n 2.685\\n 18\\n 1.330\\n 1.734\\n 2.101\\n 2.552\\n 2.878\\n \\n 48\\n 1.299\\n 1.677\\n 2.011\\n 2.407\\n 2.682\\n 19\\n 1.328\\n 1.729\\n 2.093\\n 2.539\\n 2.861\\n \\n 49\\n 1.299\\n 1.677\\n 2.010\\n 2.405\\n 2.680\\n 20\\n 1.325\\n 1.725\\n 2.086\\n 2.528\\n 2.845\\n \\n 50\\n 1.299\\n 1.676\\n 2.009\\n 2.403\\n 2.678\\n 21\\n 1.323\\n 1.721\\n 2.080\\n 2.518\\n 2.831\\n \\n 60\\n 1.296\\n 1.671\\n 2.000\\n 2.390\\n 2.660\\n 22\\n 1.321\\n 1.717\\n 2.074\\n 2.508\\n 2.819\\n \\n 70\\n 1.294\\n 1.667\\n 1.994\\n 2.381\\n 2.648\\n 23\\n 1.319\\n 1.714\\n 2.069\\n 2.500\\n 2.807\\n \\n 80\\n 1.292\\n 1.664\\n 1.990\\n 2.374\\n 2.639\\n 24\\n 1.318\\n 1.711\\n 2.064\\n 2.492\\n 2.797\\n \\n 90\\n 1.291\\n 1.662\\n 1.987\\n 2.368\\n 2.632\\n 25\\n 1.316\\n 1.708\\n 2.060\\n 2.485\\n 2.787\\n \\n 100\\n 1.290\\n 1.660\\n 1.984\\n 2.364\\n 2.626\\n 26\\n 1.315\\n 1.706\\n 2.056\\n 2.479\\n 2.779\\n \\n 110\\n 1.289\\n 1.659\\n 1.982\\n 2.361\\n 2.621\\n 27\\n 1.314\\n 1.703\\n 2.052\\n 2.473\\n 2.771\\n \\n 120\\n 1.289\\n 1.658\\n 1.980\\n 2.358\\n 2.617\\n 28\\n 1.313\\n 1.701\\n 2.048\\n 2.467\\n 2.763\\n \\n 200\\n 1.286\\n 1.653\\n 1.972\\n 2.345\\n 2.601\\n 29\\n 1.311\\n 1.699\\n 2.045\\n 2.462\\n 2.756\\n \\n ∞\\n 1.282\\n 1.645\\n 1.960\\n 2.326\\n 2.576\\n 30\\n 1.310\\n 1.697\\n 2.042\\n 2.457\\n 2.750\\n \\n \\n \\n \\n \\n \\n \\n To f nd a critical t-value, enter the table with df and a specif ed value for a, the signif cance level. For example, with 5 df, α = 0.05 and a one- tailed test, the desired probability in the tail would be \\np = 0.05 and the critical t-value would be t(5, 0.05) = 2.015. With α =0.05 and a two- tailed test, the desired probability in each tail would be p = 0.025\\xa0=α/2, giving t(0.025) = 2.571. Table generated \\nusing Excel. \\n Quantitative Methods for Investment Analysis, Second Edition, by Richard A. DeFusco, CFA, Dennis W. McLeavey, CFA, Jerald E. Pinto, CFA, and \\nCFA Institute.\\nDavid E. Runkle, CFA. Copyright © 2004 by \\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n497\\n\\xa0Appendix C  \\n\\xa0Values of χ2 (Degrees of Freedom, Level of Significance) \\n Degrees of \\nFreedom\\n Probability in Right Tail\\n 0.99\\n 0.975\\n 0.95\\n 0.9\\n 0.1\\n 0.05\\n 0.025\\n 0.01\\n 0.005\\n 1\\n 0.000157\\n 0.000982\\n 0.003932\\n 0.0158\\n 2.706\\n 3.841\\n 5.024\\n 6.635\\n 7.879\\n 2\\n 0.020100\\n 0.050636\\n 0.102586\\n 0.2107\\n 4.605\\n 5.991\\n 7.378\\n 9.210\\n 10.597\\n 3\\n 0.1148\\n 0.2158\\n 0.3518\\n 0.5844\\n 6.251\\n 7.815\\n 9.348\\n 11.345\\n 12.838\\n 4\\n 0.297\\n 0.484\\n 0.711\\n 1.064\\n 7.779\\n 9.488\\n 11.143\\n 13.277\\n 14.860\\n 5\\n 0.554\\n 0.831\\n 1.145\\n 1.610\\n 9.236\\n 11.070\\n 12.832\\n 15.086\\n 16.750\\n 6\\n 0.872\\n 1.237\\n 1.635\\n 2.204\\n 10.645\\n 12.592\\n 14.449\\n 16.812\\n 18.548\\n 7\\n 1.239\\n 1.690\\n 2.167\\n 2.833\\n 12.017\\n 14.067\\n 16.013\\n 18.475\\n 20.278\\n 8\\n 1.647\\n 2.180\\n 2.733\\n 3.490\\n 13.362\\n 15.507\\n 17.535\\n 20.090\\n 21.955\\n 9\\n 2.088\\n 2.700\\n 3.325\\n 4.168\\n 14.684\\n 16.919\\n 19.023\\n 21.666\\n 23.589\\n 10\\n 2.558\\n 3.247\\n 3.940\\n 4.865\\n 15.987\\n 18.307\\n 20.483\\n 23.209\\n 25.188\\n 11\\n 3.053\\n 3.816\\n 4.575\\n 5.578\\n 17.275\\n 19.675\\n 21.920\\n 24.725\\n 26.757\\n 12\\n 3.571\\n 4.404\\n 5.226\\n 6.304\\n 18.549\\n 21.026\\n 23.337\\n 26.217\\n 28.300\\n 13\\n 4.107\\n 5.009\\n 5.892\\n 7.041\\n 19.812\\n 22.362\\n 24.736\\n 27.688\\n 29.819\\n 14\\n 4.660\\n 5.629\\n 6.571\\n 7.790\\n 21.064\\n 23.685\\n 26.119\\n 29.141\\n 31.319\\n 15\\n 5.229\\n 6.262\\n 7.261\\n 8.547\\n 22.307\\n 24.996\\n 27.488\\n 30.578\\n 32.801\\n 16\\n 5.812\\n 6.908\\n 7.962\\n 9.312\\n 23.542\\n 26.296\\n 28.845\\n 32.000\\n 34.267\\n 17\\n 6.408\\n 7.564\\n 8.672\\n 10.085\\n 24.769\\n 27.587\\n 30.191\\n 33.409\\n 35.718\\n 18\\n 7.015\\n 8.231\\n 9.390\\n 10.865\\n 25.989\\n 28.869\\n 31.526\\n 34.805\\n 37.156\\n 19\\n 7.633\\n 8.907\\n 10.117\\n 11.651\\n 27.204\\n 30.144\\n 32.852\\n 36.191\\n 38.582\\n 20\\n 8.260\\n 9.591\\n 10.851\\n 12.443\\n 28.412\\n 31.410\\n 34.170\\n 37.566\\n 39.997\\n 21\\n 8.897\\n 10.283\\n 11.591\\n 13.240\\n 29.615\\n 32.671\\n 35.479\\n 38.932\\n 41.401\\n 22\\n 9.542\\n 10.982\\n 12.338\\n 14.041\\n 30.813\\n 33.924\\n 36.781\\n 40.289\\n 42.796\\n 23\\n 10.196\\n 11.689\\n 13.091\\n 14.848\\n 32.007\\n 35.172\\n 38.076\\n 41.638\\n 44.181\\n 24\\n 10.856\\n 12.401\\n 13.848\\n 15.659\\n 33.196\\n 36.415\\n 39.364\\n 42.980\\n 45.558\\n 25\\n 11.524\\n 13.120\\n 14.611\\n 16.473\\n 34.382\\n 37.652\\n 40.646\\n 44.314\\n 46.928\\n 26\\n 12.198\\n 13.844\\n 15.379\\n 17.292\\n 35.563\\n 38.885\\n 41.923\\n 45.642\\n 48.290\\n 27\\n 12.878\\n 14.573\\n 16.151\\n 18.114\\n 36.741\\n 40.113\\n 43.195\\n 46.963\\n 49.645\\n 28\\n 13.565\\n 15.308\\n 16.928\\n 18.939\\n 37.916\\n 41.337\\n 44.461\\n 48.278\\n 50.994\\n 29\\n 14.256\\n 16.047\\n 17.708\\n 19.768\\n 39.087\\n 42.557\\n 45.722\\n 49.588\\n 52.335\\n 30\\n 14.953\\n 16.791\\n 18.493\\n 20.599\\n 40.256\\n 43.773\\n 46.979\\n 50.892\\n 53.672\\n 50\\n 29.707\\n 32.357\\n 34.764\\n 37.689\\n 63.167\\n 67.505\\n 71.420\\n 76.154\\n 79.490\\n 60\\n 37.485\\n 40.482\\n 43.188\\n 46.459\\n 74.397\\n 79.082\\n 83.298\\n 88.379\\n 91.952\\n 80\\n 53.540\\n 57.153\\n 60.391\\n 64.278\\n 96.578\\n 101.879\\n 106.629\\n 112.329\\n 116.321\\n 100\\n 70.065\\n 74.222\\n 77.929\\n 82.358\\n 118.498\\n 124.342\\n 129.561\\n 135.807\\n 140.170\\n To have a probability of 0.05 in the right tail when df = 5, the tabled value is χ2(5, 0.05) = 11.070.\\n Quantitative Methods for Investment Analysis, Second Edition, by Richard A. DeFusco, CFA, Dennis W. McLeavey, CFA, Jerald E. Pinto, \\nCFA, and David E. Runkle, CFA. Copyright © 2004 by CFA Institute.\\n© CFA Institute. For candidate use only. Not for distribution.\\n498\\n\\xa0Appendix D \\n\\xa0Table of the F-Distribution \\n Panel A. Critical values for right- hand tail area equal to 0.05\\nNumerator: df1 and Denominator: df2\\n \\n df1:1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n 11\\n 12\\n 15\\n 20\\n 21\\n 22\\n 23\\n 24\\n 25\\n 30\\n 40\\n 60\\n 120\\n ∞\\n df2: 1\\n 161\\n 200\\n 216\\n 225\\n 230\\n 234\\n 237\\n 239\\n 241\\n 242\\n 243\\n 244\\n 246\\n 248\\n 248\\n 249\\n 249\\n 249\\n 249\\n 250\\n 251\\n 252\\n 253\\n 254\\n 2\\n 18.5\\n 19.0\\n 19.2\\n 19.2\\n 19.3\\n 19.3\\n 19.4\\n 19.4\\n 19.4\\n 19.4\\n 19.4\\n 19.4\\n 19.4\\n 19.4\\n 19.4\\n 19.5\\n 19.5\\n 19.5\\n 19.5\\n 19.5\\n 19.5\\n 19.5\\n 19.5\\n 19.5\\n 3\\n 10.1\\n 9.55\\n 9.28\\n 9.12\\n 9.01\\n 8.94\\n 8.89\\n 8.85\\n 8.81\\n 8.79\\n 8.76\\n 8.74\\n 8.70\\n 8.66\\n 8.65\\n 8.65\\n 8.64\\n 8.64\\n 8.63\\n 8.62\\n 8.59\\n 8.57\\n 8.55\\n 8.53\\n 4\\n 7.71\\n 6.94\\n 6.59\\n 6.39\\n 6.26\\n 6.16\\n 6.09\\n 6.04\\n 6.00\\n 5.96\\n 5.94\\n 5.91\\n 5.86\\n 5.80\\n 5.79\\n 5.79\\n 5.78\\n 5.77\\n 5.77\\n 5.75\\n 5.72\\n 5.69\\n 5.66\\n 5.63\\n 5\\n 6.61\\n 5.79\\n 5.41\\n 5.19\\n 5.05\\n 4.95\\n 4.88\\n 4.82\\n 4.77\\n 4.74\\n 4.70\\n 4.68\\n 4.62\\n 4.56\\n 4.55\\n 4.54\\n 4.53\\n 4.53\\n 4.52\\n 4.50\\n 4.46\\n 4.43\\n 4.40\\n 4.37\\n 6\\n 5.99\\n 5.14\\n 4.76\\n 4.53\\n 4.39\\n 4.28\\n 4.21\\n 4.15\\n 4.10\\n 4.06\\n 4.03\\n 4.00\\n 3.94\\n 3.87\\n 3.86\\n 3.86\\n 3.85\\n 3.84\\n 3.83\\n 3.81\\n 3.77\\n 3.74\\n 3.70\\n 3.67\\n 7\\n 5.59\\n 4.74\\n 4.35\\n 4.12\\n 3.97\\n 3.87\\n 3.79\\n 3.73\\n 3.68\\n 3.64\\n 3.60\\n 3.57\\n 3.51\\n 3.44\\n 3.43\\n 3.43\\n 3.42\\n 3.41\\n 3.40\\n 3.38\\n 3.34\\n 3.30\\n 3.27\\n 3.23\\n 8\\n 5.32\\n 4.46\\n 4.07\\n 3.84\\n 3.69\\n 3.58\\n 3.50\\n 3.44\\n 3.39\\n 3.35\\n 3.31\\n 3.28\\n 3.22\\n 3.15\\n 3.14\\n 3.13\\n 3.12\\n 3.12\\n 3.11\\n 3.08\\n 3.04\\n 3.01\\n 2.97\\n 2.93\\n 9\\n 5.12\\n 4.26\\n 3.86\\n 3.63\\n 3.48\\n 3.37\\n 3.29\\n 3.23\\n 3.18\\n 3.14\\n 3.10\\n 3.07\\n 3.01\\n 2.94\\n 2.93\\n 2.92\\n 2.91\\n 2.90\\n 2.89\\n 2.86\\n 2.83\\n 2.79\\n 2.75\\n 2.71\\n 10\\n 4.96\\n 4.10\\n 3.71\\n 3.48\\n 3.33\\n 3.22\\n 3.14\\n 3.07\\n 3.02\\n 2.98\\n 2.94\\n 2.91\\n 2.85\\n 2.77\\n 2.76\\n 2.75\\n 2.75\\n 2.74\\n 2.73\\n 2.70\\n 2.66\\n 2.62\\n 2.58\\n 2.54\\n 11\\n 4.84\\n 3.98\\n 3.59\\n 3.36\\n 3.20\\n 3.09\\n 3.01\\n 2.95\\n 2.90\\n 2.85\\n 2.82\\n 2.79\\n 2.72\\n 2.65\\n 2.64\\n 2.63\\n 2.62\\n 2.61\\n 2.60\\n 2.57\\n 2.53\\n 2.49\\n 2.45\\n 2.40\\n 12\\n 4.75\\n 3.89\\n 3.49\\n 3.26\\n 3.11\\n 3.00\\n 2.91\\n 2.85\\n 2.80\\n 2.75\\n 2.72\\n 2.69\\n 2.62\\n 2.54\\n 2.53\\n 2.52\\n 2.51\\n 2.51\\n 2.50\\n 2.47\\n 2.43\\n 2.38\\n 2.34\\n 2.30\\n 13\\n 4.67\\n 3.81\\n 3.41\\n 3.18\\n 3.03\\n 2.92\\n 2.83\\n 2.77\\n 2.71\\n 2.67\\n 2.63\\n 2.60\\n 2.53\\n 2.46\\n 2.45\\n 2.44\\n 2.43\\n 2.42\\n 2.41\\n 2.38\\n 2.34\\n 2.30\\n 2.25\\n 2.21\\n 14\\n 4.60\\n 3.74\\n 3.34\\n 3.11\\n 2.96\\n 2.85\\n 2.76\\n 2.70\\n 2.65\\n 2.60\\n 2.57\\n 2.53\\n 2.46\\n 2.39\\n 2.38\\n 2.37\\n 2.36\\n 2.35\\n 2.34\\n 2.31\\n 2.27\\n 2.22\\n 2.18\\n 2.13\\n 15\\n 4.54\\n 3.68\\n 3.29\\n 3.06\\n 2.90\\n 2.79\\n 2.71\\n 2.64\\n 2.59\\n 2.54\\n 2.51\\n 2.48\\n 2.40\\n 2.33\\n 2.32\\n 2.31\\n 2.30\\n 2.29\\n 2.28\\n 2.25\\n 2.20\\n 2.16\\n 2.11\\n 2.07\\n 16\\n 4.49\\n 3.63\\n 3.24\\n 3.01\\n 2.85\\n 2.74\\n 2.66\\n 2.59\\n 2.54\\n 2.49\\n 2.46\\n 2.42\\n 2.35\\n 2.28\\n 2.26\\n 2.25\\n 2.24\\n 2.24\\n 2.23\\n 2.19\\n 2.15\\n 2.11\\n 2.06\\n 2.01\\n 17\\n 4.45\\n 3.59\\n 3.20\\n 2.96\\n 2.81\\n 2.70\\n 2.61\\n 2.55\\n 2.49\\n 2.45\\n 2.41\\n 2.38\\n 2.31\\n 2.23\\n 2.22\\n 2.21\\n 2.20\\n 2.19\\n 2.18\\n 2.15\\n 2.10\\n 2.06\\n 2.01\\n 1.96\\n 18\\n 4.41\\n 3.55\\n 3.16\\n 2.93\\n 2.77\\n 2.66\\n 2.58\\n 2.51\\n 2.46\\n 2.41\\n 2.37\\n 2.34\\n 2.27\\n 2.19\\n 2.18\\n 2.17\\n 2.16\\n 2.15\\n 2.14\\n 2.11\\n 2.06\\n 2.02\\n 1.97\\n 1.92\\n 19\\n 4.38\\n 3.52\\n 3.13\\n 2.90\\n 2.74\\n 2.63\\n 2.54\\n 2.48\\n 2.42\\n 2.38\\n 2.34\\n 2.31\\n 2.23\\n 2.16\\n 2.14\\n 2.13\\n 2.12\\n 2.11\\n 2.11\\n 2.07\\n 2.03\\n 1.98\\n 1.93\\n 1.88\\n 20\\n 4.35\\n 3.49\\n 3.10\\n 2.87\\n 2.71\\n 2.60\\n 2.51\\n 2.45\\n 2.39\\n 2.35\\n 2.31\\n 2.28\\n 2.20\\n 2.12\\n 2.11\\n 2.10\\n 2.09\\n 2.08\\n 2.07\\n 2.04\\n 1.99\\n 1.95\\n 1.90\\n 1.84\\n 21\\n 4.32\\n 3.47\\n 3.07\\n 2.84\\n 2.68\\n 2.57\\n 2.49\\n 2.42\\n 2.37\\n 2.32\\n 2.28\\n 2.25\\n 2.18\\n 2.10\\n 2.08\\n 2.07\\n 2.06\\n 2.05\\n 2.05\\n 2.01\\n 1.96\\n 1.92\\n 1.87\\n 1.81\\n 22\\n 4.30\\n 3.44\\n 3.05\\n 2.82\\n 2.66\\n 2.55\\n 2.46\\n 2.40\\n 2.34\\n 2.30\\n 2.26\\n 2.23\\n 2.15\\n 2.07\\n 2.06\\n 2.05\\n 2.04\\n 2.03\\n 2.02\\n 1.98\\n 1.94\\n 1.89\\n 1.84\\n 1.78\\n 23\\n 4.28\\n 3.42\\n 3.03\\n 2.80\\n 2.64\\n 2.53\\n 2.44\\n 2.37\\n 2.32\\n 2.27\\n 2.24\\n 2.20\\n 2.13\\n 2.05\\n 2.04\\n 2.02\\n 2.01\\n 2.01\\n 2.00\\n 1.96\\n 1.91\\n 1.86\\n 1.81\\n 1.76\\n 24\\n 4.26\\n 3.40\\n 3.01\\n 2.78\\n 2.62\\n 2.51\\n 2.42\\n 2.36\\n 2.30\\n 2.25\\n 2.22\\n 2.18\\n 2.11\\n 2.03\\n 2.01\\n 2.00\\n 1.99\\n 1.98\\n 1.97\\n 1.94\\n 1.89\\n 1.84\\n 1.79\\n 1.73\\n 25\\n 4.24\\n 3.39\\n 2.99\\n 2.76\\n 2.60\\n 2.49\\n 2.40\\n 2.34\\n 2.28\\n 2.24\\n 2.20\\n 2.16\\n 2.09\\n 2.01\\n 2.00\\n 1.98\\n 1.97\\n 1.96\\n 1.96\\n 1.92\\n 1.87\\n 1.82\\n 1.77\\n 1.71\\n 30\\n 4.17\\n 3.32\\n 2.92\\n 2.69\\n 2.53\\n 2.42\\n 2.33\\n 2.27\\n 2.21\\n 2.16\\n 2.13\\n 2.09\\n 2.01\\n 1.93\\n 1.92\\n 1.91\\n 1.90\\n 1.89\\n 1.88\\n 1.84\\n 1.79\\n 1.74\\n 1.68\\n 1.62\\n 40\\n 4.08\\n 3.23\\n 2.84\\n 2.61\\n 2.45\\n 2.34\\n 2.25\\n 2.18\\n 2.12\\n 2.08\\n 2.04\\n 2.00\\n 1.92\\n 1.84\\n 1.83\\n 1.81\\n 1.80\\n 1.79\\n 1.78\\n 1.74\\n 1.69\\n 1.64\\n 1.58\\n 1.51\\n 60\\n 4.00\\n 3.15\\n 2.76\\n 2.53\\n 2.37\\n 2.25\\n 2.17\\n 2.10\\n 2.04\\n 1.99\\n 1.95\\n 1.92\\n 1.84\\n 1.75\\n 1.73\\n 1.72\\n 1.71\\n 1.70\\n 1.69\\n 1.65\\n 1.59\\n 1.53\\n 1.47\\n 1.39\\n 120\\n 3.92\\n 3.07\\n 2.68\\n 2.45\\n 2.29\\n 2.18\\n 2.09\\n 2.02\\n 1.96\\n 1.91\\n 1.87\\n 1.83\\n 1.75\\n 1.66\\n 1.64\\n 1.63\\n 1.62\\n 1.61\\n 1.60\\n 1.55\\n 1.50\\n 1.43\\n 1.35\\n 1.25\\n Inf nity\\n 3.84\\n 3.00\\n 2.60\\n 2.37\\n 2.21\\n 2.10\\n 2.01\\n 1.94\\n 1.88\\n 1.83\\n 1.79\\n 1.75\\n 1.67\\n 1.57\\n 1.56\\n 1.54\\n 1.53\\n 1.52\\n 1.51\\n 1.46\\n 1.39\\n 1.32\\n 1.22\\n 1.00\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n499\\n\\xa0Appendix D (continued) \\n\\xa0Table of the F-Distribution \\n Panel B. Critical values for right- hand tail area equal to 0.025\\nNumerator: df1 and Denominator: df2\\n \\n df1: 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n 11\\n 12\\n 15\\n 20\\n 21\\n 22\\n 23\\n 24\\n 25\\n 30\\n 40\\n 60\\n 120\\n ∞\\n df2: 1\\n 648\\n 799\\n 864\\n 900\\n 922\\n 937\\n 948\\n 957\\n 963\\n 969\\n 973\\n 977\\n 985\\n 993\\n 994\\n 995\\n 996\\n 997\\n 998\\n 1001\\n 1006\\n 1010\\n 1014\\n 1018\\n 2\\n 38.51\\n 39.00\\n 39.17\\n 39.25\\n 39.30\\n 39.33\\n 39.36\\n 39.37\\n 39.39\\n 39.40\\n 39.41\\n 39.41\\n 39.43\\n 39.45\\n 39.45\\n 39.45\\n 39.45\\n 39.46\\n 39.46\\n 39.46\\n 39.47\\n 39.48\\n 39.49\\n 39.50\\n 3\\n 17.44\\n 16.04\\n 15.44\\n 15.10\\n 14.88\\n 14.73\\n 14.62\\n 14.54\\n 14.47\\n 14.42\\n 14.37\\n 14.34\\n 14.25\\n 14.17\\n 14.16\\n 14.14\\n 14.13\\n 14.12\\n 14.12\\n 14.08\\n 14.04\\n 13.99\\n 13.95\\n 13.90\\n 4\\n 12.22\\n 10.65\\n 9.98\\n 9.60\\n 9.36\\n 9.20\\n 9.07\\n 8.98\\n 8.90\\n 8.84\\n 8.79\\n 8.75\\n 8.66\\n 8.56\\n 8.55\\n 8.53\\n 8.52\\n 8.51\\n 8.50\\n 8.46\\n 8.41\\n 8.36\\n 8.31\\n 8.26\\n 5\\n 10.01\\n 8.43\\n 7.76\\n 7.39\\n 7.15\\n 6.98\\n 6.85\\n 6.76\\n 6.68\\n 6.62\\n 6.57\\n 6.52\\n 6.43\\n 6.33\\n 6.31\\n 6.30\\n 6.29\\n 6.28\\n 6.27\\n 6.23\\n 6.18\\n 6.12\\n 6.07\\n 6.02\\n 6\\n 8.81\\n 7.26\\n 6.60\\n 6.23\\n 5.99\\n 5.82\\n 5.70\\n 5.60\\n 5.52\\n 5.46\\n 5.41\\n 5.37\\n 5.27\\n 5.17\\n 5.15\\n 5.14\\n 5.13\\n 5.12\\n 5.11\\n 5.07\\n 5.01\\n 4.96\\n 4.90\\n 4.85\\n 7\\n 8.07\\n 6.54\\n 5.89\\n 5.52\\n 5.29\\n 5.12\\n 4.99\\n 4.90\\n 4.82\\n 4.76\\n 4.71\\n 4.67\\n 4.57\\n 4.47\\n 4.45\\n 4.44\\n 4.43\\n 4.41\\n 4.40\\n 4.36\\n 4.31\\n 4.25\\n 4.20\\n 4.14\\n 8\\n 7.57\\n 6.06\\n 5.42\\n 5.05\\n 4.82\\n 4.65\\n 4.53\\n 4.43\\n 4.36\\n 4.30\\n 4.24\\n 4.20\\n 4.10\\n 4.00\\n 3.98\\n 3.97\\n 3.96\\n 3.95\\n 3.94\\n 3.89\\n 3.84\\n 3.78\\n 3.73\\n 3.67\\n 9\\n 7.21\\n 5.71\\n 5.08\\n 4.72\\n 4.48\\n 4.32\\n 4.20\\n 4.10\\n 4.03\\n 3.96\\n 3.91\\n 3.87\\n 3.77\\n 3.67\\n 3.65\\n 3.64\\n 3.63\\n 3.61\\n 3.60\\n 3.56\\n 3.51\\n 3.45\\n 3.39\\n 3.33\\n 10\\n 6.94\\n 5.46\\n 4.83\\n 4.47\\n 4.24\\n 4.07\\n 3.95\\n 3.85\\n 3.78\\n 3.72\\n 3.66\\n 3.62\\n 3.52\\n 3.42\\n 3.40\\n 3.39\\n 3.38\\n 3.37\\n 3.35\\n 3.31\\n 3.26\\n 3.20\\n 3.14\\n 3.08\\n 11\\n 6.72\\n 5.26\\n 4.63\\n 4.28\\n 4.04\\n 3.88\\n 3.76\\n 3.66\\n 3.59\\n 3.53\\n 3.47\\n 3.43\\n 3.33\\n 3.23\\n 3.21\\n 3.20\\n 3.18\\n 3.17\\n 3.16\\n 3.12\\n 3.06\\n 3.00\\n 2.94\\n 2.88\\n 12\\n 6.55\\n 5.10\\n 4.47\\n 4.12\\n 3.89\\n 3.73\\n 3.61\\n 3.51\\n 3.44\\n 3.37\\n 3.32\\n 3.28\\n 3.18\\n 3.07\\n 3.06\\n 3.04\\n 3.03\\n 3.02\\n 3.01\\n 2.96\\n 2.91\\n 2.85\\n 2.79\\n 2.72\\n 13\\n 6.41\\n 4.97\\n 4.35\\n 4.00\\n 3.77\\n 3.60\\n 3.48\\n 3.39\\n 3.31\\n 3.25\\n 3.20\\n 3.15\\n 3.05\\n 2.95\\n 2.93\\n 2.92\\n 2.91\\n 2.89\\n 2.88\\n 2.84\\n 2.78\\n 2.72\\n 2.66\\n 2.60\\n 14\\n 6.30\\n 4.86\\n 4.24\\n 3.89\\n 3.66\\n 3.50\\n 3.38\\n 3.29\\n 3.21\\n 3.15\\n 3.09\\n 3.05\\n 2.95\\n 2.84\\n 2.83\\n 2.81\\n 2.80\\n 2.79\\n 2.78\\n 2.73\\n 2.67\\n 2.61\\n 2.55\\n 2.49\\n 15\\n 6.20\\n 4.77\\n 4.15\\n 3.80\\n 3.58\\n 3.41\\n 3.29\\n 3.20\\n 3.12\\n 3.06\\n 3.01\\n 2.96\\n 2.86\\n 2.76\\n 2.74\\n 2.73\\n 2.71\\n 2.70\\n 2.69\\n 2.64\\n 2.59\\n 2.52\\n 2.46\\n 2.40\\n 16\\n 6.12\\n 4.69\\n 4.08\\n 3.73\\n 3.50\\n 3.34\\n 3.22\\n 3.12\\n 3.05\\n 2.99\\n 2.93\\n 2.89\\n 2.79\\n 2.68\\n 2.67\\n 2.65\\n 2.64\\n 2.63\\n 2.61\\n 2.57\\n 2.51\\n 2.45\\n 2.38\\n 2.32\\n 17\\n 6.04\\n 4.62\\n 4.01\\n 3.66\\n 3.44\\n 3.28\\n 3.16\\n 3.06\\n 2.98\\n 2.92\\n 2.87\\n 2.82\\n 2.72\\n 2.62\\n 2.60\\n 2.59\\n 2.57\\n 2.56\\n 2.55\\n 2.50\\n 2.44\\n 2.38\\n 2.32\\n 2.25\\n 18\\n 5.98\\n 4.56\\n 3.95\\n 3.61\\n 3.38\\n 3.22\\n 3.10\\n 3.01\\n 2.93\\n 2.87\\n 2.81\\n 2.77\\n 2.67\\n 2.56\\n 2.54\\n 2.53\\n 2.52\\n 2.50\\n 2.49\\n 2.44\\n 2.38\\n 2.32\\n 2.26\\n 2.19\\n 19\\n 5.92\\n 4.51\\n 3.90\\n 3.56\\n 3.33\\n 3.17\\n 3.05\\n 2.96\\n 2.88\\n 2.82\\n 2.76\\n 2.72\\n 2.62\\n 2.51\\n 2.49\\n 2.48\\n 2.46\\n 2.45\\n 2.44\\n 2.39\\n 2.33\\n 2.27\\n 2.20\\n 2.13\\n 20\\n 5.87\\n 4.46\\n 3.86\\n 3.51\\n 3.29\\n 3.13\\n 3.01\\n 2.91\\n 2.84\\n 2.77\\n 2.72\\n 2.68\\n 2.57\\n 2.46\\n 2.45\\n 2.43\\n 2.42\\n 2.41\\n 2.40\\n 2.35\\n 2.29\\n 2.22\\n 2.16\\n 2.09\\n 21\\n 5.83\\n 4.42\\n 3.82\\n 3.48\\n 3.25\\n 3.09\\n 2.97\\n 2.87\\n 2.80\\n 2.73\\n 2.68\\n 2.64\\n 2.53\\n 2.42\\n 2.41\\n 2.39\\n 2.38\\n 2.37\\n 2.36\\n 2.31\\n 2.25\\n 2.18\\n 2.11\\n 2.04\\n 22\\n 5.79\\n 4.38\\n 3.78\\n 3.44\\n 3.22\\n 3.05\\n 2.93\\n 2.84\\n 2.76\\n 2.70\\n 2.65\\n 2.60\\n 2.50\\n 2.39\\n 2.37\\n 2.36\\n 2.34\\n 2.33\\n 2.32\\n 2.27\\n 2.21\\n 2.14\\n 2.08\\n 2.00\\n 23\\n 5.75\\n 4.35\\n 3.75\\n 3.41\\n 3.18\\n 3.02\\n 2.90\\n 2.81\\n 2.73\\n 2.67\\n 2.62\\n 2.57\\n 2.47\\n 2.36\\n 2.34\\n 2.33\\n 2.31\\n 2.30\\n 2.29\\n 2.24\\n 2.18\\n 2.11\\n 2.04\\n 1.97\\n 24\\n 5.72\\n 4.32\\n 3.72\\n 3.38\\n 3.15\\n 2.99\\n 2.87\\n 2.78\\n 2.70\\n 2.64\\n 2.59\\n 2.54\\n 2.44\\n 2.33\\n 2.31\\n 2.30\\n 2.28\\n 2.27\\n 2.26\\n 2.21\\n 2.15\\n 2.08\\n 2.01\\n 1.94\\n 25\\n 5.69\\n 4.29\\n 3.69\\n 3.35\\n 3.13\\n 2.97\\n 2.85\\n 2.75\\n 2.68\\n 2.61\\n 2.56\\n 2.51\\n 2.41\\n 2.30\\n 2.28\\n 2.27\\n 2.26\\n 2.24\\n 2.23\\n 2.18\\n 2.12\\n 2.05\\n 1.98\\n 1.91\\n 30\\n 5.57\\n 4.18\\n 3.59\\n 3.25\\n 3.03\\n 2.87\\n 2.75\\n 2.65\\n 2.57\\n 2.51\\n 2.46\\n 2.41\\n 2.31\\n 2.20\\n 2.18\\n 2.16\\n 2.15\\n 2.14\\n 2.12\\n 2.07\\n 2.01\\n 1.94\\n 1.87\\n 1.79\\n 40\\n 5.42\\n 4.05\\n 3.46\\n 3.13\\n 2.90\\n 2.74\\n 2.62\\n 2.53\\n 2.45\\n 2.39\\n 2.33\\n 2.29\\n 2.18\\n 2.07\\n 2.05\\n 2.03\\n 2.02\\n 2.01\\n 1.99\\n 1.94\\n 1.88\\n 1.80\\n 1.72\\n 1.64\\n 60\\n 5.29\\n 3.93\\n 3.34\\n 3.01\\n 2.79\\n 2.63\\n 2.51\\n 2.41\\n 2.33\\n 2.27\\n 2.22\\n 2.17\\n 2.06\\n 1.94\\n 1.93\\n 1.91\\n 1.90\\n 1.88\\n 1.87\\n 1.82\\n 1.74\\n 1.67\\n 1.58\\n 1.48\\n 120\\n 5.15\\n 3.80\\n 3.23\\n 2.89\\n 2.67\\n 2.52\\n 2.39\\n 2.30\\n 2.22\\n 2.16\\n 2.10\\n 2.05\\n 1.94\\n 1.82\\n 1.81\\n 1.79\\n 1.77\\n 1.76\\n 1.75\\n 1.69\\n 1.61\\n 1.53\\n 1.43\\n 1.31\\n Inf nity\\n 5.02\\n 3.69\\n 3.12\\n 2.79\\n 2.57\\n 2.41\\n 2.29\\n 2.19\\n 2.11\\n 2.05\\n 1.99\\n 1.94\\n 1.83\\n 1.71\\n 1.69\\n 1.67\\n 1.66\\n 1.64\\n 1.63\\n 1.57\\n 1.48\\n 1.39\\n 1.27\\n 1.00\\n© CFA Institute. For candidate use only. Not for distribution.\\n500\\n\\xa0Appendix D (continued)  \\n\\xa0Table of the F-Distribution \\n Panel C. Critical values for right- hand tail area equal to 0.01\\nNumerator: df1 and Denominator: df2\\n \\n df1: 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n 11\\n 12\\n 15\\n 20\\n 21\\n 22\\n 23\\n 24\\n 25\\n 30\\n 40\\n 60\\n 120\\n ∞\\n df2: 1\\n 4052\\n 5000\\n 5403\\n 5625\\n 5764\\n 5859\\n 5928\\n 5982\\n 6023\\n 6056\\n 6083\\n 6106\\n 6157\\n 6209\\n 6216\\n 6223\\n 6229\\n 6235\\n 6240\\n 6261\\n 6287\\n 6313\\n 6339\\n 6366\\n 2\\n 98.5\\n 99.0\\n 99.2\\n 99.2\\n 99.3\\n 99.3\\n 99.4\\n 99.4\\n 99.4\\n 99.4\\n 99.4\\n 99.4\\n 99.4\\n 99.4\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 99.5\\n 3\\n 34.1\\n 30.8\\n 29.5\\n 28.7\\n 28.2\\n 27.9\\n 27.7\\n 27.5\\n 27.3\\n 27.2\\n 27.1\\n 27.1\\n 26.9\\n 26.7\\n 26.7\\n 26.6\\n 26.6\\n 26.6\\n 26.6\\n 26.5\\n 26.4\\n 26.3\\n 26.2\\n 26.1\\n 4\\n 21.2\\n 18.0\\n 16.7\\n 16.0\\n 15.5\\n 15.2\\n 15.0\\n 14.8\\n 14.7\\n 14.5\\n 14.5\\n 14.4\\n 14.2\\n 14.0\\n 14.0\\n 14.0\\n 13.9\\n 13.9\\n 13.9\\n 13.8\\n 13.7\\n 13.7\\n 13.6\\n 13.5\\n 5\\n 16.3\\n 13.3\\n 12.1\\n 11.4\\n 11.0\\n 10.7\\n 10.5\\n 10.3\\n 10.2\\n 10.1\\n 10.0\\n 9.89\\n 9.72\\n 9.55\\n 9.53\\n 9.51\\n 9.49\\n 9.47\\n 9.45\\n 9.38\\n 9.29\\n 9.20\\n 9.11\\n 9.02\\n 6\\n 13.7\\n 10.9\\n 9.78\\n 9.15\\n 8.75\\n 8.47\\n 8.26\\n 8.10\\n 7.98\\n 7.87\\n 7.79\\n 7.72\\n 7.56\\n 7.40\\n 7.37\\n 7.35\\n 7.33\\n 7.31\\n 7.30\\n 7.23\\n 7.14\\n 7.06\\n 6.97\\n 6.88\\n 7\\n 12.2\\n 9.55\\n 8.45\\n 7.85\\n 7.46\\n 7.19\\n 6.99\\n 6.84\\n 6.72\\n 6.62\\n 6.54\\n 6.47\\n 6.31\\n 6.16\\n 6.13\\n 6.11\\n 6.09\\n 6.07\\n 6.06\\n 5.99\\n 5.91\\n 5.82\\n 5.74\\n 5.65\\n 8\\n 11.3\\n 8.65\\n 7.59\\n 7.01\\n 6.63\\n 6.37\\n 6.18\\n 6.03\\n 5.91\\n 5.81\\n 5.73\\n 5.67\\n 5.52\\n 5.36\\n 5.34\\n 5.32\\n 5.30\\n 5.28\\n 5.26\\n 5.20\\n 5.12\\n 5.03\\n 4.95\\n 4.86\\n 9\\n 10.6\\n 8.02\\n 6.99\\n 6.42\\n 6.06\\n 5.80\\n 5.61\\n 5.47\\n 5.35\\n 5.26\\n 5.18\\n 5.11\\n 4.96\\n 4.81\\n 4.79\\n 4.77\\n 4.75\\n 4.73\\n 4.71\\n 4.65\\n 4.57\\n 4.48\\n 4.40\\n 4.31\\n 10\\n 10.0\\n 7.56\\n 6.55\\n 5.99\\n 5.64\\n 5.39\\n 5.20\\n 5.06\\n 4.94\\n 4.85\\n 4.77\\n 4.71\\n 4.56\\n 4.41\\n 4.38\\n 4.36\\n 4.34\\n 4.33\\n 4.31\\n 4.25\\n 4.17\\n 4.08\\n 4.00\\n 3.91\\n 11\\n 9.65\\n 7.21\\n 6.22\\n 5.67\\n 5.32\\n 5.07\\n 4.89\\n 4.74\\n 4.63\\n 4.54\\n 4.46\\n 4.40\\n 4.25\\n 4.10\\n 4.08\\n 4.06\\n 4.04\\n 4.02\\n 4.01\\n 3.94\\n 3.86\\n 3.78\\n 3.69\\n 3.60\\n 12\\n 9.33\\n 6.93\\n 5.95\\n 5.41\\n 5.06\\n 4.82\\n 4.64\\n 4.50\\n 4.39\\n 4.30\\n 4.22\\n 4.16\\n 4.01\\n 3.86\\n 3.84\\n 3.82\\n 3.80\\n 3.78\\n 3.76\\n 3.70\\n 3.62\\n 3.54\\n 3.45\\n 3.36\\n 13\\n 9.07\\n 6.70\\n 5.74\\n 5.21\\n 4.86\\n 4.62\\n 4.44\\n 4.30\\n 4.19\\n 4.10\\n 4.02\\n 3.96\\n 3.82\\n 3.66\\n 3.64\\n 3.62\\n 3.60\\n 3.59\\n 3.57\\n 3.51\\n 3.43\\n 3.34\\n 3.25\\n 3.17\\n 14\\n 8.86\\n 6.51\\n 5.56\\n 5.04\\n 4.70\\n 4.46\\n 4.28\\n 4.14\\n 4.03\\n 3.94\\n 3.86\\n 3.80\\n 3.66\\n 3.51\\n 3.48\\n 3.46\\n 3.44\\n 3.43\\n 3.41\\n 3.35\\n 3.27\\n 3.18\\n 3.09\\n 3.00\\n 15\\n 8.68\\n 6.36\\n 5.42\\n 4.89\\n 4.56\\n 4.32\\n 4.14\\n 4.00\\n 3.89\\n 3.80\\n 3.73\\n 3.67\\n 3.52\\n 3.37\\n 3.35\\n 3.33\\n 3.31\\n 3.29\\n 3.28\\n 3.21\\n 3.13\\n 3.05\\n 2.96\\n 2.87\\n 16\\n 8.53\\n 6.23\\n 5.29\\n 4.77\\n 4.44\\n 4.20\\n 4.03\\n 3.89\\n 3.78\\n 3.69\\n 3.62\\n 3.55\\n 3.41\\n 3.26\\n 3.24\\n 3.22\\n 3.20\\n 3.18\\n 3.16\\n 3.10\\n 3.02\\n 2.93\\n 2.84\\n 2.75\\n 17\\n 8.40\\n 6.11\\n 5.19\\n 4.67\\n 4.34\\n 4.10\\n 3.93\\n 3.79\\n 3.68\\n 3.59\\n 3.52\\n 3.46\\n 3.31\\n 3.16\\n 3.14\\n 3.12\\n 3.10\\n 3.08\\n 3.07\\n 3.00\\n 2.92\\n 2.83\\n 2.75\\n 2.65\\n 18\\n 8.29\\n 6.01\\n 5.09\\n 4.58\\n 4.25\\n 4.01\\n 3.84\\n 3.71\\n 3.60\\n 3.51\\n 3.43\\n 3.37\\n 3.23\\n 3.08\\n 3.05\\n 3.03\\n 3.02\\n 3.00\\n 2.98\\n 2.92\\n 2.84\\n 2.75\\n 2.66\\n 2.57\\n 19\\n 8.19\\n 5.93\\n 5.01\\n 4.50\\n 4.17\\n 3.94\\n 3.77\\n 3.63\\n 3.52\\n 3.43\\n 3.36\\n 3.30\\n 3.15\\n 3.00\\n 2.98\\n 2.96\\n 2.94\\n 2.92\\n 2.91\\n 2.84\\n 2.76\\n 2.67\\n 2.58\\n 2.49\\n 20\\n 8.10\\n 5.85\\n 4.94\\n 4.43\\n 4.10\\n 3.87\\n 3.70\\n 3.56\\n 3.46\\n 3.37\\n 3.29\\n 3.23\\n 3.09\\n 2.94\\n 2.92\\n 2.90\\n 2.88\\n 2.86\\n 2.84\\n 2.78\\n 2.69\\n 2.61\\n 2.52\\n 2.42\\n 21\\n 8.02\\n 5.78\\n 4.87\\n 4.37\\n 4.04\\n 3.81\\n 3.64\\n 3.51\\n 3.40\\n 3.31\\n 3.24\\n 3.17\\n 3.03\\n 2.88\\n 2.86\\n 2.84\\n 2.82\\n 2.80\\n 2.79\\n 2.72\\n 2.64\\n 2.55\\n 2.46\\n 2.36\\n 22\\n 7.95\\n 5.72\\n 4.82\\n 4.31\\n 3.99\\n 3.76\\n 3.59\\n 3.45\\n 3.35\\n 3.26\\n 3.18\\n 3.12\\n 2.98\\n 2.83\\n 2.81\\n 2.78\\n 2.77\\n 2.75\\n 2.73\\n 2.67\\n 2.58\\n 2.50\\n 2.40\\n 2.31\\n 23\\n 7.88\\n 5.66\\n 4.76\\n 4.26\\n 3.94\\n 3.71\\n 3.54\\n 3.41\\n 3.30\\n 3.21\\n 3.14\\n 3.07\\n 2.93\\n 2.78\\n 2.76\\n 2.74\\n 2.72\\n 2.70\\n 2.69\\n 2.62\\n 2.54\\n 2.45\\n 2.35\\n 2.26\\n 24\\n 7.82\\n 5.61\\n 4.72\\n 4.22\\n 3.90\\n 3.67\\n 3.50\\n 3.36\\n 3.26\\n 3.17\\n 3.09\\n 3.03\\n 2.89\\n 2.74\\n 2.72\\n 2.70\\n 2.68\\n 2.66\\n 2.64\\n 2.58\\n 2.49\\n 2.40\\n 2.31\\n 2.21\\n 25\\n 7.77\\n 5.57\\n 4.68\\n 4.18\\n 3.86\\n 3.63\\n 3.46\\n 3.32\\n 3.22\\n 3.13\\n 3.06\\n 2.99\\n 2.85\\n 2.70\\n 2.68\\n 2.66\\n 2.64\\n 2.62\\n 2.60\\n 2.53\\n 2.45\\n 2.36\\n 2.27\\n 2.17\\n 30\\n 7.56\\n 5.39\\n 4.51\\n 4.02\\n 3.70\\n 3.47\\n 3.30\\n 3.17\\n 3.07\\n 2.98\\n 2.91\\n 2.84\\n 2.70\\n 2.55\\n 2.53\\n 2.51\\n 2.49\\n 2.47\\n 2.45\\n 2.39\\n 2.30\\n 2.21\\n 2.11\\n 2.01\\n 40\\n 7.31\\n 5.18\\n 4.31\\n 3.83\\n 3.51\\n 3.29\\n 3.12\\n 2.99\\n 2.89\\n 2.80\\n 2.73\\n 2.66\\n 2.52\\n 2.37\\n 2.35\\n 2.33\\n 2.31\\n 2.29\\n 2.27\\n 2.20\\n 2.11\\n 2.02\\n 1.92\\n 1.80\\n 60\\n 7.08\\n 4.98\\n 4.13\\n 3.65\\n 3.34\\n 3.12\\n 2.95\\n 2.82\\n 2.72\\n 2.63\\n 2.56\\n 2.50\\n 2.35\\n 2.20\\n 2.17\\n 2.15\\n 2.13\\n 2.12\\n 2.10\\n 2.03\\n 1.94\\n 1.84\\n 1.73\\n 1.60\\n 120\\n 6.85\\n 4.79\\n 3.95\\n 3.48\\n 3.17\\n 2.96\\n 2.79\\n 2.66\\n 2.56\\n 2.47\\n 2.40\\n 2.34\\n 2.19\\n 2.03\\n 2.01\\n 1.99\\n 1.97\\n 1.95\\n 1.93\\n 1.86\\n 1.76\\n 1.66\\n 1.53\\n 1.38\\n Inf nity\\n 6.63\\n 4.61\\n 3.78\\n 3.32\\n 3.02\\n 2.80\\n 2.64\\n 2.51\\n 2.41\\n 2.32\\n 2.25\\n 2.18\\n 2.04\\n 1.88\\n 1.85\\n 1.83\\n 1.81\\n 1.79\\n 1.77\\n 1.70\\n 1.59\\n 1.47\\n 1.32\\n 1.00\\n© CFA Institute. For candidate use only. Not for distribution.\\nFunctional Forms for Simple Linear Regression\\n501\\n\\xa0Appendix D (continued) \\n Table of the F-Distribution \\n Panel D. Critical values for right- hand tail area equal to 0.005\\nNumerator: df1 and Denominator: df2\\n \\n df1: 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n 11\\n 12\\n 15\\n 20\\n 21\\n 22\\n 23\\n 24\\n 25\\n 30\\n 40\\n 60\\n 120\\n ∞\\n df2: 1\\n 16211\\n 20000\\n 21615\\n 22500\\n 23056\\n 23437\\n 23715\\n 23925\\n 24091\\n 24222\\n 24334\\n 24426\\n 24630\\n 24836\\n 24863\\n 24892\\n 24915\\n 24940\\n 24959\\n 25044\\n 25146\\n 25253\\n 25359\\n 25464\\n 2\\n 198.5\\n 199.0\\n 199.2\\n 199.2\\n 199.3\\n 199.3\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.4\\n 199.5\\n 199.5\\n 199.5\\n 199.5\\n 200\\n 3\\n 55.55\\n 49.80\\n 47.47\\n 46.20\\n 45.39\\n 44.84\\n 44.43\\n 44.13\\n 43.88\\n 43.68\\n 43.52\\n 43.39\\n 43.08\\n 42.78\\n 42.73\\n 42.69\\n 42.66\\n 42.62\\n 42.59\\n 42.47\\n 42.31\\n 42.15\\n 41.99\\n 41.83\\n 4\\n 31.33\\n 26.28\\n 24.26\\n 23.15\\n 22.46\\n 21.98\\n 21.62\\n 21.35\\n 21.14\\n 20.97\\n 20.82\\n 20.70\\n 20.44\\n 20.17\\n 20.13\\n 20.09\\n 20.06\\n 20.03\\n 20.00\\n 19.89\\n 19.75\\n 19.61\\n 19.47\\n 19.32\\n 5\\n 22.78\\n 18.31\\n 16.53\\n 15.56\\n 14.94\\n 14.51\\n 14.20\\n 13.96\\n 13.77\\n 13.62\\n 13.49\\n 13.38\\n 13.15\\n 12.90\\n 12.87\\n 12.84\\n 12.81\\n 12.78\\n 12.76\\n 12.66\\n 12.53\\n 12.40\\n 12.27\\n 12.14\\n 6\\n 18.63\\n 14.54\\n 12.92\\n 12.03\\n 11.46\\n 11.07\\n 10.79\\n 10.57\\n 10.39\\n 10.25\\n 10.13\\n 10.03\\n 9.81\\n 9.59\\n 9.56\\n 9.53\\n 9.50\\n 9.47\\n 9.45\\n 9.36\\n 9.24\\n 9.12\\n 9.00\\n 8.88\\n 7\\n 16.24\\n 12.40\\n 10.88\\n 10.05\\n 9.52\\n 9.16\\n 8.89\\n 8.68\\n 8.51\\n 8.38\\n 8.27\\n 8.18\\n 7.97\\n 7.75\\n 7.72\\n 7.69\\n 7.67\\n 7.64\\n 7.62\\n 7.53\\n 7.42\\n 7.31\\n 7.19\\n 7.08\\n 8\\n 14.69\\n 11.04\\n 9.60\\n 8.81\\n 8.30\\n 7.95\\n 7.69\\n 7.50\\n 7.34\\n 7.21\\n 7.10\\n 7.01\\n 6.81\\n 6.61\\n 6.58\\n 6.55\\n 6.53\\n 6.50\\n 6.48\\n 6.40\\n 6.29\\n 6.18\\n 6.06\\n 5.95\\n 9\\n 13.61\\n 10.11\\n 8.72\\n 7.96\\n 7.47\\n 7.13\\n 6.88\\n 6.69\\n 6.54\\n 6.42\\n 6.31\\n 6.23\\n 6.03\\n 5.83\\n 5.80\\n 5.78\\n 5.75\\n 5.73\\n 5.71\\n 5.62\\n 5.52\\n 5.41\\n 5.30\\n 5.19\\n 10\\n 12.83\\n 9.43\\n 8.08\\n 7.34\\n 6.87\\n 6.54\\n 6.30\\n 6.12\\n 5.97\\n 5.85\\n 5.75\\n 5.66\\n 5.47\\n 5.27\\n 5.25\\n 5.22\\n 5.20\\n 5.17\\n 5.15\\n 5.07\\n 4.97\\n 4.86\\n 4.75\\n 4.64\\n 11\\n 12.23\\n 8.91\\n 7.60\\n 6.88\\n 6.42\\n 6.10\\n 5.86\\n 5.68\\n 5.54\\n 5.42\\n 5.32\\n 5.24\\n 5.05\\n 4.86\\n 4.83\\n 4.80\\n 4.78\\n 4.76\\n 4.74\\n 4.65\\n 4.55\\n 4.45\\n 4.34\\n 4.23\\n 12\\n 11.75\\n 8.51\\n 7.23\\n 6.52\\n 6.07\\n 5.76\\n 5.52\\n 5.35\\n 5.20\\n 5.09\\n 4.99\\n 4.91\\n 4.72\\n 4.53\\n 4.50\\n 4.48\\n 4.45\\n 4.43\\n 4.41\\n 4.33\\n 4.23\\n 4.12\\n 4.01\\n 3.90\\n 13\\n 11.37\\n 8.19\\n 6.93\\n 6.23\\n 5.79\\n 5.48\\n 5.25\\n 5.08\\n 4.94\\n 4.82\\n 4.72\\n 4.64\\n 4.46\\n 4.27\\n 4.24\\n 4.22\\n 4.19\\n 4.17\\n 4.15\\n 4.07\\n 3.97\\n 3.87\\n 3.76\\n 3.65\\n 14\\n 11.06\\n 7.92\\n 6.68\\n 6.00\\n 5.56\\n 5.26\\n 5.03\\n 4.86\\n 4.72\\n 4.60\\n 4.51\\n 4.43\\n 4.25\\n 4.06\\n 4.03\\n 4.01\\n 3.98\\n 3.96\\n 3.94\\n 3.86\\n 3.76\\n 3.66\\n 3.55\\n 3.44\\n 15\\n 10.80\\n 7.70\\n 6.48\\n 5.80\\n 5.37\\n 5.07\\n 4.85\\n 4.67\\n 4.54\\n 4.42\\n 4.33\\n 4.25\\n 4.07\\n 3.88\\n 3.86\\n 3.83\\n 3.81\\n 3.79\\n 3.77\\n 3.69\\n 3.59\\n 3.48\\n 3.37\\n 3.26\\n 16\\n 10.58\\n 7.51\\n 6.30\\n 5.64\\n 5.21\\n 4.91\\n 4.69\\n 4.52\\n 4.38\\n 4.27\\n 4.18\\n 4.10\\n 3.92\\n 3.73\\n 3.71\\n 3.68\\n 3.66\\n 3.64\\n 3.62\\n 3.54\\n 3.44\\n 3.33\\n 3.22\\n 3.11\\n 17\\n 10.38\\n 7.35\\n 6.16\\n 5.50\\n 5.07\\n 4.78\\n 4.56\\n 4.39\\n 4.25\\n 4.14\\n 4.05\\n 3.97\\n 3.79\\n 3.61\\n 3.58\\n 3.56\\n 3.53\\n 3.51\\n 3.49\\n 3.41\\n 3.31\\n 3.21\\n 3.10\\n 2.98\\n 18\\n 10.22\\n 7.21\\n 6.03\\n 5.37\\n 4.96\\n 4.66\\n 4.44\\n 4.28\\n 4.14\\n 4.03\\n 3.94\\n 3.86\\n 3.68\\n 3.50\\n 3.47\\n 3.45\\n 3.42\\n 3.40\\n 3.38\\n 3.30\\n 3.20\\n 3.10\\n 2.99\\n 2.87\\n 19\\n 10.07\\n 7.09\\n 5.92\\n 5.27\\n 4.85\\n 4.56\\n 4.34\\n 4.18\\n 4.04\\n 3.93\\n 3.84\\n 3.76\\n 3.59\\n 3.40\\n 3.37\\n 3.35\\n 3.33\\n 3.31\\n 3.29\\n 3.21\\n 3.11\\n 3.00\\n 2.89\\n 2.78\\n 20\\n 9.94\\n 6.99\\n 5.82\\n 5.17\\n 4.76\\n 4.47\\n 4.26\\n 4.09\\n 3.96\\n 3.85\\n 3.76\\n 3.68\\n 3.50\\n 3.32\\n 3.29\\n 3.27\\n 3.24\\n 3.22\\n 3.20\\n 3.12\\n 3.02\\n 2.92\\n 2.81\\n 2.69\\n 21\\n 9.83\\n 6.89\\n 5.73\\n 5.09\\n 4.68\\n 4.39\\n 4.18\\n 4.01\\n 3.88\\n 3.77\\n 3.68\\n 3.60\\n 3.43\\n 3.24\\n 3.22\\n 3.19\\n 3.17\\n 3.15\\n 3.13\\n 3.05\\n 2.95\\n 2.84\\n 2.73\\n 2.61\\n 22\\n 9.73\\n 6.81\\n 5.65\\n 5.02\\n 4.61\\n 4.32\\n 4.11\\n 3.94\\n 3.81\\n 3.70\\n 3.61\\n 3.54\\n 3.36\\n 3.18\\n 3.15\\n 3.12\\n 3.10\\n 3.08\\n 3.06\\n 2.98\\n 2.88\\n 2.77\\n 2.66\\n 2.55\\n 23\\n 9.63\\n 6.73\\n 5.58\\n 4.95\\n 4.54\\n 4.26\\n 4.05\\n 3.88\\n 3.75\\n 3.64\\n 3.55\\n 3.47\\n 3.30\\n 3.12\\n 3.09\\n 3.06\\n 3.04\\n 3.02\\n 3.00\\n 2.92\\n 2.82\\n 2.71\\n 2.60\\n 2.48\\n 24\\n 9.55\\n 6.66\\n 5.52\\n 4.89\\n 4.49\\n 4.20\\n 3.99\\n 3.83\\n 3.69\\n 3.59\\n 3.50\\n 3.42\\n 3.25\\n 3.06\\n 3.04\\n 3.01\\n 2.99\\n 2.97\\n 2.95\\n 2.87\\n 2.77\\n 2.66\\n 2.55\\n 2.43\\n 25\\n 9.48\\n 6.60\\n 5.46\\n 4.84\\n 4.43\\n 4.15\\n 3.94\\n 3.78\\n 3.64\\n 3.54\\n 3.45\\n 3.37\\n 3.20\\n 3.01\\n 2.99\\n 2.96\\n 2.94\\n 2.92\\n 2.90\\n 2.82\\n 2.72\\n 2.61\\n 2.50\\n 2.38\\n 30\\n 9.18\\n 6.35\\n 5.24\\n 4.62\\n 4.23\\n 3.95\\n 3.74\\n 3.58\\n 3.45\\n 3.34\\n 3.25\\n 3.18\\n 3.01\\n 2.82\\n 2.80\\n 2.77\\n 2.75\\n 2.73\\n 2.71\\n 2.63\\n 2.52\\n 2.42\\n 2.30\\n 2.18\\n 40\\n 8.83\\n 6.07\\n 4.98\\n 4.37\\n 3.99\\n 3.71\\n 3.51\\n 3.35\\n 3.22\\n 3.12\\n 3.03\\n 2.95\\n 2.78\\n 2.60\\n 2.57\\n 2.55\\n 2.52\\n 2.50\\n 2.48\\n 2.40\\n 2.30\\n 2.18\\n 2.06\\n 1.93\\n 60\\n 8.49\\n 5.79\\n 4.73\\n 4.14\\n 3.76\\n 3.49\\n 3.29\\n 3.13\\n 3.01\\n 2.90\\n 2.82\\n 2.74\\n 2.57\\n 2.39\\n 2.36\\n 2.33\\n 2.31\\n 2.29\\n 2.27\\n 2.19\\n 2.08\\n 1.96\\n 1.83\\n 1.69\\n 120\\n 8.18\\n 5.54\\n 4.50\\n 3.92\\n 3.55\\n 3.28\\n 3.09\\n 2.93\\n 2.81\\n 2.71\\n 2.62\\n 2.54\\n 2.37\\n 2.19\\n 2.16\\n 2.13\\n 2.11\\n 2.09\\n 2.07\\n 1.98\\n 1.87\\n 1.75\\n 1.61\\n 1.43\\n Inf nity\\n 7.88\\n 5.30\\n 4.28\\n 3.72\\n 3.35\\n 3.09\\n 2.90\\n 2.74\\n 2.62\\n 2.52\\n 2.43\\n 2.36\\n 2.19\\n 2.00\\n 1.97\\n 1.95\\n 1.92\\n 1.90\\n 1.88\\n 1.79\\n 1.67\\n 1.53\\n 1.36\\n 1.00\\n With 1 degree of freedom (df) in the numerator and 3 df in the denominator, the critical F-value is 10.1 for a right- hand tail area equal to 0.05. \\n Quantitative Methods for Investment Analysis, Second Edition, by Richard A. DeFusco, CFA, Dennis W. McLeavey, CFA, Jerald E. Pinto, CFA, and \\nCFA Institute.\\nDavid E. Runkle, CFA. Copyright © 2004 by \\n© CFA Institute. For candidate use only. Not for distribution.\\n502\\n\\xa0Appendix E  \\n\\xa0Critical Values for the Durbin-Watson Statistic (α = .05)\\n \\n K = 1\\n K = 2\\n K = 3\\n K = 4\\nK = 5\\n n\\n dl\\n du\\n dl\\n du\\n dl\\n du\\n dl\\n du\\n dl\\n du\\n 15\\n 1.08\\n 1.36\\n 0.95\\n 1.54\\n 0.82\\n 1.75\\n 0.69\\n 1.97\\n 0.56\\n 2.21\\n 16\\n 1.10\\n 1.37\\n 0.98\\n 1.54\\n 0.86\\n 1.73\\n 0.74\\n 1.93\\n 0.62\\n 2.15\\n 17\\n 1.13\\n 1.38\\n 1.02\\n 1.54\\n 0.90\\n 1.71\\n 0.78\\n 1.90\\n 0.67\\n 2.10\\n 18\\n 1.16\\n 1.39\\n 1.05\\n 1.53\\n 0.93\\n 1.69\\n 0.82\\n 1.87\\n 0.71\\n 2.06\\n 19\\n 1.18\\n 1.40\\n 1.08\\n 1.53\\n 0.97\\n 1.68\\n 0.86\\n 1.85\\n 0.75\\n 2.02\\n 20\\n 1.20\\n 1.41\\n 1.10\\n 1.54\\n 1.00\\n 1.68\\n 0.90\\n 1.83\\n 0.79\\n 1.99\\n 21\\n 1.22\\n 1.42\\n 1.13\\n 1.54\\n 1.03\\n 1.67\\n 0.93\\n 1.81\\n 0.83\\n 1.96\\n 22\\n 1.24\\n 1.43\\n 1.15\\n 1.54\\n 1.05\\n 1.66\\n 0.96\\n 1.80\\n 0.86\\n 1.94\\n 23\\n 1.26\\n 1.44\\n 1.17\\n 1.54\\n 1.08\\n 1.66\\n 0.99\\n 1.79\\n 0.90\\n 1.92\\n 24\\n 1.27\\n 1.45\\n 1.19\\n 1.55\\n 1.10\\n 1.66\\n 1.01\\n 1.78\\n 0.93\\n 1.90\\n 25\\n 1.29\\n 1.45\\n 1.21\\n 1.55\\n 1.12\\n 1.66\\n 1.04\\n 1.77\\n 0.95\\n 1.89\\n 26\\n 1.30\\n 1.46\\n 1.22\\n 1.55\\n 1.14\\n 1.65\\n 1.06\\n 1.76\\n 0.98\\n 1.88\\n 27\\n 1.32\\n 1.47\\n 1.24\\n 1.56\\n 1.16\\n 1.65\\n 1.08\\n 1.76\\n 1.01\\n 1.86\\n 28\\n 1.33\\n 1.48\\n 1.26\\n 1.56\\n 1.18\\n 1.65\\n 1.10\\n 1.75\\n 1.03\\n 1.85\\n 29\\n 1.34\\n 1.48\\n 1.27\\n 1.56\\n 1.20\\n 1.65\\n 1.12\\n 1.74\\n 1.05\\n 1.84\\n 30\\n 1.35\\n 1.49\\n 1.28\\n 1.57\\n 1.21\\n 1.65\\n 1.14\\n 1.74\\n 1.07\\n 1.83\\n 31\\n 1.36\\n 1.50\\n 1.30\\n 1.57\\n 1.23\\n 1.65\\n 1.16\\n 1.74\\n 1.09\\n 1.83\\n 32\\n 1.37\\n 1.50\\n 1.31\\n 1.57\\n 1.24\\n 1.65\\n 1.18\\n 1.73\\n 1.11\\n 1.82\\n 33\\n 1.38\\n 1.51\\n 1.32\\n 1.58\\n 1.26\\n 1.65\\n 1.19\\n 1.73\\n 1.13\\n 1.81\\n 34\\n 1.39\\n 1.51\\n 1.33\\n 1.58\\n 1.27\\n 1.65\\n 1.21\\n 1.73\\n 1.15\\n 1.81\\n 35\\n 1.40\\n 1.52\\n 1.34\\n 1.58\\n 1.28\\n 1.65\\n 1.22\\n 1.73\\n 1.16\\n 1.80\\n 36\\n 1.41\\n 1.52\\n 1.35\\n 1.59\\n 1.29\\n 1.65\\n 1.24\\n 1.73\\n 1.18\\n 1.80\\n 37\\n 1.42\\n 1.53\\n 1.36\\n 1.59\\n 1.31\\n 1.66\\n 1.25\\n 1.72\\n 1.19\\n 1.80\\n 38\\n 1.43\\n 1.54\\n 1.37\\n 1.59\\n 1.32\\n 1.66\\n 1.26\\n 1.72\\n 1.21\\n 1.79\\n 39\\n 1.43\\n 1.54\\n 1.38\\n 1.60\\n 1.33\\n 1.66\\n 1.27\\n 1.72\\n 1.22\\n 1.79\\n 40\\n 1.44\\n 1.54\\n 1.39\\n 1.60\\n 1.34\\n 1.66\\n 1.29\\n 1.72\\n 1.23\\n 1.79\\n 45\\n 1.48\\n 1.57\\n 1.43\\n 1.62\\n 1.38\\n 1.67\\n 1.34\\n 1.72\\n 1.29\\n 1.78\\n 50\\n 1.50\\n 1.59\\n 1.46\\n 1.63\\n 1.42\\n 1.67\\n 1.38\\n 1.72\\n 1.34\\n 1.77\\n 55\\n 1.53\\n 1.60\\n 1.49\\n 1.64\\n 1.45\\n 1.68\\n 1.41\\n 1.72\\n 1.38\\n 1.77\\n 60\\n 1.55\\n 1.62\\n 1.51\\n 1.65\\n 1.48\\n 1.69\\n 1.44\\n 1.73\\n 1.41\\n 1.77\\n 65\\n 1.57\\n 1.63\\n 1.54\\n 1.66\\n 1.50\\n 1.70\\n 1.47\\n 1.73\\n 1.44\\n 1.77\\n 70\\n 1.58\\n 1.64\\n 1.55\\n 1.67\\n 1.52\\n 1.70\\n 1.49\\n 1.74\\n 1.46\\n 1.77\\n 75\\n 1.60\\n 1.65\\n 1.57\\n 1.68\\n 1.54\\n 1.71\\n 1.51\\n 1.74\\n 1.49\\n 1.77\\n 80\\n 1.61\\n 1.66\\n 1.59\\n 1.69\\n 1.56\\n 1.72\\n 1.53\\n 1.74\\n 1.51\\n 1.77\\n 85\\n 1.62\\n 1.67\\n 1.60\\n 1.70\\n 1.57\\n 1.72\\n 1.55\\n 1.75\\n 1.52\\n 1.77\\n 90\\n 1.63\\n 1.68\\n 1.61\\n 1.70\\n 1.59\\n 1.73\\n 1.57\\n 1.75\\n 1.54\\n 1.78\\n 95\\n 1.64\\n 1.69\\n 1.62\\n 1.71\\n 1.60\\n 1.73\\n 1.58\\n 1.75\\n 1.56\\n 1.78\\n 100\\n 1.65\\n 1.69\\n 1.63\\n 1.72\\n 1.61\\n 1.74\\n 1.59\\n 1.76\\n 1.57\\n 1.78\\n Note: K = the number of slope parameters in the model.\\n Source: From J. Durbin and G. S. Watson, “Testing for Serial Correlation in Least Squares Regression, II.” Biometrika 38 (1951): 159–178. \\n© CFA Institute. For candidate use only. Not for distribution.\\n',\n",
       "     'children': []}]}]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_content_until_learning_outcome(content):\n",
    "    match = re.search(r'LEARNING OUTCOME', content)\n",
    "    if match:\n",
    "        content = content[match.start():]\n",
    "\n",
    "    match = re.search(r'(L\\sE\\sA\\sR\\sN\\sI\\sN\\sG\\sM\\sO\\sD\\sU\\sL\\sE)', content)\n",
    "    if match:\n",
    "        content = content[:match.end() + 1]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected: LEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E, but got: LEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m remove_content_until_learning_outcome(input_text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39massert\u001b[39;00m output \u001b[39m==\u001b[39m expected_output, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_output\u001b[39m}\u001b[39;00m\u001b[39m, but got: \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_remove_content_until_learning_outcome()\n",
      "\u001b[1;32m/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb Cell 11\u001b[0m in \u001b[0;36mtest_remove_content_until_learning_outcome\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m expected_output \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mLEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m remove_content_until_learning_outcome(input_text)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/test.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39m==\u001b[39m expected_output, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_output\u001b[39m}\u001b[39;00m\u001b[39m, but got: \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected: LEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E, but got: LEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E "
     ]
    }
   ],
   "source": [
    "def test_remove_content_until_learning_outcome():\n",
    "    input_text = \"\"\"This text should be removed. LEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E blah blah blah blah this text should be removed\"\"\"\n",
    "\n",
    "    expected_output = \"\"\"LEARNING OUTCOME Mastery The candidate should be able to: interpret interest rates as required rates of return, Therefore, a smaller amount 1 L E A R N I N G M O D U L E\"\"\"\n",
    "\n",
    "    output = remove_content_until_learning_outcome(input_text)\n",
    "    assert output == expected_output, f\"Expected: {expected_output}, but got: {output}\"\n",
    "\n",
    "test_remove_content_until_learning_outcome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_content_until_learning_outcome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "formula = \"FVN = PV(1 + r)N\"\n",
    "lhs, rhs = formula.split(\"=\")\n",
    "rhs = rhs.replace(\" \", \"\").replace(\"(\", \"*(\")  # Add a multiplication symbol before the parentheses\n",
    "expr = sympy.sympify(f\"{lhs.strip()} - ({rhs.strip()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"FVN = PV(1 + r)N\"\n",
    "lhs, rhs = formula.split(\"=\")\n",
    "rhs = rhs.replace(\" \", \"\").replace(\"(\", \"*(\")  # Add a multiplication symbol before the parentheses\n",
    "expr = sympy.sympify(f\"{lhs.strip()} - ({rhs.strip()})\")\n",
    "\n",
    "# Solve the equation for FVN\n",
    "FVN = sympy.Symbol('FVN')\n",
    "solved_expr = sympy.solve(expr, FVN)[0]\n",
    "\n",
    "# Convert the expression to LaTeX\n",
    "latex_formula = sympy.latex(solved_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
