{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db_langchain\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import AnalyzeDocumentChain, RetrievalQA\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "persist_directory = 'db_langchain'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "# summary_chain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"map_reduce\")\n",
    "# summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs  = vectordb.similarity_search(\"What methods are on the Agent Class?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "\n",
      "Agent Types\n",
      "\n",
      "\n",
      "For documentation on how to create a custom agent, see the below.\n",
      "\n",
      "\n",
      "Custom Agent\n",
      "Custom LLM Agent\n",
      "Custom LLM Agent (with a ChatModel)\n",
      "Custom MRKL Agent\n",
      "Custom MultiAction Agent\n",
      "Custom Agent with Tool Retrieval\n",
      "\n",
      "\n",
      "We also have documentation for an in-depth dive into each agent type.\n",
      "\n",
      "\n",
      "Conversation Agent (for Chat Models)\n",
      "Conversation Agent\n",
      "MRKL\n",
      "MRKL Chat\n",
      "ReAct\n",
      "Self Ask With Search\n",
      "Structured Tool Chat Agent\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "previous\n",
      "Zapier Natural Language Actions API\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "next\n",
      "Agent Types\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "By Harrison Chase\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "      Â© Copyright 2023, Harrison Chase.\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Last updated on May 25, 2023.\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n\\nAgents\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nAgents and Vectorstores\\nAsync API for Agent\\nConversation Agent (for Chat Models)\\nChatGPT Plugins\\nCustom Agent\\nDefining Custom Tools\\nHuman as a tool\\nIntermediate Steps\\nLoading from LangChainHub\\nMax Iterations\\nMulti Input Tools\\nSearch Tools\\nSerialization\\nAdding SharedMemory to an Agent and its Tools\\nCSV Agent\\nJSON Agent\\nOpenAPI Agent\\nPandas Dataframe Agent\\nPython Agent\\nSQL Database Agent\\nVectorstore Agent\\nMRKL\\nMRKL Chat\\nReAct\\nSelf Ask With Search\\n\\n\\nReference\\n\\n\\nMemory\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nConversationBufferMemory\\nConversationBufferWindowMemory\\nEntity Memory\\nConversation Knowledge Graph Memory\\nConversationSummaryMemory\\nConversationSummaryBufferMemory\\nConversationTokenBufferMemory\\nAdding Memory To an LLMChain\\nAdding Memory to a Multi-Input Chain\\nAdding Memory to an Agent\\nChatGPT Clone\\nConversation Agent\\nConversational Memory Customization\\nCustom Memory\\nMultiple Memory\\n\\n\\n\\n\\nChat\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nAgent\\nChat Vector DB\\nFew Shot Examples\\nMemory\\nPromptLayer ChatOpenAI\\nStreaming\\nRetrieval Question/Answering\\nRetrieval Question Answering with Sources\\n\\n\\n\\n\\n\\nUse Cases\\n\\nAgents\\nChatbots\\nGenerate Examples\\nData Augmented Generation\\nQuestion Answering\\nSummarization\\nQuerying Tabular Data\\nExtraction\\nEvaluation\\nAgent Benchmarking: Search + Calculator\\nAgent VectorDB Question Answering Benchmarking\\nBenchmarking Template\\nData Augmented Question Answering\\nUsing Hugging Face Datasets\\nLLM Math\\nQuestion Answering Benchmarking: Paul Graham Essay\\nQuestion\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import astor\n",
    "\n",
    "def extract_definitions(source_code):\n",
    "    module = ast.parse(source_code)\n",
    "    classes = []\n",
    "    functions = []\n",
    "\n",
    "    for node in module.body:\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            classes.append({\n",
    "                'name': node.name,\n",
    "                'source': ast.unparse(node)\n",
    "            })\n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            functions.append({\n",
    "                'name': node.name,\n",
    "                'source': ast.unparse(node)\n",
    "            })\n",
    "\n",
    "    return classes, functions\n",
    "\n",
    "source_code = \"\"\"\n",
    "# Your source code here\n",
    "\"\"\"\n",
    "\n",
    "classes, functions = extract_definitions(source_code)\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Class name: {cls['name']}\")\n",
    "    print(f\"Source code:\\n{cls['source']}\\n\")\n",
    "\n",
    "for func in functions:\n",
    "    print(f\"Function name: {func['name']}\")\n",
    "    print(f\"Source code:\\n{func['source']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 63] File name too long: 'from langchain import OpenAI, GoogleSearchAPIWrapper, LLMChain\\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\\nimport faiss\\nimport json\\nimport os\\nfrom collections import deque\\nfrom typing import Dict, List, Optional, Any\\n\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.llms import BaseLLM\\nfrom langchain.vectorstores.base import VectorStore\\nfrom pydantic import BaseModel, Field\\nfrom langchain.chains.base import Chain\\n\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.docstore import InMemoryDocstore\\n\\nimport firebase_admin\\nfrom firebase_admin import credentials\\nfrom firebase_admin import firestore\\n\\n# Use a service account\\n# cred = credentials.Certificate(\\'cfa-creds.json\\')\\n# firebase_admin.initialize_app(cred)\\n\\n# db = firestore.client()\\n\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n\\n# Initialize the vectorstore as empty\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query,\\n                    index, InMemoryDocstore({}), {})\\n\\n\\nclass TaskProgressChain(LLMChain):\\n    \"\"\"Chain to manage the progress of tasks.\"\"\"\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        task_progress_template = (\\n            \"You are a task progress AI that uses the current task and its metadata\"\\n            \" to manage the progress of the task. The current task is: {task_name},\"\\n            \" and its metadata is: {metadata}. Based on this information,\"\\n            \" provide a summary of the progress on task completion.\"\\n        )\\n        prompt = PromptTemplate(\\n            template=task_progress_template,\\n            input_variables=[\"task_name\", \"metadata\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose)\\n\\n\\nclass TaskCreationChain(LLMChain):\\n    \"\"\"Chain to generates tasks.\"\"\"\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        task_creation_template = (\\n            \"You are an task creation AI that uses the result of an execution agent\"\\n            \" to create new tasks with the following objective: {objective},\"\\n            \" The list of completed tasks are: {completed_tasks}.\"\\n            \" These are the incomplete tasks still remaining: {incomplete_tasks}.\"\\n            \" Based on the result, create new tasks to be completed\"\\n            \" by the AI system that do not overlap with incomplete tasks.\"\\n            \" Return the tasks as an array.\"\\n        )\\n        prompt = PromptTemplate(\\n            template=task_creation_template,\\n            input_variables=[\"completed_tasks\", \"incomplete_tasks\", \"objective\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose)\\n\\n\\nclass TaskPrioritizationChain(LLMChain):\\n    \"\"\"Chain to prioritize tasks.\"\"\"\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        task_prioritization_template = (\\n            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\\n            \" the following tasks: {task_names}.\"\\n            \" Consider the ultimate objective of your team: {objective}.\"\\n            \" Do not remove any tasks. Return the result as a numbered list, like:\"\\n            \" #. First task\"\\n            \" #. Second task\"\\n            \" Start the task list with number {next_task_id}.\"\\n        )\\n        prompt = PromptTemplate(\\n            template=task_prioritization_template,\\n            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose)\\n\\n\\ntodo_prompt = PromptTemplate.from_template(\\n    \"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\\ntodo_chain = LLMChain(llm=OpenAI(temperature=0), prompt=todo_prompt)\\n\\nsearch = GoogleSearchAPIWrapper()\\n\\n\\ndef search_parser(string: str):\\n    \"\"\"Parse the search results.\"\"\"\\n    return search.results(string, num_results=4)\\n\\n\\n# Create a prompt template for text summarization\\nsummarize_prompt = PromptTemplate.from_template(\"Please summarize the following text: {text}\")\\n\\n# Create an LLMChain for text summarization using the OpenAI language model and the prompt template\\nsummarize_chain = LLMChain(llm=OpenAI(temperature=0.3), prompt=summarize_prompt)\\n\\n\\ndef save_summary_to_file(text: str, file_path: str) -> str:\\n    \"\"\"Summarize the given text and save the summary to a file.\\n\\n    Args:\\n        text: The text to be summarized.\\n        file_path: The path to the file where the summary will be saved.\\n\\n    Returns:\\n        A string indicating the path to the file where the summary was saved.\\n    \"\"\"\\n    # Use the summarize_chain to generate a summary of the text\\n    summary = summarize_chain.run({\"text\": text})\\n\\n    # Save the summary to a file\\n    with open(file_path, \\'w\\') as f:\\n        f.write(summary)\\n\\n    return f\"Summary saved to {file_path}\"\\n\\n\\n# Define the file path where the summary will be saved\\nfile_path = os.path.join(os.getcwd(), \\'summary.txt\\')\\n\\n# Add a tool to your tools list that summarizes text and saves the summary to a file\\ntools.append(\\n    Tool(\\n        name=\"Summarize and Save\",\\n        func=save_summary_to_file,\\n        description=f\"useful for when you need to summarize a large amount of text and save the summary to a file. Input: a large amount of text. Output: a string indicating the path to the file where the summary was saved. The summary will be saved to {file_path}.\"\\n    )\\n)\\n\\ntools = [\\n    Tool(\\n        name=\"Search\",\\n        func=search_parser,\\n        description=\"useful for when you need to gather real time information using Google Search. Input: a search query. Output: the top 4 search results.\"\\n    ),\\n    # Tool(\\n    #     name=\"Take Notes\",\\n    #     func=note_chain.run,\\n    #     description=\"useful for when you need to take notes. Input: a string of information you want to keep. Output: the string.\"\\n    # )\\n]\\n\\n\\nprefix = \"\"\"\\nYou are an AI who performs one task keeping in mind that your final objective is: {objective}. \\nTake into account these previously completed tasks: {context} \\nAlso take into account a summary of your current progress: {summary}\\n\"\"\"\\nFORMAT_INSTRUCTIONS = \"\"\"Use the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\"\"\"\\nsuffix = \"\"\"Question: {task}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools,\\n    prefix=prefix,\\n    format_instructions=FORMAT_INSTRUCTIONS,\\n    suffix=suffix,\\n    input_variables=[\"objective\", \"task\", \"context\", \"summary\", \"agent_scratchpad\"]\\n)\\n\\n\\nclass BabyAGI(Chain, BaseModel):\\n    \"\"\"Controller model for the BabyAGI agent.\"\"\"\\n    task_creation_chain: TaskCreationChain = Field(...)\\n    task_prioritization_chain: TaskPrioritizationChain = Field(...)\\n    task_progress_chain: TaskProgressChain = Field(...)\\n    todo_chain: LLMChain = Field(...)\\n    execution_chain: AgentExecutor = Field(...)\\n    task_id_counter: int = Field(0)\\n    task_list: List = Field([])\\n    completed_tasks: List = Field([])\\n    vectorstore: VectorStore = Field(init=False)\\n    objective: str = Field(\"\")\\n    max_iterations: Optional[int] = None\\n    task_file_path: str = Field(\"task_list.json\")\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [\"objective\"]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return []\\n\\n    def initialize_task_file(self):\\n        if os.path.exists(self.task_file_path):\\n            os.remove(self.task_file_path)\\n\\n    def get_task(self, task_id: str):\\n        return [t for t in self.task_list if t[\"task_id\"] == str(task_id)]\\n\\n    def get_max_task_id(self):\\n        return max([int(t[\"task_id\"]) for t in self.task_list])\\n\\n    def add_task(self, task):\\n        self.task_list.append(task)\\n\\n    def initialize_task_list(self):\\n        \"\"\"\\n        Initialize the task list with a TODO task.\\n        \"\"\"\\n        self.add_task({\"task_id\": \"0\", \"task_name\": \"Make a Todo List\", \"iteration\": 0,\\n                       \"status\": \"incomplete\", \"summary\": \"\"})\\n        self.refresh_task_list()\\n        self.save_task_list()\\n\\n    def execute_and_update_task(self, task):\\n        response = self.execution_chain(\\n            inputs={\\n                \"objective\": self.objective,\\n                \"context\": self.completed_tasks,\\n                \"task\": task,\\n                \"summary\": task[\"summary\"]\\n            }\\n        )\\n        result = response[\"output\"]\\n        intermediate_steps = response[\"intermediate_steps\"]\\n        task.update({f\"result_{task[\\'iteration\\']}\": result})\\n        task.update({f\"step_{task[\\'iteration\\']}\": intermediate_steps})\\n        task.update({\"iteration\": str(int(task[\"iteration\"]) + 1)})\\n\\n        self.save_task_list()\\n\\n    def reprioritize_tasks(self):\\n        starting_task_id = self.task_id_counter + 1\\n        task_names = [i for i in self.get_task_names(starting_task_id)]\\n        reprioritized_tasks = self.task_prioritization_chain(\\n            inputs={\"objective\": self.objective, \"next_task_id\": starting_task_id, \"task_names\": task_names}\\n        )\\n        reprioritized_task_list = reprioritized_tasks[\"text\"].split(\"\\\\n\")\\n        task = self.get_task(str(self.task_id_counter))\\n        for task_name in reprioritized_task_list:\\n            if task_name.strip():\\n                task.append({\"task_id\": str(starting_task_id), \"task_name\": task_name,\\n                             \"iteration\": 0, \"status\": \"incomplete\", \"summary\": \"\"})\\n                starting_task_id += 1\\n\\n        self.task_list = task\\n        self.save_task_list()\\n\\n    def save_task_list(self):\\n        with open(self.task_file_path, \\'w\\') as file:\\n            json.dump(self.task_list, file)\\n\\n    def load_task_list(self):\\n        with open(self.task_file_path, \\'r\\') as file:\\n            self.task_list = json.load(file)\\n\\n    def get_task_names(self, starting_task_id: int):\\n        return [t[\"task_name\"] for t in self.task_list if int(t[\"task_id\"]) >= starting_task_id]\\n\\n    def remove_task(self, task):\\n        new_tasks = [t for t in self.task_list if t[\"task_id\"] != task[\"task_id\"]]\\n        self.task_list = new_tasks\\n        self.save_task_list()\\n\\n    def refresh_task_list(self):\\n        \"\"\"Get the next task.\"\"\"\\n        max_id = self.get_max_task_id()\\n        response = todo_chain.run(\\n            incomplete_tasks=self.task_list, objective=self.objective, completed_tasks=self.completed_tasks)\\n        new_tasks = response.split(\\'\\\\n\\')\\n        new_task_list = [{\"task_name\": task_name, \"iteration\": \"0\", \"status\": \"incomplete\", \"summary\": \"\", \"task_id\": j + max_id}\\n                         for j, task_name in enumerate(new_tasks) if task_name.strip()]\\n        for i in new_task_list:\\n            self.add_task(i)\\n        self.save_task_list()\\n\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Run the agent.\"\"\"\\n        self.objective = inputs[\\'objective\\']\\n        self.initialize_task_file()\\n        self.initialize_task_list()\\n        while True:\\n            if self.task_list:\\n                input(\"Check Progress and update Status. Press Enter to continue...\")\\n                self.load_task_list()\\n                task = self.get_task(str(self.task_id_counter))[0]\\n                iteration = task[\"iteration\"]\\n                if task[\\'status\\'] == \"complete\":\\n                    print(\"Completed task!\\\\t- \", task[\\'task_name\\'])\\n                    self.completed_tasks.append(task)\\n                    self.remove_task(task)\\n                    self.reprioritize_tasks()\\n                    self.task_id_counter += 1\\n                else:\\n                    self.execute_and_update_task(task)\\n                    self.save_task_list()\\n            else:\\n                print(\"No Tasks left to do!\")\\n                break\\n        return {}\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLLM,\\n        vectorstore: VectorStore,\\n        verbose: bool = False,\\n        **kwargs\\n    ) -> \"BabyAGI\":\\n        \"\"\"Initialize the BabyAGI Controller.\"\"\"\\n        task_creation_chain = TaskCreationChain.from_llm(\\n            llm, verbose=verbose\\n        )\\n        task_prioritization_chain = TaskPrioritizationChain.from_llm(\\n            llm, verbose=verbose\\n        )\\n        task_progress_chain = TaskProgressChain.from_llm(\\n            llm, verbose=verbose\\n        )\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        tool_names = [tool.name for tool in tools]\\n        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\\n        agent_executor = AgentExecutor.from_agent_and_tools(\\n            agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)  # Change this line\\n        return cls(\\n            task_creation_chain=task_creation_chain,\\n            task_prioritization_chain=task_prioritization_chain,\\n            execution_chain=agent_executor,\\n            task_progress_chain=task_progress_chain,\\n            vectorstore=vectorstore,\\n            todo_chain=todo_chain,\\n            **kwargs\\n        )\\n\\n\\nOBJECTIVE = \"Research the Langchain Python documentation and generate some product ideas leveraging the python package\"\\n\\nllm = OpenAI(temperature=0)\\n# Logging of LLMChains\\nverbose = False\\n# If None, will keep on going forever\\nmax_iterations: Optional[int] = 7\\nbaby_agi = BabyAGI.from_llm(\\n    llm=llm,\\n    vectorstore=vectorstore,\\n    verbose=verbose,\\n    max_iterations=max_iterations\\n)\\n\\nbaby_agi({\"objective\": OBJECTIVE})\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb Cell 6\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBabyAGI.py\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     text \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m classes, functions \u001b[39m=\u001b[39m extract_definitions(text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m classes:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClass name: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb Cell 6\u001b[0m in \u001b[0;36mextract_definitions\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_definitions\u001b[39m(file_path):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         tree \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mparse(file\u001b[39m.\u001b[39mread())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephblazick/Documents/demand-forecasting-app/_helpers/test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     classes \u001b[39m=\u001b[39m [node\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m ast\u001b[39m.\u001b[39mwalk(tree) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node, ast\u001b[39m.\u001b[39mClassDef)]\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 63] File name too long: 'from langchain import OpenAI, GoogleSearchAPIWrapper, LLMChain\\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\\nimport faiss\\nimport json\\nimport os\\nfrom collections import deque\\nfrom typing import Dict, List, Optional, Any\\n\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.llms import BaseLLM\\nfrom langchain.vectorstores.base import VectorStore\\nfrom pydantic import BaseModel, Field\\nfrom langchain.chains.base import Chain\\n\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.docstore import InMemoryDocstore\\n\\nimport firebase_admin\\nfrom firebase_admin import credentials\\nfrom firebase_admin import firestore\\n\\n# Use a service account\\n# cred = credentials.Certificate(\\'cfa-creds.json\\')\\n# firebase_admin.initialize_app(cred)\\n\\n# db = firestore.client()\\n\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n\\n# Initialize the vectorstore as empty\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query,\\n                    index, InMemoryDocstore({}), {})\\n\\n\\nclass TaskProgressChain(LLMChain):\\n    \"\"\"Chain to manage the progress of tasks.\"\"\"\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        task_progress_template = (\\n            \"You are a task progress AI that uses the current task and its metadata\"\\n            \" to manage the progress of the task. The current task is: {task_name},\"\\n            \" and its metadata is: {metadata}. Based on this information,\"\\n            \" provide a summary of the progress on task completion.\"\\n        )\\n        prompt = PromptTemplate(\\n            template=task_progress_template,\\n            input_variables=[\"task_name\", \"metadata\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose)\\n\\n\\nclass TaskCreationChain(LLMChain):\\n    \"\"\"Chain to generates tasks.\"\"\"\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        task_creation_template = (\\n            \"You are an task creation AI that uses the result of an execution agent\"\\n            \" to create new tasks with the following objective: {objective},\"\\n            \" The list of completed tasks are: {completed_tasks}.\"\\n            \" These are the incomplete tasks still remaining: {incomplete_tasks}.\"\\n            \" Based on the result, create new tasks to be completed\"\\n            \" by the AI system that do not overlap with incomplete tasks.\"\\n            \" Return the tasks as an array.\"\\n        )\\n        prompt = PromptTemplate(\\n            template=task_creation_template,\\n            input_variables=[\"completed_tasks\", \"incomplete_tasks\", \"objective\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose)\\n\\n\\nclass TaskPrioritizationChain(LLMChain):\\n    \"\"\"Chain to prioritize tasks.\"\"\"\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        task_prioritization_template = (\\n            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\\n            \" the following tasks: {task_names}.\"\\n            \" Consider the ultimate objective of your team: {objective}.\"\\n            \" Do not remove any tasks. Return the result as a numbered list, like:\"\\n            \" #. First task\"\\n            \" #. Second task\"\\n            \" Start the task list with number {next_task_id}.\"\\n        )\\n        prompt = PromptTemplate(\\n            template=task_prioritization_template,\\n            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose)\\n\\n\\ntodo_prompt = PromptTemplate.from_template(\\n    \"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\\ntodo_chain = LLMChain(llm=OpenAI(temperature=0), prompt=todo_prompt)\\n\\nsearch = GoogleSearchAPIWrapper()\\n\\n\\ndef search_parser(string: str):\\n    \"\"\"Parse the search results.\"\"\"\\n    return search.results(string, num_results=4)\\n\\n\\n# Create a prompt template for text summarization\\nsummarize_prompt = PromptTemplate.from_template(\"Please summarize the following text: {text}\")\\n\\n# Create an LLMChain for text summarization using the OpenAI language model and the prompt template\\nsummarize_chain = LLMChain(llm=OpenAI(temperature=0.3), prompt=summarize_prompt)\\n\\n\\ndef save_summary_to_file(text: str, file_path: str) -> str:\\n    \"\"\"Summarize the given text and save the summary to a file.\\n\\n    Args:\\n        text: The text to be summarized.\\n        file_path: The path to the file where the summary will be saved.\\n\\n    Returns:\\n        A string indicating the path to the file where the summary was saved.\\n    \"\"\"\\n    # Use the summarize_chain to generate a summary of the text\\n    summary = summarize_chain.run({\"text\": text})\\n\\n    # Save the summary to a file\\n    with open(file_path, \\'w\\') as f:\\n        f.write(summary)\\n\\n    return f\"Summary saved to {file_path}\"\\n\\n\\n# Define the file path where the summary will be saved\\nfile_path = os.path.join(os.getcwd(), \\'summary.txt\\')\\n\\n# Add a tool to your tools list that summarizes text and saves the summary to a file\\ntools.append(\\n    Tool(\\n        name=\"Summarize and Save\",\\n        func=save_summary_to_file,\\n        description=f\"useful for when you need to summarize a large amount of text and save the summary to a file. Input: a large amount of text. Output: a string indicating the path to the file where the summary was saved. The summary will be saved to {file_path}.\"\\n    )\\n)\\n\\ntools = [\\n    Tool(\\n        name=\"Search\",\\n        func=search_parser,\\n        description=\"useful for when you need to gather real time information using Google Search. Input: a search query. Output: the top 4 search results.\"\\n    ),\\n    # Tool(\\n    #     name=\"Take Notes\",\\n    #     func=note_chain.run,\\n    #     description=\"useful for when you need to take notes. Input: a string of information you want to keep. Output: the string.\"\\n    # )\\n]\\n\\n\\nprefix = \"\"\"\\nYou are an AI who performs one task keeping in mind that your final objective is: {objective}. \\nTake into account these previously completed tasks: {context} \\nAlso take into account a summary of your current progress: {summary}\\n\"\"\"\\nFORMAT_INSTRUCTIONS = \"\"\"Use the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\"\"\"\\nsuffix = \"\"\"Question: {task}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools,\\n    prefix=prefix,\\n    format_instructions=FORMAT_INSTRUCTIONS,\\n    suffix=suffix,\\n    input_variables=[\"objective\", \"task\", \"context\", \"summary\", \"agent_scratchpad\"]\\n)\\n\\n\\nclass BabyAGI(Chain, BaseModel):\\n    \"\"\"Controller model for the BabyAGI agent.\"\"\"\\n    task_creation_chain: TaskCreationChain = Field(...)\\n    task_prioritization_chain: TaskPrioritizationChain = Field(...)\\n    task_progress_chain: TaskProgressChain = Field(...)\\n    todo_chain: LLMChain = Field(...)\\n    execution_chain: AgentExecutor = Field(...)\\n    task_id_counter: int = Field(0)\\n    task_list: List = Field([])\\n    completed_tasks: List = Field([])\\n    vectorstore: VectorStore = Field(init=False)\\n    objective: str = Field(\"\")\\n    max_iterations: Optional[int] = None\\n    task_file_path: str = Field(\"task_list.json\")\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [\"objective\"]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return []\\n\\n    def initialize_task_file(self):\\n        if os.path.exists(self.task_file_path):\\n            os.remove(self.task_file_path)\\n\\n    def get_task(self, task_id: str):\\n        return [t for t in self.task_list if t[\"task_id\"] == str(task_id)]\\n\\n    def get_max_task_id(self):\\n        return max([int(t[\"task_id\"]) for t in self.task_list])\\n\\n    def add_task(self, task):\\n        self.task_list.append(task)\\n\\n    def initialize_task_list(self):\\n        \"\"\"\\n        Initialize the task list with a TODO task.\\n        \"\"\"\\n        self.add_task({\"task_id\": \"0\", \"task_name\": \"Make a Todo List\", \"iteration\": 0,\\n                       \"status\": \"incomplete\", \"summary\": \"\"})\\n        self.refresh_task_list()\\n        self.save_task_list()\\n\\n    def execute_and_update_task(self, task):\\n        response = self.execution_chain(\\n            inputs={\\n                \"objective\": self.objective,\\n                \"context\": self.completed_tasks,\\n                \"task\": task,\\n                \"summary\": task[\"summary\"]\\n            }\\n        )\\n        result = response[\"output\"]\\n        intermediate_steps = response[\"intermediate_steps\"]\\n        task.update({f\"result_{task[\\'iteration\\']}\": result})\\n        task.update({f\"step_{task[\\'iteration\\']}\": intermediate_steps})\\n        task.update({\"iteration\": str(int(task[\"iteration\"]) + 1)})\\n\\n        self.save_task_list()\\n\\n    def reprioritize_tasks(self):\\n        starting_task_id = self.task_id_counter + 1\\n        task_names = [i for i in self.get_task_names(starting_task_id)]\\n        reprioritized_tasks = self.task_prioritization_chain(\\n            inputs={\"objective\": self.objective, \"next_task_id\": starting_task_id, \"task_names\": task_names}\\n        )\\n        reprioritized_task_list = reprioritized_tasks[\"text\"].split(\"\\\\n\")\\n        task = self.get_task(str(self.task_id_counter))\\n        for task_name in reprioritized_task_list:\\n            if task_name.strip():\\n                task.append({\"task_id\": str(starting_task_id), \"task_name\": task_name,\\n                             \"iteration\": 0, \"status\": \"incomplete\", \"summary\": \"\"})\\n                starting_task_id += 1\\n\\n        self.task_list = task\\n        self.save_task_list()\\n\\n    def save_task_list(self):\\n        with open(self.task_file_path, \\'w\\') as file:\\n            json.dump(self.task_list, file)\\n\\n    def load_task_list(self):\\n        with open(self.task_file_path, \\'r\\') as file:\\n            self.task_list = json.load(file)\\n\\n    def get_task_names(self, starting_task_id: int):\\n        return [t[\"task_name\"] for t in self.task_list if int(t[\"task_id\"]) >= starting_task_id]\\n\\n    def remove_task(self, task):\\n        new_tasks = [t for t in self.task_list if t[\"task_id\"] != task[\"task_id\"]]\\n        self.task_list = new_tasks\\n        self.save_task_list()\\n\\n    def refresh_task_list(self):\\n        \"\"\"Get the next task.\"\"\"\\n        max_id = self.get_max_task_id()\\n        response = todo_chain.run(\\n            incomplete_tasks=self.task_list, objective=self.objective, completed_tasks=self.completed_tasks)\\n        new_tasks = response.split(\\'\\\\n\\')\\n        new_task_list = [{\"task_name\": task_name, \"iteration\": \"0\", \"status\": \"incomplete\", \"summary\": \"\", \"task_id\": j + max_id}\\n                         for j, task_name in enumerate(new_tasks) if task_name.strip()]\\n        for i in new_task_list:\\n            self.add_task(i)\\n        self.save_task_list()\\n\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Run the agent.\"\"\"\\n        self.objective = inputs[\\'objective\\']\\n        self.initialize_task_file()\\n        self.initialize_task_list()\\n        while True:\\n            if self.task_list:\\n                input(\"Check Progress and update Status. Press Enter to continue...\")\\n                self.load_task_list()\\n                task = self.get_task(str(self.task_id_counter))[0]\\n                iteration = task[\"iteration\"]\\n                if task[\\'status\\'] == \"complete\":\\n                    print(\"Completed task!\\\\t- \", task[\\'task_name\\'])\\n                    self.completed_tasks.append(task)\\n                    self.remove_task(task)\\n                    self.reprioritize_tasks()\\n                    self.task_id_counter += 1\\n                else:\\n                    self.execute_and_update_task(task)\\n                    self.save_task_list()\\n            else:\\n                print(\"No Tasks left to do!\")\\n                break\\n        return {}\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLLM,\\n        vectorstore: VectorStore,\\n        verbose: bool = False,\\n        **kwargs\\n    ) -> \"BabyAGI\":\\n        \"\"\"Initialize the BabyAGI Controller.\"\"\"\\n        task_creation_chain = TaskCreationChain.from_llm(\\n            llm, verbose=verbose\\n        )\\n        task_prioritization_chain = TaskPrioritizationChain.from_llm(\\n            llm, verbose=verbose\\n        )\\n        task_progress_chain = TaskProgressChain.from_llm(\\n            llm, verbose=verbose\\n        )\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        tool_names = [tool.name for tool in tools]\\n        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\\n        agent_executor = AgentExecutor.from_agent_and_tools(\\n            agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)  # Change this line\\n        return cls(\\n            task_creation_chain=task_creation_chain,\\n            task_prioritization_chain=task_prioritization_chain,\\n            execution_chain=agent_executor,\\n            task_progress_chain=task_progress_chain,\\n            vectorstore=vectorstore,\\n            todo_chain=todo_chain,\\n            **kwargs\\n        )\\n\\n\\nOBJECTIVE = \"Research the Langchain Python documentation and generate some product ideas leveraging the python package\"\\n\\nllm = OpenAI(temperature=0)\\n# Logging of LLMChains\\nverbose = False\\n# If None, will keep on going forever\\nmax_iterations: Optional[int] = 7\\nbaby_agi = BabyAGI.from_llm(\\n    llm=llm,\\n    vectorstore=vectorstore,\\n    verbose=verbose,\\n    max_iterations=max_iterations\\n)\\n\\nbaby_agi({\"objective\": OBJECTIVE})\\n'"
     ]
    }
   ],
   "source": [
    "with open(\"BabyAGI.py\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "classes, functions = extract_definitions(text)\n",
    "for cls in classes:\n",
    "    print(f\"Class name: {cls['name']}\")\n",
    "    print(f\"Source code:\\n{cls['source']}\\n\")\n",
    "\n",
    "for func in functions:\n",
    "    print(f\"Function name: {func['name']}\")\n",
    "    print(f\"Source code:\\n{func['source']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
