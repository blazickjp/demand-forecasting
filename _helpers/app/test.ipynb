{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def combine_python_files(root_dir, output_file_path, delimiter=\"-----\"):\n",
    "    \"\"\"\n",
    "    Combine contents of all Python files in a directory into a single text file.\n",
    "\n",
    "    :param root_dir: Path to the root directory containing Python files.\n",
    "    :param output_file_path: Path to the output text file.\n",
    "    :param delimiter: Delimiter to separate contents of different files. Default is \"-----\".\n",
    "    \"\"\"\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        \n",
    "        # Walk through the root directory and its subdirectories\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            \n",
    "            # Loop through each file in the directory\n",
    "            for filename in filenames:\n",
    "                \n",
    "                # Check if the file is a Python file\n",
    "                if filename.endswith(\".py\"):\n",
    "                    # Construct the full file path\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    # Construct the relative file path from the root directory\n",
    "                    relative_path = os.path.relpath(file_path, root_dir)\n",
    "                    \n",
    "                    # Write the relative file path to the output file\n",
    "                    output_file.write(f\"File: {relative_path}\\n\")\n",
    "                    \n",
    "                    # Write the delimiter to the output file\n",
    "                    output_file.write(delimiter + '\\n')\n",
    "                    \n",
    "                    # Open and read the contents of the Python file\n",
    "                    with open(file_path, 'r') as python_file:\n",
    "                        content = python_file.read()\n",
    "                        # Write the contents of the Python file to the output file\n",
    "                        output_file.write(content + '\\n\\n')\n",
    "\n",
    "# Example usage:\n",
    "result = combine_python_files(root_dir='.', output_file_path='combined_files.txt')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Encoding' object has no attribute 'tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcombined_files.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of tokens in \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mcount_tokens_in_file(file_path)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mcount_tokens_in_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m     \u001b[39m# Count the tokens\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tokenizer\u001b[39m.\u001b[39;49mtokenize(text))\n\u001b[1;32m     19\u001b[0m     token_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokens)\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m token_count\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Encoding' object has no attribute 'tokenize'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens_in_file(file_path):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text file.\n",
    "    \n",
    "    :param file_path: The path to the text file.\n",
    "    :return: The number of tokens in the file.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    token_count = 0\n",
    "\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "        # Count the tokens\n",
    "        tokens = list(tokenizer.encode(text))\n",
    "        token_count = len(tokens)\n",
    "    \n",
    "    return token_count\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'combined_files.txt'\n",
    "print(f\"Number of tokens in {file_path}: {count_tokens_in_file(file_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
