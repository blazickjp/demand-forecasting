File: ui_helpers.py
-----
import streamlit as st


def setup_ui():
    st.title("Chat with ChatGPT")


def add_message_to_chat_window(message, chat_window):
    chat_window.markdown(message.render(), unsafe_allow_html=True)


def get_user_input():
    return st.text_area("Your Message", height=200)  # You can adjust the height


File: config.py
-----
import os

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
BOT_NAME = "ChatGPT Bot"


File: chat_interface.py
-----
import streamlit as st
from chat_gpt_connector import ChatGPTConnector
from ui_helpers import setup_ui, get_user_input
from langchain_agent.MemoryManager import MemoryManager


class ChatInterface:
    def __init__(self):
        setup_ui()

        # Use session_state to store the connector and MemoryManager
        if "connector" not in st.session_state:
            st.session_state["connector"] = ChatGPTConnector()

        # Initialize MemoryManager
        if "memory_manager" not in st.session_state:
            st.session_state["memory_manager"] = MemoryManager(model="gpt-3.5-turbo")

        self.chat_window = st.empty()

    def run(self):
        user_message = get_user_input()
        if st.button("Send"):
            # User message
            st.session_state["memory_manager"].add_message("user", user_message)

            # Bot response
            bot_response = st.session_state["connector"].get_response(user_message)
            st.session_state["memory_manager"].add_message("assistant", bot_response)

            # Display the conversation
            conversation_html = st.session_state[
                "memory_manager"
            ].display_conversation_html()
            self.chat_window.markdown(conversation_html, unsafe_allow_html=True)


File: __init__.py
-----


File: message.py
-----
from datetime import datetime


class Message:
    def __init__(self, author, content):
        self.author = author
        self.content = content
        self.timestamp = datetime.now()

    def render(self):
        time = self.timestamp.strftime("%H:%M:%S")
        return f"[{time}] {self.author}: {self.content}"


File: chat_gpt_connector.py
-----
import openai
from config import OPENAI_API_KEY


class ChatGPTConnector:
    def __init__(self):
        self.api_key = OPENAI_API_KEY
        self.conversation_history = []

    def get_response(self, message):
        # Append the user's message to the conversation history
        self.conversation_history.append({"role": "user", "content": message})

        # Generate a response using OpenAI's chat models
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # You can change to a different chat model if available
            messages=self.conversation_history,
            max_tokens=100,  # You can adjust max tokens based on your requirement
        )

        # Append the model's response to the conversation history
        bot_response = response["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "assistant", "content": bot_response})

        return bot_response


File: main.py
-----
from chat_interface import ChatInterface


def main():
    chat_interface = ChatInterface()
    chat_interface.run()


if __name__ == "__main__":
    main()


File: langchain_agent/functions.py
-----
import os
import arxiv
import openai
from csv import writer
from openai_function_call import openai_function


from ClassLoader import PythonClassDirectoryLoader
from retry import retry
from tenacity import retry, wait_random_exponential, stop_after_attempt


class_loader = PythonClassDirectoryLoader("/Users/josephblazick/Documents/langchain", glob="**/*.py")
class_loader.load()
EMBEDDING_MODEL = "text-embedding-ada-002"
GPT_MODEL = "gpt-3.5-turbo-0613"
# Set a directory to store downloaded papers
data_dir = os.path.join(os.curdir, "data", "papers")
paper_dir_filepath = "./data/arxiv_library.csv"

# Generate a blank dataframe where we can store downloaded files
# df = pd.DataFrame(list())
# df.to_csv(paper_dir_filepath)


@openai_function
def class_lookup(item: str):
    """
    Lookup for class definitions in the langchain codebase. You should use this function when you need to see the 
    source code of a particular class.
    """
    return class_loader.get_class(item)


@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))
def embedding_request(text):
    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)
    return response


def get_articles(query, library=paper_dir_filepath, top_k=5):
    """This function gets the top_k articles based on a user's query, sorted by relevance.
    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.
    """
    search = arxiv.Search(
        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance
    )
    result_list = []
    for result in search.results():
        result_dict = {}
        result_dict.update({"title": result.title})
        result_dict.update({"summary": result.summary})

        # Taking the first url provided
        result_dict.update({"article_url": [x.href for x in result.links][0]})
        result_dict.update({"pdf_url": [x.href for x in result.links][1]})
        result_list.append(result_dict)

        # Store references in library file
        response = embedding_request(text=result.title)
        file_reference = [
            result.title,
            result.download_pdf(data_dir),
            response["data"][0]["embedding"],
        ]

        # Write to file
        with open(library, "a") as f_object:
            writer_object = writer(f_object)
            writer_object.writerow(file_reference)
            f_object.close()
    return result_list


File: langchain_agent/openai_function_call.py
-----

import json
from functools import wraps
from typing import Any, Callable
from pydantic import validate_arguments, BaseModel


class openai_function:
    def __init__(self, func: Callable) -> None:
        self.func = func
        self.validate_func = validate_arguments(func)
        self.openai_schema = {
            "name": self.func.__name__,
            "description": self.func.__doc__,
            "parameters": self.validate_func.model.schema(),
        }
        self.model = self.validate_func.model

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        @wraps(self.func)
        def wrapper(*args, **kwargs):
            return self.validate_func(*args, **kwargs)

        return wrapper(*args, **kwargs)

    def from_response(self, completion, throw_error=True):
        """Execute the function from the response of an openai chat completion"""
        message = completion.choices[0].message

        if throw_error:
            assert "function_call" in message, "No function call detected"
            assert (
                message["function_call"]["name"] == self.openai_schema["name"]
            ), "Function name does not match"

        function_call = message["function_call"]
        arguments = json.loads(function_call["arguments"])
        return self.validate_func(**arguments)


class OpenAISchema(BaseModel):
    @classmethod
    @property
    def openai_schema(cls):
        schema = cls.schema()
        return {
            "name": schema["title"],
            "description": schema["description"],
            "parameters": schema,
        }

    @classmethod
    def from_response(cls, completion, throw_error=True):
        message = completion.choices[0].message

        if throw_error:
            assert "function_call" in message, "No function call detected"
            assert (
                message["function_call"]["name"] == cls.openai_schema["name"]
            ), "Function name does not match"

        function_call = message["function_call"]
        arguments = json.loads(function_call["arguments"])
        return cls(**arguments)

File: langchain_agent/MemoryManager.py
-----
import openai
import json
import time
import psycopg2
from termcolor import colored
import tiktoken


class MemoryManager:
    def __init__(self, model, max_tokens=2048):
        self.model = model
        self.max_tokens = max_tokens
        self.messages = [
            {"role": "system", "content": "", "interaction_index": time.time() * 1000}
        ]
        self.summary = "Empty"
        # Connect to the PostgreSQL database
        self.conn = psycopg2.connect(
            host="localhost", database="memory", user="joe", password="1234"
        )

        # Create a cursor object
        self.cur = self.conn.cursor()

        # Create the table if it doesn't exist
        self.cur.execute(
            """
        CREATE TABLE IF NOT EXISTS memory (
            interaction_index BIGINT PRIMARY KEY,
            memory_item JSONB NOT NULL
        )
        """
        )
        self.cur.execute("TRUNCATE memory")
        self.conn.commit()

    def add_message(self, role, content, override_truncate=False):
        timestamp = int(time.time() * 1000)  # Current timestamp in milliseconds
        self.messages.append(
            {"role": role, "content": content, "interaction_index": timestamp}
        )

        if not override_truncate and self.get_total_tokens() > self.max_tokens:
            self.truncate_history()

    def truncate_history(self):
        """Truncate the history to fit within the max_tokens limit."""
        print("Truncating history...")
        total_tokens = sum(len(item["content"]) for item in self.messages)
        while total_tokens > self.max_tokens // 2:
            # Check if the message to be removed is the first message and is a system message
            if len(self.messages) > 1:
                removed_item = self.messages.pop(
                    1
                )  # Pop the second message instead of the first
                total_tokens -= len(removed_item["content"])
                self.archive_memory_item(removed_item)
            else:
                # If there is only the system message, do nothing.
                break

    def archive_memory_item(self, memory_item):
        """Archive a memory item in the database."""
        print("Archiving memory item...")
        interaction_index = memory_item.get("interaction_index")
        if interaction_index is not None:
            interaction_index = int(interaction_index)  # Make sure it's an integer
        else:
            interaction_index = int(
                time.time() * 1000
            )  # Default to current timestamp in milliseconds

        # Check if the content is already a JSON string
        if isinstance(memory_item["content"], str):
            # If it's already a JSON string, escape single quotes
            memory_item_json = json.dumps(memory_item).replace("'", "''")
        else:
            # If it's a dictionary, convert it to a JSON string
            memory_item_json = json.dumps(memory_item)

        # Try to insert into the database
        query = "INSERT INTO memory (interaction_index, memory_item) VALUES (%s, %s)"
        while True:
            try:
                self.cur.execute(query, (interaction_index, memory_item_json))
                self.conn.commit()
                break
            except psycopg2.errors.UniqueViolation as e:
                print(e)
                # If interaction_index already exists, increment it, rollback the transaction and retry
                interaction_index += 1
                self.conn.rollback()

    def summarize_history(self):
        # Combine the content of all messages into a single string
        full_history = "\n".join(
            [
                f"{item['role'].capitalize()}: {item['content']}"
                for item in self.messages
            ]
        )

        # Use Chat API to summarize the history
        prompt = f"User: Please summarize the following conversation: \n{full_history}\n\nAssistant:"
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are a helpful AI assistant."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=100,  # You can adjust this value
        )

        summary = response["choices"][0]["message"]["content"].strip()
        # Clear the messages and add the summarized history as a single system message
        self.summary = summary
        self.system = self.system + f"\n\nSummary of Conversation so far: {summary}"
        self.messages = [{"role": "system", "content": self.system}]

    def get_memory_item(self, interaction_index):
        # Retrieve memory item by interaction index from PostgreSQL
        self.cur.execute(
            f"SELECT memory_item FROM memory WHERE interaction_index = {interaction_index}"
        )
        result = self.cur.fetchone()
        if result:
            return result[0]  # Convert the JSON string back to a dictionary
        else:
            return None

    def get_total_tokens(self):
        """Returns the number of tokens in a text string."""
        total_tokens = 0
        for item in self.messages:
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
            num_tokens = len(encoding.encode(item["content"]))
            total_tokens += num_tokens
        return total_tokens

    def display_conversation(self, detailed=False):
        role_to_color = {
            "system": "red",
            "user": "green",
            "assistant": "blue",
            "function": "magenta",
        }
        for message in self.messages:
            print(
                colored(
                    f"{message['role']}: {message['content']}\n\n",
                    role_to_color[message["role"]],
                )
            )

    def display_conversation_html(self, detailed=False):
        role_to_color = {
            "system": "#FF0000",
            "user": "#00FF00",
            "assistant": "#0000FF",
            "function": "#FF00FF",
        }
        conversation_html = ""
        for message in self.messages:
            color = role_to_color.get(message["role"], "#000000")
            # escape markdown specific characters
            content = message["content"].replace("<", "&lt;").replace(">", "&gt;")
            # wrap content in a div with a color style
            conversation_html += f"""
            <div style="color: {color}"><strong>{message["role"].capitalize()}:</strong><br>{content}</div><br>
            """
        return conversation_html


if __name__ == "__main__":
    # Example usage:
    memory_manager = MemoryManager(model="gpt-3.5-turbo-0613")

    # Add messages with interaction indices
    memory_manager.add_message("user", "What's the weather like in Boston?")
    memory_manager.add_message("assistant", "The weather in Boston is sunny.")
    memory_manager.add_message("user", "Tell me a joke.")
    # Archive a memory item manually
    idx = int(time.time() * 1000)  # Convert to integer here as well
    memory_manager.archive_memory_item(
        {"role": "user", "content": "This is a test message.", "interaction_index": idx}
    )

    # Retrieve the manually archived memory item
    memory_item = memory_manager.get_memory_item(idx)
    memory_manager.display_conversation()


File: langchain_agent/ClassLoader.py
-----

import os
import ast
import re
from langchain.document_loaders import DirectoryLoader
from langchain.docstore.document import Document
from pathlib import Path
from typing import List, Any, Optional


def _is_visible(p: Path) -> bool:
    parts = p.parts
    for _p in parts:
        if _p.startswith("."):
            return False
    return True


class ClassCollector(ast.NodeVisitor):
    def __init__(self):
        self.classes = []

    def visit_ClassDef(self, node):
        self.classes.append(ast.unparse(node))
        self.generic_visit(node)

    def get_class_by_name(self, class_name: str) -> Optional[str]:
        for class_def in self.classes:
            if f"class {class_name}" in class_def:
                return class_def
        return None


class PythonClassDirectoryLoader(DirectoryLoader):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.docs = []

    def load_file(self, item: Path, path: Path, docs: List[Document], pbar: Optional[Any]) -> None:
        if item.is_file() and os.path.abspath(item).endswith(".py"):
            full_path = os.path.abspath(item)
            if full_path.endswith("__init__.py"):
                return
            if full_path.endswith(".json"):
                return
            if full_path.startswith("/Users/josephblazick/Documents/langchain/docs"):
                return
            if full_path.startswith("/Users/josephblazick/Documents/langchain/tests"):
                return
            if _is_visible(item.relative_to(path)) or self.load_hidden:
                try:
                    # Load the Python source code
                    with open(item, 'r') as file:
                        code = file.read()

                    # Parse the code and extract class definitions
                    tree = ast.parse(code)
                    collector = ClassCollector()
                    collector.visit(tree)
                    classes = collector.classes

                    # Create a Document object for each class definition
                    for class_def in classes:
                        try:
                            class_name = re.search('(?<=class\s)\w+', class_def).group(0)  # Extract the class name
                            self.docs.append(
                                Document(
                                  page_content=class_def, 
                                  metadata={
                                      "file_path": str(item), 
                                      "class_name": class_name
                                  }
                                )
                            )
                        except Exception as e:
                            print('Skipping class definition due to error: ', e)

                except Exception as e:
                    if self.silent_errors:
                        print(e)
                    else:
                        print(e)
                        print(os.path.abspath(item))
                        raise e
                finally:
                    if pbar:
                        pbar.update(1)

    def get_file_data(self, file_path: str) -> List[Document]:
        return [doc for doc in self.docs if doc.metadata["file_path"] == file_path]

    def get_class(self, class_name: str) -> List[Document]:
        content = [doc.page_content for doc in self.docs if doc.metadata.get("class_name") == class_name]
        return "\n".join(content)






File: langchain_agent/PydanticPlayground.py
-----
from functools import lru_cache
import openai
import enum
import json
import asyncio

from pydantic import Field
from typing import List, Tuple

from pyparsing import C
from openai_function_call import OpenAISchema
from tenacity import retry, stop_after_attempt
from pprint import pprint
from collections import deque


class QueryType(str, enum.Enum):
    """
    Enumeration representing the types of queries that can be asked to a question answer system.
    """

    # When i call it anything beyond 'merge multiple responses' the accuracy drops significantly.
    SINGLE_QUESTION = "SINGLE"
    MERGE_MULTIPLE_RESPONSES = "MERGE_MULTIPLE_RESPONSES"


class Model(enum.Enum):
    """
    Enumeration representing the models that can be used to answer questions.
    """
    GPT4 = "gpt-4-0613"
    GPT3_5: str = "gpt-3.5-turbo-0613"


class ComputeQuery(OpenAISchema):
    """
    Models a computation of a query, assume this can be some RAG system like llamaindex
    """

    query: str
    response: str = "..."

    @classmethod
    def from_query(cls, query: str) -> 'ComputeQuery':
        messages = [{'role': 'system', 'content': 'You are a helpful assistant answering my questions.'}]
        messages.append({'role': 'user', 'content': query})
        completion = openai.ChatCompletion.create(
            model=Model.GPT3_5.value,
            temperature=0,
            messages=messages,
            max_tokens=1000,
        )
        return cls(query=query, response=completion.choices[0].message['content'])


class MergedResponses(OpenAISchema):
    """
    Models a merged response of multiple queries.
    Currently we just concatinate them but we can do much more complex things.
    """

    responses: List[ComputeQuery]


class Query(OpenAISchema):
    """
    Class representing a single question in a question answer subquery.
    Can be either a single question or a multi question merge.
    """

    id: int = Field(..., description="Unique id of the query")
    question: str = Field(
        ...,
        description="The question we are asking from our question answer system. If we are asking multiple questions"
        ", this question is asked by also providing the answers to the sub questions",
    )
    dependencies: List[int] = Field(
        default_factory=list,
        description="List of children questions which need to be answered before we can ask the main question."
        " Use a subquery to answer known unknowns to increase confidence in the final answer, when we"
        " need to ask multiple questions to get the answer. Dependencies may only be other queries.",
    )
    node_type: QueryType = Field(
        default=QueryType.SINGLE_QUESTION,
        description="Type of question we are asking, either a single question or a multi question merge"
        " when there are multiple questions",
    )

    async def execute(self, dependency_func):
        print("Executing", f"{self.question}")
        print("Executing with", len(self.dependencies), "dependancies")

        if self.node_type == QueryType.SINGLE_QUESTION:
            resp = ComputeQuery.from_query(
                query=self.question,
            )
            pprint(resp.dict())
            return resp

        sub_queries = dependency_func(self.dependencies)
        computed_queries = await asyncio.gather(
            *[q.execute(dependency_func=dependency_func) for q in sub_queries]
        )
        sub_answers = MergedResponses(responses=computed_queries)
        merged_query = f"{self.question}\nContext: {sub_answers.json()}"
        resp = ComputeQuery(
            query=merged_query,
        )
        pprint(resp.dict())
        return resp


class QueryPlan(OpenAISchema):
    """
    Container class representing a tree of questions to ask a question answer system.
    Make sure every question is included in the tree and every question is only asked a single time.
    """
    query_graph: List[Query] = Field(
        ..., description="The original question we are asking"
    )

    async def execute(self):
        # this should be done with a topological sort, but this is easier to understand
        original_question = self.query_graph[-1]
        print(f"Executing query plan from `{original_question.question}`")
        return await original_question.execute(dependency_func=self.dependencies)

    def dependencies(self, idz: List[int]) -> List[Query]:
        """
        Returns the dependencies of the query with the given id.
        """
        return [q for q in self.query_graph if q.id in idz]


Query.update_forward_refs()
QueryPlan.update_forward_refs()


def query_planner(question: str) -> QueryPlan:
    """
    Function that takes a question and returns a query plan.

    Args:
        question (str): The question we are asking

    Returns:
        QueryPlan: The query plan to answer the question
    """
    messages = [
        {
            "role": "system",
            "content": "You are a world class query planning algorithm capable of breaking questions up in to its"
                       " dependent sub-queries such that the answers can be used to inform the parent question. Do"
                       " not answer the questions, simply provide correct compute graph with good specific"
                       " questions to ask and relevant dependencies. Before you call the function, think step by"
                       " step to get a better understanding the problem.",
        },
        {
            "role": "user",
            "content": f"Consider: {question}\nGenerate the correct query plan.",
        },
    ]

    completion = openai.ChatCompletion.create(
        model=Model.GPT4.value,
        temperature=0,
        functions=[QueryPlan.openai_schema],
        function_call={"name": QueryPlan.openai_schema["name"]},
        messages=messages,
        max_tokens=1000,
    )
    root = QueryPlan.from_response(completion)
    return root


if __name__ == "__main__":
    plan = query_planner(
        "What is the difference in populations of Canada and Sydney Crosby's home country?",
    )
    pprint(plan.dict())
    asyncio.run(plan.execute())


File: langchain_agent/__init__.py
-----


File: langchain_agent/LangChainAgent.py
-----
# import os
import openai
import json
from MemoryManager import MemoryManager
from functions import class_lookup

# git_loader = GitLoader(repo_path="/Users/josephblazick/Documents/langchain", branch="v0.0.196", file_filter=file_filter)
# package_docs = git_loader.load_and_split()

GPT_MODEL = "gpt-3.5-turbo-0613"  # or any other chat model you want to use
FUNCTIONS = [class_lookup.openai_schema]


class LangChainAgent:
    def __init__(self, memory_manager, functions=None):
        self.memory_manager = memory_manager
        self.functions = functions

    def query(self, input_text, function_call="auto"):
        # Add user input to memory
        self.memory_manager.add_message("user", input_text)
        messages_without_index = [
            {k: v for k, v in item.items() if k != 'interaction_index'} for item in self.memory_manager.messages
        ]

        # Initialize the conversation with the model
        response = openai.ChatCompletion.create(
            model=GPT_MODEL,
            messages=messages_without_index,
            functions=self.functions,
            function_call=function_call,
            max_tokens=1000,
            temperature=0.3
        )

        message = response["choices"][0]["message"]
        # Check if the model wants to call a function
        if message.get("function_call"):
            function_name = message["function_call"]["name"]
            arguments = json.loads(message["function_call"]["arguments"])

            # Execute the function (needs to be defined before using it)
            function_response = {
                "role": "assistant",
                "content": json.dumps(obj=globals()[function_name](**arguments))
            }

            # Add function response to memory
            messages_without_index = [
                {k: v for k, v in item.items() if k != 'interaction_index'} for item in self.memory_manager.messages
            ]
            messages_without_index.append(function_response)

            # Continue the conversation with the function response
            second_response = openai.ChatCompletion.create(
                model=GPT_MODEL,
                messages=messages_without_index,
                max_tokens=500,
                temperature=0.7
            )
            output_text = second_response.choices[0].message["content"].strip()
        else:
            output_text = message["content"].strip()

        # Add the output to memory
        self.memory_manager.add_message("assistant", output_text)
        return output_text


memory_manager = MemoryManager(model=GPT_MODEL)
agent = LangChainAgent(memory_manager, functions=FUNCTIONS)

response = agent.query("What's the weather like in Boston?")
# print(response)

response1 = agent.query("What is the capital of France?")
# print(response1)

response2 = agent.query("Tell me more about its history.")
# print(response2)
# print(f"Total Tokens: {agent.memory_manager.get_total_tokens()}")

response2 = agent.query("""
    Write a python script using the GitLoader class from Langchain to load and parse
    the files from the pandas package. Use the master branch.
""")
# print(response2)

print(f"Total Tokens: {agent.memory_manager.get_total_tokens()}")
print(f"Total Messages: {len(agent.memory_manager.messages)}")
memory_manager.display_conversation()
memory_manager.summarize_history()
memory_manager.display_conversation()


